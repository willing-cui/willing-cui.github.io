
============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\accelerate.md
============================================================

# ğŸ¤— åŠ é€Ÿåˆ†å¸ƒå¼è®­ç»ƒ

éšç€æ¨¡å‹å˜å¾—è¶Šæ¥è¶Šå¤§ï¼Œå¹¶è¡Œæ€§å·²ç»æˆä¸ºåœ¨æœ‰é™ç¡¬ä»¶ä¸Šè®­ç»ƒæ›´å¤§æ¨¡å‹å’ŒåŠ é€Ÿè®­ç»ƒé€Ÿåº¦çš„ç­–ç•¥ï¼Œå¢åŠ äº†æ•°ä¸ªæ•°é‡çº§ã€‚åœ¨Hugging Faceï¼Œæˆ‘ä»¬åˆ›å»ºäº†[ğŸ¤— åŠ é€Ÿ](https://huggingface.co/docs/accelerate)åº“ï¼Œä»¥å¸®åŠ©ç”¨æˆ·åœ¨ä»»ä½•ç±»å‹çš„åˆ†å¸ƒå¼è®¾ç½®ä¸Šè½»æ¾è®­ç»ƒğŸ¤— Transformersæ¨¡å‹ï¼Œæ— è®ºæ˜¯åœ¨ä¸€å°æœºå™¨ä¸Šçš„å¤šä¸ªGPUè¿˜æ˜¯åœ¨å¤šä¸ªæœºå™¨ä¸Šçš„å¤šä¸ªGPUã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œäº†è§£å¦‚ä½•è‡ªå®šä¹‰æ‚¨çš„åŸç”ŸPyTorchè®­ç»ƒå¾ªç¯ï¼Œä»¥å¯ç”¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­çš„è®­ç»ƒã€‚

## è®¾ç½®

é€šè¿‡å®‰è£…ğŸ¤— åŠ é€Ÿå¼€å§‹:

```bash
pip install accelerate
```

ç„¶åå¯¼å…¥å¹¶åˆ›å»º[`~accelerate.Accelerator`]å¯¹è±¡ã€‚[`~accelerate.Accelerator`]å°†è‡ªåŠ¨æ£€æµ‹æ‚¨çš„åˆ†å¸ƒå¼è®¾ç½®ç±»å‹ï¼Œå¹¶åˆå§‹åŒ–æ‰€æœ‰å¿…è¦çš„è®­ç»ƒç»„ä»¶ã€‚æ‚¨ä¸éœ€è¦æ˜¾å¼åœ°å°†æ¨¡å‹æ”¾åœ¨è®¾å¤‡ä¸Šã€‚

```py
>>> from accelerate import Accelerator

>>> accelerator = Accelerator()
```

## å‡†å¤‡åŠ é€Ÿ

ä¸‹ä¸€æ­¥æ˜¯å°†æ‰€æœ‰ç›¸å…³çš„è®­ç»ƒå¯¹è±¡ä¼ é€’ç»™[`~accelerate.Accelerator.prepare`]æ–¹æ³•ã€‚è¿™åŒ…æ‹¬æ‚¨çš„è®­ç»ƒå’Œè¯„ä¼°DataLoaderã€ä¸€ä¸ªæ¨¡å‹å’Œä¸€ä¸ªä¼˜åŒ–å™¨:

```py
>>> train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
...     train_dataloader, eval_dataloader, model, optimizer
... )
```

## åå‘ä¼ æ’­

æœ€åä¸€æ­¥æ˜¯ç”¨ğŸ¤— åŠ é€Ÿçš„[`~accelerate.Accelerator.backward`]æ–¹æ³•æ›¿æ¢è®­ç»ƒå¾ªç¯ä¸­çš„å…¸å‹`loss.backward()`:

```py
>>> for epoch in range(num_epochs):
...     for batch in train_dataloader:
...         outputs = model(**batch)
...         loss = outputs.loss
...         accelerator.backward(loss)

...         optimizer.step()
...         lr_scheduler.step()
...         optimizer.zero_grad()
...         progress_bar.update(1)
```

å¦‚æ‚¨åœ¨ä¸‹é¢çš„ä»£ç ä¸­æ‰€è§ï¼Œæ‚¨åªéœ€è¦æ·»åŠ å››è¡Œé¢å¤–çš„ä»£ç åˆ°æ‚¨çš„è®­ç»ƒå¾ªç¯ä¸­å³å¯å¯ç”¨åˆ†å¸ƒå¼è®­ç»ƒï¼

```diff
+ from accelerate import Accelerator
  from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

+ accelerator = Accelerator()

  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
  optimizer = AdamW(model.parameters(), lr=3e-5)

- device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
- model.to(device)

+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
+     train_dataloader, eval_dataloader, model, optimizer
+ )

  num_epochs = 3
  num_training_steps = num_epochs * len(train_dataloader)
  lr_scheduler = get_scheduler(
      "linear",
      optimizer=optimizer,
      num_warmup_steps=0,
      num_training_steps=num_training_steps
  )

  progress_bar = tqdm(range(num_training_steps))

  model.train()
  for epoch in range(num_epochs):
      for batch in train_dataloader:
-         batch = {k: v.to(device) for k, v in batch.items()}
          outputs = model(**batch)
          loss = outputs.loss
-         loss.backward()
+         accelerator.backward(loss)

          optimizer.step()
          lr_scheduler.step()
          optimizer.zero_grad()
          progress_bar.update(1)
```

## è®­ç»ƒ

åœ¨æ·»åŠ äº†ç›¸å…³ä»£ç è¡Œåï¼Œå¯ä»¥åœ¨è„šæœ¬æˆ–ç¬”è®°æœ¬ï¼ˆå¦‚Colaboratoryï¼‰ä¸­å¯åŠ¨è®­ç»ƒã€‚

### ç”¨è„šæœ¬è®­ç»ƒ

å¦‚æœæ‚¨ä»è„šæœ¬ä¸­è¿è¡Œè®­ç»ƒï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ä»¥åˆ›å»ºå’Œä¿å­˜é…ç½®æ–‡ä»¶:

```bash
accelerate config
```

ç„¶åä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨è®­ç»ƒ:

```bash
accelerate launch train.py
```

### ç”¨ç¬”è®°æœ¬è®­ç»ƒ

ğŸ¤— åŠ é€Ÿè¿˜å¯ä»¥åœ¨ç¬”è®°æœ¬ä¸­è¿è¡Œï¼Œå¦‚æœæ‚¨è®¡åˆ’ä½¿ç”¨Colaboratoryçš„TPUï¼Œåˆ™å¯åœ¨å…¶ä¸­è¿è¡Œã€‚å°†è´Ÿè´£è®­ç»ƒçš„æ‰€æœ‰ä»£ç åŒ…è£…åœ¨ä¸€ä¸ªå‡½æ•°ä¸­ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™[`~accelerate.notebook_launcher`]:

```py
>>> from accelerate import notebook_launcher

>>> notebook_launcher(training_function)
```

æœ‰å…³ğŸ¤— åŠ é€ŸåŠå…¶ä¸°å¯ŒåŠŸèƒ½çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[æ–‡æ¡£](https://huggingface.co/docs/accelerate)ã€‚

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\add_new_pipeline.md
============================================================

# å¦‚ä½•åˆ›å»ºè‡ªå®šä¹‰æµæ°´çº¿ï¼Ÿ

åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†æ¼”ç¤ºå¦‚ä½•åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰æµæ°´çº¿å¹¶åˆ†äº«åˆ° [Hub](https://hf.co/models)ï¼Œæˆ–å°†å…¶æ·»åŠ åˆ° ğŸ¤— Transformers åº“ä¸­ã€‚

é¦–å…ˆï¼Œä½ éœ€è¦å†³å®šæµæ°´çº¿å°†èƒ½å¤Ÿæ¥å—çš„åŸå§‹æ¡ç›®ã€‚å®ƒå¯ä»¥æ˜¯å­—ç¬¦ä¸²ã€åŸå§‹å­—èŠ‚ã€å­—å…¸æˆ–ä»»ä½•çœ‹èµ·æ¥æœ€å¯èƒ½æ˜¯æœŸæœ›çš„è¾“å…¥ã€‚
å°½é‡ä¿æŒè¾“å…¥ä¸ºçº¯ Python è¯­è¨€ï¼Œå› ä¸ºè¿™æ ·å¯ä»¥æ›´å®¹æ˜“åœ°å®ç°å…¼å®¹æ€§ï¼ˆç”šè‡³é€šè¿‡ JSON åœ¨å…¶ä»–è¯­è¨€ä¹‹é—´ï¼‰ã€‚
è¿™äº›å°†æ˜¯æµæ°´çº¿ (`preprocess`) çš„ `inputs`ã€‚

ç„¶åå®šä¹‰ `outputs`ã€‚ä¸ `inputs` ç›¸åŒçš„ç­–ç•¥ã€‚è¶Šç®€å•è¶Šå¥½ã€‚è¿™äº›å°†æ˜¯ `postprocess` æ–¹æ³•çš„è¾“å‡ºã€‚

é¦–å…ˆç»§æ‰¿åŸºç±» `Pipeline`ï¼Œå…¶ä¸­åŒ…å«å®ç° `preprocess`ã€`_forward`ã€`postprocess` å’Œ `_sanitize_parameters` æ‰€éœ€çš„ 4 ä¸ªæ–¹æ³•ã€‚

```python
from transformers import Pipeline


class MyPipeline(Pipeline):
    def _sanitize_parameters(self, **kwargs):
        preprocess_kwargs = {}
        if "maybe_arg" in kwargs:
            preprocess_kwargs["maybe_arg"] = kwargs["maybe_arg"]
        return preprocess_kwargs, {}, {}

    def preprocess(self, inputs, maybe_arg=2):
        model_input = Tensor(inputs["input_ids"])
        return {"model_input": model_input}

    def _forward(self, model_inputs):
        # model_inputs == {"model_input": model_input}
        outputs = self.model(**model_inputs)
        # Maybe {"logits": Tensor(...)}
        return outputs

    def postprocess(self, model_outputs):
        best_class = model_outputs["logits"].softmax(-1)
        return best_class
```

è¿™ç§åˆ†è§£çš„ç»“æ„æ—¨åœ¨ä¸º CPU/GPU æä¾›ç›¸å¯¹æ— ç¼çš„æ”¯æŒï¼ŒåŒæ—¶æ”¯æŒåœ¨ä¸åŒçº¿ç¨‹ä¸Šå¯¹ CPU è¿›è¡Œé¢„å¤„ç†/åå¤„ç†ã€‚

`preprocess` å°†æ¥å—æœ€åˆå®šä¹‰çš„è¾“å…¥ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºå¯ä¾›æ¨¡å‹è¾“å…¥çš„å†…å®¹ã€‚å®ƒå¯èƒ½åŒ…å«æ›´å¤šä¿¡æ¯ï¼Œé€šå¸¸æ˜¯ä¸€ä¸ª `Dict`ã€‚

`_forward` æ˜¯å®ç°ç»†èŠ‚ï¼Œä¸åº”ç›´æ¥è°ƒç”¨ã€‚`forward` æ˜¯é¦–é€‰çš„è°ƒç”¨æ–¹æ³•ï¼Œå› ä¸ºå®ƒåŒ…å«ä¿éšœæªæ–½ï¼Œä»¥ç¡®ä¿ä¸€åˆ‡éƒ½åœ¨é¢„æœŸçš„è®¾å¤‡ä¸Šè¿ä½œã€‚
å¦‚æœä»»ä½•å†…å®¹ä¸å®é™…æ¨¡å‹ç›¸å…³ï¼Œå®ƒåº”è¯¥å±äº `_forward` æ–¹æ³•ï¼Œå…¶ä»–å†…å®¹åº”è¯¥åœ¨ preprocess/postprocess ä¸­ã€‚

`postprocess` æ–¹æ³•å°†æ¥å— `_forward` çš„è¾“å‡ºï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºä¹‹å‰ç¡®å®šçš„æœ€ç»ˆè¾“å‡ºã€‚

`_sanitize_parameters` å­˜åœ¨æ˜¯ä¸ºäº†å…è®¸ç”¨æˆ·åœ¨ä»»ä½•æ—¶å€™ä¼ é€’ä»»ä½•å‚æ•°ï¼Œæ— è®ºæ˜¯åœ¨åˆå§‹åŒ–æ—¶ `pipeline(...., maybe_arg=4)`
è¿˜æ˜¯åœ¨è°ƒç”¨æ—¶ `pipe = pipeline(...); output = pipe(...., maybe_arg=4)`ã€‚

`_sanitize_parameters` çš„è¿”å›å€¼æ˜¯å°†ç›´æ¥ä¼ é€’ç»™ `preprocess`ã€`_forward` å’Œ `postprocess` çš„ 3 ä¸ªå…³é”®å­—å‚æ•°å­—å…¸ã€‚
å¦‚æœè°ƒç”¨æ–¹æ²¡æœ‰ä½¿ç”¨ä»»ä½•é¢å¤–å‚æ•°è°ƒç”¨ï¼Œåˆ™ä¸è¦å¡«å†™ä»»ä½•å†…å®¹ã€‚è¿™æ ·å¯ä»¥ä¿ç•™å‡½æ•°å®šä¹‰ä¸­çš„é»˜è®¤å‚æ•°ï¼Œè¿™æ€»æ˜¯æ›´"è‡ªç„¶"çš„ã€‚

åœ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œä¸€ä¸ªç»å…¸çš„ä¾‹å­æ˜¯åœ¨åå¤„ç†ä¸­ä½¿ç”¨ `top_k` å‚æ•°ã€‚

```python
>>> pipe = pipeline("my-new-task")
>>> pipe("This is a test")
[{"label": "1-star", "score": 0.8}, {"label": "2-star", "score": 0.1}, {"label": "3-star", "score": 0.05}
{"label": "4-star", "score": 0.025}, {"label": "5-star", "score": 0.025}]

>>> pipe("This is a test", top_k=2)
[{"label": "1-star", "score": 0.8}, {"label": "2-star", "score": 0.1}]
```

ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å°†æ›´æ–°æˆ‘ä»¬çš„ `postprocess` æ–¹æ³•ï¼Œå°†é»˜è®¤å‚æ•°è®¾ç½®ä¸º `5`ï¼Œ
å¹¶ç¼–è¾‘ `_sanitize_parameters` æ–¹æ³•ï¼Œä»¥å…è®¸è¿™ä¸ªæ–°å‚æ•°ã€‚

```python
def postprocess(self, model_outputs, top_k=5):
    best_class = model_outputs["logits"].softmax(-1)
    # Add logic to handle top_k
    return best_class


def _sanitize_parameters(self, **kwargs):
    preprocess_kwargs = {}
    if "maybe_arg" in kwargs:
        preprocess_kwargs["maybe_arg"] = kwargs["maybe_arg"]

    postprocess_kwargs = {}
    if "top_k" in kwargs:
        postprocess_kwargs["top_k"] = kwargs["top_k"]
    return preprocess_kwargs, {}, postprocess_kwargs
```

å°½é‡ä¿æŒç®€å•è¾“å…¥/è¾“å‡ºï¼Œæœ€å¥½æ˜¯å¯ JSON åºåˆ—åŒ–çš„ï¼Œå› ä¸ºè¿™æ ·å¯ä»¥ä½¿æµæ°´çº¿çš„ä½¿ç”¨éå¸¸ç®€å•ï¼Œè€Œä¸éœ€è¦ç”¨æˆ·äº†è§£æ–°çš„å¯¹è±¡ç±»å‹ã€‚
é€šå¸¸ä¹Ÿç›¸å¯¹å¸¸è§åœ°æ”¯æŒè®¸å¤šä¸åŒç±»å‹çš„å‚æ•°ä»¥ä¾¿ä½¿ç”¨ï¼ˆä¾‹å¦‚éŸ³é¢‘æ–‡ä»¶ï¼Œå¯ä»¥æ˜¯æ–‡ä»¶åã€URL æˆ–çº¯å­—èŠ‚ï¼‰ã€‚

## å°†å…¶æ·»åŠ åˆ°æ”¯æŒçš„ä»»åŠ¡åˆ—è¡¨ä¸­

è¦å°†ä½ çš„ `new-task` æ³¨å†Œåˆ°æ”¯æŒçš„ä»»åŠ¡åˆ—è¡¨ä¸­ï¼Œä½ éœ€è¦å°†å…¶æ·»åŠ åˆ° `PIPELINE_REGISTRY` ä¸­ï¼š

```python
from transformers.pipelines import PIPELINE_REGISTRY

PIPELINE_REGISTRY.register_pipeline(
    "new-task",
    pipeline_class=MyPipeline,
    pt_model=AutoModelForSequenceClassification,
)
```

å¦‚æœéœ€è¦ï¼Œä½ å¯ä»¥æŒ‡å®šä¸€ä¸ªé»˜è®¤æ¨¡å‹ï¼Œæ­¤æ—¶å®ƒåº”è¯¥å¸¦æœ‰ä¸€ä¸ªç‰¹å®šçš„ä¿®è®¢ç‰ˆæœ¬ï¼ˆå¯ä»¥æ˜¯åˆ†æ”¯åç§°æˆ–æäº¤å“ˆå¸Œï¼Œè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨äº† `"abcdef"`ï¼‰ï¼Œä»¥åŠç±»å‹ï¼š

```python
PIPELINE_REGISTRY.register_pipeline(
    "new-task",
    pipeline_class=MyPipeline,
    pt_model=AutoModelForSequenceClassification,
    default={"pt": ("user/awesome_model", "abcdef")},
    type="text",  # current support type: text, audio, image, multimodal
)
```

## åœ¨ Hub ä¸Šåˆ†äº«ä½ çš„æµæ°´çº¿

è¦åœ¨ Hub ä¸Šåˆ†äº«ä½ çš„è‡ªå®šä¹‰æµæ°´çº¿ï¼Œä½ åªéœ€è¦å°† `Pipeline` å­ç±»çš„è‡ªå®šä¹‰ä»£ç ä¿å­˜åœ¨ä¸€ä¸ª Python æ–‡ä»¶ä¸­ã€‚
ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬æƒ³ä½¿ç”¨ä¸€ä¸ªè‡ªå®šä¹‰æµæ°´çº¿è¿›è¡Œå¥å¯¹åˆ†ç±»ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```py
import numpy as np

from transformers import Pipeline


def softmax(outputs):
    maxes = np.max(outputs, axis=-1, keepdims=True)
    shifted_exp = np.exp(outputs - maxes)
    return shifted_exp / shifted_exp.sum(axis=-1, keepdims=True)


class PairClassificationPipeline(Pipeline):
    def _sanitize_parameters(self, **kwargs):
        preprocess_kwargs = {}
        if "second_text" in kwargs:
            preprocess_kwargs["second_text"] = kwargs["second_text"]
        return preprocess_kwargs, {}, {}

    def preprocess(self, text, second_text=None):
        return self.tokenizer(text, text_pair=second_text, return_tensors=self.framework)

    def _forward(self, model_inputs):
        return self.model(**model_inputs)

    def postprocess(self, model_outputs):
        logits = model_outputs.logits[0].numpy()
        probabilities = softmax(logits)

        best_class = np.argmax(probabilities)
        label = self.model.config.id2label[best_class]
        score = probabilities[best_class].item()
        logits = logits.tolist()
        return {"label": label, "score": score, "logits": logits}
```

è¿™ä¸ªå®ç°ä¸æ¡†æ¶æ— å…³ï¼Œé€‚ç”¨äº PyTorch å’Œ TensorFlow æ¨¡å‹ã€‚å¦‚æœæˆ‘ä»¬å°†å…¶ä¿å­˜åœ¨ä¸€ä¸ªåä¸º
`pair_classification.py` çš„æ–‡ä»¶ä¸­ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥åƒè¿™æ ·å¯¼å…¥å¹¶æ³¨å†Œå®ƒï¼š

```py
from pair_classification import PairClassificationPipeline
from transformers.pipelines import PIPELINE_REGISTRY
from transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification

PIPELINE_REGISTRY.register_pipeline(
    "pair-classification",
    pipeline_class=PairClassificationPipeline,
    pt_model=AutoModelForSequenceClassification,
    tf_model=TFAutoModelForSequenceClassification,
)
```

å®Œæˆè¿™äº›æ­¥éª¤åï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶ä¸é¢„è®­ç»ƒæ¨¡å‹ä¸€èµ·ä½¿ç”¨ã€‚ä¾‹å¦‚ï¼Œ`sgugger/finetuned-bert-mrpc`
å·²ç»åœ¨ MRPC æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¾®è°ƒï¼Œç”¨äºå°†å¥å­å¯¹åˆ†ç±»ä¸ºæ˜¯é‡Šä¹‰æˆ–ä¸æ˜¯é‡Šä¹‰ã€‚

```py
from transformers import pipeline

classifier = pipeline("pair-classification", model="sgugger/finetuned-bert-mrpc")
```

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡åœ¨ `Repository` ä¸­ä½¿ç”¨ `save_pretrained` æ–¹æ³•å°†å…¶åˆ†äº«åˆ° Hub ä¸Šï¼š

```py
from huggingface_hub import Repository

repo = Repository("test-dynamic-pipeline", clone_from="{your_username}/test-dynamic-pipeline")
classifier.save_pretrained("test-dynamic-pipeline")
repo.push_to_hub()
```

è¿™å°†ä¼šå¤åˆ¶åŒ…å«ä½ å®šä¹‰çš„ `PairClassificationPipeline` çš„æ–‡ä»¶åˆ°æ–‡ä»¶å¤¹ `"test-dynamic-pipeline"` ä¸­ï¼Œ
åŒæ—¶ä¿å­˜æµæ°´çº¿çš„æ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œç„¶åå°†æ‰€æœ‰å†…å®¹æ¨é€åˆ°ä»“åº“ `{your_username}/test-dynamic-pipeline` ä¸­ã€‚
ä¹‹åï¼Œåªè¦æä¾›é€‰é¡¹ `trust_remote_code=True`ï¼Œä»»ä½•äººéƒ½å¯ä»¥ä½¿ç”¨å®ƒï¼š

```py
from transformers import pipeline

classifier = pipeline(model="{your_username}/test-dynamic-pipeline", trust_remote_code=True)
```

## å°†æµæ°´çº¿æ·»åŠ åˆ° ğŸ¤— Transformers

å¦‚æœä½ æƒ³å°†ä½ çš„æµæ°´çº¿è´¡çŒ®ç»™ ğŸ¤— Transformersï¼Œä½ éœ€è¦åœ¨ `pipelines` å­æ¨¡å—ä¸­æ·»åŠ ä¸€ä¸ªæ–°æ¨¡å—ï¼Œ
å…¶ä¸­åŒ…å«ä½ çš„æµæ°´çº¿çš„ä»£ç ï¼Œç„¶åå°†å…¶æ·»åŠ åˆ° `pipelines/__init__.py` ä¸­å®šä¹‰çš„ä»»åŠ¡åˆ—è¡¨ä¸­ã€‚

ç„¶åï¼Œä½ éœ€è¦æ·»åŠ æµ‹è¯•ã€‚åˆ›å»ºä¸€ä¸ªæ–°æ–‡ä»¶ `tests/test_pipelines_MY_PIPELINE.py`ï¼Œå…¶ä¸­åŒ…å«å…¶ä»–æµ‹è¯•çš„ç¤ºä¾‹ã€‚

`run_pipeline_test` å‡½æ•°å°†éå¸¸é€šç”¨ï¼Œå¹¶åœ¨æ¯ç§å¯èƒ½çš„æ¶æ„ä¸Šè¿è¡Œå°å‹éšæœºæ¨¡å‹ï¼Œå¦‚ `model_mapping` å’Œ `tf_model_mapping` æ‰€å®šä¹‰ã€‚

è¿™å¯¹äºæµ‹è¯•æœªæ¥çš„å…¼å®¹æ€§éå¸¸é‡è¦ï¼Œè¿™æ„å‘³ç€å¦‚æœæœ‰äººä¸º `XXXForQuestionAnswering` æ·»åŠ äº†ä¸€ä¸ªæ–°æ¨¡å‹ï¼Œ
æµæ°´çº¿æµ‹è¯•å°†å°è¯•åœ¨å…¶ä¸Šè¿è¡Œã€‚ç”±äºæ¨¡å‹æ˜¯éšæœºçš„ï¼Œæ‰€ä»¥ä¸å¯èƒ½æ£€æŸ¥å®é™…å€¼ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆæœ‰ä¸€ä¸ªå¸®åŠ©å‡½æ•° `ANY`ï¼Œå®ƒåªæ˜¯å°è¯•åŒ¹é…æµæ°´çº¿çš„è¾“å‡ºç±»å‹ã€‚

ä½ è¿˜ **éœ€è¦** å®ç° 2ï¼ˆæœ€å¥½æ˜¯ 4ï¼‰ä¸ªæµ‹è¯•ã€‚

- `test_small_model_pt`ï¼šä¸ºè¿™ä¸ªæµæ°´çº¿å®šä¹‰ä¸€ä¸ªå°å‹æ¨¡å‹ï¼ˆç»“æœæ˜¯å¦åˆç†å¹¶ä¸é‡è¦ï¼‰ï¼Œå¹¶æµ‹è¯•æµæ°´çº¿çš„è¾“å‡ºã€‚
  ç»“æœåº”è¯¥ä¸ `test_small_model_tf` çš„ç»“æœç›¸åŒã€‚
- `test_small_model_tf`ï¼šä¸ºè¿™ä¸ªæµæ°´çº¿å®šä¹‰ä¸€ä¸ªå°å‹æ¨¡å‹ï¼ˆç»“æœæ˜¯å¦åˆç†å¹¶ä¸é‡è¦ï¼‰ï¼Œå¹¶æµ‹è¯•æµæ°´çº¿çš„è¾“å‡ºã€‚
  ç»“æœåº”è¯¥ä¸ `test_small_model_pt` çš„ç»“æœç›¸åŒã€‚
- `test_large_model_pt`ï¼ˆå¯é€‰ï¼‰ï¼šåœ¨ä¸€ä¸ªçœŸå®çš„æµæ°´çº¿ä¸Šæµ‹è¯•æµæ°´çº¿ï¼Œç»“æœåº”è¯¥æ˜¯æœ‰æ„ä¹‰çš„ã€‚
  è¿™äº›æµ‹è¯•é€Ÿåº¦è¾ƒæ…¢ï¼Œåº”è¯¥è¢«å¦‚æ­¤æ ‡è®°ã€‚è¿™é‡Œçš„ç›®æ ‡æ˜¯å±•ç¤ºæµæ°´çº¿ï¼Œå¹¶ç¡®ä¿åœ¨æœªæ¥çš„å‘å¸ƒä¸­æ²¡æœ‰æ¼‚ç§»ã€‚
- `test_large_model_tf`ï¼ˆå¯é€‰ï¼‰ï¼šåœ¨ä¸€ä¸ªçœŸå®çš„æµæ°´çº¿ä¸Šæµ‹è¯•æµæ°´çº¿ï¼Œç»“æœåº”è¯¥æ˜¯æœ‰æ„ä¹‰çš„ã€‚
  è¿™äº›æµ‹è¯•é€Ÿåº¦è¾ƒæ…¢ï¼Œåº”è¯¥è¢«å¦‚æ­¤æ ‡è®°ã€‚è¿™é‡Œçš„ç›®æ ‡æ˜¯å±•ç¤ºæµæ°´çº¿ï¼Œå¹¶ç¡®ä¿åœ¨æœªæ¥çš„å‘å¸ƒä¸­æ²¡æœ‰æ¼‚ç§»ã€‚

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\attention.md
============================================================

# æ³¨æ„åŠ›æœºåˆ¶

å¤§å¤šæ•° transformer æ¨¡å‹ä½¿ç”¨å®Œå…¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œè¯¥æœºåˆ¶é‡‡ç”¨æ­£æ–¹å½¢çš„æ³¨æ„åŠ›çŸ©é˜µã€‚å½“è¾“å…¥å¾ˆé•¿çš„æ–‡æœ¬æ—¶ï¼Œè¿™å°†å¯¼è‡´å·¨å¤§çš„è®¡ç®—ç“¶é¢ˆã€‚Longformer å’Œ Reformer æ˜¯æé«˜æ³¨æ„åŠ›æœºåˆ¶æ•ˆç‡çš„æ”¹è¿›æ¨¡å‹ï¼Œå®ƒä»¬ä½¿ç”¨ç¨€ç–åŒ–çš„æ³¨æ„åŠ›çŸ©é˜µæ¥åŠ é€Ÿè®­ç»ƒã€‚

## å±€éƒ¨æ•æ„Ÿå“ˆå¸Œæ³¨æ„åŠ›æœºåˆ¶ï¼ˆLSH attentionï¼‰

[Reformer](model_doc/reformer)ä½¿ç”¨LSHï¼ˆå±€éƒ¨æ•æ„Ÿå“ˆå¸Œï¼‰çš„æ³¨æ„åŠ›æœºåˆ¶ã€‚åœ¨è®¡ç®—softmax(QK^t)æ—¶ï¼Œåªæœ‰çŸ©é˜µQK^tä¸­çš„æœ€å¤§å…ƒç´ ï¼ˆåœ¨softmaxç»´åº¦ä¸Šï¼‰ä¼šåšå‡ºæœ‰ç”¨çš„è´¡çŒ®ã€‚æ‰€ä»¥å¯¹äºQä¸­çš„æ¯ä¸ªæŸ¥è¯¢qï¼Œæˆ‘ä»¬åªéœ€è¦è€ƒè™‘Kä¸­ä¸qæ¥è¿‘çš„é”®kï¼Œè¿™é‡Œä½¿ç”¨äº†ä¸€ä¸ªå“ˆå¸Œå‡½æ•°æ¥ç¡®å®šqå’Œkæ˜¯å¦æ¥è¿‘ã€‚æ³¨æ„åŠ›æ©ç è¢«ä¿®æ”¹ä»¥æ©ç›–å½“å‰çš„è¯ç¬¦ï¼ˆtokenï¼‰ï¼ˆé™¤äº†ç¬¬ä¸€ä¸ªä½ç½®ä¹‹å¤–ï¼‰ï¼Œå› ä¸ºè¿™æ ·ä¼šä½¿å¾—æŸ¥è¯¢å’Œé”®ç›¸ç­‰ï¼ˆå› æ­¤éå¸¸ç›¸ä¼¼ï¼‰ã€‚ç”±äºå“ˆå¸Œå¯èƒ½ä¼šæœ‰äº›éšæœºæ€§ï¼Œæ‰€ä»¥åœ¨å®è·µä¸­ä½¿ç”¨å¤šä¸ªå“ˆå¸Œå‡½æ•°ï¼ˆç”±n_roundså‚æ•°ç¡®å®šï¼‰,ç„¶åä¸€èµ·æ±‚å¹³å‡ã€‚

## å±€éƒ¨æ³¨æ„åŠ›æœºåˆ¶ï¼ˆLocal attentionï¼‰
[Longformer](model_doc/longformer)ä½¿ç”¨å±€éƒ¨æ³¨æ„åŠ›æœºåˆ¶:é€šå¸¸æƒ…å†µä¸‹ï¼Œå±€éƒ¨ä¸Šä¸‹æ–‡ï¼ˆä¾‹å¦‚ï¼Œå·¦è¾¹å’Œå³è¾¹çš„ä¸¤ä¸ªè¯ç¬¦æ˜¯ä»€ä¹ˆï¼Ÿï¼‰å¯¹äºç»™å®šè¯ç¬¦çš„æ“ä½œå·²ç»è¶³å¤Ÿäº†ã€‚æ­¤å¤–ï¼Œé€šè¿‡å †å å…·æœ‰å°çª—å£çš„æ³¨æ„åŠ›å±‚ï¼Œæœ€åä¸€å±‚å°†æ‹¥æœ‰ä¸ä»…ä»…æ˜¯çª—å£å†…è¯ç¬¦çš„æ„Ÿå—é‡ï¼Œè¿™ä½¿å¾—å®ƒä»¬èƒ½æ„å»ºæ•´ä¸ªå¥å­çš„è¡¨ç¤ºã€‚

ä¸€äº›é¢„å…ˆé€‰å®šçš„è¾“å…¥è¯ç¬¦ä¹Ÿè¢«èµ‹äºˆå…¨å±€æ³¨æ„åŠ›:å¯¹äºè¿™äº›å°‘æ•°è¯ç¬¦ï¼Œæ³¨æ„åŠ›çŸ©é˜µå¯ä»¥è®¿é—®æ‰€æœ‰è¯ç¬¦ï¼ˆtokensï¼‰ï¼Œå¹¶ä¸”è¿™ä¸ªè¿‡ç¨‹æ˜¯å¯¹ç§°çš„:æ‰€æœ‰å…¶ä»–è¯ç¬¦é™¤äº†å®ƒä»¬å±€éƒ¨çª—å£å†…çš„è¯ç¬¦ä¹‹å¤–ï¼Œä¹Ÿå¯ä»¥è®¿é—®è¿™äº›ç‰¹å®šçš„è¯ç¬¦ã€‚è¿™åœ¨è®ºæ–‡çš„å›¾2dä¸­æœ‰å±•ç¤ºï¼Œä¸‹é¢æ˜¯ä¸€ä¸ªæ ·æœ¬æ³¨æ„åŠ›æ©ç ï¼š

<div class="flex justify-center">
    <img scale="50 %" align="center" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/local_attention_mask.png"/>
</div>

ä½¿ç”¨å‚æ•°æ›´å°‘çš„æ³¨æ„åŠ›çŸ©é˜µï¼Œå¯ä»¥è®©æ¨¡å‹å¤„ç†æ›´é•¿çš„è¾“å…¥åºåˆ—ã€‚

## å…¶ä»–æŠ€å·§

### è½´å‘ä½ç½®ç¼–ç 

[Reformer](model_doc/reformer)æ¨¡å‹ä½¿ç”¨è½´å‘ä½ç½®ç¼–ç ï¼šåœ¨ä¼ ç»Ÿçš„transformeræ¨¡å‹ä¸­ï¼Œä½ç½®ç¼–ç çŸ©é˜µEçš„å¤§å°æ˜¯\\(l\\)ä¹˜ä»¥\\(d\\)ï¼Œå…¶ä¸­\\(l\\)æ˜¯åºåˆ—é•¿åº¦ï¼Œ\\(d\\)æ˜¯éšè—çŠ¶æ€çš„ç»´åº¦ã€‚å¦‚æœä½ æœ‰éå¸¸é•¿çš„æ–‡æœ¬ï¼Œè¿™ä¸ªçŸ©é˜µå¯èƒ½ä¼šéå¸¸å¤§ï¼Œå°†ä¼šå ç”¨å¤§é‡çš„GPUæ˜¾å­˜ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œè½´å‘ä½ç½®ç¼–ç å°†è¿™ä¸ªå¤§çŸ©é˜µEåˆ†è§£æˆä¸¤ä¸ªè¾ƒå°çš„çŸ©é˜µE1å’ŒE2ï¼Œå®ƒä»¬çš„ç»´åº¦åˆ†åˆ«æ˜¯\\(l_{1} \times d_{1}\\) å’Œ\\(l_{2} \times d_{2}\\)ï¼Œæ»¡è¶³\\(l_{1} \times l_{2} = l\\)å’Œ\\(d_{1} + d_{2} = d\\)ï¼ˆé€šè¿‡é•¿åº¦çš„ä¹˜ç§¯ï¼Œæœ€ç»ˆå¾—åˆ°çš„çŸ©é˜µè¦å°å¾—å¤šï¼‰ã€‚åœ¨Eä¸­ï¼Œå¯¹äºæ—¶é—´æ­¥\\(j\\) çš„åµŒå…¥æ˜¯é€šè¿‡è¿æ¥E1ä¸­æ—¶é—´æ­¥ \\(j \% l1\\) çš„åµŒå…¥å’ŒE2ä¸­æ—¶é—´æ­¥\\(j // l1\\)çš„åµŒå…¥æ¥è·å¾—çš„ã€‚


============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\autoclass_tutorial.md
============================================================



# ä½¿ç”¨AutoClassåŠ è½½é¢„è®­ç»ƒå®ä¾‹

ç”±äºå­˜åœ¨è®¸å¤šä¸åŒçš„Transformeræ¶æ„ï¼Œå› æ­¤ä¸ºæ‚¨çš„checkpointåˆ›å»ºä¸€ä¸ªå¯ç”¨æ¶æ„å¯èƒ½ä¼šå…·æœ‰æŒ‘æˆ˜æ€§ã€‚é€šè¿‡`AutoClass`å¯ä»¥è‡ªåŠ¨æ¨æ–­å¹¶ä»ç»™å®šçš„checkpointåŠ è½½æ­£ç¡®çš„æ¶æ„, è¿™ä¹Ÿæ˜¯ğŸ¤— Transformersæ˜“äºä½¿ç”¨ã€ç®€å•ä¸”çµæ´»æ ¸å¿ƒè§„åˆ™çš„é‡è¦ä¸€éƒ¨åˆ†ã€‚`from_pretrained()`æ–¹æ³•å…è®¸æ‚¨å¿«é€ŸåŠ è½½ä»»ä½•æ¶æ„çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå› æ­¤æ‚¨ä¸å¿…èŠ±è´¹æ—¶é—´å’Œç²¾åŠ›ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹ã€‚ç”Ÿæˆè¿™ç§ä¸checkpointæ— å…³çš„ä»£ç æ„å‘³ç€ï¼Œå¦‚æœæ‚¨çš„ä»£ç é€‚ç”¨äºä¸€ä¸ªcheckpointï¼Œå®ƒå°†é€‚ç”¨äºå¦ä¸€ä¸ªcheckpoint - åªè¦å®ƒä»¬æ˜¯ä¸ºäº†ç±»ä¼¼çš„ä»»åŠ¡è¿›è¡Œè®­ç»ƒçš„ - å³ä½¿æ¶æ„ä¸åŒã€‚

<Tip>

è¯·è®°ä½ï¼Œæ¶æ„æŒ‡çš„æ˜¯æ¨¡å‹çš„ç»“æ„ï¼Œè€Œcheckpointsæ˜¯ç»™å®šæ¶æ„çš„æƒé‡ã€‚ä¾‹å¦‚ï¼Œ[BERT](https://huggingface.co/google-bert/bert-base-uncased)æ˜¯ä¸€ç§æ¶æ„ï¼Œè€Œ`google-bert/bert-base-uncased`æ˜¯ä¸€ä¸ªcheckpointã€‚æ¨¡å‹æ˜¯ä¸€ä¸ªé€šç”¨æœ¯è¯­ï¼Œå¯ä»¥æŒ‡ä»£æ¶æ„æˆ–checkpointã€‚


</Tip>

åœ¨è¿™ä¸ªæ•™ç¨‹ä¸­ï¼Œå­¦ä¹ å¦‚ä½•ï¼š

* åŠ è½½é¢„è®­ç»ƒçš„åˆ†è¯å™¨ï¼ˆ`tokenizer`ï¼‰
* åŠ è½½é¢„è®­ç»ƒçš„å›¾åƒå¤„ç†å™¨(`image processor`)
* åŠ è½½é¢„è®­ç»ƒçš„ç‰¹å¾æå–å™¨(`feature extractor`)
* åŠ è½½é¢„è®­ç»ƒçš„å¤„ç†å™¨(`processor`)
* åŠ è½½é¢„è®­ç»ƒçš„æ¨¡å‹ã€‚


## AutoTokenizer

å‡ ä¹æ‰€æœ‰çš„NLPä»»åŠ¡éƒ½ä»¥`tokenizer`å¼€å§‹ã€‚`tokenizer`å°†æ‚¨çš„è¾“å…¥è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥å¤„ç†çš„æ ¼å¼ã€‚

ä½¿ç”¨[`AutoTokenizer.from_pretrained`]åŠ è½½`tokenizer`ï¼š

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
```

ç„¶åæŒ‰ç…§å¦‚ä¸‹æ–¹å¼å¯¹è¾“å…¥è¿›è¡Œåˆ†è¯ï¼š

```py
>>> sequence = "In a hole in the ground there lived a hobbit."
>>> print(tokenizer(sequence))
{'input_ids': [101, 1999, 1037, 4920, 1999, 1996, 2598, 2045, 2973, 1037, 7570, 10322, 4183, 1012, 102], 
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

## AutoImageProcessor

å¯¹äºè§†è§‰ä»»åŠ¡ï¼Œ`image processor`å°†å›¾åƒå¤„ç†æˆæ­£ç¡®çš„è¾“å…¥æ ¼å¼ã€‚

```py
>>> from transformers import AutoImageProcessor

>>> image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")
```


## AutoFeatureExtractor

å¯¹äºéŸ³é¢‘ä»»åŠ¡,`feature extractor`å°†éŸ³é¢‘ä¿¡å·å¤„ç†æˆæ­£ç¡®çš„è¾“å…¥æ ¼å¼ã€‚

ä½¿ç”¨[`AutoFeatureExtractor.from_pretrained`]åŠ è½½`feature extractor`ï¼š

```py
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained(
...     "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition"
... )
```

## AutoProcessor

å¤šæ¨¡æ€ä»»åŠ¡éœ€è¦ä¸€ç§`processor`ï¼Œå°†ä¸¤ç§ç±»å‹çš„é¢„å¤„ç†å·¥å…·ç»“åˆèµ·æ¥ã€‚ä¾‹å¦‚ï¼Œ[LayoutLMV2](model_doc/layoutlmv2)æ¨¡å‹éœ€è¦ä¸€ä¸ª`image processor`æ¥å¤„ç†å›¾åƒå’Œä¸€ä¸ª`tokenizer`æ¥å¤„ç†æ–‡æœ¬ï¼›`processor`å°†ä¸¤è€…ç»“åˆèµ·æ¥ã€‚

ä½¿ç”¨[`AutoProcessor.from_pretrained`]åŠ è½½`processor`ï¼š


```py
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("microsoft/layoutlmv2-base-uncased")
```

## AutoModel


æœ€åï¼Œ`AutoModelFor`ç±»è®©ä½ å¯ä»¥åŠ è½½ç»™å®šä»»åŠ¡çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå‚è§[è¿™é‡Œ](model_doc/auto)è·å–å¯ç”¨ä»»åŠ¡çš„å®Œæ•´åˆ—è¡¨ï¼‰ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨[`AutoModelForSequenceClassification.from_pretrained`]åŠ è½½ç”¨äºåºåˆ—åˆ†ç±»çš„æ¨¡å‹ï¼š

```py
>>> from transformers import AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
```

è½»æ¾åœ°é‡å¤ä½¿ç”¨ç›¸åŒçš„checkpointæ¥ä¸ºä¸åŒä»»åŠ¡åŠ è½½æ¨¡å‹æ¶æ„ï¼š


```py
>>> from transformers import AutoModelForTokenClassification

>>> model = AutoModelForTokenClassification.from_pretrained("distilbert/distilbert-base-uncased")
```

<Tip warning={true}>

å¯¹äºPyTorchæ¨¡å‹ï¼Œ`from_pretrained()`æ–¹æ³•ä½¿ç”¨`torch.load()`ï¼Œå®ƒå†…éƒ¨ä½¿ç”¨å·²çŸ¥æ˜¯ä¸å®‰å…¨çš„`pickle`ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæ°¸è¿œä¸è¦åŠ è½½æ¥è‡ªä¸å¯ä¿¡æ¥æºæˆ–å¯èƒ½è¢«ç¯¡æ”¹çš„æ¨¡å‹ã€‚å¯¹äºæ‰˜ç®¡åœ¨Hugging Face Hubä¸Šçš„å…¬å…±æ¨¡å‹ï¼Œè¿™ç§å®‰å…¨é£é™©åœ¨ä¸€å®šç¨‹åº¦ä¸Šå¾—åˆ°äº†ç¼“è§£ï¼Œå› ä¸ºæ¯æ¬¡æäº¤éƒ½ä¼šè¿›è¡Œ[æ¶æ„è½¯ä»¶æ‰«æ](https://huggingface.co/docs/hub/security-malware)ã€‚è¯·å‚é˜…[Hubæ–‡æ¡£](https://huggingface.co/docs/hub/security)ä»¥äº†è§£æœ€ä½³å®è·µï¼Œä¾‹å¦‚ä½¿ç”¨GPGè¿›è¡Œ[ç­¾åæäº¤éªŒè¯](https://huggingface.co/docs/hub/security-gpg#signing-commits-with-gpg)ã€‚

</Tip>

ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨`AutoTokenizer`ç±»å’Œ`AutoModelFor`ç±»æ¥åŠ è½½é¢„è®­ç»ƒçš„æ¨¡å‹å®ä¾‹ã€‚è¿™æ ·å¯ä»¥ç¡®ä¿æ¯æ¬¡åŠ è½½æ­£ç¡®çš„æ¶æ„ã€‚åœ¨ä¸‹ä¸€ä¸ª[æ•™ç¨‹](preprocessing)ä¸­ï¼Œå­¦ä¹ å¦‚ä½•ä½¿ç”¨æ–°åŠ è½½çš„`tokenizer`, `image processor`, `feature extractor`å’Œ`processor`å¯¹æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†ä»¥è¿›è¡Œå¾®è°ƒã€‚


============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\bertology.md
============================================================

# åŸºäºBERTè¿›è¡Œçš„ç›¸å…³ç ”ç©¶ï¼ˆBERTologyï¼‰

å½“å‰ï¼Œä¸€ä¸ªæ–°å…´çš„ç ”ç©¶é¢†åŸŸæ­£è‡´åŠ›äºæ¢ç´¢å¤§è§„æ¨¡ transformer æ¨¡å‹ï¼ˆå¦‚BERTï¼‰çš„å†…éƒ¨å·¥ä½œæœºåˆ¶ï¼Œä¸€äº›äººç§°ä¹‹ä¸ºâ€œBERTologyâ€ã€‚ä»¥ä¸‹æ˜¯è¿™ä¸ªé¢†åŸŸçš„ä¸€äº›å…¸å‹ç¤ºä¾‹ï¼š


- BERT Rediscovers the Classical NLP Pipeline by Ian Tenney, Dipanjan Das, Ellie Pavlick:
  https://huggingface.co/papers/1905.05950
- Are Sixteen Heads Really Better than One? by Paul Michel, Omer Levy, Graham Neubig: https://huggingface.co/papers/1905.10650
- What Does BERT Look At? An Analysis of BERT's Attention by Kevin Clark, Urvashi Khandelwal, Omer Levy, Christopher D.
  Manning: https://huggingface.co/papers/1906.04341
- CAT-probing: A Metric-based Approach to Interpret How Pre-trained Models for Programming Language Attend Code Structure: https://huggingface.co/papers/2210.04633


ä¸ºäº†åŠ©åŠ›è¿™ä¸€æ–°å…´é¢†åŸŸçš„å‘å±•ï¼Œæˆ‘ä»¬åœ¨BERT/GPT/GPT-2æ¨¡å‹ä¸­å¢åŠ äº†ä¸€äº›é™„åŠ åŠŸèƒ½ï¼Œæ–¹ä¾¿äººä»¬è®¿é—®å…¶å†…éƒ¨è¡¨ç¤ºï¼Œè¿™äº›åŠŸèƒ½ä¸»è¦å€Ÿé‰´äº†Paul Michelçš„æ°å‡ºå·¥ä½œ(https://huggingface.co/papers/1905.10650)ï¼š


- è®¿é—®BERT/GPT/GPT-2çš„æ‰€æœ‰éšè—çŠ¶æ€ï¼Œ
- è®¿é—®BERT/GPT/GPT-2æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„æ‰€æœ‰æ³¨æ„åŠ›æƒé‡ï¼Œ
- æ£€ç´¢æ³¨æ„åŠ›å¤´çš„è¾“å‡ºå€¼å’Œæ¢¯åº¦ï¼Œä»¥ä¾¿è®¡ç®—å¤´çš„é‡è¦æ€§å¾—åˆ†å¹¶å¯¹å¤´è¿›è¡Œå‰ªæï¼Œè¯¦æƒ…å¯è§è®ºæ–‡ï¼šhttps://huggingface.co/papers/1905.10650ã€‚

ä¸ºäº†å¸®åŠ©æ‚¨ç†è§£å’Œä½¿ç”¨è¿™äº›åŠŸèƒ½ï¼Œæˆ‘ä»¬æ·»åŠ äº†ä¸€ä¸ªå…·ä½“çš„ç¤ºä¾‹è„šæœ¬ï¼š[bertology.py](https://github.com/huggingface/transformers-research-projects/tree/main/bertology/run_bertology.py)ï¼Œè¯¥è„šæœ¬å¯ä»¥å¯¹ä¸€ä¸ªåœ¨ GLUE æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œä¿¡æ¯æå–ä¸å‰ªæã€‚

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\big_models.md
============================================================



# å®ä¾‹åŒ–å¤§å‹æ¨¡å‹

å½“ä½ æƒ³ä½¿ç”¨ä¸€ä¸ªéå¸¸å¤§çš„é¢„è®­ç»ƒæ¨¡å‹æ—¶ï¼Œä¸€ä¸ªæŒ‘æˆ˜æ˜¯å°½é‡å‡å°‘å¯¹å†…å­˜çš„ä½¿ç”¨ã€‚é€šå¸¸ä»PyTorchå¼€å§‹çš„å·¥ä½œæµç¨‹å¦‚ä¸‹ï¼š

1. ç”¨éšæœºæƒé‡åˆ›å»ºä½ çš„æ¨¡å‹ã€‚
2. åŠ è½½ä½ çš„é¢„è®­ç»ƒæƒé‡ã€‚
3. å°†è¿™äº›é¢„è®­ç»ƒæƒé‡æ”¾å…¥ä½ çš„éšæœºæ¨¡å‹ä¸­ã€‚

æ­¥éª¤1å’Œ2éƒ½éœ€è¦å®Œæ•´ç‰ˆæœ¬çš„æ¨¡å‹åœ¨å†…å­˜ä¸­ï¼Œè¿™åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ä¸æ˜¯é—®é¢˜ï¼Œä½†å¦‚æœä½ çš„æ¨¡å‹å¼€å§‹è¾¾åˆ°å‡ ä¸ªGBçš„å¤§å°ï¼Œè¿™ä¸¤ä¸ªå‰¯æœ¬å¯èƒ½ä¼šè®©ä½ è¶…å‡ºå†…å­˜çš„é™åˆ¶ã€‚æ›´ç³Ÿç³•çš„æ˜¯ï¼Œå¦‚æœä½ ä½¿ç”¨`torch.distributed`æ¥å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒï¼Œæ¯ä¸ªè¿›ç¨‹éƒ½ä¼šåŠ è½½é¢„è®­ç»ƒæ¨¡å‹å¹¶å°†è¿™ä¸¤ä¸ªå‰¯æœ¬å­˜å‚¨åœ¨å†…å­˜ä¸­ã€‚

<Tip>

è¯·æ³¨æ„ï¼Œéšæœºåˆ›å»ºçš„æ¨¡å‹ä½¿ç”¨â€œç©ºâ€å¼ é‡è¿›è¡Œåˆå§‹åŒ–ï¼Œè¿™äº›å¼ é‡å ç”¨å†…å­˜ç©ºé—´ä½†ä¸å¡«å……å®ƒï¼ˆå› æ­¤éšæœºå€¼æ˜¯ç»™å®šæ—¶é—´å†…è¯¥å†…å­˜å—ä¸­çš„ä»»ä½•å†…å®¹ï¼‰ã€‚åœ¨ç¬¬3æ­¥ä¹‹åï¼Œå¯¹æœªåˆå§‹åŒ–çš„æƒé‡æ‰§è¡Œé€‚åˆæ¨¡å‹/å‚æ•°ç§ç±»çš„éšæœºåˆå§‹åŒ–ï¼ˆä¾‹å¦‚æ­£æ€åˆ†å¸ƒï¼‰ï¼Œä»¥å°½å¯èƒ½æé«˜é€Ÿåº¦ï¼

</Tip>

åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨ Transformers æä¾›çš„è§£å†³æ–¹æ¡ˆæ¥å¤„ç†è¿™ä¸ªé—®é¢˜ã€‚è¯·æ³¨æ„ï¼Œè¿™æ˜¯ä¸€ä¸ªç§¯æå¼€å‘çš„é¢†åŸŸï¼Œå› æ­¤è¿™é‡Œè§£é‡Šçš„APIåœ¨å°†æ¥å¯èƒ½ä¼šç•¥æœ‰å˜åŒ–ã€‚

## åˆ†ç‰‡checkpoints

è‡ª4.18.0ç‰ˆæœ¬èµ·ï¼Œå ç”¨ç©ºé—´è¶…è¿‡10GBçš„æ¨¡å‹æ£€æŸ¥ç‚¹å°†è‡ªåŠ¨åˆ†æˆè¾ƒå°çš„ç‰‡æ®µã€‚åœ¨ä½¿ç”¨`model.save_pretrained(save_dir)`æ—¶ï¼Œæ‚¨æœ€ç»ˆä¼šå¾—åˆ°å‡ ä¸ªéƒ¨åˆ†`checkpoints`ï¼ˆæ¯ä¸ªçš„å¤§å°éƒ½å°äº10GBï¼‰ä»¥åŠä¸€ä¸ªç´¢å¼•ï¼Œè¯¥ç´¢å¼•å°†å‚æ•°åç§°æ˜ å°„åˆ°å­˜å‚¨å®ƒä»¬çš„æ–‡ä»¶ã€‚

æ‚¨å¯ä»¥ä½¿ç”¨`max_shard_size`å‚æ•°æ¥æ§åˆ¶åˆ†ç‰‡ä¹‹å‰çš„æœ€å¤§å¤§å°ã€‚ä¸ºäº†ç¤ºä¾‹çš„ç›®çš„ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å…·æœ‰è¾ƒå°åˆ†ç‰‡å¤§å°çš„æ™®é€šå¤§å°çš„æ¨¡å‹ï¼šè®©æˆ‘ä»¬ä»¥ä¼ ç»Ÿçš„BERTæ¨¡å‹ä¸ºä¾‹ã€‚


```py
from transformers import AutoModel

model = AutoModel.from_pretrained("google-bert/bert-base-cased")
```

å¦‚æœæ‚¨ä½¿ç”¨ [`PreTrainedModel.save_pretrained`](æ¨¡å‹é¢„è®­ç»ƒä¿å­˜) è¿›è¡Œä¿å­˜ï¼Œæ‚¨å°†å¾—åˆ°ä¸€ä¸ªæ–°çš„æ–‡ä»¶å¤¹ï¼Œå…¶ä¸­åŒ…å«ä¸¤ä¸ªæ–‡ä»¶ï¼šæ¨¡å‹çš„é…ç½®å’Œæƒé‡ï¼š

```py
>>> import os
>>> import tempfile

>>> with tempfile.TemporaryDirectory() as tmp_dir:
...     model.save_pretrained(tmp_dir)
...     print(sorted(os.listdir(tmp_dir)))
['config.json', 'pytorch_model.bin']
```

ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨æœ€å¤§åˆ†ç‰‡å¤§å°ä¸º200MBï¼š

```py
>>> with tempfile.TemporaryDirectory() as tmp_dir:
...     model.save_pretrained(tmp_dir, max_shard_size="200MB")
...     print(sorted(os.listdir(tmp_dir)))
['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']
```

åœ¨æ¨¡å‹é…ç½®æ–‡ä»¶æœ€ä¸Šæ–¹ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸‰ä¸ªä¸åŒçš„æƒé‡æ–‡ä»¶ï¼Œä»¥åŠä¸€ä¸ª`index.json`ç´¢å¼•æ–‡ä»¶ã€‚è¿™æ ·çš„`checkpoint`å¯ä»¥ä½¿ç”¨[`~PreTrainedModel.from_pretrained`]æ–¹æ³•å®Œå…¨é‡æ–°åŠ è½½ï¼š

```py
>>> with tempfile.TemporaryDirectory() as tmp_dir:
...     model.save_pretrained(tmp_dir, max_shard_size="200MB")
...     new_model = AutoModel.from_pretrained(tmp_dir)
```

å¯¹äºå¤§å‹æ¨¡å‹æ¥è¯´ï¼Œè¿™æ ·åšçš„ä¸»è¦ä¼˜ç‚¹æ˜¯åœ¨ä¸Šè¿°å·¥ä½œæµç¨‹çš„æ­¥éª¤2ä¸­ï¼Œæ¯ä¸ª`checkpoint`çš„åˆ†ç‰‡åœ¨å‰ä¸€ä¸ªåˆ†ç‰‡ä¹‹ååŠ è½½ï¼Œä»è€Œå°†å†…å­˜ä¸­çš„å†…å­˜ä½¿ç”¨é™åˆ¶åœ¨æ¨¡å‹å¤§å°åŠ ä¸Šæœ€å¤§åˆ†ç‰‡çš„å¤§å°ã€‚

åœ¨åå°ï¼Œç´¢å¼•æ–‡ä»¶ç”¨äºç¡®å®š`checkpoint`ä¸­åŒ…å«å“ªäº›é”®ä»¥åŠç›¸åº”çš„æƒé‡å­˜å‚¨åœ¨å“ªé‡Œã€‚æˆ‘ä»¬å¯ä»¥åƒåŠ è½½ä»»ä½•jsonä¸€æ ·åŠ è½½è¯¥ç´¢å¼•ï¼Œå¹¶è·å¾—ä¸€ä¸ªå­—å…¸ï¼š

```py
>>> import json

>>> with tempfile.TemporaryDirectory() as tmp_dir:
...     model.save_pretrained(tmp_dir, max_shard_size="200MB")
...     with open(os.path.join(tmp_dir, "pytorch_model.bin.index.json"), "r") as f:
...         index = json.load(f)

>>> print(index.keys())
dict_keys(['metadata', 'weight_map'])
```

ç›®å‰å…ƒæ•°æ®ä»…åŒ…æ‹¬æ¨¡å‹çš„æ€»å¤§å°ã€‚æˆ‘ä»¬è®¡åˆ’åœ¨å°†æ¥æ·»åŠ å…¶ä»–ä¿¡æ¯ï¼š
```py
>>> index["metadata"]
{'total_size': 433245184}
```

æƒé‡æ˜ å°„æ˜¯è¯¥ç´¢å¼•çš„ä¸»è¦éƒ¨åˆ†ï¼Œå®ƒå°†æ¯ä¸ªå‚æ•°çš„åç§°ï¼ˆé€šå¸¸åœ¨PyTorchæ¨¡å‹çš„`state_dict`ä¸­æ‰¾åˆ°ï¼‰æ˜ å°„åˆ°å­˜å‚¨è¯¥å‚æ•°çš„æ–‡ä»¶ï¼š

```py
>>> index["weight_map"]
{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',
 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',
 ...
```

å¦‚æœæ‚¨æƒ³ç›´æ¥åœ¨æ¨¡å‹å†…éƒ¨åŠ è½½è¿™æ ·çš„åˆ†ç‰‡`checkpoint`ï¼Œè€Œä¸ä½¿ç”¨ [`PreTrainedModel.from_pretrained`](å°±åƒæ‚¨ä¼šä¸ºå®Œæ•´`checkpoint`æ‰§è¡Œ `model.load_state_dict()` ä¸€æ ·)ï¼Œæ‚¨åº”è¯¥ä½¿ç”¨ [`trainer_utils.load_sharded_checkpoint`]ï¼š


```py
>>> from transformers.trainer_utils import load_sharded_checkpoint

>>> with tempfile.TemporaryDirectory() as tmp_dir:
...     model.save_pretrained(tmp_dir, max_shard_size="200MB")
...     load_sharded_checkpoint(model, tmp_dir)
```

## ä½å†…å­˜åŠ è½½

åˆ†ç‰‡`checkpoints`åœ¨ä¸Šè¿°å·¥ä½œæµçš„ç¬¬2æ­¥ä¸­é™ä½äº†å†…å­˜ä½¿ç”¨ï¼Œä½†ä¸ºäº†åœ¨ä½å†…å­˜ç¯å¢ƒä¸­ä½¿ç”¨è¯¥æ¨¡å‹ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨åŸºäº Accelerate åº“çš„å·¥å…·ã€‚

è¯·é˜…è¯»ä»¥ä¸‹æŒ‡å—ä»¥è·å–æ›´å¤šä¿¡æ¯ï¼š[ä½¿ç”¨ Accelerate è¿›è¡Œå¤§æ¨¡å‹åŠ è½½](./main_classes/model#large-model-loading)

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\chat_templating.md
============================================================



# èŠå¤©æ¨¡å‹çš„æ¨¡æ¿

## ä»‹ç»

LLM çš„ä¸€ä¸ªå¸¸è§åº”ç”¨åœºæ™¯æ˜¯èŠå¤©ã€‚åœ¨èŠå¤©ä¸Šä¸‹æ–‡ä¸­ï¼Œä¸å†æ˜¯è¿ç»­çš„æ–‡æœ¬å­—ç¬¦ä¸²æ„æˆçš„è¯­å¥ï¼ˆä¸åŒäºæ ‡å‡†çš„è¯­è¨€æ¨¡å‹ï¼‰ï¼Œ
èŠå¤©æ¨¡å‹ç”±ä¸€æ¡æˆ–å¤šæ¡æ¶ˆæ¯ç»„æˆçš„å¯¹è¯ç»„æˆï¼Œæ¯æ¡æ¶ˆæ¯éƒ½æœ‰ä¸€ä¸ªâ€œç”¨æˆ·â€æˆ–â€œåŠ©æ‰‹â€ç­‰ **è§’è‰²**ï¼Œè¿˜åŒ…æ‹¬æ¶ˆæ¯æ–‡æœ¬ã€‚

ä¸`Tokenizer`ç±»ä¼¼ï¼Œä¸åŒçš„æ¨¡å‹å¯¹èŠå¤©çš„è¾“å…¥æ ¼å¼è¦æ±‚ä¹Ÿä¸åŒã€‚è¿™å°±æ˜¯æˆ‘ä»¬æ·»åŠ **èŠå¤©æ¨¡æ¿**ä½œä¸ºä¸€ä¸ªåŠŸèƒ½çš„åŸå› ã€‚
èŠå¤©æ¨¡æ¿æ˜¯`Tokenizer`çš„ä¸€éƒ¨åˆ†ã€‚ç”¨æ¥æŠŠé—®ç­”çš„å¯¹è¯å†…å®¹è½¬æ¢ä¸ºæ¨¡å‹çš„è¾“å…¥`prompt`ã€‚


è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå¿«é€Ÿçš„ç¤ºä¾‹æ¥å…·ä½“è¯´æ˜ï¼Œä½¿ç”¨`BlenderBot`æ¨¡å‹ã€‚
BlenderBotæœ‰ä¸€ä¸ªéå¸¸ç®€å•çš„é»˜è®¤æ¨¡æ¿ï¼Œä¸»è¦æ˜¯åœ¨å¯¹è¯è½®ä¹‹é—´æ·»åŠ ç©ºæ ¼ï¼š

```python
>>> from transformers import AutoTokenizer
>>> tokenizer = AutoTokenizer.from_pretrained("facebook/blenderbot-400M-distill")

>>> chat = [
...    {"role": "user", "content": "Hello, how are you?"},
...    {"role": "assistant", "content": "I'm doing great. How can I help you today?"},
...    {"role": "user", "content": "I'd like to show off how chat templating works!"},
... ]

>>> tokenizer.apply_chat_template(chat, tokenize=False)
" Hello, how are you?  I'm doing great. How can I help you today?   I'd like to show off how chat templating works!</s>"
```

æ³¨æ„ï¼Œæ•´ä¸ªèŠå¤©å¯¹è¯å†…å®¹è¢«å‹ç¼©æˆäº†ä¸€æ•´ä¸ªå­—ç¬¦ä¸²ã€‚å¦‚æœæˆ‘ä»¬ä½¿ç”¨é»˜è®¤è®¾ç½®çš„`tokenize=True`ï¼Œé‚£ä¹ˆè¯¥å­—ç¬¦ä¸²ä¹Ÿå°†è¢«tokenizedå¤„ç†ã€‚
ä¸è¿‡ï¼Œä¸ºäº†çœ‹åˆ°æ›´å¤æ‚çš„æ¨¡æ¿å®é™…è¿è¡Œï¼Œè®©æˆ‘ä»¬ä½¿ç”¨`mistralai/Mistral-7B-Instruct-v0.1`æ¨¡å‹ã€‚

```python
>>> from transformers import AutoTokenizer
>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1")

>>> chat = [
...   {"role": "user", "content": "Hello, how are you?"},
...   {"role": "assistant", "content": "I'm doing great. How can I help you today?"},
...   {"role": "user", "content": "I'd like to show off how chat templating works!"},
... ]

>>> tokenizer.apply_chat_template(chat, tokenize=False)
"<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]"
```

å¯ä»¥çœ‹åˆ°ï¼Œè¿™ä¸€æ¬¡tokenizerå·²ç»æ·»åŠ äº†[INST]å’Œ[/INST]æ¥è¡¨ç¤ºç”¨æˆ·æ¶ˆæ¯çš„å¼€å§‹å’Œç»“æŸã€‚
Mistral-instructæ˜¯æœ‰ä½¿ç”¨è¿™äº›tokenè¿›è¡Œè®­ç»ƒçš„ï¼Œä½†BlenderBotæ²¡æœ‰ã€‚

## æˆ‘å¦‚ä½•ä½¿ç”¨èŠå¤©æ¨¡æ¿ï¼Ÿ

æ­£å¦‚æ‚¨åœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­æ‰€çœ‹åˆ°çš„ï¼ŒèŠå¤©æ¨¡æ¿éå¸¸å®¹æ˜“ä½¿ç”¨ã€‚åªéœ€æ„å»ºä¸€ç³»åˆ—å¸¦æœ‰`role`å’Œ`content`é”®çš„æ¶ˆæ¯ï¼Œ
ç„¶åå°†å…¶ä¼ é€’ç»™[`~PreTrainedTokenizer.apply_chat_template`]æ–¹æ³•ã€‚
å¦å¤–ï¼Œåœ¨å°†èŠå¤©æ¨¡æ¿ç”¨ä½œæ¨¡å‹é¢„æµ‹çš„è¾“å…¥æ—¶ï¼Œè¿˜å»ºè®®ä½¿ç”¨`add_generation_prompt=True`æ¥æ·»åŠ [generation prompt](#ä»€ä¹ˆæ˜¯generation-prompts)ã€‚

è¿™æ˜¯ä¸€ä¸ªå‡†å¤‡`model.generate()`çš„ç¤ºä¾‹ï¼Œä½¿ç”¨`Zephyr`æ¨¡å‹ï¼š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

checkpoint = "HuggingFaceH4/zephyr-7b-beta"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForCausalLM.from_pretrained(checkpoint)  # You may want to use bfloat16 and/or move to GPU here

messages = [
    {
        "role": "system",
        "content": "You are a friendly chatbot who always responds in the style of a pirate",
    },
    {"role": "user", "content": "How many helicopters can a human eat in one sitting?"},
 ]
tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt")
print(tokenizer.decode(tokenized_chat[0]))
```
è¿™å°†ç”ŸæˆZephyræœŸæœ›çš„è¾“å…¥æ ¼å¼çš„å­—ç¬¦ä¸²ã€‚å®ƒçœ‹èµ·æ¥åƒè¿™æ ·ï¼š
```text
<|system|>
You are a friendly chatbot who always responds in the style of a pirate</s> 
<|user|>
How many helicopters can a human eat in one sitting?</s> 
<|assistant|>
```

ç°åœ¨æˆ‘ä»¬å·²ç»æŒ‰ç…§`Zephyr`çš„è¦æ±‚ä¼ å…¥promptäº†ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ¨¡å‹æ¥ç”Ÿæˆå¯¹ç”¨æˆ·é—®é¢˜çš„å›å¤ï¼š

```python
outputs = model.generate(tokenized_chat, max_new_tokens=128) 
print(tokenizer.decode(outputs[0]))
```

è¾“å‡ºç»“æœæ˜¯ï¼š

```text
<|system|>
You are a friendly chatbot who always responds in the style of a pirate</s> 
<|user|>
How many helicopters can a human eat in one sitting?</s> 
<|assistant|>
Matey, I'm afraid I must inform ye that humans cannot eat helicopters. Helicopters are not food, they are flying machines. Food is meant to be eaten, like a hearty plate o' grog, a savory bowl o' stew, or a delicious loaf o' bread. But helicopters, they be for transportin' and movin' around, not for eatin'. So, I'd say none, me hearties. None at all.
```
å•Šï¼ŒåŸæ¥è¿™ä¹ˆå®¹æ˜“ï¼

## æœ‰è‡ªåŠ¨åŒ–çš„èŠå¤©`pipeline`å—ï¼Ÿ

æœ‰çš„ï¼Œ[`TextGenerationPipeline`]ã€‚è¿™ä¸ª`pipeline`çš„è®¾è®¡æ˜¯ä¸ºäº†æ–¹ä¾¿ä½¿ç”¨èŠå¤©æ¨¡å‹ã€‚è®©æˆ‘ä»¬å†è¯•ä¸€æ¬¡ Zephyr çš„ä¾‹å­ï¼Œä½†è¿™æ¬¡ä½¿ç”¨`pipeline`ï¼š

```python
from transformers import pipeline

pipe = pipeline("text-generation", "HuggingFaceH4/zephyr-7b-beta")
messages = [
    {
        "role": "system",
        "content": "You are a friendly chatbot who always responds in the style of a pirate",
    },
    {"role": "user", "content": "How many helicopters can a human eat in one sitting?"},
]
print(pipe(messages, max_new_tokens=256)['generated_text'][-1])
```

```text
{'role': 'assistant', 'content': "Matey, I'm afraid I must inform ye that humans cannot eat helicopters. Helicopters are not food, they are flying machines. Food is meant to be eaten, like a hearty plate o' grog, a savory bowl o' stew, or a delicious loaf o' bread. But helicopters, they be for transportin' and movin' around, not for eatin'. So, I'd say none, me hearties. None at all."}
```

[`TextGenerationPipeline`]å°†è´Ÿè´£å¤„ç†æ‰€æœ‰çš„`tokenized`å¹¶è°ƒç”¨`apply_chat_template`ï¼Œä¸€æ—¦æ¨¡å‹æœ‰äº†èŠå¤©æ¨¡æ¿ï¼Œæ‚¨åªéœ€è¦åˆå§‹åŒ–pipelineå¹¶ä¼ é€’æ¶ˆæ¯åˆ—è¡¨ï¼

## ä»€ä¹ˆæ˜¯"generation prompts"?

æ‚¨å¯èƒ½å·²ç»æ³¨æ„åˆ°`apply_chat_template`æ–¹æ³•æœ‰ä¸€ä¸ª`add_generation_prompt`å‚æ•°ã€‚
è¿™ä¸ªå‚æ•°å‘Šè¯‰æ¨¡æ¿æ·»åŠ æ¨¡å‹å¼€å§‹ç­”å¤çš„æ ‡è®°ã€‚ä¾‹å¦‚ï¼Œè€ƒè™‘ä»¥ä¸‹å¯¹è¯ï¼š

```python
messages = [
    {"role": "user", "content": "Hi there!"},
    {"role": "assistant", "content": "Nice to meet you!"},
    {"role": "user", "content": "Can I ask a question?"}
]
```

è¿™æ˜¯`add_generation_prompt=False`çš„ç»“æœï¼Œä½¿ç”¨ChatMLæ¨¡æ¿ï¼š
```python
tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
"""<|im_start|>user
Hi there!<|im_end|>
<|im_start|>assistant
Nice to meet you!<|im_end|>
<|im_start|>user
Can I ask a question?<|im_end|>
"""
```

ä¸‹é¢è¿™æ˜¯`add_generation_prompt=True`çš„ç»“æœï¼š

```python
tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
"""<|im_start|>user
Hi there!<|im_end|>
<|im_start|>assistant
Nice to meet you!<|im_end|>
<|im_start|>user
Can I ask a question?<|im_end|>
<|im_start|>assistant
"""
```

è¿™ä¸€æ¬¡æˆ‘ä»¬æ·»åŠ äº†æ¨¡å‹å¼€å§‹ç­”å¤çš„æ ‡è®°ã€‚è¿™å¯ä»¥ç¡®ä¿æ¨¡å‹ç”Ÿæˆæ–‡æœ¬æ—¶åªä¼šç»™å‡ºç­”å¤ï¼Œè€Œä¸ä¼šåšå‡ºæ„å¤–çš„è¡Œä¸ºï¼Œæ¯”å¦‚ç»§ç»­ç”¨æˆ·çš„æ¶ˆæ¯ã€‚
è®°ä½ï¼ŒèŠå¤©æ¨¡å‹åªæ˜¯è¯­è¨€æ¨¡å‹ï¼Œå®ƒä»¬è¢«è®­ç»ƒæ¥ç»§ç»­æ–‡æœ¬ï¼Œè€ŒèŠå¤©å¯¹å®ƒä»¬æ¥è¯´åªæ˜¯ä¸€ç§ç‰¹æ®Šçš„æ–‡æœ¬ï¼
ä½ éœ€è¦ç”¨é€‚å½“çš„æ§åˆ¶æ ‡è®°æ¥å¼•å¯¼å®ƒä»¬ï¼Œè®©å®ƒä»¬çŸ¥é“è‡ªå·±åº”è¯¥åšä»€ä¹ˆã€‚

å¹¶éæ‰€æœ‰æ¨¡å‹éƒ½éœ€è¦ç”Ÿæˆæç¤ºã€‚ä¸€äº›æ¨¡å‹ï¼Œå¦‚BlenderBotå’ŒLLaMAï¼Œåœ¨æ¨¡å‹å›å¤ä¹‹å‰æ²¡æœ‰ä»»ä½•ç‰¹æ®Šæ ‡è®°ã€‚
åœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œ`add_generation_prompt`å‚æ•°å°†ä¸èµ·ä½œç”¨ã€‚`add_generation_prompt`å‚æ•°å–å†³äºä½ æ‰€ä½¿ç”¨çš„æ¨¡æ¿ã€‚

## æˆ‘å¯ä»¥åœ¨è®­ç»ƒä¸­ä½¿ç”¨èŠå¤©æ¨¡æ¿å—ï¼Ÿ

å¯ä»¥ï¼æˆ‘ä»¬å»ºè®®æ‚¨å°†èŠå¤©æ¨¡æ¿åº”ç”¨ä¸ºæ•°æ®é›†çš„é¢„å¤„ç†æ­¥éª¤ã€‚ä¹‹åï¼Œæ‚¨å¯ä»¥åƒè¿›è¡Œä»»ä½•å…¶ä»–è¯­è¨€æ¨¡å‹è®­ç»ƒä»»åŠ¡ä¸€æ ·ç»§ç»­ã€‚
åœ¨è®­ç»ƒæ—¶ï¼Œé€šå¸¸åº”è¯¥è®¾ç½®`add_generation_prompt=False`ï¼Œå› ä¸ºæ·»åŠ çš„åŠ©æ‰‹æ ‡è®°åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¹¶ä¸ä¼šæœ‰å¸®åŠ©ã€‚
è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªä¾‹å­ï¼š

```python
from transformers import AutoTokenizer
from datasets import Dataset

tokenizer = AutoTokenizer.from_pretrained("HuggingFaceH4/zephyr-7b-beta")

chat1 = [
    {"role": "user", "content": "Which is bigger, the moon or the sun?"},
    {"role": "assistant", "content": "The sun."}
]
chat2 = [
    {"role": "user", "content": "Which is bigger, a virus or a bacterium?"},
    {"role": "assistant", "content": "A bacterium."}
]

dataset = Dataset.from_dict({"chat": [chat1, chat2]})
dataset = dataset.map(lambda x: {"formatted_chat": tokenizer.apply_chat_template(x["chat"], tokenize=False, add_generation_prompt=False)})
print(dataset['formatted_chat'][0])
```
ç»“æœæ˜¯ï¼š
```text
<|user|>
Which is bigger, the moon or the sun?</s>
<|assistant|>
The sun.</s>
```

è¿™æ ·ï¼Œåé¢ä½ å¯ä»¥ä½¿ç”¨`formatted_chat`åˆ—ï¼Œè·Ÿæ ‡å‡†è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸­ä¸€æ ·è®­ç»ƒå³å¯ã€‚
## é«˜çº§ï¼šèŠå¤©æ¨¡æ¿æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ

æ¨¡å‹çš„èŠå¤©æ¨¡æ¿å­˜å‚¨åœ¨`tokenizer.chat_template`å±æ€§ä¸Šã€‚å¦‚æœæ²¡æœ‰è®¾ç½®ï¼Œåˆ™å°†ä½¿ç”¨è¯¥æ¨¡å‹çš„é»˜è®¤æ¨¡æ¿ã€‚
è®©æˆ‘ä»¬æ¥çœ‹çœ‹`BlenderBot`çš„æ¨¡æ¿ï¼š
```python

>>> from transformers import AutoTokenizer
>>> tokenizer = AutoTokenizer.from_pretrained("facebook/blenderbot-400M-distill")

>>> tokenizer.chat_template
"{% for message in messages %}{% if message['role'] == 'user' %}{{ ' ' }}{% endif %}{{ message['content'] }}{% if not loop.last %}{{ '  ' }}{% endif %}{% endfor %}{{ eos_token }}"
```

è¿™çœ‹ç€æœ‰ç‚¹å¤æ‚ã€‚è®©æˆ‘ä»¬æ·»åŠ ä¸€äº›æ¢è¡Œå’Œç¼©è¿›ï¼Œä½¿å…¶æ›´æ˜“è¯»ã€‚
è¯·æ³¨æ„ï¼Œé»˜è®¤æƒ…å†µä¸‹å¿½ç•¥æ¯ä¸ªå—åçš„ç¬¬ä¸€ä¸ªæ¢è¡Œä»¥åŠå—ä¹‹å‰çš„ä»»ä½•å‰å¯¼ç©ºæ ¼ï¼Œ
ä½¿ç”¨Jinjaçš„`trim_blocks`å’Œ`lstrip_blocks`æ ‡ç­¾ã€‚
è¿™é‡Œï¼Œè¯·æ³¨æ„ç©ºæ ¼çš„ä½¿ç”¨ã€‚æˆ‘ä»¬å¼ºçƒˆå»ºè®®æ‚¨ä»”ç»†æ£€æŸ¥æ¨¡æ¿æ˜¯å¦æ‰“å°äº†å¤šä½™çš„ç©ºæ ¼ï¼
```
{% for message in messages %}
    {% if message['role'] == 'user' %}
        {{ ' ' }}
    {% endif %}
    {{ message['content'] }}
    {% if not loop.last %}
        {{ '  ' }}
    {% endif %}
{% endfor %}
{{ eos_token }}
```

å¦‚æœä½ ä¹‹å‰ä¸äº†è§£[Jinja template](https://jinja.palletsprojects.com/en/3.1.x/templates/)ã€‚
Jinjaæ˜¯ä¸€ç§æ¨¡æ¿è¯­è¨€ï¼Œå…è®¸ä½ ç¼–å†™ç®€å•çš„ä»£ç æ¥ç”Ÿæˆæ–‡æœ¬ã€‚
åœ¨è®¸å¤šæ–¹é¢ï¼Œä»£ç å’Œè¯­æ³•ç±»ä¼¼äºPythonã€‚åœ¨çº¯Pythonä¸­ï¼Œè¿™ä¸ªæ¨¡æ¿çœ‹èµ·æ¥ä¼šåƒè¿™æ ·ï¼š
```python
for idx, message in enumerate(messages):
    if message['role'] == 'user':
        print(' ')
    print(message['content'])
    if not idx == len(messages) - 1:  # Check for the last message in the conversation
        print('  ')
print(eos_token)
```

è¿™é‡Œä½¿ç”¨Jinjaæ¨¡æ¿å¤„ç†å¦‚ä¸‹ä¸‰æ­¥ï¼š
1. å¯¹äºæ¯æ¡æ¶ˆæ¯ï¼Œå¦‚æœæ¶ˆæ¯æ˜¯ç”¨æˆ·æ¶ˆæ¯ï¼Œåˆ™åœ¨å…¶å‰é¢æ·»åŠ ä¸€ä¸ªç©ºæ ¼ï¼Œå¦åˆ™ä¸æ‰“å°ä»»ä½•å†…å®¹
2. æ·»åŠ æ¶ˆæ¯å†…å®¹
3. å¦‚æœæ¶ˆæ¯ä¸æ˜¯æœ€åä¸€æ¡ï¼Œè¯·åœ¨å…¶åæ·»åŠ ä¸¤ä¸ªç©ºæ ¼ã€‚åœ¨æœ€åä¸€æ¡æ¶ˆæ¯ä¹‹åï¼Œæ‰“å°`EOS`ã€‚

è¿™æ˜¯ä¸€ä¸ªç®€å•çš„æ¨¡æ¿ï¼Œå®ƒä¸æ·»åŠ ä»»ä½•æ§åˆ¶tokensï¼Œä¹Ÿä¸æ”¯æŒ`system`æ¶ˆæ¯ï¼ˆå¸¸ç”¨äºæŒ‡å¯¼æ¨¡å‹åœ¨åç»­å¯¹è¯ä¸­å¦‚ä½•è¡¨ç°ï¼‰ã€‚
ä½† Jinja ç»™äº†ä½ å¾ˆå¤§çš„çµæ´»æ€§æ¥åšè¿™äº›äº‹æƒ…ï¼è®©æˆ‘ä»¬çœ‹ä¸€ä¸ª Jinja æ¨¡æ¿ï¼Œ
å®ƒå¯ä»¥å®ç°ç±»ä¼¼äºLLaMAçš„promptè¾“å…¥ï¼ˆè¯·æ³¨æ„ï¼ŒçœŸæ­£çš„LLaMAæ¨¡æ¿åŒ…æ‹¬`system`æ¶ˆæ¯ï¼Œè¯·ä¸è¦åœ¨å®é™…ä»£ç ä¸­ä½¿ç”¨è¿™ä¸ªç®€å•æ¨¡æ¿ï¼ï¼‰
```
{% for message in messages %}
    {% if message['role'] == 'user' %}
        {{ bos_token + '[INST] ' + message['content'] + ' [/INST]' }}
    {% elif message['role'] == 'system' %}
        {{ '<<SYS>>\\n' + message['content'] + '\\n<</SYS>>\\n\\n' }}
    {% elif message['role'] == 'assistant' %}
        {{ ' '  + message['content'] + ' ' + eos_token }}
    {% endif %}
{% endfor %}
```

è¿™é‡Œç¨å¾®çœ‹ä¸€ä¸‹ï¼Œå°±èƒ½æ˜ç™½è¿™ä¸ªæ¨¡æ¿çš„ä½œç”¨ï¼šå®ƒæ ¹æ®æ¯æ¡æ¶ˆæ¯çš„â€œè§’è‰²â€æ·»åŠ å¯¹åº”çš„æ¶ˆæ¯ã€‚
`user`ã€`assistant`ã€`system`çš„æ¶ˆæ¯éœ€è¦åˆ†åˆ«å¤„ç†ï¼Œå› ä¸ºå®ƒä»¬ä»£è¡¨ä¸åŒçš„è§’è‰²è¾“å…¥ã€‚

## é«˜çº§ï¼šç¼–è¾‘èŠå¤©æ¨¡æ¿

### å¦‚ä½•åˆ›å»ºèŠå¤©æ¨¡æ¿ï¼Ÿ

å¾ˆç®€å•ï¼Œä½ åªéœ€ç¼–å†™ä¸€ä¸ªjinjaæ¨¡æ¿å¹¶è®¾ç½®`tokenizer.chat_template`ã€‚ä½ ä¹Ÿå¯ä»¥ä»ä¸€ä¸ªç°æœ‰æ¨¡æ¿å¼€å§‹ï¼Œåªéœ€è¦ç®€å•ç¼–è¾‘ä¾¿å¯ä»¥ï¼
ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥é‡‡ç”¨ä¸Šé¢çš„LLaMAæ¨¡æ¿ï¼Œå¹¶åœ¨åŠ©æ‰‹æ¶ˆæ¯ä¸­æ·»åŠ "[ASST]"å’Œ"[/ASST]"ï¼š
```
{% for message in messages %}
    {% if message['role'] == 'user' %}
        {{ bos_token + '[INST] ' + message['content'].strip() + ' [/INST]' }}
    {% elif message['role'] == 'system' %}
        {{ '<<SYS>>\\n' + message['content'].strip() + '\\n<</SYS>>\\n\\n' }}
    {% elif message['role'] == 'assistant' %}
        {{ '[ASST] '  + message['content'] + ' [/ASST]' + eos_token }}
    {% endif %}
{% endfor %}
```

ç°åœ¨ï¼Œåªéœ€è®¾ç½®`tokenizer.chat_template`å±æ€§ã€‚ä¸‹æ¬¡ä½¿ç”¨[`~PreTrainedTokenizer.apply_chat_template`]æ—¶ï¼Œå®ƒå°†ä½¿ç”¨æ‚¨çš„æ–°æ¨¡æ¿ï¼
æ­¤å±æ€§å°†ä¿å­˜åœ¨`tokenizer_config.json`æ–‡ä»¶ä¸­ï¼Œå› æ­¤æ‚¨å¯ä»¥ä½¿ç”¨[`~utils.PushToHubMixin.push_to_hub`]å°†æ–°æ¨¡æ¿ä¸Šä¼ åˆ° Hubï¼Œ
è¿™æ ·æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨ä½ æ¨¡å‹çš„æ¨¡æ¿ï¼

```python
template = tokenizer.chat_template
template = template.replace("SYS", "SYSTEM")  # Change the system token
tokenizer.chat_template = template  # Set the new template
tokenizer.push_to_hub("model_name")  # Upload your new template to the Hub!
```

ç”±äº[`~PreTrainedTokenizer.apply_chat_template`]æ–¹æ³•æ˜¯ç”±[`TextGenerationPipeline`]ç±»è°ƒç”¨ï¼Œ
å› æ­¤ä¸€æ—¦ä½ è®¾ç½®äº†èŠå¤©æ¨¡æ¿ï¼Œæ‚¨çš„æ¨¡å‹å°†è‡ªåŠ¨ä¸[`TextGenerationPipeline`]å…¼å®¹ã€‚
### â€œé»˜è®¤â€æ¨¡æ¿æ˜¯ä»€ä¹ˆï¼Ÿ

åœ¨å¼•å…¥èŠå¤©æ¨¡æ¿ï¼ˆchat_templateï¼‰ä¹‹å‰ï¼ŒèŠå¤©promptæ˜¯åœ¨æ¨¡å‹ä¸­é€šè¿‡ç¡¬ç¼–ç å¤„ç†çš„ã€‚ä¸ºäº†å‘å‰å…¼å®¹ï¼Œæˆ‘ä»¬ä¿ç•™äº†è¿™ç§ç¡¬ç¼–ç å¤„ç†èŠå¤©promptçš„æ–¹æ³•ã€‚
å¦‚æœä¸€ä¸ªæ¨¡å‹æ²¡æœ‰è®¾ç½®èŠå¤©æ¨¡æ¿ï¼Œä½†å…¶æ¨¡å‹æœ‰é»˜è®¤æ¨¡æ¿ï¼Œ`TextGenerationPipeline`ç±»å’Œ`apply_chat_template`ç­‰æ–¹æ³•å°†ä½¿ç”¨è¯¥æ¨¡å‹çš„èŠå¤©æ¨¡æ¿ã€‚
æ‚¨å¯ä»¥é€šè¿‡æ£€æŸ¥`tokenizer.default_chat_template`å±æ€§æ¥æŸ¥æ‰¾`tokenizer`çš„é»˜è®¤æ¨¡æ¿ã€‚

è¿™æ˜¯æˆ‘ä»¬çº¯ç²¹ä¸ºäº†å‘å‰å…¼å®¹æ€§è€Œåšçš„äº‹æƒ…ï¼Œä»¥é¿å…ç ´åä»»ä½•ç°æœ‰çš„å·¥ä½œæµç¨‹ã€‚å³ä½¿é»˜è®¤çš„èŠå¤©æ¨¡æ¿é€‚ç”¨äºæ‚¨çš„æ¨¡å‹ï¼Œ
æˆ‘ä»¬å¼ºçƒˆå»ºè®®é€šè¿‡æ˜¾å¼è®¾ç½®`chat_template`å±æ€§æ¥è¦†ç›–é»˜è®¤æ¨¡æ¿ï¼Œä»¥ä¾¿å‘ç”¨æˆ·æ¸…æ¥šåœ°è¡¨æ˜æ‚¨çš„æ¨¡å‹å·²ç»æ­£ç¡®çš„é…ç½®äº†èŠå¤©æ¨¡æ¿ï¼Œ
å¹¶ä¸”ä¸ºäº†æœªæ¥é˜²èŒƒé»˜è®¤æ¨¡æ¿è¢«ä¿®æ”¹æˆ–å¼ƒç”¨çš„æƒ…å†µã€‚
### æˆ‘åº”è¯¥ä½¿ç”¨å“ªä¸ªæ¨¡æ¿ï¼Ÿ

åœ¨ä¸ºå·²ç»è®­ç»ƒè¿‡çš„èŠå¤©æ¨¡å‹è®¾ç½®æ¨¡æ¿æ—¶ï¼Œæ‚¨åº”ç¡®ä¿æ¨¡æ¿ä¸æ¨¡å‹åœ¨è®­ç»ƒæœŸé—´çœ‹åˆ°çš„æ¶ˆæ¯æ ¼å¼å®Œå…¨åŒ¹é…ï¼Œå¦åˆ™å¯èƒ½ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚
å³ä½¿æ‚¨ç»§ç»­å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œä¹Ÿåº”ä¿æŒèŠå¤©æ¨¡æ¿ä¸å˜ï¼Œè¿™æ ·å¯èƒ½ä¼šè·å¾—æœ€ä½³æ€§èƒ½ã€‚
è¿™ä¸`tokenization`éå¸¸ç±»ä¼¼ï¼Œåœ¨æ¨æ–­æ—¶ï¼Œä½ é€‰ç”¨è·Ÿè®­ç»ƒæ—¶ä¸€æ ·çš„`tokenization`ï¼Œé€šå¸¸ä¼šè·å¾—æœ€ä½³æ€§èƒ½ã€‚

å¦‚æœæ‚¨ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼Œæˆ–è€…åœ¨å¾®è°ƒåŸºç¡€è¯­è¨€æ¨¡å‹è¿›è¡ŒèŠå¤©æ—¶ï¼Œæ‚¨æœ‰å¾ˆå¤§çš„è‡ªç”±é€‰æ‹©é€‚å½“çš„æ¨¡æ¿ï¼
LLMsè¶³å¤Ÿèªæ˜ï¼Œå¯ä»¥å­¦ä¼šå¤„ç†è®¸å¤šä¸åŒçš„è¾“å…¥æ ¼å¼ã€‚æˆ‘ä»¬ä¸ºæ²¡æœ‰ç‰¹å®šç±»åˆ«æ¨¡æ¿çš„æ¨¡å‹æä¾›ä¸€ä¸ªé»˜è®¤æ¨¡æ¿ï¼Œè¯¥æ¨¡æ¿éµå¾ª
`ChatML` formatæ ¼å¼è¦æ±‚ï¼Œå¯¹äºè®¸å¤šç”¨ä¾‹æ¥è¯´ï¼Œ
è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ã€çµæ´»çš„é€‰æ‹©ã€‚

é»˜è®¤æ¨¡æ¿çœ‹èµ·æ¥åƒè¿™æ ·ï¼š

```
{% for message in messages %}
    {{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}
{% endfor %}
```


å¦‚æœæ‚¨å–œæ¬¢è¿™ä¸ªæ¨¡æ¿ï¼Œä¸‹é¢æ˜¯ä¸€è¡Œä»£ç çš„æ¨¡æ¿å½¢å¼ï¼Œå®ƒå¯ä»¥ç›´æ¥å¤åˆ¶åˆ°æ‚¨çš„ä»£ç ä¸­ã€‚è¿™ä¸€è¡Œä»£ç è¿˜åŒ…æ‹¬äº†[generation prompts](#ä»€ä¹ˆæ˜¯"generation prompts"?)ï¼Œ
ä½†è¯·æ³¨æ„å®ƒä¸ä¼šæ·»åŠ `BOS`æˆ–`EOS`tokenã€‚
å¦‚æœæ‚¨çš„æ¨¡å‹éœ€è¦è¿™äº›tokenï¼Œå®ƒä»¬ä¸ä¼šè¢«`apply_chat_template`è‡ªåŠ¨æ·»åŠ ï¼Œæ¢å¥è¯è¯´ï¼Œæ–‡æœ¬çš„é»˜è®¤å¤„ç†å‚æ•°æ˜¯`add_special_tokens=False`ã€‚
è¿™æ˜¯ä¸ºäº†é¿å…æ¨¡æ¿å’Œ`add_special_tokens`é€»è¾‘äº§ç”Ÿå†²çªï¼Œå¦‚æœæ‚¨çš„æ¨¡å‹éœ€è¦ç‰¹æ®Štokensï¼Œè¯·ç¡®ä¿å°†å®ƒä»¬æ·»åŠ åˆ°æ¨¡æ¿ä¸­ï¼

```
tokenizer.chat_template = "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}"
```

è¯¥æ¨¡æ¿å°†æ¯æ¡æ¶ˆæ¯åŒ…è£…åœ¨`<|im_start|>`å’Œ`<|im_end|>`tokensé‡Œé¢ï¼Œå¹¶å°†è§’è‰²ç®€å•åœ°å†™ä¸ºå­—ç¬¦ä¸²ï¼Œè¿™æ ·å¯ä»¥çµæ´»åœ°è®­ç»ƒè§’è‰²ã€‚è¾“å‡ºå¦‚ä¸‹ï¼š
```text
<|im_start|>system
You are a helpful chatbot that will do its best not to say anything so stupid that people tweet about it.<|im_end|>
<|im_start|>user
How are you?<|im_end|>
<|im_start|>assistant
I'm doing great!<|im_end|>
```

`user`ï¼Œ`system`å’Œ`assistant`æ˜¯å¯¹è¯åŠ©æ‰‹æ¨¡å‹çš„æ ‡å‡†è§’è‰²ï¼Œå¦‚æœæ‚¨çš„æ¨¡å‹è¦ä¸[`TextGenerationPipeline`]å…¼å®¹ï¼Œæˆ‘ä»¬å»ºè®®ä½ ä½¿ç”¨è¿™äº›è§’è‰²ã€‚
ä½†æ‚¨å¯ä»¥ä¸å±€é™äºè¿™äº›è§’è‰²ï¼Œæ¨¡æ¿éå¸¸çµæ´»ï¼Œä»»ä½•å­—ç¬¦ä¸²éƒ½å¯ä»¥æˆä¸ºè§’è‰²ã€‚

### å¦‚ä½•æ·»åŠ èŠå¤©æ¨¡æ¿ï¼Ÿ

å¦‚æœæ‚¨æœ‰ä»»ä½•èŠå¤©æ¨¡å‹ï¼Œæ‚¨åº”è¯¥è®¾ç½®å®ƒä»¬çš„`tokenizer.chat_template`å±æ€§ï¼Œå¹¶ä½¿ç”¨[`~PreTrainedTokenizer.apply_chat_template`]æµ‹è¯•ï¼Œ
ç„¶åå°†æ›´æ–°åçš„`tokenizer`æ¨é€åˆ° Hubã€‚
å³ä½¿æ‚¨ä¸æ˜¯æ¨¡å‹æ‰€æœ‰è€…ï¼Œå¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨ä¸€ä¸ªç©ºçš„èŠå¤©æ¨¡æ¿æˆ–è€…ä»åœ¨ä½¿ç”¨é»˜è®¤çš„èŠå¤©æ¨¡æ¿ï¼Œ
è¯·å‘èµ·ä¸€ä¸ª[pull request](https://huggingface.co/docs/hub/repositories-pull-requests-discussions)ï¼Œä»¥ä¾¿æ­£ç¡®è®¾ç½®è¯¥å±æ€§ï¼

ä¸€æ—¦å±æ€§è®¾ç½®å®Œæˆï¼Œå°±å®Œæˆäº†ï¼`tokenizer.apply_chat_template`ç°åœ¨å°†åœ¨è¯¥æ¨¡å‹ä¸­æ­£å¸¸å·¥ä½œï¼Œ
è¿™æ„å‘³ç€å®ƒä¹Ÿä¼šè‡ªåŠ¨æ”¯æŒåœ¨è¯¸å¦‚`TextGenerationPipeline`çš„åœ°æ–¹ï¼

é€šè¿‡ç¡®ä¿æ¨¡å‹å…·æœ‰è¿™ä¸€å±æ€§ï¼Œæˆ‘ä»¬å¯ä»¥ç¡®ä¿æ•´ä¸ªç¤¾åŒºéƒ½èƒ½å……åˆ†åˆ©ç”¨å¼€æºæ¨¡å‹çš„å…¨éƒ¨åŠŸèƒ½ã€‚
æ ¼å¼ä¸åŒ¹é…å·²ç»å›°æ‰°è¿™ä¸ªé¢†åŸŸå¹¶æ‚„æ‚„åœ°æŸå®³äº†æ€§èƒ½å¤ªä¹…äº†ï¼Œæ˜¯æ—¶å€™ç»“æŸå®ƒä»¬äº†ï¼


## é«˜çº§ï¼šæ¨¡æ¿å†™ä½œæŠ€å·§

å¦‚æœä½ å¯¹Jinjaä¸ç†Ÿæ‚‰ï¼Œæˆ‘ä»¬é€šå¸¸å‘ç°ç¼–å†™èŠå¤©æ¨¡æ¿çš„æœ€ç®€å•æ–¹æ³•æ˜¯å…ˆç¼–å†™ä¸€ä¸ªç®€çŸ­çš„Pythonè„šæœ¬ï¼ŒæŒ‰ç…§ä½ æƒ³è¦çš„æ–¹å¼æ ¼å¼åŒ–æ¶ˆæ¯ï¼Œç„¶åå°†è¯¥è„šæœ¬è½¬æ¢ä¸ºæ¨¡æ¿ã€‚

è¯·è®°ä½ï¼Œæ¨¡æ¿å¤„ç†ç¨‹åºå°†æ¥æ”¶å¯¹è¯å†å²ä½œä¸ºåä¸º`messages`çš„å˜é‡ã€‚æ¯æ¡`message`éƒ½æ˜¯ä¸€ä¸ªå¸¦æœ‰ä¸¤ä¸ªé”®`role`å’Œ`content`çš„å­—å…¸ã€‚
æ‚¨å¯ä»¥åœ¨æ¨¡æ¿ä¸­åƒåœ¨Pythonä¸­ä¸€æ ·è®¿é—®`messages`ï¼Œè¿™æ„å‘³ç€æ‚¨å¯ä»¥ä½¿ç”¨`{% for message in messages %}`è¿›è¡Œå¾ªç¯ï¼Œ
æˆ–è€…ä¾‹å¦‚ä½¿ç”¨`{{ messages[0] }}`è®¿é—®å•ä¸ªæ¶ˆæ¯ã€‚

æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ä»¥ä¸‹æç¤ºå°†æ‚¨çš„ä»£ç è½¬æ¢ä¸ºJinjaï¼š
### Forå¾ªç¯

åœ¨Jinjaä¸­ï¼Œforå¾ªç¯çœ‹èµ·æ¥åƒè¿™æ ·ï¼š

```
{% for message in messages %}
{{ message['content'] }}
{% endfor %}
```

è¯·æ³¨æ„ï¼Œ`{{ expression block }}`ä¸­çš„å†…å®¹å°†è¢«æ‰“å°åˆ°è¾“å‡ºã€‚æ‚¨å¯ä»¥åœ¨è¡¨è¾¾å¼å—ä¸­ä½¿ç”¨åƒ`+`è¿™æ ·çš„è¿ç®—ç¬¦æ¥ç»„åˆå­—ç¬¦ä¸²ã€‚
### Ifè¯­å¥

Jinjaä¸­çš„ifè¯­å¥å¦‚ä¸‹æ‰€ç¤ºï¼š

```
{% if message['role'] == 'user' %}
{{ message['content'] }}
{% endif %}
```
æ³¨æ„Jinjaä½¿ç”¨`{% endfor %}`å’Œ`{% endif %}`æ¥è¡¨ç¤º`for`å’Œ`if`çš„ç»“æŸã€‚

### ç‰¹æ®Šå˜é‡

åœ¨æ‚¨çš„æ¨¡æ¿ä¸­ï¼Œæ‚¨å°†å¯ä»¥è®¿é—®`messages`åˆ—è¡¨ï¼Œä½†æ‚¨è¿˜å¯ä»¥è®¿é—®å…¶ä»–å‡ ä¸ªç‰¹æ®Šå˜é‡ã€‚
è¿™äº›åŒ…æ‹¬ç‰¹æ®Š`token`ï¼Œå¦‚`bos_token`å’Œ`eos_token`ï¼Œä»¥åŠæˆ‘ä»¬ä¸Šé¢è®¨è®ºè¿‡çš„`add_generation_prompt`å˜é‡ã€‚
æ‚¨è¿˜å¯ä»¥ä½¿ç”¨`loop`å˜é‡æ¥è®¿é—®æœ‰å…³å½“å‰å¾ªç¯è¿­ä»£çš„ä¿¡æ¯ï¼Œä¾‹å¦‚ä½¿ç”¨`{% if loop.last %}`æ¥æ£€æŸ¥å½“å‰æ¶ˆæ¯æ˜¯å¦æ˜¯å¯¹è¯ä¸­çš„æœ€åä¸€æ¡æ¶ˆæ¯ã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼Œå¦‚æœ`add_generation_prompt=True`éœ€è¦åœ¨å¯¹è¯ç»“æŸæ—¶æ·»åŠ `generate_prompt`ï¼š


```
{% if loop.last and add_generation_prompt %}
{{ bos_token + 'Assistant:\n' }}
{% endif %}
```

### ç©ºæ ¼çš„æ³¨æ„äº‹é¡¹

æˆ‘ä»¬å·²ç»å°½å¯èƒ½å°è¯•è®©Jinjaå¿½ç•¥é™¤`{{ expressions }}`ä¹‹å¤–çš„ç©ºæ ¼ã€‚
ç„¶è€Œï¼Œè¯·æ³¨æ„Jinjaæ˜¯ä¸€ä¸ªé€šç”¨çš„æ¨¡æ¿å¼•æ“ï¼Œå®ƒå¯èƒ½ä¼šå°†åŒä¸€è¡Œæ–‡æœ¬å—ä¹‹é—´çš„ç©ºæ ¼è§†ä¸ºé‡è¦ï¼Œå¹¶å°†å…¶æ‰“å°åˆ°è¾“å‡ºä¸­ã€‚
æˆ‘ä»¬**å¼ºçƒˆ**å»ºè®®åœ¨ä¸Šä¼ æ¨¡æ¿ä¹‹å‰æ£€æŸ¥ä¸€ä¸‹ï¼Œç¡®ä¿æ¨¡æ¿æ²¡æœ‰åœ¨ä¸åº”è¯¥çš„åœ°æ–¹æ‰“å°é¢å¤–çš„ç©ºæ ¼ï¼

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\community.md
============================================================

<!--âš ï¸è¯·æ³¨æ„ï¼Œæ­¤æ–‡ä»¶è™½ç„¶æ˜¯Markdownæ ¼å¼ï¼Œä½†åŒ…å«äº†æˆ‘ä»¬æ–‡æ¡£æ„å»ºå™¨ï¼ˆç±»ä¼¼äºMDXï¼‰çš„ç‰¹å®šè¯­æ³•ï¼Œå¯èƒ½æ— æ³•åœ¨ä½ çš„MarkdownæŸ¥çœ‹å™¨ä¸­æ­£ç¡®æ˜¾ç¤ºã€‚
-->

# ç¤¾åŒº

è¿™ä¸ªé¡µé¢æ±‡é›†äº†ç¤¾åŒºå¼€å‘çš„ğŸ¤—Transformersç›¸å…³çš„èµ„æºã€‚

## ç¤¾åŒºèµ„æº

| èµ„æº     |      æè¿°      |      ä½œè€…      |
|:----------|:-------------|------:|
| [Hugging Face Transformers Glossary Flashcards](https://www.darigovresearch.com/huggingface-transformers-glossary-flashcards) | è¿™æ˜¯ä¸€å¥—åŸºäº [Transformersæ–‡æ¡£æœ¯è¯­è¡¨](glossary) çš„æŠ½è®¤å¡ï¼Œå®ƒä»¬å·²è¢«æ•´ç†æˆå¯ä»¥é€šè¿‡ [Anki](https://apps.ankiweb.net/) ï¼ˆä¸€æ¬¾ä¸“ä¸ºé•¿æœŸçŸ¥è¯†ä¿ç•™è€Œè®¾è®¡çš„å¼€æºã€è·¨å¹³å°çš„åº”ç”¨ï¼‰æ¥è¿›è¡Œå­¦ä¹ å’Œå¤ä¹ çš„å½¢å¼ã€‚ä½¿ç”¨æ–¹æ³•å‚è§ï¼š [ä»‹ç»å¦‚ä½•ä½¿ç”¨æŠ½è®¤å¡çš„è§†é¢‘](https://www.youtube.com/watch?v=Dji_h7PILrw)ã€‚ | [Darigov Research](https://www.darigovresearch.com/) |

## ç¤¾åŒºç¬”è®°æœ¬

| ç¬”è®°æœ¬     |      æè¿°      |      ä½œè€…      |      |
|:----------|:-------------|:-------------|------:|
| [Fine-tune a pre-trained Transformer to generate lyrics](https://github.com/AlekseyKorshuk/huggingartists) | å¦‚ä½•é€šè¿‡å¾®è°ƒGPT-2æ¨¡å‹æ¥ç”Ÿæˆä½ æœ€å–œæ¬¢çš„è‰ºæœ¯å®¶é£æ ¼çš„æ­Œè¯ |  [Aleksey Korshuk](https://github.com/AlekseyKorshuk) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AlekseyKorshuk/huggingartists/blob/master/huggingartists-demo.ipynb) |
| [Train T5 in Tensorflow 2](https://github.com/snapthat/TF-T5-text-to-text) | å¦‚ä½•ä½¿ç”¨ Tensorflow 2 è®­ç»ƒ T5 å¯ä»¥å®Œæˆä»»ä½•ä»»åŠ¡ã€‚æœ¬ç¬”è®°æœ¬æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨ SQUAD åœ¨ Tensorflow 2 ä¸­å®ç°é—®ç­”ä»»åŠ¡ | [Muhammad Harris](https://github.com/HarrisDePerceptron) |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/snapthat/TF-T5-text-to-text/blob/master/snapthatT5/notebooks/TF-T5-Datasets%20Training.ipynb) |
| [Train T5 on TPU](https://github.com/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb)  | å¦‚ä½•ä½¿ç”¨ Transformers å’Œ Nlp åœ¨ SQUAD ä¸Šè®­ç»ƒ T5 | [Suraj Patil](https://github.com/patil-suraj) |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb#scrollTo=QLGiFCDqvuil) |
| [Fine-tune T5 for Classification and Multiple Choice](https://github.com/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb)  | å¦‚ä½•ä½¿ç”¨ PyTorch Lightning çš„text-to-textæ ¼å¼å¯¹ T5 è¿›è¡Œå¾®è°ƒä»¥å®Œæˆåˆ†ç±»å’Œå¤šé¡¹é€‰æ‹©ä»»åŠ¡ |  [Suraj Patil](https://github.com/patil-suraj) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb) |
| [Fine-tune DialoGPT on New Datasets and Languages](https://github.com/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb)  | å¦‚ä½•åœ¨æ–°æ•°æ®é›†ä¸Šå¾®è°ƒ DialoGPT æ¨¡å‹ï¼Œä»¥å®ç°å¼€æ”¾å¼å¯¹è¯èŠå¤©æœºå™¨äºº |  [Nathan Cooper](https://github.com/ncoop57) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb) |
| [Long Sequence Modeling with Reformer](https://github.com/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb)  | å¦‚ä½•ä½¿ç”¨ Reformer å¯¹é•¿è¾¾ 500,000 ä¸ª token çš„åºåˆ—è¿›è¡Œè®­ç»ƒ |  [Patrick von Platen](https://github.com/patrickvonplaten) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb)  |
| [Fine-tune BART for Summarization](https://github.com/ohmeow/ohmeow_website/blob/master/posts/2021-05-25-mbart-sequence-classification-with-blurr.ipynb) | å¦‚ä½•ä½¿ç”¨ blurr å¯¹ BART è¿›è¡Œå¾®è°ƒï¼Œä»¥ä¾¿ä½¿ç”¨ fastai è¿›è¡Œæ±‡æ€» | [Wayde Gilliam](https://ohmeow.com/) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ohmeow/ohmeow_website/blob/master/posts/2021-05-25-mbart-sequence-classification-with-blurr.ipynb) |
| [Fine-tune a pre-trained Transformer on anyone's tweets](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb) | å¦‚ä½•é€šè¿‡å¾®è°ƒ GPT-2 æ¨¡å‹ç”Ÿæˆä»¥ä½ æœ€å–œæ¬¢çš„ Twitter å¸æˆ·é£æ ¼å‘å¸ƒçš„æ¨æ–‡ |  [Boris Dayma](https://github.com/borisdayma) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb) |
| [Optimize ğŸ¤— Hugging Face models with Weights & Biases](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/huggingface/Optimize_Hugging_Face_models_with_Weights_%26_Biases.ipynb) | å±•ç¤º W&B ä¸ Hugging Face é›†æˆçš„å®Œæ•´æ•™ç¨‹ | [Boris Dayma](https://github.com/borisdayma) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/huggingface/Optimize_Hugging_Face_models_with_Weights_%26_Biases.ipynb) |
| [Pretrain Longformer](https://github.com/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb)  | å¦‚ä½•æ„å»ºç°æœ‰é¢„è®­ç»ƒæ¨¡å‹çš„â€œé•¿â€ç‰ˆæœ¬ |  [Iz Beltagy](https://beltagy.net) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb) |
| [Fine-tune Longformer for QA](https://github.com/patil-suraj/Notebooks/blob/master/longformer_qa_training.ipynb) | å¦‚ä½•é’ˆå¯¹é—®ç­”ä»»åŠ¡å¾®è°ƒé•¿æ¨¡å‹ | [Suraj Patil](https://github.com/patil-suraj) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patil-suraj/Notebooks/blob/master/longformer_qa_training.ipynb) |
| [Evaluate Model with ğŸ¤—nlp](https://github.com/patrickvonplaten/notebooks/blob/master/How_to_evaluate_Longformer_on_TriviaQA_using_NLP.ipynb) | å¦‚ä½•ä½¿ç”¨`nlp`åº“åœ¨TriviaQAæ•°æ®é›†ä¸Šè¯„ä¼°Longformeræ¨¡å‹| [Patrick von Platen](https://github.com/patrickvonplaten) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1m7eTGlPmLRgoPkkA7rkhQdZ9ydpmsdLE?usp=sharing) |
| [Fine-tune T5 for Sentiment Span Extraction](https://github.com/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb)  | å¦‚ä½•ä½¿ç”¨PyTorch Lightningä»¥text-to-textçš„æ ¼å¼å¯¹T5è¿›è¡Œå¾®è°ƒï¼Œä»¥è¿›è¡Œæƒ…æ„Ÿè·¨åº¦æå– |  [Lorenzo Ampil](https://github.com/enzoampil) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb) |
| [Fine-tune DistilBert for Multiclass Classification](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb) | å¦‚ä½•ä½¿ç”¨ PyTorch å¾®è°ƒ DistilBert è¿›è¡Œå¤šç±»åˆ†ç±» | [Abhishek Kumar Mishra](https://github.com/abhimishra91) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb)|
|[Fine-tune BERT for Multi-label Classification](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb)|å¦‚ä½•ä½¿ç”¨ PyTorch å¯¹ BERT è¿›è¡Œå¾®è°ƒä»¥è¿›è¡Œå¤šæ ‡ç­¾åˆ†ç±»|[Abhishek Kumar Mishra](https://github.com/abhimishra91) |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb)|
|[Fine-tune T5 for Summarization](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb)|å¦‚ä½•åœ¨ PyTorch ä¸­å¾®è°ƒ T5 è¿›è¡Œæ€»ç»“å¹¶ä½¿ç”¨ WandB è·Ÿè¸ªå®éªŒ|[Abhishek Kumar Mishra](https://github.com/abhimishra91) |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb)|
|[Speed up Fine-Tuning in Transformers with Dynamic Padding / Bucketing](https://github.com/ELS-RD/transformers-notebook/blob/master/Divide_Hugging_Face_Transformers_training_time_by_2_or_more.ipynb)|å¦‚ä½•é€šè¿‡ä½¿ç”¨åŠ¨æ€å¡«å……/æ¡¶æ’åºå°†å¾®è°ƒé€Ÿåº¦æé«˜ä¸¤å€|[Michael Benesty](https://github.com/pommedeterresautee) |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1CBfRU1zbfu7-ijiOqAAQUA-RJaxfcJoO?usp=sharing)|
|[Pretrain Reformer for Masked Language Modeling](https://github.com/patrickvonplaten/notebooks/blob/master/Reformer_For_Masked_LM.ipynb)| å¦‚ä½•è®­ç»ƒä¸€ä¸ªå¸¦æœ‰åŒå‘è‡ªæ³¨æ„åŠ›å±‚çš„Reformeræ¨¡å‹ | [Patrick von Platen](https://github.com/patrickvonplaten) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1tzzh0i8PgDQGV3SMFUGxM7_gGae3K-uW?usp=sharing)|
|[Expand and Fine Tune Sci-BERT](https://github.com/lordtt13/word-embeddings/blob/master/COVID-19%20Research%20Data/COVID-SciBERT.ipynb)| å¦‚ä½•åœ¨ CORD æ•°æ®é›†ä¸Šå¢åŠ  AllenAI é¢„è®­ç»ƒçš„ SciBERT æ¨¡å‹çš„è¯æ±‡é‡ï¼Œå¹¶å¯¹å…¶è¿›è¡Œæµæ°´çº¿åŒ– | [Tanmay Thakur](https://github.com/lordtt13) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1rqAR40goxbAfez1xvF3hBJphSCsvXmh8)|
|[Fine Tune BlenderBotSmall for Summarization using the Trainer API](https://github.com/lordtt13/transformers-experiments/blob/master/Custom%20Tasks/fine-tune-blenderbot_small-for-summarization.ipynb)| å¦‚ä½•ä½¿ç”¨Trainer APIåœ¨è‡ªå®šä¹‰æ•°æ®é›†ä¸Šå¯¹BlenderBotSmallè¿›è¡Œå¾®è°ƒä»¥è¿›è¡Œæ–‡æœ¬æ‘˜è¦ | [Tanmay Thakur](https://github.com/lordtt13) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/19Wmupuls7mykSGyRN_Qo6lPQhgp56ymq?usp=sharing)|
|[Fine-tune Electra and interpret with Integrated Gradients](https://github.com/elsanns/xai-nlp-notebooks/blob/master/electra_fine_tune_interpret_captum_ig.ipynb) | å¦‚ä½•å¯¹Electraæ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥è¿›è¡Œæƒ…æ„Ÿåˆ†æï¼Œå¹¶ä½¿ç”¨Captumé›†æˆæ¢¯åº¦æ¥è§£é‡Šé¢„æµ‹ç»“æœ | [Eliza Szczechla](https://elsanns.github.io) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/elsanns/xai-nlp-notebooks/blob/master/electra_fine_tune_interpret_captum_ig.ipynb)|
|[fine-tune a non-English GPT-2 Model with Trainer class](https://github.com/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb) | å¦‚ä½•ä½¿ç”¨ Trainer ç±»å¾®è°ƒéè‹±è¯­ GPT-2 æ¨¡å‹ | [Philipp Schmid](https://www.philschmid.de) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb)|
|[Fine-tune a DistilBERT Model for Multi Label Classification task](https://github.com/DhavalTaunk08/Transformers_scripts/blob/master/Transformers_multilabel_distilbert.ipynb) | å¦‚ä½•é’ˆå¯¹å¤šæ ‡ç­¾åˆ†ç±»ä»»åŠ¡å¾®è°ƒ DistilBERT æ¨¡å‹ | [Dhaval Taunk](https://github.com/DhavalTaunk08) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DhavalTaunk08/Transformers_scripts/blob/master/Transformers_multilabel_distilbert.ipynb)|
|[Fine-tune ALBERT for sentence-pair classification](https://github.com/NadirEM/nlp-notebooks/blob/master/Fine_tune_ALBERT_sentence_pair_classification.ipynb) | å¦‚ä½•é’ˆå¯¹å¥å­å¯¹åˆ†ç±»ä»»åŠ¡å¯¹ ALBERT æ¨¡å‹æˆ–å…¶ä»–åŸºäº BERT çš„æ¨¡å‹è¿›è¡Œå¾®è°ƒ | [Nadir El Manouzi](https://github.com/NadirEM) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NadirEM/nlp-notebooks/blob/master/Fine_tune_ALBERT_sentence_pair_classification.ipynb)|
|[Fine-tune Roberta for sentiment analysis](https://github.com/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb) | å¦‚ä½•å¾®è°ƒ Roberta æ¨¡å‹è¿›è¡Œæƒ…ç»ªåˆ†æ | [Dhaval Taunk](https://github.com/DhavalTaunk08) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb)|
|[Evaluating Question Generation Models](https://github.com/flexudy-pipe/qugeev) | ä½ çš„ seq2seq è½¬æ¢å™¨æ¨¡å‹ç”Ÿæˆçš„é—®é¢˜çš„ç­”æ¡ˆæœ‰å¤šå‡†ç¡®ï¼Ÿ | [Pascal Zoleko](https://github.com/zolekode) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1bpsSqCQU-iw_5nNoRm_crPq6FRuJthq_?usp=sharing)|
|[Classify text with DistilBERT and Tensorflow](https://github.com/peterbayerle/huggingface_notebook/blob/main/distilbert_tf.ipynb) | å¦‚ä½•åœ¨ TensorFlow ä¸­å¾®è°ƒ DistilBERT ä»¥è¿›è¡Œæ–‡æœ¬åˆ†ç±» | [Peter Bayerle](https://github.com/peterbayerle) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/peterbayerle/huggingface_notebook/blob/main/distilbert_tf.ipynb)|
|[Leverage BERT for Encoder-Decoder Summarization on CNN/Dailymail](https://github.com/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb) | å¦‚ä½•åœ¨CNN/Dailymailæ‘˜è¦ä»»åŠ¡ä¸Šä½¿ç”¨*google-bert/bert-base-uncased*æ£€æŸ¥ç‚¹å¯¹*EncoderDecoderModel*è¿›è¡Œçƒ­å¯åŠ¨ | [Patrick von Platen](https://github.com/patrickvonplaten) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb)|
|[Leverage RoBERTa for Encoder-Decoder Summarization on BBC XSum](https://github.com/patrickvonplaten/notebooks/blob/master/RoBERTaShared_for_BBC_XSum.ipynb) | å¦‚ä½•åœ¨BBC/XSumæ‘˜è¦ä»»åŠ¡ä¸Šä½¿ç”¨*FacebookAI/roberta-base*æ£€æŸ¥ç‚¹å¯¹å…±äº«çš„*EncoderDecoderModel*è¿›è¡Œçƒ­å¯åŠ¨ | [Patrick von Platen](https://github.com/patrickvonplaten) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/RoBERTaShared_for_BBC_XSum.ipynb)|
|[Fine-tune TAPAS on Sequential Question Answering (SQA)](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb) | å¦‚ä½•åœ¨Sequential Question Answering (SQA)æ•°æ®é›†ä¸Šä½¿ç”¨*tapas-base*æ£€æŸ¥ç‚¹å¯¹*TapasForQuestionAnswering*è¿›è¡Œå¾®è°ƒ | [Niels Rogge](https://github.com/nielsrogge) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb)|
|[Evaluate TAPAS on Table Fact Checking (TabFact)](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Evaluating_TAPAS_on_the_Tabfact_test_set.ipynb) | å¦‚ä½•ç»“åˆä½¿ç”¨ ğŸ¤— æ•°æ®é›†å’Œ ğŸ¤— transformers åº“ï¼Œä½¿ç”¨*tapas-base-finetuned-tabfact*æ£€æŸ¥ç‚¹è¯„ä¼°ç»è¿‡å¾®è°ƒçš„*TapasForSequenceClassification* | [Niels Rogge](https://github.com/nielsrogge) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Evaluating_TAPAS_on_the_Tabfact_test_set.ipynb)|
|[Fine-tuning mBART for translation](https://colab.research.google.com/github/vasudevgupta7/huggingface-tutorials/blob/main/translation_training.ipynb) | å¦‚ä½•ä½¿ç”¨ Seq2SeqTrainer å¯¹ mBART è¿›è¡Œå¾®è°ƒä»¥å®ç°å°åœ°è¯­åˆ°è‹±è¯­çš„ç¿»è¯‘ | [Vasudev Gupta](https://github.com/vasudevgupta7) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vasudevgupta7/huggingface-tutorials/blob/main/translation_training.ipynb)|
|[Fine-tune LayoutLM on FUNSD (a form understanding dataset)](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb) | å¦‚ä½•åœ¨FUNSDæ•°æ®é›†ä¸Šå¯¹*LayoutLMForTokenClassification*è¿›è¡Œå¾®è°ƒä»¥ä»æ‰«ææ–‡æ¡£ä¸­æå–ä¿¡æ¯ | [Niels Rogge](https://github.com/nielsrogge) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb)|
|[Fine-Tune DistilGPT2 and Generate Text](https://colab.research.google.com/github/tripathiaakash/DistilGPT2-Tutorial/blob/main/distilgpt2_fine_tuning.ipynb) | å¦‚ä½•å¾®è°ƒ DistilGPT2 å¹¶ç”Ÿæˆæ–‡æœ¬ | [Aakash Tripathi](https://github.com/tripathiaakash) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tripathiaakash/DistilGPT2-Tutorial/blob/main/distilgpt2_fine_tuning.ipynb)|
|[Fine-Tune LED on up to 8K tokens](https://github.com/patrickvonplaten/notebooks/blob/master/Fine_tune_Longformer_Encoder_Decoder_(LED)_for_Summarization_on_pubmed.ipynb) | å¦‚ä½•å¯¹LEDæ¨¡å‹åœ¨PubMedæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒä»¥è¿›è¡Œé•¿æ–‡æœ¬æ‘˜è¦ | [Patrick von Platen](https://github.com/patrickvonplaten) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_tune_Longformer_Encoder_Decoder_(LED)_for_Summarization_on_pubmed.ipynb)|
|[Evaluate LED on Arxiv](https://github.com/patrickvonplaten/notebooks/blob/master/LED_on_Arxiv.ipynb) | å¦‚ä½•æœ‰æ•ˆè¯„ä¼°LEDæ¨¡å‹çš„é•¿è¿œå‘å±• | [Patrick von Platen](https://github.com/patrickvonplaten) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/LED_on_Arxiv.ipynb)|
|[Fine-tune LayoutLM on RVL-CDIP (a document image classification dataset)](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForSequenceClassification_on_RVL_CDIP.ipynb) | å¦‚ä½•åœ¨ RVL-CDIP æ•°æ®é›†ä¸Šå¾®è°ƒ*LayoutLMForSequenceClassification*ä»¥è¿›è¡Œæ‰«ææ–‡æ¡£åˆ†ç±» | [Niels Rogge](https://github.com/nielsrogge) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForSequenceClassification_on_RVL_CDIP.ipynb)|
|[Wav2Vec2 CTC decoding with GPT2 adjustment](https://github.com/voidful/huggingface_notebook/blob/main/xlsr_gpt.ipynb) | å¦‚ä½•é€šè¿‡è¯­è¨€æ¨¡å‹è°ƒæ•´è§£ç  CTC åºåˆ— | [Eric Lam](https://github.com/voidful) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1e_z5jQHYbO2YKEaUgzb1ww1WwiAyydAj?usp=sharing)|
|[Fine-tune BART for summarization in two languages with Trainer class](https://github.com/elsanns/xai-nlp-notebooks/blob/master/fine_tune_bart_summarization_two_langs.ipynb) | å¦‚ä½•ä½¿ç”¨Trainerç±»å¯¹BARTæ¨¡å‹è¿›è¡Œå¤šè¯­è¨€æ‘˜è¦ä»»åŠ¡çš„å¾®è°ƒ | [Eliza Szczechla](https://github.com/elsanns) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/elsanns/xai-nlp-notebooks/blob/master/fine_tune_bart_summarization_two_langs.ipynb)|
|[Evaluate Big Bird on Trivia QA](https://github.com/patrickvonplaten/notebooks/blob/master/Evaluating_Big_Bird_on_TriviaQA.ipynb) | è¯„ä¼°BigBirdæ¨¡å‹åœ¨é•¿æ–‡æ¡£é—®ç­”ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨Trivia QAæ•°æ®é›†ä¸Š| [Patrick von Platen](https://github.com/patrickvonplaten) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Evaluating_Big_Bird_on_TriviaQA.ipynb)|
| [Create video captions using Wav2Vec2](https://github.com/Muennighoff/ytclipcc/blob/main/wav2vec_youtube_captions.ipynb) | å¦‚ä½•ä½¿ç”¨Wav2Vecå¯¹ä»»ä½•è§†é¢‘çš„éŸ³é¢‘è¿›è¡Œè½¬å½•ä»¥åˆ›å»ºYouTubeå­—å¹• | [Niklas Muennighoff](https://github.com/Muennighoff) |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Muennighoff/ytclipcc/blob/main/wav2vec_youtube_captions.ipynb) |
| [Fine-tune the Vision Transformer on CIFAR-10 using PyTorch Lightning](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_PyTorch_Lightning.ipynb) | å¦‚ä½•ä½¿ç”¨HuggingFaceçš„Transformersã€Datasetså’ŒPyTorch Lightningåœ¨CIFAR-10æ•°æ®é›†ä¸Šå¯¹Vision Transformerï¼ˆViTï¼‰è¿›è¡Œå¾®è°ƒ | [Niels Rogge](https://github.com/nielsrogge) |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_PyTorch_Lightning.ipynb) |
| [Fine-tune the Vision Transformer on CIFAR-10 using the ğŸ¤— Trainer](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb) | å¦‚ä½•ä½¿ç”¨HuggingFaceçš„Transformersã€Datasetså’ŒğŸ¤— Traineråœ¨CIFAR-10æ•°æ®é›†ä¸Šå¯¹Vision Transformerï¼ˆViTï¼‰è¿›è¡Œå¾®è°ƒ| [Niels Rogge](https://github.com/nielsrogge) |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb) |
| [Evaluate LUKE on Open Entity, an entity typing dataset](https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_open_entity.ipynb) | å¦‚ä½•åœ¨å¼€æ”¾å®ä½“æ•°æ®é›†ä¸Šè¯„ä¼°*LukeForEntityClassification*| [Ikuya Yamada](https://github.com/ikuyamada) |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_open_entity.ipynb) |
| [Evaluate LUKE on TACRED, a relation extraction dataset](https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_tacred.ipynb) | å¦‚ä½•åœ¨ TACRED æ•°æ®é›†ä¸Šè¯„ä¼°*LukeForEntityPairClassification* | [Ikuya Yamada](https://github.com/ikuyamada) |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_tacred.ipynb) |
| [Evaluate LUKE on CoNLL-2003, an important NER benchmark](https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_conll_2003.ipynb) | å¦‚ä½•åœ¨ CoNLL-2003 æ•°æ®é›†ä¸Šè¯„ä¼°*LukeForEntitySpanClassification* | [Ikuya Yamada](https://github.com/ikuyamada) |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_conll_2003.ipynb) |
| [Evaluate BigBird-Pegasus on PubMed dataset](https://github.com/vasudevgupta7/bigbird/blob/main/notebooks/bigbird_pegasus_evaluation.ipynb) | å¦‚ä½•åœ¨ PubMed æ•°æ®é›†ä¸Šè¯„ä¼°*BigBirdPegasusForConditionalGeneration*| [Vasudev Gupta](https://github.com/vasudevgupta7) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vasudevgupta7/bigbird/blob/main/notebooks/bigbird_pegasus_evaluation.ipynb) |
| [Speech Emotion Classification with Wav2Vec2](https://github.com/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb) |å¦‚ä½•åˆ©ç”¨é¢„è®­ç»ƒçš„ Wav2Vec2 æ¨¡å‹åœ¨ MEGA æ•°æ®é›†ä¸Šè¿›è¡Œæƒ…ç»ªåˆ†ç±»| [Mehrdad Farahani](https://github.com/m3hrdadfi) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb) |
| [Detect objects in an image with DETR](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DETR/DETR_minimal_example_(with_DetrFeatureExtractor).ipynb) | å¦‚ä½•ä½¿ç”¨ç»è¿‡è®­ç»ƒçš„*DetrForObjectDetection*æ¨¡å‹æ£€æµ‹å›¾åƒä¸­çš„ç‰©ä½“å¹¶å¯è§†åŒ–æ³¨æ„åŠ› | [Niels Rogge](https://github.com/NielsRogge) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/DETR_minimal_example_(with_DetrFeatureExtractor).ipynb) |
| [Fine-tune DETR on a custom object detection dataset](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DETR/Fine_tuning_DetrForObjectDetection_on_custom_dataset_(balloon).ipynb) | å¦‚ä½•åœ¨è‡ªå®šä¹‰å¯¹è±¡æ£€æµ‹æ•°æ®é›†ä¸Šå¾®è°ƒ*DetrForObjectDetection* | [Niels Rogge](https://github.com/NielsRogge) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/Fine_tuning_DetrForObjectDetection_on_custom_dataset_(balloon).ipynb) |
| [Finetune T5 for Named Entity Recognition](https://github.com/ToluClassics/Notebooks/blob/main/T5_Ner_Finetuning.ipynb) | å¦‚ä½•åœ¨å‘½åå®ä½“è¯†åˆ«ä»»åŠ¡ä¸­å¾®è°ƒ*T5*| [Ogundepo Odunayo](https://github.com/ToluClassics) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1obr78FY_cBmWY5ODViCmzdY6O1KB65Vc?usp=sharing) |
| [Fine-Tuning Open-Source LLM using QLoRA with MLflow and PEFT](https://github.com/mlflow/mlflow/blob/master/docs/source/llms/transformers/tutorials/fine-tuning/transformers-peft.ipynb) | å¦‚ä½•ä½¿ç”¨[QLoRA](https://github.com/artidoro/qlora) å’Œ[PEFT](https://huggingface.co/docs/peft/en/index)ä»¥å†…å­˜é«˜æ•ˆçš„æ–¹å¼å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼ŒåŒæ—¶ä½¿ç”¨ [MLflow](https://mlflow.org/docs/latest/llms/transformers/index.html)è¿›è¡Œå®éªŒè·Ÿè¸ª| [Yuki Watanabe](https://github.com/B-Step62) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mlflow/mlflow/blob/master/docs/source/llms/transformers/tutorials/fine-tuning/transformers-peft.ipynb) |

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\contributing.md
============================================================

# ä¸º ğŸ¤— Transformers åšè´¡çŒ®

æ¬¢è¿æ‰€æœ‰äººä¸º ğŸ¤— Transformers åšå‡ºè´¡çŒ®ï¼Œæˆ‘ä»¬é‡è§†æ¯ä¸ªäººçš„è´¡çŒ®ã€‚ä»£ç è´¡çŒ®å¹¶ä¸æ˜¯å¸®åŠ©ç¤¾åŒºçš„å”¯ä¸€é€”å¾„ã€‚å›ç­”é—®é¢˜ã€å¸®åŠ©ä»–äººå’Œæ”¹è¿›æ–‡æ¡£ä¹Ÿéå¸¸æœ‰ä»·å€¼ã€‚

å®£ä¼  ğŸ¤— Transformers ä¹Ÿä¼šå¸®åŠ©æˆ‘ä»¬ï¼æ¯”å¦‚åœ¨åšå®¢æ–‡ç« é‡Œä»‹ç»ä¸€ä¸‹è¿™ä¸ªåº“æ˜¯å¦‚ä½•å¸®åŠ©ä½ å®Œæˆäº†å¾ˆæ£’çš„é¡¹ç›®ï¼Œæ¯æ¬¡å®ƒå¸®åŠ©ä½ æ—¶éƒ½åœ¨ Twitter ä¸Šå¤§å£°å®£ä¼ ï¼Œæˆ–è€…ç»™è¿™ä¸ªä»£ç ä»“åº“ç‚¹â­ï¸æ¥è¡¨ç¤ºæ„Ÿè°¢ã€‚

æ— è®ºä½ é€‰æ‹©ä»¥å“ªç§æ–¹å¼åšå‡ºè´¡çŒ®ï¼Œè¯·æ³¨æ„å¹¶å°Šé‡æˆ‘ä»¬çš„[è¡Œä¸ºå‡†åˆ™](https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md)ã€‚

**æœ¬æŒ‡å—çš„çµæ„Ÿæ¥æºäº [scikit-learnè´¡çŒ®æŒ‡å—](https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md) ï¼Œå®ƒä»¤äººå°è±¡æ·±åˆ».**

## åšè´¡çŒ®çš„æ–¹æ³•

æœ‰å¤šç§æ–¹æ³•å¯ä»¥ä¸º ğŸ¤— Transformers åšè´¡çŒ®ï¼š

* ä¿®å¤ç°æœ‰ä»£ç ä¸­å°šæœªè§£å†³çš„é—®é¢˜ã€‚
* æäº¤ä¸ bug æˆ–æ‰€éœ€æ–°åŠŸèƒ½ç›¸å…³çš„ issueã€‚
* å®ç°æ–°çš„æ¨¡å‹ã€‚
* ä¸ºç¤ºä¾‹æˆ–æ–‡æ¡£åšè´¡çŒ®ã€‚

å¦‚æœä½ ä¸çŸ¥é“ä»å“ªé‡Œå¼€å§‹ï¼Œæœ‰ä¸€ä¸ªç‰¹åˆ«çš„ [Good First Issue](https://github.com/huggingface/transformers/contribute) åˆ—è¡¨ã€‚å®ƒä¼šåˆ—å‡ºä¸€äº›é€‚åˆåˆå­¦è€…çš„å¼€æ”¾çš„ issuesï¼Œå¹¶å¸®åŠ©ä½ å¼€å§‹ä¸ºå¼€æºé¡¹ç›®åšè´¡çŒ®ã€‚åªéœ€è¦åœ¨ä½ æƒ³è¦å¤„ç†çš„ issue ä¸‹å‘è¡¨è¯„è®ºå°±è¡Œã€‚

å¦‚æœæƒ³è¦ç¨å¾®æ›´æœ‰æŒ‘æˆ˜æ€§çš„å†…å®¹ï¼Œä½ ä¹Ÿå¯ä»¥æŸ¥çœ‹ [Good Second Issue](https://github.com/huggingface/transformers/labels/Good%20Second%20Issue) åˆ—è¡¨ã€‚æ€»çš„æ¥è¯´ï¼Œå¦‚æœä½ è§‰å¾—è‡ªå·±çŸ¥é“è¯¥æ€ä¹ˆåšï¼Œå°±å»åšå§ï¼Œæˆ‘ä»¬ä¼šå¸®åŠ©ä½ è¾¾åˆ°ç›®æ ‡çš„ï¼ğŸš€

> æ‰€æœ‰çš„è´¡çŒ®å¯¹ç¤¾åŒºæ¥è¯´éƒ½åŒæ ·å®è´µã€‚ğŸ¥°

## ä¿®å¤å°šæœªè§£å†³çš„é—®é¢˜

å¦‚æœä½ å‘ç°ç°æœ‰ä»£ç ä¸­å­˜åœ¨é—®é¢˜ï¼Œå¹¶ä¸”å·²ç»æƒ³åˆ°äº†è§£å†³æ–¹æ³•ï¼Œè¯·éšæ—¶[å¼€å§‹è´¡çŒ®](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md/#create-a-pull-request) å¹¶åˆ›å»ºä¸€ä¸ª Pull Requestï¼

## æäº¤ä¸ bug ç›¸å…³çš„ issue æˆ–åŠŸèƒ½è¯·æ±‚

åœ¨æäº¤ä¸é”™è¯¯ç›¸å…³çš„ issue æˆ–åŠŸèƒ½è¯·æ±‚æ—¶ï¼Œè¯·å°½é‡éµå¾ªä¸‹é¢çš„æŒ‡å—ã€‚è¿™èƒ½è®©æˆ‘ä»¬æ›´å®¹æ˜“è¿…é€Ÿå›å¤ä½ ï¼Œå¹¶æä¾›è‰¯å¥½çš„åé¦ˆæ„è§ã€‚

### ä½ å‘ç°äº† bug å—ï¼Ÿ

ğŸ¤— Transformers ä¹‹æ‰€ä»¥å¼ºå¤§å¯é ï¼Œè¦æ„Ÿè°¢ç”¨æˆ·æŠ¥å‘Šäº†ä»–ä»¬é‡åˆ°çš„é—®é¢˜ã€‚

åœ¨æå‡ºissueä¹‹å‰ï¼Œè¯·ä½ **ç¡®è®¤è¯¥ bug å°šæœªè¢«æŠ¥å‘Š**ï¼ˆä½¿ç”¨ GitHub çš„ Issues ä¸‹é¢çš„æœç´¢æ ï¼‰ã€‚issue ä¹Ÿåº”è¯¥æ˜¯ä¸åº“æœ¬èº«çš„ bug æœ‰å…³ï¼Œè€Œä¸æ˜¯ä¸ä½ çš„ä»£ç æœ‰å…³ã€‚å¦‚æœä¸ç¡®å®š bug æ˜¯åœ¨ä½ çš„ä»£ç ä¸­è¿˜æ˜¯åœ¨åº“ä¸­ï¼Œè¯·å…ˆåœ¨[è®ºå›](https://discuss.huggingface.co/)ä¸­è¯¢é—®ã€‚è¿™æœ‰åŠ©äºæˆ‘ä»¬æ›´å¿«åœ°è§£å†³ä¸åº“ç›¸å…³çš„é—®é¢˜ã€‚

ä¸€æ—¦ä½ ç¡®è®¤è¯¥ bug å°šæœªè¢«æŠ¥å‘Šï¼Œè¯·åœ¨ä½ çš„ issue ä¸­åŒ…å«ä»¥ä¸‹ä¿¡æ¯ï¼Œä»¥ä¾¿æˆ‘ä»¬å¿«é€Ÿè§£å†³ï¼š

* ä½¿ç”¨çš„**æ“ä½œç³»ç»Ÿç±»å‹å’Œç‰ˆæœ¬**ï¼Œä»¥åŠ **Python** å’Œ **PyTorch** çš„ç‰ˆæœ¬ã€‚
* ä¸€ä¸ªç®€çŸ­ã€ç‹¬ç«‹çš„ä»£ç ç‰‡æ®µï¼Œå¯ä»¥è®©æˆ‘ä»¬åœ¨ä¸åˆ°30ç§’å†…é‡ç°è¿™ä¸ªé—®é¢˜ã€‚
* å¦‚æœå‘ç”Ÿå¼‚å¸¸ï¼Œè¯·æä¾›*å®Œæ•´çš„* tracebackã€‚
* é™„ä¸Šä½ è®¤ä¸ºå¯èƒ½æœ‰å¸®åŠ©çš„ä»»ä½•å…¶ä»–é™„åŠ ä¿¡æ¯ï¼Œå¦‚å±å¹•æˆªå›¾ã€‚

æƒ³è¦è‡ªåŠ¨è·å–æ“ä½œç³»ç»Ÿå’Œè½¯ä»¶ç‰ˆæœ¬ï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```bash
transformers env
```

ä½ ä¹Ÿå¯ä»¥ä»ä»£ç ä»“åº“çš„æ ¹ç›®å½•ä¸‹è¿è¡Œç›¸åŒçš„å‘½ä»¤ï¼š

```bash
python src/transformers/commands/transformers_cli.py env
```

### ä½ æƒ³è¦æ–°åŠŸèƒ½å—ï¼Ÿ

å¦‚æœä½ å¸Œæœ›åœ¨ ğŸ¤— Transformers ä¸­çœ‹åˆ°æ–°åŠŸèƒ½ï¼Œè¯·æå‡ºä¸€ä¸ª issue å¹¶åŒ…å«ä»¥ä¸‹å†…å®¹ï¼š

1. è¿™ä¸ªæ–°åŠŸèƒ½çš„*åŠ¨æœº*æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿæ˜¯å› ä¸ºä½¿ç”¨è¿™ä¸ªåº“æ—¶é‡åˆ°äº†é—®é¢˜æˆ–è€…æ„Ÿåˆ°äº†æŸç§ä¸æ»¡å—ï¼Ÿæ˜¯å› ä¸ºä½ çš„é¡¹ç›®éœ€è¦è¿™ä¸ªåŠŸèƒ½å—ï¼Ÿæˆ–è€…æ˜¯ä½ è‡ªå·±å¼€å‘äº†æŸé¡¹å†…å®¹ï¼Œå¹¶ä¸”è®¤ä¸ºå®ƒå¯èƒ½ä¼šå¯¹ç¤¾åŒºæœ‰æ‰€å¸®åŠ©ï¼Ÿ

   ä¸ç®¡æ˜¯ä»€ä¹ˆï¼Œæˆ‘ä»¬éƒ½å¾ˆæƒ³å¬ï¼

2. è¯·å°½å¯èƒ½è¯¦ç»†åœ°æè¿°ä½ æƒ³è¦çš„åŠŸèƒ½ã€‚ä½ å‘Šè¯‰æˆ‘ä»¬çš„è¶Šå¤šï¼Œæˆ‘ä»¬å°±èƒ½æ›´å¥½åœ°å¸®åŠ©ä½ ã€‚
3. è¯·æä¾›ä¸€ä¸ª*ä»£ç ç‰‡æ®µ*ï¼Œæ¼”ç¤ºè¯¥åŠŸèƒ½çš„ä½¿ç”¨æ–¹æ³•ã€‚
4. å¦‚æœè¿™ä¸ªåŠŸèƒ½ä¸æŸç¯‡è®ºæ–‡ç›¸å…³ï¼Œè¯·åŒ…å«é“¾æ¥ã€‚

å¦‚æœä½ æè¿°å¾—è¶³å¤Ÿæ¸…æ™°ï¼Œé‚£ä¹ˆåœ¨ä½ åˆ›å»º issue æ—¶ï¼Œæˆ‘ä»¬å·²ç»å®Œæˆäº†80%çš„å·¥ä½œã€‚

æˆ‘ä»¬å·²ç»æ·»åŠ äº†[æ¨¡æ¿](https://github.com/huggingface/transformers/tree/main/templates)ï¼Œå¯èƒ½æœ‰åŠ©äºä½ æå‡º issueã€‚

## ä½ æƒ³è¦å®ç°ä¸€ä¸ªæ–°æ¨¡å‹å—ï¼Ÿ

æˆ‘ä»¬ä¼šæŒç»­å‘å¸ƒæ–°æ¨¡å‹ï¼Œå¦‚æœä½ æƒ³è¦å®ç°ä¸€ä¸ªæ–°æ¨¡å‹ï¼Œè¯·æä¾›ä»¥ä¸‹ä¿¡æ¯:

* æ¨¡å‹çš„ç®€è¦æè¿°å’Œè®ºæ–‡é“¾æ¥ã€‚
* å¦‚æœå®ç°æ˜¯å¼€æºçš„ï¼Œè¯·æä¾›å®ç°çš„é“¾æ¥ã€‚
* å¦‚æœæ¨¡å‹æƒé‡å¯ç”¨ï¼Œè¯·æä¾›æ¨¡å‹æƒé‡çš„é“¾æ¥ã€‚

å¦‚æœä½ æƒ³äº²è‡ªè´¡çŒ®æ¨¡å‹ï¼Œè¯·å‘Šè¯‰æˆ‘ä»¬ã€‚è®©æˆ‘ä»¬å¸®ä½ æŠŠå®ƒæ·»åŠ åˆ° ğŸ¤— Transformersï¼

æˆ‘ä»¬è¿˜æœ‰ä¸€ä¸ªæ›´æŠ€æœ¯æ€§çš„æŒ‡å—ï¼Œå‘Šè¯‰ä½ [å¦‚ä½•å°†æ¨¡å‹æ·»åŠ åˆ° ğŸ¤— Transformers](https://huggingface.co/docs/transformers/add_new_model)ã€‚

## ä½ æƒ³è¦æ·»åŠ æ–‡æ¡£å—ï¼Ÿ

æˆ‘ä»¬å§‹ç»ˆåœ¨å¯»æ±‚æ”¹è¿›æ–‡æ¡£ï¼Œä½¿å…¶æ›´æ¸…æ™°å‡†ç¡®ã€‚è¯·å‘Šè¯‰æˆ‘ä»¬å¦‚ä½•æ”¹è¿›æ–‡æ¡£ï¼Œæ¯”å¦‚æ‹¼å†™é”™è¯¯ä»¥åŠä»»ä½•ç¼ºå¤±ã€ä¸æ¸…æ¥šæˆ–ä¸å‡†ç¡®çš„å†…å®¹ã€‚æˆ‘ä»¬éå¸¸ä¹æ„è¿›è¡Œä¿®æ”¹ï¼Œå¦‚æœä½ æœ‰å…´è¶£ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å¸®åŠ©ä½ åšå‡ºè´¡çŒ®ï¼

æœ‰å…³å¦‚ä½•ç”Ÿæˆã€æ„å»ºå’Œç¼–å†™æ–‡æ¡£çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹æ–‡æ¡£ [README](https://github.com/huggingface/transformers/tree/main/docs)ã€‚

## åˆ›å»º Pull Request

åœ¨å¼€å§‹ç¼–å†™ä»»ä½•ä»£ç ä¹‹å‰ï¼Œæˆ‘ä»¬å¼ºçƒˆå»ºè®®ä½ å…ˆæœç´¢ç°æœ‰çš„ PR(Pull Request) æˆ– issueï¼Œä»¥ç¡®ä¿æ²¡æœ‰å…¶ä»–äººå·²ç»åœ¨åšåŒæ ·çš„äº‹æƒ…ã€‚å¦‚æœä½ ä¸ç¡®å®šï¼Œæå‡º issue æ¥è·å–åé¦ˆæ„è§æ˜¯ä¸€ä¸ªå¥½åŠæ³•ã€‚

è¦ä¸º ğŸ¤— Transformers åšè´¡çŒ®ï¼Œä½ éœ€è¦åŸºæœ¬çš„ `git` ä½¿ç”¨æŠ€èƒ½ã€‚è™½ç„¶ `git` ä¸æ˜¯ä¸€ä¸ªå¾ˆå®¹æ˜“ä½¿ç”¨çš„å·¥å…·ï¼Œä½†å®ƒæä¾›äº†éå¸¸å…¨é¢çš„æ‰‹å†Œï¼Œåœ¨å‘½ä»¤è¡Œä¸­è¾“å…¥ `git --help` å¹¶äº«å—å§ï¼å¦‚æœä½ æ›´å–œæ¬¢ä¹¦ç±ï¼Œ[Pro Git](https://git-scm.com/book/en/v2)æ˜¯ä¸€æœ¬å¾ˆå¥½çš„å‚è€ƒä¹¦ã€‚

è¦ä¸º ğŸ¤— Transformers åšè´¡çŒ®ï¼Œä½ éœ€è¦ **[Python 3.9](https://github.com/huggingface/transformers/blob/main/setup.py#L426)** æˆ–æ›´é«˜ç‰ˆæœ¬ã€‚è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å¼€å§‹è´¡çŒ®ï¼š

1. ç‚¹å‡»[ä»“åº“](https://github.com/huggingface/transformers)é¡µé¢ä¸Šçš„ **[Fork](https://github.com/huggingface/transformers/fork)** æŒ‰é’®ï¼Œè¿™ä¼šåœ¨ä½ çš„ GitHub è´¦å·ä¸‹æ‹·è´ä¸€ä»½ä»£ç ã€‚

2. æŠŠæ´¾ç”Ÿä»“åº“å…‹éš†åˆ°æœ¬åœ°ç£ç›˜ï¼Œå¹¶å°†åŸºç¡€ä»“åº“æ·»åŠ ä¸ºè¿œç¨‹ä»“åº“ï¼š

   ```bash
   git clone git@github.com:<your Github handle>/transformers.git
   cd transformers
   git remote add upstream https://github.com/huggingface/transformers.git
   ```

3. åˆ›å»ºä¸€ä¸ªæ–°çš„åˆ†æ”¯æ¥ä¿å­˜ä½ çš„æ›´æ”¹ï¼š

   ```bash
   git checkout -b a-descriptive-name-for-my-changes
   ```

   ğŸš¨ **ä¸è¦**åœ¨ `main` åˆ†æ”¯å·¥ä½œ!

4. åœ¨è™šæ‹Ÿç¯å¢ƒä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥è®¾ç½®å¼€å‘ç¯å¢ƒï¼š

   ```bash
   pip install -e ".[dev]"
   ```

   å¦‚æœåœ¨è™šæ‹Ÿç¯å¢ƒä¸­å·²ç»å®‰è£…äº† ğŸ¤— Transformersï¼Œè¯·å…ˆä½¿ç”¨ `pip uninstall transformers` å¸è½½å®ƒï¼Œç„¶åå†ç”¨ `-e` å‚æ•°ä»¥å¯ç¼–è¾‘æ¨¡å¼é‡æ–°å®‰è£…ã€‚

   æ ¹æ®ä½ çš„æ“ä½œç³»ç»Ÿï¼Œä»¥åŠ Transformers çš„å¯é€‰ä¾èµ–é¡¹æ•°é‡çš„å¢åŠ ï¼Œå¯èƒ½ä¼šåœ¨æ‰§è¡Œæ­¤å‘½ä»¤æ—¶å‡ºç°å¤±è´¥ã€‚å¦‚æœå‡ºç°è¿™ç§æƒ…å†µï¼Œè¯·ç¡®ä¿å·²ç»å®‰è£…äº†ä½ æƒ³ä½¿ç”¨çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ˆPyTorchï¼‰ï¼Œç„¶åæ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š

   ```bash
   pip install -e ".[quality]"
   ```

   å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œè¿™äº›åº”è¯¥å¤Ÿç”¨äº†ã€‚

5. åœ¨ä½ çš„åˆ†æ”¯ä¸Šå¼€å‘ç›¸å…³åŠŸèƒ½ã€‚

   åœ¨ç¼–å†™ä»£ç æ—¶ï¼Œè¯·ç¡®ä¿æµ‹è¯•å¥—ä»¶é€šè¿‡ã€‚ç”¨ä¸‹é¢çš„æ–¹å¼è¿è¡Œå—ä½ çš„æ›´æ”¹å½±å“çš„æµ‹è¯•ï¼š

   ```bash
   pytest tests/<TEST_TO_RUN>.py
   ```

   æƒ³äº†è§£æ›´å¤šå…³äºæµ‹è¯•çš„ä¿¡æ¯ï¼Œè¯·é˜…è¯»[æµ‹è¯•](https://huggingface.co/docs/transformers/testing)æŒ‡å—ã€‚

   ğŸ¤— Transformers ä½¿ç”¨ `black` å’Œ `ruff` æ¥ä¿æŒä»£ç é£æ ¼çš„ä¸€è‡´æ€§ã€‚è¿›è¡Œæ›´æ”¹åï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è‡ªåŠ¨æ‰§è¡Œæ ¼å¼æ›´æ­£å’Œä»£ç éªŒè¯ï¼š


   ```bash
   make style
   ```

   ğŸ¤— Transformers è¿˜ä½¿ç”¨äº† `ruff` å’Œä¸€äº›è‡ªå®šä¹‰è„šæœ¬æ¥æ£€æŸ¥ç¼–ç é”™è¯¯ã€‚è™½ç„¶è´¨é‡ç®¡ç†æ˜¯é€šè¿‡ CI è¿›è¡Œçš„ï¼Œä½†ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ¥è¿è¡Œç›¸åŒçš„æ£€æŸ¥ï¼š

   ```bash
   make check-repo
   ```

   æƒ³è¦äº†è§£æœ‰å…³è¿™äº›æ£€æŸ¥åŠå¦‚ä½•è§£å†³ç›¸å…³é—®é¢˜çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·é˜…è¯» [æ£€æŸ¥ Pull Request](https://huggingface.co/docs/transformers/pr_checks) æŒ‡å—ã€‚

   å¦‚æœä½ ä¿®æ”¹äº† `docs/source` ç›®å½•ä¸‹çš„æ–‡æ¡£ï¼Œè¯·ç¡®ä¿æ–‡æ¡£ä»ç„¶èƒ½å¤Ÿè¢«æ„å»ºã€‚è¿™ä¸ªæ£€æŸ¥ä¹Ÿä¼šåœ¨ä½ åˆ›å»º PR æ—¶åœ¨ CI ä¸­è¿è¡Œã€‚å¦‚æœè¦è¿›è¡Œæœ¬åœ°æ£€æŸ¥ï¼Œè¯·ç¡®ä¿å®‰è£…äº†æ–‡æ¡£æ„å»ºå·¥å…·ï¼š

   ```bash
   pip install ".[docs]"
   ```

   åœ¨ä»“åº“çš„æ ¹ç›®å½•ä¸‹è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

   ```bash
   doc-builder build transformers docs/source/en --build_dir ~/tmp/test-build
   ```

   è¿™å°†ä¼šåœ¨ `~/tmp/test-build` æ–‡ä»¶å¤¹ä¸­æ„å»ºæ–‡æ¡£ï¼Œä½ å¯ä»¥ä½¿ç”¨è‡ªå·±å–œæ¬¢çš„ç¼–è¾‘å™¨æŸ¥çœ‹ç”Ÿæˆçš„ Markdown æ–‡ä»¶ã€‚å½“ä½ åˆ›å»º PR æ—¶ï¼Œä¹Ÿå¯ä»¥åœ¨GitHubä¸Šé¢„è§ˆæ–‡æ¡£ã€‚

   å½“ä½ å¯¹ä¿®æ”¹æ»¡æ„åï¼Œä½¿ç”¨ `git add` æŠŠä¿®æ”¹çš„æ–‡ä»¶æ·»åŠ åˆ°æš‚å­˜åŒºï¼Œç„¶åä½¿ç”¨ `git commit` åœ¨æœ¬åœ°è®°å½•ä½ çš„æ›´æ”¹:

   ```bash
   git add modified_file.py
   git commit
   ```

   è¯·è®°å¾—å†™ä¸€ä¸ª[å¥½çš„æäº¤ä¿¡æ¯](https://chris.beams.io/posts/git-commit/)æ¥æ¸…æ™°åœ°ä¼ è¾¾ä½ æ‰€åšçš„æ›´æ”¹ï¼

   ä¸ºäº†ä¿æŒä½ çš„ä»£ç å‰¯æœ¬ä¸åŸå§‹ä»“åº“çš„æœ€æ–°çŠ¶æ€ä¸€è‡´ï¼Œåœ¨ä½ åˆ›å»º PR *ä¹‹å‰*æˆ–è€…åœ¨ç®¡ç†å‘˜è¦æ±‚çš„æƒ…å†µä¸‹ï¼ŒæŠŠä½ çš„åˆ†æ”¯åœ¨ `upstream/branch` ä¸Šè¿›è¡Œ rebaseï¼š

   ```bash
   git fetch upstream
   git rebase upstream/main
   ```

   æŠŠä½ çš„æ›´æ”¹æ¨é€åˆ°ä½ çš„åˆ†æ”¯ï¼š

   ```bash
   git push -u origin a-descriptive-name-for-my-changes
   ```

   å¦‚æœä½ å·²ç»åˆ›å»ºäº†ä¸€ä¸ª PRï¼Œä½ éœ€è¦ä½¿ç”¨ `--force` å‚æ•°è¿›è¡Œå¼ºåˆ¶æ¨é€ã€‚å¦‚æœ PR è¿˜æ²¡æœ‰è¢«åˆ›å»ºï¼Œä½ å¯ä»¥æ­£å¸¸æ¨é€ä½ çš„æ›´æ”¹ã€‚

6. ç°åœ¨ä½ å¯ä»¥è½¬åˆ° GitHub ä¸Šä½ çš„è´¦å·ä¸‹çš„æ´¾ç”Ÿä»“åº“ï¼Œç‚¹å‡» **Pull Request** æ¥åˆ›å»ºä¸€ä¸ª PRã€‚ è¯·ç¡®ä¿å‹¾é€‰æˆ‘ä»¬ [checklist](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md/#pull-request-checklist) ä¸‹çš„æ‰€æœ‰é¡¹ç›®ã€‚å‡†å¤‡å¥½è¿™äº›åï¼Œå¯ä»¥å°†ä½ çš„æ›´æ”¹å‘é€ç»™é¡¹ç›®ç®¡ç†å‘˜è¿›è¡Œå®¡æŸ¥ã€‚

7. å¦‚æœç®¡ç†å‘˜è¦æ±‚ä½ è¿›è¡Œæ›´æ”¹ï¼Œåˆ«æ°”é¦ï¼Œæˆ‘ä»¬çš„æ ¸å¿ƒè´¡çŒ®è€…ä¹Ÿä¼šç»å†ç›¸åŒçš„äº‹æƒ…ï¼è¯·åœ¨ä½ çš„æœ¬åœ°åˆ†æ”¯ä¸Šè¿›è¡Œå·¥ä½œï¼Œå¹¶å°†æ›´æ”¹æ¨é€åˆ°æ´¾ç”Ÿä»“åº“ï¼Œä»¥ä¾¿äºæ¯ä¸ªäººéƒ½å¯ä»¥åœ¨ PR ä¸­çœ‹åˆ°ä½ çš„æ›´æ”¹ã€‚è¿™æ ·å®ƒä»¬ä¼šè‡ªåŠ¨å‡ºç°åœ¨ PR ä¸­ã€‚

### Pull request çš„æ£€æŸ¥æ¸…å•

â˜ Pull request çš„æ ‡é¢˜åº”è¯¥æ€»ç»“ä½ çš„è´¡çŒ®å†…å®¹ã€‚<br>
â˜ å¦‚æœä½ çš„ Pull request è§£å†³äº†ä¸€ä¸ªissueï¼Œè¯·åœ¨ Pull request æè¿°ä¸­æåŠè¯¥ issue çš„ç¼–å·ï¼Œä»¥ç¡®ä¿å®ƒä»¬è¢«å…³è”èµ·æ¥ï¼ˆè¿™æ ·æŸ¥çœ‹ issue çš„äººå°±çŸ¥é“ä½ æ­£åœ¨å¤„ç†å®ƒï¼‰ã€‚<br>
â˜ å¦‚æœæ˜¯æ­£åœ¨è¿›è¡Œä¸­çš„å·¥ä½œï¼Œè¯·åœ¨æ ‡é¢˜å‰åŠ ä¸Š [WIP]ã€‚è¿™æœ‰åŠ©äºé¿å…é‡å¤å·¥ä½œå’ŒåŒºåˆ†å“ªäº› PR å¯ä»¥åˆå¹¶ã€‚<br>
â˜ ç¡®ä¿å¯ä»¥é€šè¿‡ç°æœ‰çš„æµ‹è¯•ã€‚<br>
â˜ å¦‚æœæ·»åŠ äº†æ–°åŠŸèƒ½ï¼Œè¯·åŒæ—¶æ·»åŠ å¯¹åº”çš„æµ‹è¯•ã€‚<br>
   - å¦‚æœæ·»åŠ ä¸€ä¸ªæ–°æ¨¡å‹ï¼Œè¯·ä½¿ç”¨ `ModelTester.all_model_classes = (MyModel, MyModelWithLMHead,...)` æ¥è§¦å‘é€šç”¨æµ‹è¯•ã€‚
   - å¦‚æœä½ æ­£åœ¨æ·»åŠ æ–°çš„ `@slow` æµ‹è¯•ï¼Œè¯·ç¡®ä¿é€šè¿‡ä»¥ä¸‹æ£€æŸ¥ï¼š`RUN_SLOW=1 python -m pytest tests/models/my_new_model/test_my_new_model.py`
   - å¦‚æœä½ æ­£åœ¨æ·»åŠ ä¸€ä¸ªæ–°çš„åˆ†è¯å™¨ï¼Œè¯·ç¼–å†™æµ‹è¯•å¹¶ç¡®ä¿é€šè¿‡ä»¥ä¸‹æ£€æŸ¥ï¼š`RUN_SLOW=1 python -m pytest tests/models/{your_model_name}/test_tokenization_{your_model_name}.py`
   - CircleCI ä¸ä¼šè¿è¡Œæ—¶é—´è¾ƒé•¿çš„æµ‹è¯•ï¼Œä½† GitHub Actions æ¯æ™šä¼šè¿è¡Œæ‰€æœ‰æµ‹è¯•ï¼<br>

â˜ æ‰€æœ‰å…¬å…± method å¿…é¡»å…·æœ‰ä¿¡æ¯æ–‡æ¡£ï¼ˆæ¯”å¦‚ [`modeling_bert.py`](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_bert.py)ï¼‰ã€‚<br>
â˜ ç”±äºä»£ç ä»“åº“çš„ä½“ç§¯æ­£åœ¨è¿…é€Ÿå¢é•¿ï¼Œè¯·é¿å…æ·»åŠ å›¾åƒã€è§†é¢‘å’Œå…¶ä»–éæ–‡æœ¬æ–‡ä»¶ï¼Œå®ƒä»¬ä¼šå¢åŠ ä»“åº“çš„è´Ÿæ‹…ã€‚è¯·ä½¿ç”¨ [`hf-internal-testing`](https://huggingface.co/hf-internal-testing) ç­‰ Hub ä»“åº“æ¥æ‰˜ç®¡è¿™äº›æ–‡ä»¶ï¼Œå¹¶é€šè¿‡ URL å¼•ç”¨å®ƒä»¬ã€‚æˆ‘ä»¬å»ºè®®å°†ä¸æ–‡æ¡£ç›¸å…³çš„å›¾ç‰‡æ”¾ç½®åœ¨ä»¥ä¸‹ä»“åº“ä¸­ï¼š[huggingface/documentation-images](https://huggingface.co/datasets/huggingface/documentation-images)ã€‚ä½ å¯ä»¥åœ¨è¿™ä¸ªæ•°æ®é›†ä»“åº“ä¸Šåˆ›å»ºä¸€ä¸ª PRï¼Œå¹¶è¯·æ±‚ Hugging Face æˆå‘˜è¿›è¡Œåˆå¹¶ã€‚

è¦äº†è§£æ›´å¤šæœ‰å…³åœ¨ Pull request ä¸Šè¿è¡Œçš„æ£€æŸ¥çš„ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹æˆ‘ä»¬çš„ [æ£€æŸ¥ Pull Request](https://huggingface.co/docs/transformers/pr_checks) æŒ‡å—ã€‚

### æµ‹è¯•

åŒ…å«äº†å¹¿æ³›çš„æµ‹è¯•å¥—ä»¶æ¥æµ‹è¯•åº“çš„è¡Œä¸ºå’Œä¸€äº›ç¤ºä¾‹ã€‚åº“æµ‹è¯•å¯ä»¥åœ¨ [tests](https://github.com/huggingface/transformers/tree/main/tests) æ–‡ä»¶å¤¹ä¸­æ‰¾åˆ°ï¼Œç¤ºä¾‹æµ‹è¯•å¯ä»¥åœ¨ [examples](https://github.com/huggingface/transformers/tree/main/examples) æ–‡ä»¶å¤¹ä¸­æ‰¾åˆ°ã€‚

æˆ‘ä»¬å–œæ¬¢ä½¿ç”¨ `pytest` å’Œ `pytest-xdist`ï¼Œå› ä¸ºå®ƒè¿è¡Œæ›´å¿«ã€‚åœ¨ä»“åº“çš„æ ¹ç›®å½•ï¼ŒæŒ‡å®šä¸€ä¸ª*å­æ–‡ä»¶å¤¹çš„è·¯å¾„æˆ–æµ‹è¯•æ–‡ä»¶*æ¥è¿è¡Œæµ‹è¯•ï¼š

```bash
python -m pytest -n auto --dist=loadfile -s -v ./tests/models/my_new_model
```

åŒæ ·åœ°ï¼Œåœ¨ `examples` ç›®å½•ï¼ŒæŒ‡å®šä¸€ä¸ª*å­æ–‡ä»¶å¤¹çš„è·¯å¾„æˆ–æµ‹è¯•æ–‡ä»¶* æ¥è¿è¡Œæµ‹è¯•ã€‚ä¾‹å¦‚ï¼Œä»¥ä¸‹å‘½ä»¤ä¼šæµ‹è¯• PyTorch `examples` ç›®å½•ä¸­çš„æ–‡æœ¬åˆ†ç±»å­æ–‡ä»¶å¤¹ï¼š

```bash
pip install -r examples/xxx/requirements.txt  # ä»…åœ¨ç¬¬ä¸€æ¬¡éœ€è¦
python -m pytest -n auto --dist=loadfile -s -v ./examples/pytorch/text-classification
```

å®é™…ä¸Šè¿™å°±æ˜¯æˆ‘ä»¬çš„ `make test` å’Œ `make test-examples` å‘½ä»¤çš„å®ç°æ–¹å¼ï¼ˆä¸åŒ…æ‹¬ `pip install`ï¼‰ï¼

ä½ ä¹Ÿå¯ä»¥æŒ‡å®šä¸€ä¸ªè¾ƒå°çš„æµ‹è¯•é›†æ¥ä»…æµ‹è¯•ç‰¹å®šåŠŸèƒ½ã€‚

é»˜è®¤æƒ…å†µä¸‹ï¼Œä¼šè·³è¿‡æ—¶é—´è¾ƒé•¿çš„æµ‹è¯•ï¼Œä½†ä½ å¯ä»¥å°† `RUN_SLOW` ç¯å¢ƒå˜é‡è®¾ç½®ä¸º `yes` æ¥è¿è¡Œå®ƒä»¬ã€‚è¿™å°†ä¸‹è½½ä»¥ GB ä¸ºå•ä½çš„æ¨¡å‹æ–‡ä»¶ï¼Œæ‰€ä»¥ç¡®ä¿ä½ æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´ã€è‰¯å¥½çš„ç½‘ç»œè¿æ¥å’Œè¶³å¤Ÿçš„è€å¿ƒï¼

<Tip warning={true}>

è®°å¾—æŒ‡å®šä¸€ä¸ª*å­æ–‡ä»¶å¤¹çš„è·¯å¾„æˆ–æµ‹è¯•æ–‡ä»¶*æ¥è¿è¡Œæµ‹è¯•ã€‚å¦åˆ™ä½ å°†ä¼šè¿è¡Œ `tests` æˆ– `examples` æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æµ‹è¯•ï¼Œå®ƒä¼šèŠ±è´¹å¾ˆé•¿æ—¶é—´ï¼

</Tip>

```bash
RUN_SLOW=yes python -m pytest -n auto --dist=loadfile -s -v ./tests/models/my_new_model
RUN_SLOW=yes python -m pytest -n auto --dist=loadfile -s -v ./examples/pytorch/text-classification
```

å’Œæ—¶é—´è¾ƒé•¿çš„æµ‹è¯•ä¸€æ ·ï¼Œè¿˜æœ‰å…¶ä»–ç¯å¢ƒå˜é‡åœ¨æµ‹è¯•è¿‡ç¨‹ä¸­ï¼Œåœ¨é»˜è®¤æƒ…å†µä¸‹æ˜¯æœªå¯ç”¨çš„ï¼š
- `RUN_CUSTOM_TOKENIZERS`: å¯ç”¨è‡ªå®šä¹‰åˆ†è¯å™¨çš„æµ‹è¯•ã€‚

æ›´å¤šç¯å¢ƒå˜é‡å’Œé¢å¤–ä¿¡æ¯å¯ä»¥åœ¨ [testing_utils.py](src/transformers/testing_utils.py) ä¸­æ‰¾åˆ°ã€‚

ğŸ¤— Transformers åªæ˜¯ä½¿ç”¨ `pytest` ä½œä¸ºæµ‹è¯•è¿è¡Œç¨‹åºï¼Œä½†æµ‹è¯•å¥—ä»¶æœ¬èº«æ²¡ç”¨ä»»ä½•ä¸ `pytest` ç›¸å…³çš„åŠŸèƒ½ã€‚

è¿™æ„å‘³ç€å®Œå…¨æ”¯æŒ `unittest` ã€‚ä»¥ä¸‹æ˜¯å¦‚ä½•ä½¿ç”¨ `unittest` è¿è¡Œæµ‹è¯•çš„æ–¹æ³•ï¼š

```bash
python -m unittest discover -s tests -t . -v
python -m unittest discover -s examples -t examples -v
```

### é£æ ¼æŒ‡å—

ğŸ¤— Transformers çš„æ–‡æ¡£éµå¾ª [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html)ã€‚è¯·æŸ¥çœ‹æˆ‘ä»¬çš„ [æ–‡æ¡£ç¼–å†™æŒ‡å—](https://github.com/huggingface/transformers/tree/main/docs#writing-documentation---specification) æ¥è·å–æ›´å¤šä¿¡æ¯ã€‚

### åœ¨ Windows ä¸Šå¼€å‘

åœ¨ Windows ä¸Šï¼ˆé™¤éä½ æ­£åœ¨ä½¿ç”¨ [Windows Subsystem for Linux](https://learn.microsoft.com/en-us/windows/wsl/) æˆ– WSLï¼‰ï¼Œä½ éœ€è¦é…ç½® git å°† Windows çš„ `CRLF` è¡Œç»“æŸç¬¦è½¬æ¢ä¸º Linux çš„ `LF` è¡Œç»“æŸç¬¦ï¼š

```bash
git config core.autocrlf input
```

åœ¨ Windows ä¸Šæœ‰ä¸€ç§æ–¹æ³•å¯ä»¥è¿è¡Œ `make` å‘½ä»¤ï¼Œé‚£å°±æ˜¯ä½¿ç”¨ MSYS2ï¼š

1. [ä¸‹è½½ MSYS2](https://www.msys2.org/)ï¼Œå‡è®¾å·²ç»å®‰è£…åœ¨ `C:\msys64`ã€‚
2. ä»å‘½ä»¤è¡Œæ‰“å¼€ `C:\msys64\msys2.exe` ï¼ˆå¯ä»¥åœ¨ **å¼€å§‹** èœå•ä¸­æ‰¾åˆ°ï¼‰ã€‚
3. åœ¨ shell ä¸­è¿è¡Œï¼š `pacman -Syu` ï¼Œå¹¶ä½¿ç”¨ `pacman -S make` å®‰è£… `make`ã€‚
4. æŠŠ `C:\msys64\usr\bin` æ·»åŠ åˆ°ä½ çš„ PATH ç¯å¢ƒå˜é‡ä¸­ã€‚

ç°åœ¨ä½ å¯ä»¥åœ¨ä»»ä½•ç»ˆç«¯ï¼ˆPowerShellã€cmd.exe ç­‰ï¼‰ä¸­ä½¿ç”¨ `make` å‘½ä»¤äº†ï¼ ğŸ‰

### å°†æ´¾ç”Ÿä»“åº“ä¸ä¸Šæ¸¸ä¸»ä»“åº“ï¼ˆHugging Face ä»“åº“ï¼‰åŒæ­¥

æ›´æ–°æ´¾ç”Ÿä»“åº“çš„ä¸»åˆ†æ”¯æ—¶ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ“ä½œã€‚è¿™æ˜¯ä¸ºäº†é¿å…å‘æ¯ä¸ªä¸Šæ¸¸ PR æ·»åŠ å‚è€ƒæ³¨é‡Šï¼ŒåŒæ—¶é¿å…å‘å‚ä¸è¿™äº› PR çš„å¼€å‘äººå‘˜å‘é€ä¸å¿…è¦çš„é€šçŸ¥ã€‚

1. å¯ä»¥çš„è¯ï¼Œè¯·é¿å…ä½¿ç”¨æ´¾ç”Ÿä»“åº“ä¸Šçš„åˆ†æ”¯å’Œ PR æ¥ä¸ä¸Šæ¸¸è¿›è¡ŒåŒæ­¥ï¼Œè€Œæ˜¯ç›´æ¥åˆå¹¶åˆ°æ´¾ç”Ÿä»“åº“çš„ä¸»åˆ†æ”¯ã€‚
2. å¦‚æœç¡®å®éœ€è¦ä¸€ä¸ª PRï¼Œåœ¨æ£€æŸ¥ä½ çš„åˆ†æ”¯åï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š

   ```bash
   git checkout -b your-branch-for-syncing
   git pull --squash --no-commit upstream main
   git commit -m '<your message without GitHub references>'
   git push --set-upstream origin your-branch-for-syncing
   ```

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\create_a_model.md
============================================================



# åˆ›å»ºè‡ªå®šä¹‰æ¶æ„

[`AutoClass`](model_doc/auto) è‡ªåŠ¨æ¨æ–­æ¨¡å‹æ¶æ„å¹¶ä¸‹è½½é¢„è®­ç»ƒçš„é…ç½®å’Œæƒé‡ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨ `AutoClass` ç”Ÿæˆä¸æ£€æŸ¥ç‚¹ï¼ˆcheckpointï¼‰æ— å…³çš„ä»£ç ã€‚å¸Œæœ›å¯¹ç‰¹å®šæ¨¡å‹å‚æ•°æœ‰æ›´å¤šæ§åˆ¶çš„ç”¨æˆ·ï¼Œå¯ä»¥ä»…ä»å‡ ä¸ªåŸºç±»åˆ›å»ºè‡ªå®šä¹‰çš„ ğŸ¤— Transformers æ¨¡å‹ã€‚è¿™å¯¹äºä»»ä½•æœ‰å…´è¶£å­¦ä¹ ã€è®­ç»ƒæˆ–è¯•éªŒ ğŸ¤— Transformers æ¨¡å‹çš„äººå¯èƒ½ç‰¹åˆ«æœ‰ç”¨ã€‚é€šè¿‡æœ¬æŒ‡å—ï¼Œæ·±å…¥äº†è§£å¦‚ä½•ä¸é€šè¿‡ `AutoClass` åˆ›å»ºè‡ªå®šä¹‰æ¨¡å‹ã€‚äº†è§£å¦‚ä½•ï¼š

- åŠ è½½å¹¶è‡ªå®šä¹‰æ¨¡å‹é…ç½®ã€‚
- åˆ›å»ºæ¨¡å‹æ¶æ„ã€‚
- ä¸ºæ–‡æœ¬åˆ›å»ºæ…¢é€Ÿå’Œå¿«é€Ÿåˆ†è¯å™¨ã€‚
- ä¸ºè§†è§‰ä»»åŠ¡åˆ›å»ºå›¾åƒå¤„ç†å™¨ã€‚
- ä¸ºéŸ³é¢‘ä»»åŠ¡åˆ›å»ºç‰¹å¾æå–å™¨ã€‚
- ä¸ºå¤šæ¨¡æ€ä»»åŠ¡åˆ›å»ºå¤„ç†å™¨ã€‚

## é…ç½®

[é…ç½®](main_classes/configuration) æ¶‰åŠåˆ°æ¨¡å‹çš„å…·ä½“å±æ€§ã€‚æ¯ä¸ªæ¨¡å‹é…ç½®éƒ½æœ‰ä¸åŒçš„å±æ€§ï¼›ä¾‹å¦‚ï¼Œæ‰€æœ‰ NLP æ¨¡å‹éƒ½å…±äº« `hidden_size`ã€`num_attention_heads`ã€ `num_hidden_layers` å’Œ `vocab_size` å±æ€§ã€‚è¿™äº›å±æ€§ç”¨äºæŒ‡å®šæ„å»ºæ¨¡å‹æ—¶çš„æ³¨æ„åŠ›å¤´æ•°é‡æˆ–éšè—å±‚å±‚æ•°ã€‚

è®¿é—® [`DistilBertConfig`] ä»¥æ›´è¿‘ä¸€æ­¥äº†è§£ [DistilBERT](model_doc/distilbert)ï¼Œæ£€æŸ¥å®ƒçš„å±æ€§ï¼š

```py
>>> from transformers import DistilBertConfig

>>> config = DistilBertConfig()
>>> print(config)
DistilBertConfig {
  "activation": "gelu",
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "transformers_version": "4.16.2",
  "vocab_size": 30522
}
```

[`DistilBertConfig`] æ˜¾ç¤ºäº†æ„å»ºåŸºç¡€ [`DistilBertModel`] æ‰€ä½¿ç”¨çš„æ‰€æœ‰é»˜è®¤å±æ€§ã€‚æ‰€æœ‰å±æ€§éƒ½å¯ä»¥è¿›è¡Œè‡ªå®šä¹‰ï¼Œä¸ºå®éªŒåˆ›é€ äº†ç©ºé—´ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥å°†é»˜è®¤æ¨¡å‹è‡ªå®šä¹‰ä¸ºï¼š

- ä½¿ç”¨ `activation` å‚æ•°å°è¯•ä¸åŒçš„æ¿€æ´»å‡½æ•°ã€‚
- ä½¿ç”¨ `attention_dropout` å‚æ•°ä¸º attention probabilities ä½¿ç”¨æ›´é«˜çš„ dropout ratioã€‚

```py
>>> my_config = DistilBertConfig(activation="relu", attention_dropout=0.4)
>>> print(my_config)
DistilBertConfig {
  "activation": "relu",
  "attention_dropout": 0.4,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "transformers_version": "4.16.2",
  "vocab_size": 30522
}
```

é¢„è®­ç»ƒæ¨¡å‹çš„å±æ€§å¯ä»¥åœ¨ [`~PreTrainedConfig.from_pretrained`] å‡½æ•°ä¸­è¿›è¡Œä¿®æ”¹ï¼š

```py
>>> my_config = DistilBertConfig.from_pretrained("distilbert/distilbert-base-uncased", activation="relu", attention_dropout=0.4)
```

å½“ä½ å¯¹æ¨¡å‹é…ç½®æ»¡æ„æ—¶ï¼Œå¯ä»¥ä½¿ç”¨ [`~PreTrainedConfig.save_pretrained`] æ¥ä¿å­˜é…ç½®ã€‚ä½ çš„é…ç½®æ–‡ä»¶å°†ä»¥ JSON æ–‡ä»¶çš„å½¢å¼å­˜å‚¨åœ¨æŒ‡å®šçš„ä¿å­˜ç›®å½•ä¸­ï¼š

```py
>>> my_config.save_pretrained(save_directory="./your_model_save_path")
```

è¦é‡ç”¨é…ç½®æ–‡ä»¶ï¼Œè¯·ä½¿ç”¨ [`~PreTrainedConfig.from_pretrained`] è¿›è¡ŒåŠ è½½ï¼š

```py
>>> my_config = DistilBertConfig.from_pretrained("./your_model_save_path/config.json")
```

<Tip>

ä½ è¿˜å¯ä»¥å°†é…ç½®æ–‡ä»¶ä¿å­˜ä¸ºå­—å…¸ï¼Œç”šè‡³åªä¿å­˜è‡ªå®šä¹‰é…ç½®å±æ€§ä¸é»˜è®¤é…ç½®å±æ€§ä¹‹é—´çš„å·®å¼‚ï¼æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [é…ç½®](main_classes/configuration) æ–‡æ¡£ã€‚

</Tip>

## æ¨¡å‹

æ¥ä¸‹æ¥ï¼Œåˆ›å»ºä¸€ä¸ª[æ¨¡å‹](main_classes/models)ã€‚æ¨¡å‹ï¼Œä¹Ÿå¯æ³›æŒ‡æ¶æ„ï¼Œå®šä¹‰äº†æ¯ä¸€å±‚ç½‘ç»œçš„è¡Œä¸ºä»¥åŠè¿›è¡Œçš„æ“ä½œã€‚é…ç½®ä¸­çš„ `num_hidden_layers` ç­‰å±æ€§ç”¨äºå®šä¹‰æ¶æ„ã€‚æ¯ä¸ªæ¨¡å‹éƒ½å…±äº«åŸºç±» [`PreTrainedModel`] å’Œä¸€äº›å¸¸ç”¨æ–¹æ³•ï¼Œä¾‹å¦‚è°ƒæ•´è¾“å…¥åµŒå…¥çš„å¤§å°å’Œä¿®å‰ªè‡ªæ³¨æ„åŠ›å¤´ã€‚æ­¤å¤–ï¼Œæ‰€æœ‰æ¨¡å‹éƒ½æ˜¯ [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) çš„å­ç±»ã€‚è¿™æ„å‘³ç€æ¨¡å‹ä¸å„è‡ªæ¡†æ¶çš„ç”¨æ³•å…¼å®¹ã€‚

å°†è‡ªå®šä¹‰é…ç½®å±æ€§åŠ è½½åˆ°æ¨¡å‹ä¸­ï¼š

```py
>>> from transformers import DistilBertModel

>>> my_config = DistilBertConfig.from_pretrained("./your_model_save_path/config.json")
>>> model = DistilBertModel(my_config)
```

è¿™æ®µä»£ç åˆ›å»ºäº†ä¸€ä¸ªå…·æœ‰éšæœºå‚æ•°è€Œä¸æ˜¯é¢„è®­ç»ƒæƒé‡çš„æ¨¡å‹ã€‚åœ¨è®­ç»ƒè¯¥æ¨¡å‹ä¹‹å‰ï¼Œæ‚¨è¿˜æ— æ³•å°†è¯¥æ¨¡å‹ç”¨äºä»»ä½•ç”¨é€”ã€‚è®­ç»ƒæ˜¯ä¸€é¡¹æ˜‚è´µä¸”è€—æ—¶çš„è¿‡ç¨‹ã€‚é€šå¸¸æ¥è¯´ï¼Œæœ€å¥½ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æ¥æ›´å¿«åœ°è·å¾—æ›´å¥½çš„ç»“æœï¼ŒåŒæ—¶ä»…ä½¿ç”¨è®­ç»ƒæ‰€éœ€èµ„æºçš„ä¸€å°éƒ¨åˆ†ã€‚

ä½¿ç”¨ [`~PreTrainedModel.from_pretrained`] åˆ›å»ºé¢„è®­ç»ƒæ¨¡å‹ï¼š

```py
>>> model = DistilBertModel.from_pretrained("distilbert/distilbert-base-uncased")
```

å½“åŠ è½½é¢„è®­ç»ƒæƒé‡æ—¶ï¼Œå¦‚æœæ¨¡å‹æ˜¯ç”± ğŸ¤— Transformers æä¾›çš„ï¼Œå°†è‡ªåŠ¨åŠ è½½é»˜è®¤æ¨¡å‹é…ç½®ã€‚ç„¶è€Œï¼Œå¦‚æœä½ æ„¿æ„ï¼Œä»ç„¶å¯ä»¥å°†é»˜è®¤æ¨¡å‹é…ç½®çš„æŸäº›æˆ–è€…æ‰€æœ‰å±æ€§æ›¿æ¢æˆä½ è‡ªå·±çš„é…ç½®ï¼š

```py
>>> model = DistilBertModel.from_pretrained("distilbert/distilbert-base-uncased", config=my_config)
```

### æ¨¡å‹å¤´ï¼ˆModel headsï¼‰

æ­¤æ—¶ï¼Œä½ å·²ç»æœ‰äº†ä¸€ä¸ªè¾“å‡º*éšè—çŠ¶æ€*çš„åŸºç¡€ DistilBERT æ¨¡å‹ã€‚éšè—çŠ¶æ€ä½œä¸ºè¾“å…¥ä¼ é€’åˆ°æ¨¡å‹å¤´ä»¥ç”Ÿæˆæœ€ç»ˆè¾“å‡ºã€‚ğŸ¤— Transformers ä¸ºæ¯ä¸ªä»»åŠ¡æä¾›ä¸åŒçš„æ¨¡å‹å¤´ï¼Œåªè¦æ¨¡å‹æ”¯æŒè¯¥ä»»åŠ¡ï¼ˆå³ï¼Œæ‚¨ä¸èƒ½ä½¿ç”¨ DistilBERT æ¥æ‰§è¡Œåƒç¿»è¯‘è¿™æ ·çš„åºåˆ—åˆ°åºåˆ—ä»»åŠ¡ï¼‰ã€‚

ä¾‹å¦‚ï¼Œ[`DistilBertForSequenceClassification`] æ˜¯ä¸€ä¸ªå¸¦æœ‰åºåˆ—åˆ†ç±»å¤´ï¼ˆsequence classification headï¼‰çš„åŸºç¡€ DistilBERT æ¨¡å‹ã€‚åºåˆ—åˆ†ç±»å¤´æ˜¯æ± åŒ–è¾“å‡ºä¹‹ä¸Šçš„çº¿æ€§å±‚ã€‚

```py
>>> from transformers import DistilBertForSequenceClassification

>>> model = DistilBertForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
```

é€šè¿‡åˆ‡æ¢åˆ°ä¸åŒçš„æ¨¡å‹å¤´ï¼Œå¯ä»¥è½»æ¾åœ°å°†æ­¤æ£€æŸ¥ç‚¹é‡å¤ç”¨äºå…¶ä»–ä»»åŠ¡ã€‚å¯¹äºé—®ç­”ä»»åŠ¡ï¼Œä½ å¯ä»¥ä½¿ç”¨ [`DistilBertForQuestionAnswering`] æ¨¡å‹å¤´ã€‚é—®ç­”å¤´ï¼ˆquestion answering headï¼‰ä¸åºåˆ—åˆ†ç±»å¤´ç±»ä¼¼ï¼Œä¸åŒç‚¹åœ¨äºå®ƒæ˜¯éšè—çŠ¶æ€è¾“å‡ºä¹‹ä¸Šçš„çº¿æ€§å±‚ã€‚

```py
>>> from transformers import DistilBertForQuestionAnswering

>>> model = DistilBertForQuestionAnswering.from_pretrained("distilbert/distilbert-base-uncased")
```

## åˆ†è¯å™¨

åœ¨å°†æ¨¡å‹ç”¨äºæ–‡æœ¬æ•°æ®ä¹‹å‰ï¼Œä½ éœ€è¦çš„æœ€åä¸€ä¸ªåŸºç±»æ˜¯ [tokenizer](main_classes/tokenizer)ï¼Œå®ƒç”¨äºå°†åŸå§‹æ–‡æœ¬è½¬æ¢ä¸ºå¼ é‡ã€‚ğŸ¤— Transformers æ”¯æŒä¸¤ç§ç±»å‹çš„åˆ†è¯å™¨ï¼š

- [`PreTrainedTokenizer`]ï¼šåˆ†è¯å™¨çš„Pythonå®ç°
- [`PreTrainedTokenizerFast`]ï¼šæ¥è‡ªæˆ‘ä»¬åŸºäº Rust çš„ [ğŸ¤— Tokenizer](https://huggingface.co/docs/tokenizers/python/latest/) åº“çš„åˆ†è¯å™¨ã€‚å› ä¸ºå…¶ä½¿ç”¨äº† Rust å®ç°ï¼Œè¿™ç§åˆ†è¯å™¨ç±»å‹çš„é€Ÿåº¦è¦å¿«å¾—å¤šï¼Œå°¤å…¶æ˜¯åœ¨æ‰¹é‡åˆ†è¯ï¼ˆbatch tokenizationï¼‰çš„æ—¶å€™ã€‚å¿«é€Ÿåˆ†è¯å™¨è¿˜æä¾›å…¶ä»–çš„æ–¹æ³•ï¼Œä¾‹å¦‚*åç§»æ˜ å°„ï¼ˆoffset mappingï¼‰*ï¼Œå®ƒå°†æ ‡è®°ï¼ˆtokenï¼‰æ˜ å°„åˆ°å…¶åŸå§‹å•è¯æˆ–å­—ç¬¦ã€‚

è¿™ä¸¤ç§åˆ†è¯å™¨éƒ½æ”¯æŒå¸¸ç”¨çš„æ–¹æ³•ï¼Œå¦‚ç¼–ç å’Œè§£ç ã€æ·»åŠ æ–°æ ‡è®°ä»¥åŠç®¡ç†ç‰¹æ®Šæ ‡è®°ã€‚

<Tip warning={true}>

å¹¶éæ¯ä¸ªæ¨¡å‹éƒ½æ”¯æŒå¿«é€Ÿåˆ†è¯å™¨ã€‚å‚ç…§è¿™å¼  [è¡¨æ ¼](index#supported-frameworks) æŸ¥çœ‹æ¨¡å‹æ˜¯å¦æ”¯æŒå¿«é€Ÿåˆ†è¯å™¨ã€‚

</Tip>

å¦‚æœæ‚¨è®­ç»ƒäº†è‡ªå·±çš„åˆ†è¯å™¨ï¼Œåˆ™å¯ä»¥ä»*è¯è¡¨*æ–‡ä»¶åˆ›å»ºä¸€ä¸ªåˆ†è¯å™¨ï¼š

```py
>>> from transformers import DistilBertTokenizer

>>> my_tokenizer = DistilBertTokenizer(vocab_file="my_vocab_file.txt", do_lower_case=False, padding_side="left")
```

è¯·åŠ¡å¿…è®°ä½ï¼Œè‡ªå®šä¹‰åˆ†è¯å™¨ç”Ÿæˆçš„è¯è¡¨ä¸é¢„è®­ç»ƒæ¨¡å‹åˆ†è¯å™¨ç”Ÿæˆçš„è¯è¡¨æ˜¯ä¸åŒçš„ã€‚å¦‚æœä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œåˆ™éœ€è¦ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„è¯è¡¨ï¼Œå¦åˆ™è¾“å…¥å°†æ²¡æœ‰æ„ä¹‰ã€‚ ä½¿ç”¨ [`DistilBertTokenizer`] ç±»åˆ›å»ºå…·æœ‰é¢„è®­ç»ƒæ¨¡å‹è¯è¡¨çš„åˆ†è¯å™¨ï¼š

```py
>>> from transformers import DistilBertTokenizer

>>> slow_tokenizer = DistilBertTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
```

ä½¿ç”¨ [`DistilBertTokenizerFast`] ç±»åˆ›å»ºå¿«é€Ÿåˆ†è¯å™¨ï¼š

```py
>>> from transformers import DistilBertTokenizerFast

>>> fast_tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert/distilbert-base-uncased")
```

<Tip>

é»˜è®¤æƒ…å†µä¸‹ï¼Œ[`AutoTokenizer`] å°†å°è¯•åŠ è½½å¿«é€Ÿæ ‡è®°ç”Ÿæˆå™¨ã€‚ä½ å¯ä»¥é€šè¿‡åœ¨ `from_pretrained` ä¸­è®¾ç½® `use_fast=False` ä»¥ç¦ç”¨æ­¤è¡Œä¸ºã€‚

</Tip>

## å›¾åƒå¤„ç†å™¨

å›¾åƒå¤„ç†å™¨ç”¨äºå¤„ç†è§†è§‰è¾“å…¥ã€‚å®ƒç»§æ‰¿è‡ª [`~image_processing_utils.ImageProcessingMixin`] åŸºç±»ã€‚

è¦ä½¿ç”¨å®ƒï¼Œéœ€è¦åˆ›å»ºä¸€ä¸ªä¸ä½ ä½¿ç”¨çš„æ¨¡å‹å…³è”çš„å›¾åƒå¤„ç†å™¨ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ ä½¿ç”¨ [ViT](model_doc/vit) è¿›è¡Œå›¾åƒåˆ†ç±»ï¼Œå¯ä»¥åˆ›å»ºä¸€ä¸ªé»˜è®¤çš„ [`ViTImageProcessor`]ï¼š

```py
>>> from transformers import ViTImageProcessor

>>> vit_extractor = ViTImageProcessor()
>>> print(vit_extractor)
ViTImageProcessor {
  "do_normalize": true,
  "do_resize": true,
  "image_processor_type": "ViTImageProcessor",
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "resample": 2,
  "size": 224
}
```

<Tip>

å¦‚æœæ‚¨ä¸éœ€è¦è¿›è¡Œä»»ä½•è‡ªå®šä¹‰ï¼Œåªéœ€ä½¿ç”¨ `from_pretrained` æ–¹æ³•åŠ è½½æ¨¡å‹çš„é»˜è®¤å›¾åƒå¤„ç†å™¨å‚æ•°ã€‚

</Tip>

ä¿®æ”¹ä»»ä½• [`ViTImageProcessor`] å‚æ•°ä»¥åˆ›å»ºè‡ªå®šä¹‰å›¾åƒå¤„ç†å™¨ï¼š

```py
>>> from transformers import ViTImageProcessor

>>> my_vit_extractor = ViTImageProcessor(resample="PIL.Image.BOX", do_normalize=False, image_mean=[0.3, 0.3, 0.3])
>>> print(my_vit_extractor)
ViTImageProcessor {
  "do_normalize": false,
  "do_resize": true,
  "image_processor_type": "ViTImageProcessor",
  "image_mean": [
    0.3,
    0.3,
    0.3
  ],
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "resample": "PIL.Image.BOX",
  "size": 224
}
```

## ç‰¹å¾æå–å™¨

ç‰¹å¾æå–å™¨ç”¨äºå¤„ç†éŸ³é¢‘è¾“å…¥ã€‚å®ƒç»§æ‰¿è‡ª [`~feature_extraction_utils.FeatureExtractionMixin`] åŸºç±»ï¼Œäº¦å¯ç»§æ‰¿ [`SequenceFeatureExtractor`] ç±»æ¥å¤„ç†éŸ³é¢‘è¾“å…¥ã€‚

è¦ä½¿ç”¨å®ƒï¼Œåˆ›å»ºä¸€ä¸ªä¸ä½ ä½¿ç”¨çš„æ¨¡å‹å…³è”çš„ç‰¹å¾æå–å™¨ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ ä½¿ç”¨ [Wav2Vec2](model_doc/wav2vec2) è¿›è¡ŒéŸ³é¢‘åˆ†ç±»ï¼Œå¯ä»¥åˆ›å»ºä¸€ä¸ªé»˜è®¤çš„ [`Wav2Vec2FeatureExtractor`]ï¼š

```py
>>> from transformers import Wav2Vec2FeatureExtractor

>>> w2v2_extractor = Wav2Vec2FeatureExtractor()
>>> print(w2v2_extractor)
Wav2Vec2FeatureExtractor {
  "do_normalize": true,
  "feature_extractor_type": "Wav2Vec2FeatureExtractor",
  "feature_size": 1,
  "padding_side": "right",
  "padding_value": 0.0,
  "return_attention_mask": false,
  "sampling_rate": 16000
}
```

<Tip>

å¦‚æœæ‚¨ä¸éœ€è¦è¿›è¡Œä»»ä½•è‡ªå®šä¹‰ï¼Œåªéœ€ä½¿ç”¨ `from_pretrained` æ–¹æ³•åŠ è½½æ¨¡å‹çš„é»˜è®¤ç‰¹å¾æå–å™¨å‚æ•°ã€‚

</Tip>

ä¿®æ”¹ä»»ä½• [`Wav2Vec2FeatureExtractor`] å‚æ•°ä»¥åˆ›å»ºè‡ªå®šä¹‰ç‰¹å¾æå–å™¨ï¼š

```py
>>> from transformers import Wav2Vec2FeatureExtractor

>>> w2v2_extractor = Wav2Vec2FeatureExtractor(sampling_rate=8000, do_normalize=False)
>>> print(w2v2_extractor)
Wav2Vec2FeatureExtractor {
  "do_normalize": false,
  "feature_extractor_type": "Wav2Vec2FeatureExtractor",
  "feature_size": 1,
  "padding_side": "right",
  "padding_value": 0.0,
  "return_attention_mask": false,
  "sampling_rate": 8000
}
```


## å¤„ç†å™¨

å¯¹äºæ”¯æŒå¤šæ¨¡å¼ä»»åŠ¡çš„æ¨¡å‹ï¼ŒğŸ¤— Transformers æä¾›äº†ä¸€ä¸ªå¤„ç†å™¨ç±»ï¼Œå¯ä»¥æ–¹ä¾¿åœ°å°†ç‰¹å¾æå–å™¨å’Œåˆ†è¯å™¨ç­‰å¤„ç†ç±»åŒ…è£…åˆ°å•ä¸ªå¯¹è±¡ä¸­ã€‚ä¾‹å¦‚ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨ [`Wav2Vec2Processor`] æ¥æ‰§è¡Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä»»åŠ¡ (ASR)ã€‚ ASR å°†éŸ³é¢‘è½¬å½•ä¸ºæ–‡æœ¬ï¼Œå› æ­¤æ‚¨å°†éœ€è¦ä¸€ä¸ªç‰¹å¾æå–å™¨å’Œä¸€ä¸ªåˆ†è¯å™¨ã€‚

åˆ›å»ºä¸€ä¸ªç‰¹å¾æå–å™¨æ¥å¤„ç†éŸ³é¢‘è¾“å…¥ï¼š

```py
>>> from transformers import Wav2Vec2FeatureExtractor

>>> feature_extractor = Wav2Vec2FeatureExtractor(padding_value=1.0, do_normalize=True)
```

åˆ›å»ºä¸€ä¸ªåˆ†è¯å™¨æ¥å¤„ç†æ–‡æœ¬è¾“å…¥ï¼š

```py
>>> from transformers import Wav2Vec2CTCTokenizer

>>> tokenizer = Wav2Vec2CTCTokenizer(vocab_file="my_vocab_file.txt")
```

å°†ç‰¹å¾æå–å™¨å’Œåˆ†è¯å™¨åˆå¹¶åˆ° [`Wav2Vec2Processor`] ä¸­ï¼š

```py
>>> from transformers import Wav2Vec2Processor

>>> processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)
```

é€šè¿‡ä¸¤ä¸ªåŸºç±» - é…ç½®ç±»å’Œæ¨¡å‹ç±» - ä»¥åŠä¸€ä¸ªé™„åŠ çš„é¢„å¤„ç†ç±»ï¼ˆåˆ†è¯å™¨ã€å›¾åƒå¤„ç†å™¨ã€ç‰¹å¾æå–å™¨æˆ–å¤„ç†å™¨ï¼‰ï¼Œä½ å¯ä»¥åˆ›å»º ğŸ¤— Transformers æ”¯æŒçš„ä»»ä½•æ¨¡å‹ã€‚ æ¯ä¸ªåŸºç±»éƒ½æ˜¯å¯é…ç½®çš„ï¼Œå…è®¸ä½ ä½¿ç”¨æ‰€éœ€çš„ç‰¹å®šå±æ€§ã€‚ ä½ å¯ä»¥è½»æ¾è®¾ç½®æ¨¡å‹è¿›è¡Œè®­ç»ƒæˆ–ä¿®æ”¹ç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\custom_models.md
============================================================



# å…±äº«è‡ªå®šä¹‰æ¨¡å‹

ğŸ¤— Transformers åº“è®¾è®¡å¾—æ˜“äºæ‰©å±•ã€‚æ¯ä¸ªæ¨¡å‹çš„ä»£ç éƒ½åœ¨ä»“åº“ç»™å®šçš„å­æ–‡ä»¶å¤¹ä¸­ï¼Œæ²¡æœ‰è¿›è¡ŒæŠ½è±¡ï¼Œå› æ­¤ä½ å¯ä»¥è½»æ¾å¤åˆ¶æ¨¡å‹ä»£ç æ–‡ä»¶å¹¶æ ¹æ®éœ€è¦è¿›è¡Œè°ƒæ•´ã€‚

å¦‚æœä½ è¦ç¼–å†™å…¨æ–°çš„æ¨¡å‹ï¼Œä»å¤´å¼€å§‹å¯èƒ½æ›´å®¹æ˜“ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†å‘ä½ å±•ç¤ºå¦‚ä½•ç¼–å†™è‡ªå®šä¹‰æ¨¡å‹åŠå…¶é…ç½®ï¼Œä»¥ä¾¿å¯ä»¥åœ¨ Transformers ä¸­ä½¿ç”¨å®ƒï¼›ä»¥åŠå¦‚ä½•ä¸ç¤¾åŒºå…±äº«å®ƒï¼ˆåŠå…¶ä¾èµ–çš„ä»£ç ï¼‰ï¼Œä»¥ä¾¿ä»»ä½•äººéƒ½å¯ä»¥ä½¿ç”¨ï¼Œå³ä½¿å®ƒä¸åœ¨ ğŸ¤— Transformers åº“ä¸­ã€‚

æˆ‘ä»¬å°†ä»¥ ResNet æ¨¡å‹ä¸ºä¾‹ï¼Œé€šè¿‡å°† [timm åº“](https://github.com/rwightman/pytorch-image-models) çš„ ResNet ç±»å°è£…åˆ° [`PreTrainedModel`] ä¸­æ¥è¿›è¡Œè¯´æ˜ã€‚

## ç¼–å†™è‡ªå®šä¹‰é…ç½®

åœ¨æ·±å…¥ç ”ç©¶æ¨¡å‹ä¹‹å‰ï¼Œè®©æˆ‘ä»¬é¦–å…ˆç¼–å†™å…¶é…ç½®ã€‚æ¨¡å‹çš„é…ç½®æ˜¯ä¸€ä¸ªå¯¹è±¡ï¼Œå…¶ä¸­åŒ…å«æ„å»ºæ¨¡å‹æ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚ä¸­çœ‹åˆ°ï¼Œæ¨¡å‹åªèƒ½æ¥å—ä¸€ä¸ª `config` æ¥è¿›è¡Œåˆå§‹åŒ–ï¼Œå› æ­¤æˆ‘ä»¬å¾ˆéœ€è¦ä½¿è¯¥å¯¹è±¡å°½å¯èƒ½å®Œæ•´ã€‚

æˆ‘ä»¬å°†é‡‡ç”¨ä¸€äº›æˆ‘ä»¬å¯èƒ½æƒ³è¦è°ƒæ•´çš„ ResNet ç±»çš„å‚æ•°ä¸¾ä¾‹ã€‚ä¸åŒçš„é…ç½®å°†ä¸ºæˆ‘ä»¬æä¾›ä¸åŒç±»å‹å¯èƒ½çš„ ResNet æ¨¡å‹ã€‚åœ¨ç¡®è®¤å…¶ä¸­ä¸€äº›å‚æ•°çš„æœ‰æ•ˆæ€§åï¼Œæˆ‘ä»¬åªéœ€å­˜å‚¨è¿™äº›å‚æ•°ã€‚

```python
from transformers import PreTrainedConfig
from typing import List


class ResnetConfig(PreTrainedConfig):
    model_type = "resnet"

    def __init__(
        self,
        block_type="bottleneck",
        layers: list[int] = [3, 4, 6, 3],
        num_classes: int = 1000,
        input_channels: int = 3,
        cardinality: int = 1,
        base_width: int = 64,
        stem_width: int = 64,
        stem_type: str = "",
        avg_down: bool = False,
        **kwargs,
    ):
        if block_type not in ["basic", "bottleneck"]:
            raise ValueError(f"`block_type` must be 'basic' or bottleneck', got {block_type}.")
        if stem_type not in ["", "deep", "deep-tiered"]:
            raise ValueError(f"`stem_type` must be '', 'deep' or 'deep-tiered', got {stem_type}.")

        self.block_type = block_type
        self.layers = layers
        self.num_classes = num_classes
        self.input_channels = input_channels
        self.cardinality = cardinality
        self.base_width = base_width
        self.stem_width = stem_width
        self.stem_type = stem_type
        self.avg_down = avg_down
        super().__init__(**kwargs)
```

ç¼–å†™è‡ªå®šä¹‰é…ç½®æ—¶éœ€è¦è®°ä½çš„ä¸‰ä¸ªé‡è¦äº‹é¡¹å¦‚ä¸‹ï¼š
- å¿…é¡»ç»§æ‰¿è‡ª `PreTrainedConfig`ï¼Œ
- `PreTrainedConfig` çš„ `__init__` æ–¹æ³•å¿…é¡»æ¥å—ä»»ä½• kwargsï¼Œ
- è¿™äº› `kwargs` éœ€è¦ä¼ é€’ç»™è¶…ç±»çš„ `__init__` æ–¹æ³•ã€‚

ç»§æ‰¿æ˜¯ä¸ºäº†ç¡®ä¿ä½ è·å¾—æ¥è‡ª ğŸ¤— Transformers åº“çš„æ‰€æœ‰åŠŸèƒ½ï¼Œè€Œå¦å¤–ä¸¤ä¸ªçº¦æŸæºäº `PreTrainedConfig` çš„å­—æ®µæ¯”ä½ è®¾ç½®çš„å­—æ®µå¤šã€‚åœ¨ä½¿ç”¨ `from_pretrained` æ–¹æ³•é‡æ–°åŠ è½½é…ç½®æ—¶ï¼Œè¿™äº›å­—æ®µéœ€è¦è¢«ä½ çš„é…ç½®æ¥å—ï¼Œç„¶åä¼ é€’ç»™è¶…ç±»ã€‚

ä¸ºä½ çš„é…ç½®å®šä¹‰ `model_type`ï¼ˆæ­¤å¤„ä¸º `model_type="resnet"`ï¼‰ä¸æ˜¯å¿…é¡»çš„ï¼Œé™¤éä½ æƒ³ä½¿ç”¨è‡ªåŠ¨ç±»æ³¨å†Œä½ çš„æ¨¡å‹ï¼ˆè¯·å‚é˜…æœ€åä¸€èŠ‚ï¼‰ã€‚

åšå®Œè¿™äº›ä»¥åï¼Œå°±å¯ä»¥åƒä½¿ç”¨åº“é‡Œä»»ä½•å…¶ä»–æ¨¡å‹é…ç½®ä¸€æ ·ï¼Œè½»æ¾åœ°åˆ›å»ºå’Œä¿å­˜é…ç½®ã€‚ä»¥ä¸‹ä»£ç å±•ç¤ºäº†å¦‚ä½•åˆ›å»ºå¹¶ä¿å­˜ resnet50d é…ç½®ï¼š

```py
resnet50d_config = ResnetConfig(block_type="bottleneck", stem_width=32, stem_type="deep", avg_down=True)
resnet50d_config.save_pretrained("custom-resnet")
```

è¿™è¡Œä»£ç å°†åœ¨ `custom-resnet` æ–‡ä»¶å¤¹å†…ä¿å­˜ä¸€ä¸ªåä¸º `config.json` çš„æ–‡ä»¶ã€‚ç„¶åï¼Œä½ å¯ä»¥ä½¿ç”¨ `from_pretrained` æ–¹æ³•é‡æ–°åŠ è½½é…ç½®ï¼š

```py
resnet50d_config = ResnetConfig.from_pretrained("custom-resnet")
```

ä½ è¿˜å¯ä»¥ä½¿ç”¨ [`PreTrainedConfig`] ç±»çš„ä»»ä½•å…¶ä»–æ–¹æ³•ï¼Œä¾‹å¦‚ [`~PreTrainedConfig.push_to_hub`]ï¼Œç›´æ¥å°†é…ç½®ä¸Šä¼ åˆ° Hubã€‚

## ç¼–å†™è‡ªå®šä¹‰æ¨¡å‹

æœ‰äº† ResNet é…ç½®åï¼Œå°±å¯ä»¥ç»§ç»­ç¼–å†™æ¨¡å‹äº†ã€‚å®é™…ä¸Šï¼Œæˆ‘ä»¬å°†ç¼–å†™ä¸¤ä¸ªæ¨¡å‹ï¼šä¸€ä¸ªæ¨¡å‹ç”¨äºä»ä¸€æ‰¹å›¾åƒä¸­æå–éšè—ç‰¹å¾ï¼ˆç±»ä¼¼äº [`BertModel`]ï¼‰ï¼Œå¦ä¸€ä¸ªæ¨¡å‹é€‚ç”¨äºå›¾åƒåˆ†ç±»ï¼ˆç±»ä¼¼äº [`BertForSequenceClassification`]ï¼‰ã€‚

æ­£å¦‚ä¹‹å‰æåˆ°çš„ï¼Œæˆ‘ä»¬åªä¼šç¼–å†™ä¸€ä¸ªæ¾æ•£çš„æ¨¡å‹åŒ…è£…ï¼Œä»¥ä½¿ç¤ºä¾‹ä¿æŒç®€æ´ã€‚åœ¨ç¼–å†™æ­¤ç±»ä¹‹å‰ï¼Œåªéœ€è¦å»ºç«‹èµ·å—ç±»å‹ï¼ˆblock typesï¼‰ä¸å®é™…å—ç±»ï¼ˆblock classesï¼‰ä¹‹é—´çš„æ˜ å°„ã€‚ç„¶åï¼Œé€šè¿‡å°†æ‰€æœ‰å†…å®¹ä¼ é€’ç»™ResNetç±»ï¼Œä»é…ç½®ä¸­å®šä¹‰æ¨¡å‹ï¼š

```py
from transformers import PreTrainedModel
from timm.models.resnet import BasicBlock, Bottleneck, ResNet
from .configuration_resnet import ResnetConfig


BLOCK_MAPPING = {"basic": BasicBlock, "bottleneck": Bottleneck}


class ResnetModel(PreTrainedModel):
    config_class = ResnetConfig

    def __init__(self, config):
        super().__init__(config)
        block_layer = BLOCK_MAPPING[config.block_type]
        self.model = ResNet(
            block_layer,
            config.layers,
            num_classes=config.num_classes,
            in_chans=config.input_channels,
            cardinality=config.cardinality,
            base_width=config.base_width,
            stem_width=config.stem_width,
            stem_type=config.stem_type,
            avg_down=config.avg_down,
        )

    def forward(self, tensor):
        return self.model.forward_features(tensor)
```

å¯¹ç”¨äºè¿›è¡Œå›¾åƒåˆ†ç±»çš„æ¨¡å‹ï¼Œæˆ‘ä»¬åªéœ€æ›´æ”¹å‰å‘æ–¹æ³•ï¼š

```py
import torch


class ResnetModelForImageClassification(PreTrainedModel):
    config_class = ResnetConfig

    def __init__(self, config):
        super().__init__(config)
        block_layer = BLOCK_MAPPING[config.block_type]
        self.model = ResNet(
            block_layer,
            config.layers,
            num_classes=config.num_classes,
            in_chans=config.input_channels,
            cardinality=config.cardinality,
            base_width=config.base_width,
            stem_width=config.stem_width,
            stem_type=config.stem_type,
            avg_down=config.avg_down,
        )

    def forward(self, tensor, labels=None):
        logits = self.model(tensor)
        if labels is not None:
            loss = torch.nn.functional.cross_entropy(logits, labels)
            return {"loss": loss, "logits": logits}
        return {"logits": logits}
```

åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œè¯·æ³¨æ„æˆ‘ä»¬å¦‚ä½•ç»§æ‰¿ `PreTrainedModel` å¹¶ä½¿ç”¨ `config` è°ƒç”¨äº†è¶…ç±»çš„åˆå§‹åŒ–ï¼ˆæœ‰ç‚¹åƒç¼–å†™å¸¸è§„çš„torch.nn.Moduleï¼‰ã€‚è®¾ç½® `config_class` çš„é‚£è¡Œä»£ç ä¸æ˜¯å¿…é¡»çš„ï¼Œé™¤éä½ æƒ³ä½¿ç”¨è‡ªåŠ¨ç±»æ³¨å†Œä½ çš„æ¨¡å‹ï¼ˆè¯·å‚é˜…æœ€åä¸€èŠ‚ï¼‰ã€‚

<Tip>

å¦‚æœä½ çš„æ¨¡å‹ä¸åº“ä¸­çš„æŸä¸ªæ¨¡å‹éå¸¸ç›¸ä¼¼ï¼Œä½ å¯ä»¥é‡ç”¨ä¸è¯¥æ¨¡å‹ç›¸åŒçš„é…ç½®ã€‚

</Tip>

ä½ å¯ä»¥è®©æ¨¡å‹è¿”å›ä»»ä½•ä½ æƒ³è¦çš„å†…å®¹ï¼Œä½†æ˜¯åƒæˆ‘ä»¬ä¸º `ResnetModelForImageClassification` åšçš„é‚£æ ·è¿”å›ä¸€ä¸ªå­—å…¸ï¼Œå¹¶åœ¨ä¼ é€’æ ‡ç­¾æ—¶åŒ…å«lossï¼Œå¯ä»¥ä½¿ä½ çš„æ¨¡å‹èƒ½å¤Ÿåœ¨ [`Trainer`] ç±»ä¸­ç›´æ¥ä½¿ç”¨ã€‚åªè¦ä½ è®¡åˆ’ä½¿ç”¨è‡ªå·±çš„è®­ç»ƒå¾ªç¯æˆ–å…¶ä»–åº“è¿›è¡Œè®­ç»ƒï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨å…¶ä»–è¾“å‡ºæ ¼å¼ã€‚

ç°åœ¨æˆ‘ä»¬å·²ç»æœ‰äº†æ¨¡å‹ç±»ï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªï¼š

```py
resnet50d = ResnetModelForImageClassification(resnet50d_config)
```

åŒæ ·çš„ï¼Œä½ å¯ä»¥ä½¿ç”¨ [`PreTrainedModel`] çš„ä»»ä½•æ–¹æ³•ï¼Œæ¯”å¦‚ [`~PreTrainedModel.save_pretrained`] æˆ–è€… [`~PreTrainedModel.push_to_hub`]ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚ä¸­ä½¿ç”¨ç¬¬äºŒç§æ–¹æ³•ï¼Œå¹¶äº†è§£å¦‚ä½•å¦‚ä½•ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹çš„ä»£ç æ¨é€æ¨¡å‹æƒé‡ã€‚ä½†é¦–å…ˆï¼Œè®©æˆ‘ä»¬åœ¨æ¨¡å‹å†…åŠ è½½ä¸€äº›é¢„è®­ç»ƒæƒé‡ã€‚

åœ¨ä½ è‡ªå·±çš„ç”¨ä¾‹ä¸­ï¼Œä½ å¯èƒ½ä¼šåœ¨è‡ªå·±çš„æ•°æ®ä¸Šè®­ç»ƒè‡ªå®šä¹‰æ¨¡å‹ã€‚ä¸ºäº†å¿«é€Ÿå®Œæˆæœ¬æ•™ç¨‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ resnet50d çš„é¢„è®­ç»ƒç‰ˆæœ¬ã€‚ç”±äºæˆ‘ä»¬çš„æ¨¡å‹åªæ˜¯å®ƒçš„åŒ…è£…ï¼Œè½¬ç§»è¿™äº›æƒé‡å°†ä¼šå¾ˆå®¹æ˜“ï¼š

```py
import timm

pretrained_model = timm.create_model("resnet50d", pretrained=True)
resnet50d.model.load_state_dict(pretrained_model.state_dict())
```

ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹ï¼Œå¦‚ä½•ç¡®ä¿åœ¨æ‰§è¡Œ [`~PreTrainedModel.save_pretrained`] æˆ– [`~PreTrainedModel.push_to_hub`] æ—¶ï¼Œæ¨¡å‹çš„ä»£ç è¢«ä¿å­˜ã€‚

## å°†ä»£ç å‘é€åˆ° Hub

<Tip warning={true}>

æ­¤ API æ˜¯å®éªŒæ€§çš„ï¼Œæœªæ¥çš„å‘å¸ƒä¸­å¯èƒ½ä¼šæœ‰ä¸€äº›è½»å¾®çš„ä¸å…¼å®¹æ›´æ”¹ã€‚

</Tip>

é¦–å…ˆï¼Œç¡®ä¿ä½ çš„æ¨¡å‹åœ¨ä¸€ä¸ª `.py` æ–‡ä»¶ä¸­å®Œå…¨å®šä¹‰ã€‚åªè¦æ‰€æœ‰æ–‡ä»¶éƒ½ä½äºåŒä¸€ç›®å½•ä¸­ï¼Œå®ƒå°±å¯ä»¥ä¾èµ–äºæŸäº›å…¶ä»–æ–‡ä»¶çš„ç›¸å¯¹å¯¼å…¥ï¼ˆç›®å‰æˆ‘ä»¬è¿˜ä¸ä¸ºå­æ¨¡å—æ”¯æŒæ­¤åŠŸèƒ½ï¼‰ã€‚å¯¹äºæˆ‘ä»¬çš„ç¤ºä¾‹ï¼Œæˆ‘ä»¬å°†åœ¨å½“å‰å·¥ä½œç›®å½•ä¸­åä¸º `resnet_model` çš„æ–‡ä»¶å¤¹ä¸­å®šä¹‰ä¸€ä¸ª `modeling_resnet.py` æ–‡ä»¶å’Œä¸€ä¸ª `configuration_resnet.py` æ–‡ä»¶ã€‚ é…ç½®æ–‡ä»¶åŒ…å« `ResnetConfig` çš„ä»£ç ï¼Œæ¨¡å‹æ–‡ä»¶åŒ…å« `ResnetModel` å’Œ `ResnetModelForImageClassification` çš„ä»£ç ã€‚

```
.
â””â”€â”€ resnet_model
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ configuration_resnet.py
    â””â”€â”€ modeling_resnet.py
```

`__init__.py` å¯ä»¥ä¸ºç©ºï¼Œå®ƒçš„å­˜åœ¨åªæ˜¯ä¸ºäº†è®© Python æ£€æµ‹åˆ° `resnet_model` å¯ä»¥ç”¨ä½œæ¨¡å—ã€‚

<Tip warning={true}>

å¦‚æœä»åº“ä¸­å¤åˆ¶æ¨¡å‹æ–‡ä»¶ï¼Œä½ éœ€è¦å°†æ–‡ä»¶é¡¶éƒ¨çš„æ‰€æœ‰ç›¸å¯¹å¯¼å…¥æ›¿æ¢ä¸ºä» `transformers` åŒ…ä¸­çš„å¯¼å…¥ã€‚

</Tip>

è¯·æ³¨æ„ï¼Œä½ å¯ä»¥é‡ç”¨ï¼ˆæˆ–å­ç±»åŒ–ï¼‰ç°æœ‰çš„é…ç½®/æ¨¡å‹ã€‚

è¦ä¸ç¤¾åŒºå…±äº«æ‚¨çš„æ¨¡å‹ï¼Œè¯·å‚ç…§ä»¥ä¸‹æ­¥éª¤ï¼šé¦–å…ˆä»æ–°åˆ›å»ºçš„æ–‡ä»¶ä¸­å¯¼å…¥ResNetæ¨¡å‹å’Œé…ç½®ï¼š

```py
from resnet_model.configuration_resnet import ResnetConfig
from resnet_model.modeling_resnet import ResnetModel, ResnetModelForImageClassification
```

æ¥ä¸‹æ¥ï¼Œä½ éœ€è¦å‘Šè¯‰åº“ï¼Œå½“ä½¿ç”¨ `save_pretrained` æ–¹æ³•æ—¶ï¼Œä½ å¸Œæœ›å¤åˆ¶è¿™äº›å¯¹è±¡çš„ä»£ç æ–‡ä»¶ï¼Œå¹¶å°†å®ƒä»¬æ­£ç¡®æ³¨å†Œåˆ°ç»™å®šçš„ Auto ç±»ï¼ˆç‰¹åˆ«æ˜¯å¯¹äºæ¨¡å‹ï¼‰ï¼Œåªéœ€è¦è¿è¡Œä»¥ä¸‹ä»£ç ï¼š

```py
ResnetConfig.register_for_auto_class()
ResnetModel.register_for_auto_class("AutoModel")
ResnetModelForImageClassification.register_for_auto_class("AutoModelForImageClassification")
```

è¯·æ³¨æ„ï¼Œå¯¹äºé…ç½®ï¼ˆåªæœ‰ä¸€ä¸ªè‡ªåŠ¨ç±» [`AutoConfig`]ï¼‰ï¼Œä¸éœ€è¦æŒ‡å®šè‡ªåŠ¨ç±»ï¼Œä½†å¯¹äºæ¨¡å‹æ¥è¯´æƒ…å†µä¸åŒã€‚ ä½ çš„è‡ªå®šä¹‰æ¨¡å‹å¯èƒ½é€‚ç”¨äºè®¸å¤šä¸åŒçš„ä»»åŠ¡ï¼Œå› æ­¤ä½ å¿…é¡»æŒ‡å®šå“ªä¸€ä¸ªè‡ªåŠ¨ç±»é€‚åˆä½ çš„æ¨¡å‹ã€‚

æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬åƒä¹‹å‰ä¸€æ ·åˆ›å»ºé…ç½®å’Œæ¨¡å‹ï¼š

```py
resnet50d_config = ResnetConfig(block_type="bottleneck", stem_width=32, stem_type="deep", avg_down=True)
resnet50d = ResnetModelForImageClassification(resnet50d_config)

pretrained_model = timm.create_model("resnet50d", pretrained=True)
resnet50d.model.load_state_dict(pretrained_model.state_dict())
```

ç°åœ¨è¦å°†æ¨¡å‹æ¨é€åˆ°é›†çº¿å™¨ï¼Œè¯·ç¡®ä¿ä½ å·²ç™»å½•ã€‚ä½ çœ‹å¯ä»¥åœ¨ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```bash
hf auth login
```

æˆ–è€…åœ¨ç¬”è®°æœ¬ä¸­è¿è¡Œä»¥ä¸‹ä»£ç ï¼š

```py
from huggingface_hub import notebook_login

notebook_login()
```

ç„¶åï¼Œå¯ä»¥è¿™æ ·å°†æ¨¡å‹æ¨é€åˆ°è‡ªå·±çš„å‘½åç©ºé—´ï¼ˆæˆ–ä½ æ‰€å±çš„ç»„ç»‡ï¼‰ï¼š

```py
resnet50d.push_to_hub("custom-resnet50d")
```

é™¤äº†æ¨¡å‹æƒé‡å’Œ JSON æ ¼å¼çš„é…ç½®å¤–ï¼Œè¿™è¡Œä»£ç ä¹Ÿä¼šå¤åˆ¶ `custom-resnet50d` æ–‡ä»¶å¤¹å†…çš„æ¨¡å‹ä»¥åŠé…ç½®çš„ `.py` æ–‡ä»¶å¹¶å°†ç»“æœä¸Šä¼ è‡³ Hubã€‚ä½ å¯ä»¥åœ¨æ­¤[æ¨¡å‹ä»“åº“](https://huggingface.co/sgugger/custom-resnet50d)ä¸­æŸ¥çœ‹ç»“æœã€‚

æœ‰å…³æ¨æ¨é€è‡³ Hub æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[å…±äº«æ•™ç¨‹](model_sharing)ã€‚

## ä½¿ç”¨å¸¦æœ‰è‡ªå®šä¹‰ä»£ç çš„æ¨¡å‹

å¯ä»¥ä½¿ç”¨è‡ªåŠ¨ç±»ï¼ˆauto-classesï¼‰å’Œ `from_pretrained` æ–¹æ³•ï¼Œä½¿ç”¨æ¨¡å‹ä»“åº“é‡Œå¸¦æœ‰è‡ªå®šä¹‰ä»£ç çš„é…ç½®ã€æ¨¡å‹æˆ–åˆ†è¯å™¨æ–‡ä»¶ã€‚æ‰€æœ‰ä¸Šä¼ åˆ° Hub çš„æ–‡ä»¶å’Œä»£ç éƒ½ä¼šè¿›è¡Œæ¶æ„è½¯ä»¶æ‰«æï¼ˆæœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜… [Hub å®‰å…¨](https://huggingface.co/docs/hub/security#malware-scanning) æ–‡æ¡£ï¼‰, ä½†ä½ ä»åº”æŸ¥çœ‹æ¨¡å‹ä»£ç å’Œä½œè€…ï¼Œä»¥é¿å…åœ¨ä½ çš„è®¡ç®—æœºä¸Šæ‰§è¡Œæ¶æ„ä»£ç ã€‚ è®¾ç½® `trust_remote_code=True` ä»¥ä½¿ç”¨å¸¦æœ‰è‡ªå®šä¹‰ä»£ç çš„æ¨¡å‹ï¼š

```py
from transformers import AutoModelForImageClassification

model = AutoModelForImageClassification.from_pretrained("sgugger/custom-resnet50d", trust_remote_code=True)
```

æˆ‘ä»¬å¼ºçƒˆå»ºè®®ä¸º `revision` å‚æ•°ä¼ é€’æäº¤å“ˆå¸Œï¼ˆcommit hashï¼‰ï¼Œä»¥ç¡®ä¿æ¨¡å‹çš„ä½œè€…æ²¡æœ‰ä½¿ç”¨ä¸€äº›æ¶æ„çš„ä»£ç è¡Œæ›´æ–°äº†ä»£ç ï¼ˆé™¤éæ‚¨å®Œå…¨ä¿¡ä»»æ¨¡å‹çš„ä½œè€…ï¼‰ã€‚

```py
commit_hash = "ed94a7c6247d8aedce4647f00f20de6875b5b292"
model = AutoModelForImageClassification.from_pretrained(
    "sgugger/custom-resnet50d", trust_remote_code=True, revision=commit_hash
)
```

åœ¨ Hub ä¸Šæµè§ˆæ¨¡å‹ä»“åº“çš„æäº¤å†å²æ—¶ï¼Œæœ‰ä¸€ä¸ªæŒ‰é’®å¯ä»¥è½»æ¾å¤åˆ¶ä»»ä½•æäº¤çš„æäº¤å“ˆå¸Œã€‚

## å°†è‡ªå®šä¹‰ä»£ç çš„æ¨¡å‹æ³¨å†Œåˆ°è‡ªåŠ¨ç±»

å¦‚æœä½ åœ¨ç¼–å†™ä¸€ä¸ªæ‰©å±• ğŸ¤— Transformers çš„åº“ï¼Œä½ å¯èƒ½æƒ³è¦æ‰©å±•è‡ªåŠ¨ç±»ä»¥åŒ…å«æ‚¨è‡ªå·±çš„æ¨¡å‹ã€‚è¿™ä¸å°†ä»£ç æ¨é€åˆ° Hub ä¸åŒï¼Œå› ä¸ºç”¨æˆ·éœ€è¦å¯¼å…¥ä½ çš„åº“æ‰èƒ½è·å–è‡ªå®šä¹‰æ¨¡å‹ï¼ˆä¸ä» Hub è‡ªåŠ¨ä¸‹è½½æ¨¡å‹ä»£ç ç›¸åï¼‰ã€‚

åªè¦ä½ çš„é…ç½® `model_type` å±æ€§ä¸ç°æœ‰æ¨¡å‹ç±»å‹ä¸åŒï¼Œå¹¶ä¸”ä½ çš„æ¨¡å‹ç±»æœ‰æ­£ç¡®çš„ `config_class` å±æ€§ï¼Œä½ å¯ä»¥åƒè¿™æ ·å°†å®ƒä»¬æ·»åŠ åˆ°è‡ªåŠ¨ç±»ä¸­ï¼š

```py
from transformers import AutoConfig, AutoModel, AutoModelForImageClassification

AutoConfig.register("resnet", ResnetConfig)
AutoModel.register(ResnetConfig, ResnetModel)
AutoModelForImageClassification.register(ResnetConfig, ResnetModelForImageClassification)
```

è¯·æ³¨æ„ï¼Œå°†è‡ªå®šä¹‰é…ç½®æ³¨å†Œåˆ° [`AutoConfig`] æ—¶ï¼Œä½¿ç”¨çš„ç¬¬ä¸€ä¸ªå‚æ•°éœ€è¦ä¸è‡ªå®šä¹‰é…ç½®çš„ `model_type` åŒ¹é…ï¼›è€Œå°†è‡ªå®šä¹‰æ¨¡å‹æ³¨å†Œåˆ°ä»»ä½•è‡ªåŠ¨æ¨¡å‹ç±»æ—¶ï¼Œä½¿ç”¨çš„ç¬¬ä¸€ä¸ªå‚æ•°éœ€è¦ä¸ `config_class` åŒ¹é…ã€‚

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\debugging.md
============================================================



# è°ƒè¯•

## å¤šGPUç½‘ç»œé—®é¢˜è°ƒè¯•

å½“ä½¿ç”¨`DistributedDataParallel`å’Œå¤šä¸ªGPUè¿›è¡Œè®­ç»ƒæˆ–æ¨ç†æ—¶ï¼Œå¦‚æœé‡åˆ°è¿›ç¨‹å’Œï¼ˆæˆ–ï¼‰èŠ‚ç‚¹ä¹‹é—´çš„äº’è”é—®é¢˜ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹è„šæœ¬æ¥è¯Šæ–­ç½‘ç»œé—®é¢˜ã€‚

```bash
wget https://raw.githubusercontent.com/huggingface/transformers/main/scripts/distributed/torch-distributed-gpu-test.py
```

ä¾‹å¦‚ï¼Œè¦æµ‹è¯•ä¸¤ä¸ªGPUä¹‹é—´çš„äº’è”ï¼Œè¯·æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š

```bash
python -m torch.distributed.run --nproc_per_node 2 --nnodes 1 torch-distributed-gpu-test.py
```

å¦‚æœä¸¤ä¸ªè¿›ç¨‹èƒ½å¤Ÿç›¸äº’é€šä¿¡å¹¶åˆ†é…GPUå†…å­˜ï¼Œå®ƒä»¬å„è‡ªå°†æ‰“å°å‡º "OK" çŠ¶æ€ã€‚

å¯¹äºæ›´å¤šçš„GPUæˆ–èŠ‚ç‚¹ï¼Œå¯ä»¥æ ¹æ®è„šæœ¬ä¸­çš„å‚æ•°è¿›è¡Œè°ƒæ•´ã€‚

åœ¨è¯Šæ–­è„šæœ¬å†…éƒ¨ï¼Œæ‚¨å°†æ‰¾åˆ°æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œç”šè‡³æœ‰å…³å¦‚ä½•åœ¨SLURMç¯å¢ƒä¸­è¿è¡Œå®ƒçš„è¯´æ˜ã€‚

å¦ä¸€ç§çº§åˆ«çš„è°ƒè¯•æ˜¯æ·»åŠ  `NCCL_DEBUG=INFO` ç¯å¢ƒå˜é‡ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š


```bash
NCCL_DEBUG=INFO python -m torch.distributed.run --nproc_per_node 2 --nnodes 1 torch-distributed-gpu-test.py
```

è¿™å°†äº§ç”Ÿå¤§é‡ä¸NCCLç›¸å…³çš„è°ƒè¯•ä¿¡æ¯ï¼Œå¦‚æœå‘ç°æœ‰é—®é¢˜æŠ¥å‘Šï¼Œæ‚¨å¯ä»¥åœ¨çº¿æœç´¢ä»¥è·å–ç›¸å…³ä¿¡æ¯ã€‚æˆ–è€…ï¼Œå¦‚æœæ‚¨ä¸ç¡®å®šå¦‚ä½•è§£é‡Šè¾“å‡ºï¼Œå¯ä»¥åœ¨`issue`ä¸­åˆ†äº«æ—¥å¿—æ–‡ä»¶ã€‚


## ä¸‹æº¢å’Œä¸Šæº¢æ£€æµ‹

<Tip>

ç›®å‰ï¼Œæ­¤åŠŸèƒ½ä»…é€‚ç”¨äºPyTorchã€‚

</Tip>

<Tip>

å¯¹äºå¤šGPUè®­ç»ƒï¼Œå®ƒéœ€è¦ä½¿ç”¨DDPï¼ˆ`torch.distributed.launch`ï¼‰ã€‚

</Tip>

<Tip>

æ­¤åŠŸèƒ½å¯ä»¥ä¸ä»»ä½•åŸºäº`nn.Module`çš„æ¨¡å‹ä¸€èµ·ä½¿ç”¨ã€‚

</Tip>

å¦‚æœæ‚¨å¼€å§‹å‘ç°`loss=NaN`æˆ–æ¨¡å‹å› æ¿€æ´»å€¼æˆ–æƒé‡ä¸­çš„`inf`æˆ–`nan`è€Œå‡ºç°ä¸€äº›å¼‚å¸¸è¡Œä¸ºï¼Œå°±éœ€è¦å‘ç°ç¬¬ä¸€ä¸ªä¸‹æº¢æˆ–ä¸Šæº¢å‘ç”Ÿçš„åœ°æ–¹ä»¥åŠå¯¼è‡´å®ƒçš„åŸå› ã€‚å¹¸è¿çš„æ˜¯ï¼Œæ‚¨å¯ä»¥é€šè¿‡æ¿€æ´»ä¸€ä¸ªç‰¹æ®Šæ¨¡å—æ¥è‡ªåŠ¨è¿›è¡Œæ£€æµ‹ã€‚

å¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨[`Trainer`]ï¼Œåªéœ€æŠŠä»¥ä¸‹å†…å®¹ï¼š


```bash
--debug underflow_overflow
```

æ·»åŠ åˆ°å¸¸è§„å‘½ä»¤è¡Œå‚æ•°ä¸­ï¼Œæˆ–åœ¨åˆ›å»º[`TrainingArguments`]å¯¹è±¡æ—¶ä¼ é€’ `debug="underflow_overflow"`ã€‚

å¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨è‡ªå·±çš„è®­ç»ƒå¾ªç¯æˆ–å…¶ä»–Trainerï¼Œæ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å®ç°ç›¸åŒçš„åŠŸèƒ½ï¼š

```python
from transformers.debug_utils import DebugUnderflowOverflow

debug_overflow = DebugUnderflowOverflow(model)
```

[`debug_utils.DebugUnderflowOverflow`] å°†`hooks`æ’å…¥æ¨¡å‹ï¼Œç´§è·Ÿåœ¨æ¯æ¬¡å‰å‘è°ƒç”¨ä¹‹åï¼Œè¿›è€Œæµ‹è¯•è¾“å…¥å’Œè¾“å‡ºå˜é‡ï¼Œä»¥åŠç›¸åº”æ¨¡å—çš„æƒé‡ã€‚ä¸€æ—¦åœ¨æ¿€æ´»å€¼æˆ–æƒé‡çš„è‡³å°‘ä¸€ä¸ªå…ƒç´ ä¸­æ£€æµ‹åˆ°`inf`æˆ–`nan`ï¼Œç¨‹åºå°†æ‰§è¡Œ`assert`å¹¶æ‰“å°æŠ¥å‘Šï¼Œå°±åƒè¿™æ ·ï¼ˆè¿™æ˜¯åœ¨`google/mt5-small`ä¸‹ä½¿ç”¨fp16æ··åˆç²¾åº¦æ•è·çš„ï¼‰ï¼š

```
Detected inf/nan during batch_number=0
Last 21 forward frames:
abs min  abs max  metadata
                  encoder.block.1.layer.1.DenseReluDense.dropout Dropout
0.00e+00 2.57e+02 input[0]
0.00e+00 2.85e+02 output
[...]
                  encoder.block.2.layer.0 T5LayerSelfAttention
6.78e-04 3.15e+03 input[0]
2.65e-04 3.42e+03 output[0]
             None output[1]
2.25e-01 1.00e+04 output[2]
                  encoder.block.2.layer.1.layer_norm T5LayerNorm
8.69e-02 4.18e-01 weight
2.65e-04 3.42e+03 input[0]
1.79e-06 4.65e+00 output
                  encoder.block.2.layer.1.DenseReluDense.wi_0 Linear
2.17e-07 4.50e+00 weight
1.79e-06 4.65e+00 input[0]
2.68e-06 3.70e+01 output
                  encoder.block.2.layer.1.DenseReluDense.wi_1 Linear
8.08e-07 2.66e+01 weight
1.79e-06 4.65e+00 input[0]
1.27e-04 2.37e+02 output
                  encoder.block.2.layer.1.DenseReluDense.dropout Dropout
0.00e+00 8.76e+03 input[0]
0.00e+00 9.74e+03 output
                  encoder.block.2.layer.1.DenseReluDense.wo Linear
1.01e-06 6.44e+00 weight
0.00e+00 9.74e+03 input[0]
3.18e-04 6.27e+04 output
                  encoder.block.2.layer.1.DenseReluDense T5DenseGatedGeluDense
1.79e-06 4.65e+00 input[0]
3.18e-04 6.27e+04 output
                  encoder.block.2.layer.1.dropout Dropout
3.18e-04 6.27e+04 input[0]
0.00e+00      inf output
```

ç”±äºç¯‡å¹…åŸå› ï¼Œç¤ºä¾‹è¾“å‡ºä¸­é—´çš„éƒ¨åˆ†å·²ç»è¢«ç¼©å‡ã€‚

ç¬¬äºŒåˆ—æ˜¾ç¤ºäº†ç»å¯¹æœ€å¤§å…ƒç´ çš„å€¼ï¼Œå› æ­¤ï¼Œå¦‚æœæ‚¨ä»”ç»†æŸ¥çœ‹æœ€å`frame`ï¼Œè¾“å…¥å’Œè¾“å‡ºéƒ½åœ¨`1e4`çš„èŒƒå›´å†…ã€‚å› æ­¤ï¼Œåœ¨ä½¿ç”¨fp16æ··åˆç²¾åº¦è¿›è¡Œè®­ç»ƒæ—¶ï¼Œæœ€åä¸€æ­¥å‘ç”Ÿäº†æº¢å‡ºï¼ˆå› ä¸ºåœ¨`fp16`ä¸‹ï¼Œåœ¨`inf`ä¹‹å‰çš„æœ€å¤§æ•°å­—æ˜¯`64e3`ï¼‰ã€‚ä¸ºäº†é¿å…åœ¨`fp16`ä¸‹å‘ç”Ÿæº¢å‡ºï¼Œæ¿€æ´»å€¼å¿…é¡»ä¿æŒä½äº`1e4`ï¼Œå› ä¸º`1e4 * 1e4 = 1e8`ï¼Œå› æ­¤ä»»ä½•å…·æœ‰å¤§æ¿€æ´»å€¼çš„çŸ©é˜µä¹˜æ³•éƒ½ä¼šå¯¼è‡´æ•°å€¼æº¢å‡ºã€‚

åœ¨è·Ÿè¸ªçš„å¼€å§‹å¤„ï¼Œæ‚¨å¯ä»¥å‘ç°é—®é¢˜å‘ç”Ÿåœ¨å“ªä¸ªæ‰¹æ¬¡ï¼ˆè¿™é‡Œçš„`Detected inf/nan during batch_number=0`è¡¨ç¤ºé—®é¢˜å‘ç”Ÿåœ¨ç¬¬ä¸€ä¸ªæ‰¹æ¬¡ï¼‰ã€‚

æ¯ä¸ªæŠ¥å‘Šçš„`frame`éƒ½ä»¥å£°æ˜ç›¸åº”æ¨¡å—çš„å±‚ä¿¡æ¯ä¸ºå¼€å¤´ï¼Œè¯´æ˜è¿™ä¸€`frame`æ˜¯ä¸ºå“ªä¸ªæ¨¡å—æŠ¥å‘Šçš„ã€‚å¦‚æœåªçœ‹è¿™ä¸ª`frame`ï¼š

```
                  encoder.block.2.layer.1.layer_norm T5LayerNorm
8.69e-02 4.18e-01 weight
2.65e-04 3.42e+03 input[0]
1.79e-06 4.65e+00 output
```

åœ¨è¿™é‡Œï¼Œ`encoder.block.2.layer.1.layer_norm` è¡¨ç¤ºå®ƒæ˜¯ç¼–ç å™¨çš„ç¬¬äºŒä¸ªå—ä¸­ç¬¬ä¸€å±‚çš„`layer norm`ã€‚è€Œ `forward` çš„å…·ä½“è°ƒç”¨æ˜¯ `T5LayerNorm`ã€‚

è®©æˆ‘ä»¬çœ‹çœ‹è¯¥æŠ¥å‘Šçš„æœ€åå‡ ä¸ª`frame`ï¼š

```
Detected inf/nan during batch_number=0
Last 21 forward frames:
abs min  abs max  metadata
[...]
                  encoder.block.2.layer.1.DenseReluDense.wi_0 Linear
2.17e-07 4.50e+00 weight
1.79e-06 4.65e+00 input[0]
2.68e-06 3.70e+01 output
                  encoder.block.2.layer.1.DenseReluDense.wi_1 Linear
8.08e-07 2.66e+01 weight
1.79e-06 4.65e+00 input[0]
1.27e-04 2.37e+02 output
                  encoder.block.2.layer.1.DenseReluDense.wo Linear
1.01e-06 6.44e+00 weight
0.00e+00 9.74e+03 input[0]
3.18e-04 6.27e+04 output
                  encoder.block.2.layer.1.DenseReluDense T5DenseGatedGeluDense
1.79e-06 4.65e+00 input[0]
3.18e-04 6.27e+04 output
                  encoder.block.2.layer.1.dropout Dropout
3.18e-04 6.27e+04 input[0]
0.00e+00      inf output
```

æœ€åä¸€ä¸ª`frame`æŠ¥å‘Šäº†`Dropout.forward`å‡½æ•°ï¼Œç¬¬ä¸€ä¸ªæ¡ç›®æ˜¯å”¯ä¸€çš„è¾“å…¥ï¼Œç¬¬äºŒä¸ªæ¡ç›®æ˜¯å”¯ä¸€çš„è¾“å‡ºã€‚æ‚¨å¯ä»¥çœ‹åˆ°ï¼Œå®ƒæ˜¯ä»`DenseReluDense`ç±»å†…çš„å±æ€§`dropout`ä¸­è°ƒç”¨çš„ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å®ƒå‘ç”Ÿåœ¨ç¬¬2ä¸ªå—çš„ç¬¬1å±‚ï¼Œä¹Ÿå°±æ˜¯åœ¨ç¬¬ä¸€ä¸ªæ‰¹æ¬¡æœŸé—´ã€‚æœ€åï¼Œç»å¯¹æœ€å¤§çš„è¾“å…¥å…ƒç´ å€¼ä¸º`6.27e+04`ï¼Œè¾“å‡ºä¹Ÿæ˜¯`inf`ã€‚

æ‚¨å¯ä»¥åœ¨è¿™é‡Œçœ‹åˆ°ï¼Œ`T5DenseGatedGeluDense.forward`äº§ç”Ÿäº†è¾“å‡ºæ¿€æ´»å€¼ï¼Œå…¶ç»å¯¹æœ€å¤§å€¼çº¦ä¸º62.7Kï¼Œéå¸¸æ¥è¿‘fp16çš„ä¸Šé™64Kã€‚åœ¨ä¸‹ä¸€ä¸ª`frame`ä¸­ï¼Œæˆ‘ä»¬æœ‰`Dropout`å¯¹æƒé‡è¿›è¡Œé‡æ–°å½’ä¸€åŒ–ï¼Œä¹‹åå°†æŸäº›å…ƒç´ å½’é›¶ï¼Œå°†ç»å¯¹æœ€å¤§å€¼æ¨åˆ°äº†64Kä»¥ä¸Šï¼Œå¯¼è‡´æº¢å‡ºï¼ˆ`inf`ï¼‰ã€‚

æ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬éœ€è¦æŸ¥çœ‹å‰é¢çš„`frame`, ä»é‚£é‡Œfp16æ•°å­—å¼€å§‹å˜å¾—éå¸¸å¤§ã€‚

è®©æˆ‘ä»¬å°†æŠ¥å‘Šä¸`models/t5/modeling_t5.py`ä¸­çš„ä»£ç åŒ¹é…ï¼š

```python
class T5DenseGatedGeluDense(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.wi_0 = nn.Linear(config.d_model, config.d_ff, bias=False)
        self.wi_1 = nn.Linear(config.d_model, config.d_ff, bias=False)
        self.wo = nn.Linear(config.d_ff, config.d_model, bias=False)
        self.dropout = nn.Dropout(config.dropout_rate)
        self.gelu_act = ACT2FN["gelu_new"]

    def forward(self, hidden_states):
        hidden_gelu = self.gelu_act(self.wi_0(hidden_states))
        hidden_linear = self.wi_1(hidden_states)
        hidden_states = hidden_gelu * hidden_linear
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.wo(hidden_states)
        return hidden_states
```

ç°åœ¨å¾ˆå®¹æ˜“çœ‹åˆ°`dropout`è°ƒç”¨ï¼Œä»¥åŠæ‰€æœ‰ä¹‹å‰çš„è°ƒç”¨ã€‚

ç”±äºæ£€æµ‹æ˜¯åœ¨å‰å‘`hook`ä¸­è¿›è¡Œçš„ï¼Œè¿™äº›æŠ¥å‘Šå°†ç«‹å³åœ¨æ¯ä¸ª`forward`è¿”å›åæ‰“å°å‡ºæ¥ã€‚

å›åˆ°å®Œæ•´çš„æŠ¥å‘Šï¼Œè¦é‡‡å–æªæ–½å¹¶è§£å†³é—®é¢˜ï¼Œæˆ‘ä»¬éœ€è¦å¾€å›çœ‹å‡ ä¸ª`frame`ï¼Œåœ¨é‚£é‡Œæ•°å­—å¼€å§‹ä¸Šå‡ï¼Œå¹¶ä¸”æœ€æœ‰å¯èƒ½åˆ‡æ¢åˆ°fp32æ¨¡å¼ä»¥ä¾¿åœ¨ä¹˜æ³•æˆ–æ±‚å’Œæ—¶æ•°å­—ä¸ä¼šæº¢å‡ºã€‚å½“ç„¶ï¼Œå¯èƒ½è¿˜æœ‰å…¶ä»–è§£å†³æ–¹æ¡ˆã€‚ä¾‹å¦‚ï¼Œå¦‚æœå¯ç”¨äº†`amp`ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å°†åŸå§‹`forward`ç§»åˆ°`helper wrapper`ä¸­åï¼Œæš‚æ—¶å…³é—­å®ƒï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
def _forward(self, hidden_states):
    hidden_gelu = self.gelu_act(self.wi_0(hidden_states))
    hidden_linear = self.wi_1(hidden_states)
    hidden_states = hidden_gelu * hidden_linear
    hidden_states = self.dropout(hidden_states)
    hidden_states = self.wo(hidden_states)
    return hidden_states


import torch


def forward(self, hidden_states):
    if torch.is_autocast_enabled():
        with torch.cuda.amp.autocast(enabled=False):
            return self._forward(hidden_states)
    else:
        return self._forward(hidden_states)
```

ç”±äºè‡ªåŠ¨æ£€æµ‹å™¨ä»…æŠ¥å‘Šå®Œæ•´`frame`çš„è¾“å…¥å’Œè¾“å‡ºï¼Œä¸€æ—¦çŸ¥é“åœ¨å“ªé‡ŒæŸ¥æ‰¾ï¼Œæ‚¨å¯èƒ½è¿˜å¸Œæœ›åˆ†æç‰¹å®š`forward`å‡½æ•°çš„ä¸­é—´é˜¶æ®µã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨`detect_overflow`è¾…åŠ©å‡½æ•°å°†æ£€æµ‹å™¨æ”¾åˆ°å¸Œæœ›çš„ä½ç½®ï¼Œä¾‹å¦‚ï¼š

```python
from debug_utils import detect_overflow


class T5LayerFF(nn.Module):
    [...]

    def forward(self, hidden_states):
        forwarded_states = self.layer_norm(hidden_states)
        detect_overflow(forwarded_states, "after layer_norm")
        forwarded_states = self.DenseReluDense(forwarded_states)
        detect_overflow(forwarded_states, "after DenseReluDense")
        return hidden_states + self.dropout(forwarded_states)
```

å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬æ·»åŠ äº†2ä¸ªæ£€æµ‹å™¨ï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥è·Ÿè¸ªæ˜¯å¦åœ¨`forwarded_states`ä¸­é—´çš„æŸä¸ªåœ°æ–¹æ£€æµ‹åˆ°äº†`inf`æˆ–`nan`ã€‚

å®é™…ä¸Šï¼Œæ£€æµ‹å™¨å·²ç»æŠ¥å‘Šäº†è¿™äº›ï¼Œå› ä¸ºä¸Šé¢ç¤ºä¾‹ä¸­çš„æ¯ä¸ªè°ƒç”¨éƒ½æ˜¯ä¸€ä¸ª`nn.Module`ï¼Œä½†å‡è®¾å¦‚æœæ‚¨æœ‰ä¸€äº›æœ¬åœ°çš„ç›´æ¥è®¡ç®—ï¼Œè¿™å°±æ˜¯æ‚¨å°†å¦‚ä½•æ‰§è¡Œçš„æ–¹å¼ã€‚

æ­¤å¤–ï¼Œå¦‚æœæ‚¨åœ¨è‡ªå·±çš„ä»£ç ä¸­å®ä¾‹åŒ–è°ƒè¯•å™¨ï¼Œæ‚¨å¯ä»¥è°ƒæ•´ä»å…¶é»˜è®¤æ‰“å°çš„`frame`æ•°ï¼Œä¾‹å¦‚ï¼š

```python
from transformers.debug_utils import DebugUnderflowOverflow

debug_overflow = DebugUnderflowOverflow(model, max_frames_to_save=100)
```

### ç‰¹å®šæ‰¹æ¬¡çš„ç»å¯¹æœ€å°å€¼å’Œæœ€å¤§å€¼è·Ÿè¸ª

å½“å…³é—­ä¸‹æº¢/ä¸Šæº¢æ£€æµ‹åŠŸèƒ½, åŒæ ·çš„è°ƒè¯•ç±»å¯ä»¥ç”¨äºæ‰¹å¤„ç†è·Ÿè¸ªã€‚

å‡è®¾æ‚¨æƒ³è¦ç›‘è§†ç»™å®šæ‰¹æ¬¡çš„æ¯ä¸ª`forward`è°ƒç”¨çš„æ‰€æœ‰æˆåˆ†çš„ç»å¯¹æœ€å°å€¼å’Œæœ€å¤§å€¼ï¼Œå¹¶ä¸”ä»…å¯¹æ‰¹æ¬¡1å’Œ3æ‰§è¡Œæ­¤æ“ä½œï¼Œæ‚¨å¯ä»¥è¿™æ ·å®ä¾‹åŒ–è¿™ä¸ªç±»ï¼š

```python
debug_overflow = DebugUnderflowOverflow(model, trace_batch_nums=[1, 3])
```

ç°åœ¨ï¼Œå®Œæ•´çš„æ‰¹æ¬¡1å’Œ3å°†ä»¥ä¸ä¸‹æº¢/ä¸Šæº¢æ£€æµ‹å™¨ç›¸åŒçš„æ ¼å¼è¿›è¡Œè·Ÿè¸ªã€‚

æ‰¹æ¬¡ä»0å¼€å§‹è®¡æ•°ã€‚

å¦‚æœæ‚¨çŸ¥é“ç¨‹åºåœ¨æŸä¸ªæ‰¹æ¬¡ç¼–å·ä¹‹åå¼€å§‹å‡ºç°é—®é¢˜ï¼Œé‚£ä¹ˆæ‚¨å¯ä»¥ç›´æ¥å¿«è¿›åˆ°è¯¥åŒºåŸŸã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªæˆªå–çš„é…ç½®ç¤ºä¾‹è¾“å‡ºï¼š

```
                  *** Starting batch number=1 ***
abs min  abs max  metadata
                  shared Embedding
1.01e-06 7.92e+02 weight
0.00e+00 2.47e+04 input[0]
5.36e-05 7.92e+02 output
[...]
                  decoder.dropout Dropout
1.60e-07 2.27e+01 input[0]
0.00e+00 2.52e+01 output
                  decoder T5Stack
     not a tensor output
                  lm_head Linear
1.01e-06 7.92e+02 weight
0.00e+00 1.11e+00 input[0]
6.06e-02 8.39e+01 output
                   T5ForConditionalGeneration
     not a tensor output

                  *** Starting batch number=3 ***
abs min  abs max  metadata
                  shared Embedding
1.01e-06 7.92e+02 weight
0.00e+00 2.78e+04 input[0]
5.36e-05 7.92e+02 output
[...]
```

åœ¨è¿™é‡Œï¼Œæ‚¨å°†è·å¾—å¤§é‡çš„`frame`è¢«`dump` - ä¸æ‚¨çš„æ¨¡å‹ä¸­çš„å‰å‘è°ƒç”¨ä¸€æ ·å¤šï¼Œå®ƒæœ‰å¯èƒ½ç¬¦åˆä¹Ÿå¯èƒ½ä¸ç¬¦åˆæ‚¨çš„è¦æ±‚ï¼Œä½†æœ‰æ—¶å¯¹äºè°ƒè¯•ç›®çš„æ¥è¯´ï¼Œå®ƒå¯èƒ½æ¯”æ­£å¸¸çš„è°ƒè¯•å™¨æ›´å®¹æ˜“ä½¿ç”¨ã€‚ä¾‹å¦‚ï¼Œå¦‚æœé—®é¢˜å¼€å§‹å‘ç”Ÿåœ¨æ‰¹æ¬¡å·150ä¸Šï¼Œæ‚¨å¯ä»¥`dump`æ‰¹æ¬¡149å’Œ150çš„è·Ÿè¸ªï¼Œå¹¶æ¯”è¾ƒæ•°å­—å¼€å§‹å‘æ•£çš„åœ°æ–¹ã€‚

ä½ è¿˜å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æŒ‡å®šåœæ­¢è®­ç»ƒçš„æ‰¹æ¬¡å·ï¼š

```python
debug_overflow = DebugUnderflowOverflow(model, trace_batch_nums=[1, 3], abort_after_batch_num=3)
```

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\fast_tokenizers.md
============================================================



# ä½¿ç”¨ ğŸ¤— Tokenizers ä¸­çš„åˆ†è¯å™¨

[`PreTrainedTokenizerFast`] ä¾èµ–äº [ğŸ¤— Tokenizers](https://huggingface.co/docs/tokenizers) åº“ã€‚ä» ğŸ¤— Tokenizers åº“è·å¾—çš„åˆ†è¯å™¨å¯ä»¥è¢«è½»æ¾åœ°åŠ è½½åˆ° ğŸ¤— Transformers ä¸­ã€‚

åœ¨äº†è§£å…·ä½“å†…å®¹ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆç”¨å‡ è¡Œä»£ç åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„åˆ†è¯å™¨ï¼š

```python
>>> from tokenizers import Tokenizer
>>> from tokenizers.models import BPE
>>> from tokenizers.trainers import BpeTrainer
>>> from tokenizers.pre_tokenizers import Whitespace

>>> tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
>>> trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])

>>> tokenizer.pre_tokenizer = Whitespace()
>>> files = [...]
>>> tokenizer.train(files, trainer)
```

ç°åœ¨ï¼Œæˆ‘ä»¬æ‹¥æœ‰äº†ä¸€ä¸ªé’ˆå¯¹æˆ‘ä»¬å®šä¹‰çš„æ–‡ä»¶è¿›è¡Œè®­ç»ƒçš„åˆ†è¯å™¨ã€‚æˆ‘ä»¬å¯ä»¥åœ¨å½“å‰è¿è¡Œæ—¶ä¸­ç»§ç»­ä½¿ç”¨å®ƒï¼Œæˆ–è€…å°†å…¶ä¿å­˜åˆ°ä¸€ä¸ª JSON æ–‡ä»¶ä»¥ä¾›å°†æ¥é‡å¤ä½¿ç”¨ã€‚

## ç›´æ¥ä»åˆ†è¯å™¨å¯¹è±¡åŠ è½½

è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åˆ©ç”¨ ğŸ¤— Transformers åº“ä¸­çš„è¿™ä¸ªåˆ†è¯å™¨å¯¹è±¡ã€‚[`PreTrainedTokenizerFast`] ç±»å…è®¸é€šè¿‡æ¥å—å·²å®ä¾‹åŒ–çš„ *tokenizer* å¯¹è±¡ä½œä¸ºå‚æ•°ï¼Œè¿›è¡Œè½»æ¾å®ä¾‹åŒ–ï¼š

```python
>>> from transformers import PreTrainedTokenizerFast

>>> fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)
```

ç°åœ¨å¯ä»¥ä½¿ç”¨è¿™ä¸ªå¯¹è±¡ï¼Œä½¿ç”¨ ğŸ¤— Transformers åˆ†è¯å™¨å…±äº«çš„æ‰€æœ‰æ–¹æ³•ï¼å‰å¾€[åˆ†è¯å™¨é¡µé¢](main_classes/tokenizer)äº†è§£æ›´å¤šä¿¡æ¯ã€‚

## ä» JSON æ–‡ä»¶åŠ è½½

ä¸ºäº†ä» JSON æ–‡ä»¶ä¸­åŠ è½½åˆ†è¯å™¨ï¼Œè®©æˆ‘ä»¬å…ˆä¿å­˜æˆ‘ä»¬çš„åˆ†è¯å™¨ï¼š

```python
>>> tokenizer.save("tokenizer.json")
```

æˆ‘ä»¬ä¿å­˜æ­¤æ–‡ä»¶çš„è·¯å¾„å¯ä»¥é€šè¿‡ `tokenizer_file` å‚æ•°ä¼ é€’ç»™ [`PreTrainedTokenizerFast`] åˆå§‹åŒ–æ–¹æ³•ï¼š

```python
>>> from transformers import PreTrainedTokenizerFast

>>> fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file="tokenizer.json")
```

ç°åœ¨å¯ä»¥ä½¿ç”¨è¿™ä¸ªå¯¹è±¡ï¼Œä½¿ç”¨ ğŸ¤— Transformers åˆ†è¯å™¨å…±äº«çš„æ‰€æœ‰æ–¹æ³•ï¼å‰å¾€[åˆ†è¯å™¨é¡µé¢](main_classes/tokenizer)äº†è§£æ›´å¤šä¿¡æ¯ã€‚

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\fsdp.md
============================================================


# å®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œ

[å®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œï¼ˆFSDPï¼‰](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)æ˜¯ä¸€ç§æ•°æ®å¹¶è¡Œæ–¹æ³•ï¼Œ
å®ƒå°†æ¨¡å‹çš„å‚æ•°ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€åœ¨å¯ç”¨ GPUï¼ˆä¹Ÿç§°ä¸º Worker æˆ– *rank*ï¼‰çš„æ•°é‡ä¸Šè¿›è¡Œåˆ†ç‰‡ã€‚
ä¸[åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œï¼ˆDDPï¼‰](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)ä¸åŒï¼Œ
FSDP å‡å°‘äº†å†…å­˜ä½¿ç”¨é‡ï¼Œå› ä¸ºæ¨¡å‹åœ¨æ¯ä¸ª GPU ä¸Šéƒ½è¢«å¤åˆ¶äº†ä¸€æ¬¡ã€‚è¿™å°±æé«˜äº† GPU å†…å­˜æ•ˆç‡ï¼Œ
ä½¿æ‚¨èƒ½å¤Ÿç”¨è¾ƒå°‘çš„ GPU è®­ç»ƒæ›´å¤§çš„æ¨¡å‹ã€‚FSDP å·²ç»é›†æˆåˆ° Accelerate ä¸­ï¼Œ
è¿™æ˜¯ä¸€ä¸ªç”¨äºåœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­è½»æ¾ç®¡ç†è®­ç»ƒçš„åº“ï¼Œè¿™æ„å‘³ç€å¯ä»¥ä» [`Trainer`] ç±»ä¸­è°ƒç”¨è¿™ä¸ªåº“ã€‚

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£… Accelerateï¼Œå¹¶ä¸”è‡³å°‘ä½¿ç”¨ PyTorch 2.1.0 æˆ–æ›´é«˜ç‰ˆæœ¬ã€‚

```bash
pip install accelerate
```

## FSDP é…ç½®

é¦–å…ˆï¼Œè¿è¡Œ [`accelerate config`](https://huggingface.co/docs/accelerate/package_reference/cli#accelerate-config)
å‘½ä»¤ä¸ºæ‚¨çš„è®­ç»ƒç¯å¢ƒåˆ›å»ºä¸€ä¸ªé…ç½®æ–‡ä»¶ã€‚Accelerate ä½¿ç”¨æ­¤é…ç½®æ–‡ä»¶æ ¹æ®æ‚¨åœ¨ `accelerate config`
ä¸­é€‰æ‹©çš„è®­ç»ƒé€‰é¡¹æ¥è‡ªåŠ¨æ­å»ºæ­£ç¡®çš„è®­ç»ƒç¯å¢ƒã€‚

```bash
accelerate config
```

è¿è¡Œ `accelerate config` æ—¶ï¼Œæ‚¨å°†è¢«æç¤ºä¸€ç³»åˆ—é€‰é¡¹æ¥é…ç½®è®­ç»ƒç¯å¢ƒã€‚
æœ¬èŠ‚æ¶µç›–äº†ä¸€äº›æœ€é‡è¦çš„ FSDP é€‰é¡¹ã€‚è¦äº†è§£æœ‰å…³å…¶ä»–å¯ç”¨çš„ FSDP é€‰é¡¹çš„æ›´å¤šä¿¡æ¯ï¼Œ
è¯·æŸ¥é˜… [fsdp_config](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.fsdp_config) å‚æ•°ã€‚

### åˆ†ç‰‡ç­–ç•¥

FSDP æä¾›äº†å¤šç§å¯é€‰æ‹©çš„åˆ†ç‰‡ç­–ç•¥ï¼š

- `FULL_SHARD` - å°†æ¨¡å‹å‚æ•°ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€è·¨ Worker è¿›è¡Œåˆ†ç‰‡ï¼›ä¸ºæ­¤é€‰é¡¹é€‰æ‹© `1`
- `SHARD_GRAD_OP`- å°†æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€è·¨ Worker è¿›è¡Œåˆ†ç‰‡ï¼›ä¸ºæ­¤é€‰é¡¹é€‰æ‹© `2`
- `NO_SHARD` - ä¸åˆ†ç‰‡ä»»ä½•å†…å®¹ï¼ˆè¿™ç­‰åŒäº DDPï¼‰ï¼›ä¸ºæ­¤é€‰é¡¹é€‰æ‹© `3`
- `HYBRID_SHARD` - åœ¨æ¯ä¸ª Worker ä¸­åˆ†ç‰‡æ¨¡å‹å‚æ•°ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€ï¼Œå…¶ä¸­æ¯ä¸ª Worker ä¹Ÿæœ‰å®Œæ•´å‰¯æœ¬ï¼›ä¸ºæ­¤é€‰é¡¹é€‰æ‹© `4`
- `HYBRID_SHARD_ZERO2` - åœ¨æ¯ä¸ª Worker ä¸­åˆ†ç‰‡æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€ï¼Œå…¶ä¸­æ¯ä¸ª Worker ä¹Ÿæœ‰å®Œæ•´å‰¯æœ¬ï¼›ä¸ºæ­¤é€‰é¡¹é€‰æ‹© `5`

è¿™ç”± `fsdp_sharding_strategy` æ ‡å¿—å¯ç”¨ã€‚

### CPU å¸è½½

å½“å‚æ•°å’Œæ¢¯åº¦åœ¨ä¸ä½¿ç”¨æ—¶å¯ä»¥å¸è½½åˆ° CPU ä¸Šï¼Œä»¥èŠ‚çœæ›´å¤š GPU å†…å­˜å¹¶å¸®åŠ©æ‚¨é€‚åº”å³ä½¿ FSDP ä¹Ÿä¸è¶³ä»¥å®¹çº³å¤§å‹æ¨¡å‹çš„æƒ…å†µã€‚
åœ¨è¿è¡Œ `accelerate config` æ—¶ï¼Œé€šè¿‡è®¾ç½® `fsdp_offload_params: true` æ¥å¯ç”¨æ­¤åŠŸèƒ½ã€‚

### åŒ…è£…ç­–ç•¥

FSDP æ˜¯é€šè¿‡åŒ…è£…ç½‘ç»œä¸­çš„æ¯ä¸ªå±‚æ¥åº”ç”¨çš„ã€‚é€šå¸¸ï¼ŒåŒ…è£…æ˜¯ä»¥åµŒå¥—æ–¹å¼åº”ç”¨çš„ï¼Œå…¶ä¸­å®Œæ•´çš„æƒé‡åœ¨æ¯æ¬¡å‰å‘ä¼ é€’åè¢«ä¸¢å¼ƒï¼Œ
ä»¥ä¾¿åœ¨ä¸‹ä¸€å±‚ä½¿ç”¨å†…å­˜ã€‚**è‡ªåŠ¨åŒ…è£…**ç­–ç•¥æ˜¯å®ç°è¿™ä¸€ç‚¹çš„æœ€ç®€å•æ–¹æ³•ï¼Œæ‚¨ä¸éœ€è¦æ›´æ”¹ä»»ä½•ä»£ç ã€‚
æ‚¨åº”è¯¥é€‰æ‹© `fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP` æ¥åŒ…è£…ä¸€ä¸ª Transformer å±‚ï¼Œ
å¹¶ä¸” `fsdp_transformer_layer_cls_to_wrap` æ¥æŒ‡å®šè¦åŒ…è£…çš„å±‚ï¼ˆä¾‹å¦‚ `BertLayer`ï¼‰ã€‚

å¦åˆ™ï¼Œæ‚¨å¯ä»¥é€‰æ‹©åŸºäºå¤§å°çš„åŒ…è£…ç­–ç•¥ï¼Œå…¶ä¸­å¦‚æœä¸€å±‚çš„å‚æ•°è¶…è¿‡ä¸€å®šæ•°é‡ï¼Œåˆ™åº”ç”¨ FSDPã€‚é€šè¿‡è®¾ç½®
`fsdp_wrap_policy: SIZE_BASED_WRAP` å’Œ `min_num_param` æ¥å¯ç”¨æ­¤åŠŸèƒ½ï¼Œå°†å‚æ•°è®¾ç½®ä¸ºæ‰€éœ€çš„å¤§å°é˜ˆå€¼ã€‚

### æ£€æŸ¥ç‚¹

åº”è¯¥ä½¿ç”¨ `fsdp_state_dict_type: SHARDED_STATE_DICT` æ¥ä¿å­˜ä¸­é—´æ£€æŸ¥ç‚¹ï¼Œ
å› ä¸ºåœ¨æ’å 0 ä¸Šä¿å­˜å®Œæ•´çŠ¶æ€å­—å…¸éœ€è¦å¾ˆé•¿æ—¶é—´ï¼Œé€šå¸¸ä¼šå¯¼è‡´ `NCCL Timeout` é”™è¯¯ï¼Œå› ä¸ºåœ¨å¹¿æ’­è¿‡ç¨‹ä¸­ä¼šæ— é™æœŸæŒ‚èµ·ã€‚
æ‚¨å¯ä»¥ä½¿ç”¨ [`~accelerate.Accelerator.load_state`] æ–¹æ³•åŠ è½½åˆ†ç‰‡çŠ¶æ€å­—å…¸ä»¥æ¢å¤è®­ç»ƒã€‚

```py
# åŒ…å«æ£€æŸ¥ç‚¹çš„ç›®å½•
accelerator.load_state("ckpt")
```

ç„¶è€Œï¼Œå½“è®­ç»ƒç»“æŸæ—¶ï¼Œæ‚¨å¸Œæœ›ä¿å­˜å®Œæ•´çŠ¶æ€å­—å…¸ï¼Œå› ä¸ºåˆ†ç‰‡çŠ¶æ€å­—å…¸ä»…ä¸ FSDP å…¼å®¹ã€‚

```py
if trainer.is_fsdp_enabled:
    trainer.accelerator.state.fsdp_plugin.set_state_dict_type("FULL_STATE_DICT")

trainer.save_model(script_args.output_dir)
```

### TPU

[PyTorch XLA](https://pytorch.org/xla/release/2.1/index.html) æ”¯æŒç”¨äº TPUs çš„ FSDP è®­ç»ƒï¼Œ
å¯ä»¥é€šè¿‡ä¿®æ”¹ç”± `accelerate config` ç”Ÿæˆçš„ FSDP é…ç½®æ–‡ä»¶æ¥å¯ç”¨ã€‚é™¤äº†ä¸Šé¢æŒ‡å®šçš„åˆ†ç‰‡ç­–ç•¥å’ŒåŒ…è£…é€‰é¡¹å¤–ï¼Œ
æ‚¨è¿˜å¯ä»¥å°†ä»¥ä¸‹å‚æ•°æ·»åŠ åˆ°æ–‡ä»¶ä¸­ã€‚

```yaml
xla: True # å¿…é¡»è®¾ç½®ä¸º True ä»¥å¯ç”¨ PyTorch/XLA
xla_fsdp_settings: # XLA ç‰¹å®šçš„ FSDP å‚æ•°
xla_fsdp_grad_ckpt: True # ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
```

[`xla_fsdp_settings`](https://github.com/pytorch/xla/blob/2e6e183e0724818f137c8135b34ef273dea33318/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py#L128)
å…è®¸æ‚¨é…ç½®ç”¨äº FSDP çš„é¢å¤– XLA ç‰¹å®šå‚æ•°ã€‚

## å¯åŠ¨è®­ç»ƒ

FSDP é…ç½®æ–‡ä»¶ç¤ºä¾‹å¦‚ä¸‹æ‰€ç¤ºï¼š

```yaml
compute_environment: LOCAL_MACHINE
debug: false
distributed_type: FSDP
downcast_bf16: "no"
fsdp_config:
  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
  fsdp_backward_prefetch_policy: BACKWARD_PRE
  fsdp_cpu_ram_efficient_loading: true
  fsdp_forward_prefetch: false
  fsdp_offload_params: true
  fsdp_sharding_strategy: 1
  fsdp_state_dict_type: SHARDED_STATE_DICT
  fsdp_sync_module_states: true
  fsdp_transformer_layer_cls_to_wrap: BertLayer
  fsdp_use_orig_params: true
machine_rank: 0
main_training_function: main
mixed_precision: bf16
num_machines: 1
num_processes: 2
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
```

è¦å¯åŠ¨è®­ç»ƒï¼Œè¯·è¿è¡Œ [`accelerate launch`](https://huggingface.co/docs/accelerate/package_reference/cli#accelerate-launch)
å‘½ä»¤ï¼Œå®ƒå°†è‡ªåŠ¨ä½¿ç”¨æ‚¨ä¹‹å‰ä½¿ç”¨ `accelerate config` åˆ›å»ºçš„é…ç½®æ–‡ä»¶ã€‚

```bash
accelerate launch my-trainer-script.py
```

```bash
accelerate launch --fsdp="full shard" --fsdp_config="path/to/fsdp_config/ my-trainer-script.py
```

## ä¸‹ä¸€æ­¥

FSDP åœ¨å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒæ–¹é¢æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å·¥å…·ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å¤šä¸ª GPU æˆ– TPUã€‚
é€šè¿‡åˆ†ç‰‡æ¨¡å‹å‚æ•°ã€ä¼˜åŒ–å™¨å’Œæ¢¯åº¦çŠ¶æ€ï¼Œç”šè‡³åœ¨å®ƒä»¬ä¸æ´»åŠ¨æ—¶å°†å…¶å¸è½½åˆ° CPU ä¸Šï¼Œ
FSDP å¯ä»¥å‡å°‘å¤§è§„æ¨¡è®­ç»ƒçš„é«˜æˆæœ¬ã€‚å¦‚æœæ‚¨å¸Œæœ›äº†è§£æ›´å¤šä¿¡æ¯ï¼Œä¸‹é¢çš„å†…å®¹å¯èƒ½ä¼šæœ‰æ‰€å¸®åŠ©ï¼š

- æ·±å…¥å‚è€ƒ Accelerate æŒ‡å—ï¼Œäº†è§£æœ‰å…³
  [FSDP](https://huggingface.co/docs/accelerate/usage_guides/fsdp)çš„æ›´å¤šä¿¡æ¯ã€‚
- é˜…è¯»[ä»‹ç» PyTorch å®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œï¼ˆFSDPï¼‰API](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) åšæ–‡ã€‚
- é˜…è¯»[ä½¿ç”¨ FSDP åœ¨äº‘ TPU ä¸Šæ‰©å±• PyTorch æ¨¡å‹](https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/)åšæ–‡ã€‚

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\generation_strategies.md
============================================================

# ç”Ÿæˆç­–ç•¥

è§£ç ç­–ç•¥å†³å®šäº†æ¨¡å‹åº”è¯¥å¦‚ä½•é€‰æ‹©ä¸‹ä¸€ä¸ªç”Ÿæˆçš„tokenã€‚æœ‰è®¸å¤šç±»å‹çš„è§£ç ç­–ç•¥ï¼Œé€‰æ‹©åˆé€‚çš„ç­–ç•¥å¯¹ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡æœ‰æ˜¾è‘—å½±å“ã€‚

æœ¬æŒ‡å—å°†å¸®åŠ©ä½ ç†è§£ Transformers ä¸­å¯ç”¨çš„ä¸åŒè§£ç ç­–ç•¥ï¼Œä»¥åŠå¦‚ä½•å’Œä½•æ—¶ä½¿ç”¨å®ƒä»¬ã€‚

## åŸºç¡€è§£ç æ–¹æ³•

è¿™äº›æ˜¯æˆç†Ÿçš„è§£ç æ–¹æ³•ï¼Œåº”è¯¥ä½œä¸ºæ–‡æœ¬ç”Ÿæˆä»»åŠ¡çš„èµ·ç‚¹ã€‚

### è´ªå©ªæœç´¢

è´ªå©ªæœç´¢æ˜¯é»˜è®¤çš„è§£ç ç­–ç•¥ã€‚å®ƒåœ¨æ¯ä¸€æ­¥é€‰æ‹©æœ€å¯èƒ½çš„ä¸‹ä¸€ä¸ªtokenã€‚é™¤éåœ¨ [`GenerationConfig`] ä¸­æŒ‡å®šï¼Œå¦åˆ™æ­¤ç­–ç•¥æœ€å¤šç”Ÿæˆ20ä¸ªæ–°tokenã€‚

è´ªå©ªæœç´¢é€‚ç”¨äºè¾“å‡ºç›¸å¯¹è¾ƒçŸ­ä¸”ä¸éœ€è¦åˆ›é€ æ€§çš„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå½“ç”Ÿæˆè¾ƒé•¿çš„åºåˆ—æ—¶ï¼Œå®ƒä¼šå¼€å§‹é‡å¤è‡ªå·±ï¼Œæ•ˆæœä¼šå˜å·®ã€‚

```py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from accelerate import Accelerator

device = Accelerator().device

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
inputs = tokenizer("Hugging Face is an open-source company", return_tensors="pt").to(device)

model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf", dtype=torch.float16).to(device)
# æ˜¾å¼è®¾ç½®ä¸ºé»˜è®¤é•¿åº¦ï¼Œå› ä¸ºLlama2çš„ç”Ÿæˆé•¿åº¦æ˜¯4096
outputs = model.generate(**inputs, max_new_tokens=20)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
'Hugging Face is an open-source company that provides a suite of tools and services for building, deploying, and maintaining natural language processing'
```

### é‡‡æ ·

é‡‡æ ·ï¼Œæˆ–å¤šé¡¹å¼é‡‡æ ·ï¼Œæ ¹æ®æ¨¡å‹æ•´ä¸ªè¯æ±‡è¡¨ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒéšæœºé€‰æ‹©ä¸€ä¸ªtokenï¼ˆè€Œä¸æ˜¯åƒè´ªå©ªæœç´¢é‚£æ ·é€‰æ‹©æœ€å¯èƒ½çš„tokenï¼‰ã€‚è¿™æ„å‘³ç€æ¯ä¸ªå…·æœ‰éé›¶æ¦‚ç‡çš„tokenéƒ½æœ‰æœºä¼šè¢«é€‰ä¸­ã€‚é‡‡æ ·ç­–ç•¥å¯ä»¥å‡å°‘é‡å¤ï¼Œå¹¶ç”Ÿæˆæ›´æœ‰åˆ›æ„å’Œå¤šæ ·æ€§çš„è¾“å‡ºã€‚

é€šè¿‡è®¾ç½® `do_sample=True` å’Œ `num_beams=1` æ¥å¯ç”¨å¤šé¡¹å¼é‡‡æ ·ã€‚

```py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from accelerate import Accelerator

device = Accelerator().device

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
inputs = tokenizer("Hugging Face is an open-source company", return_tensors="pt").to(device)

model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf", dtype=torch.float16).to(device)
# æ˜¾å¼è®¾ç½®ä¸º100ï¼Œå› ä¸ºLlama2çš„ç”Ÿæˆé•¿åº¦æ˜¯4096
outputs = model.generate(**inputs, max_new_tokens=50, do_sample=True, num_beams=1)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
'Hugging Face is an open-source company ğŸ¤—\nWe are open-source and believe that open-source is the best way to build technology. Our mission is to make AI accessible to everyone, and we believe that open-source is the best way to achieve that.'
```

### æŸæœç´¢

æŸæœç´¢åœ¨æ¯ä¸ªæ—¶é—´æ­¥ä¿æŒè·Ÿè¸ªå¤šä¸ªç”Ÿæˆçš„åºåˆ—ï¼ˆç§°ä¸º"æŸ"ï¼‰ã€‚åœ¨ä¸€å®šæ•°é‡çš„æ­¥éª¤åï¼Œå®ƒé€‰æ‹©*æ•´ä½“*æ¦‚ç‡æœ€é«˜çš„åºåˆ—ã€‚ä¸è´ªå©ªæœç´¢ä¸åŒï¼Œè¿™ç§ç­–ç•¥å¯ä»¥"å‘å‰çœ‹"ï¼Œå³ä½¿åˆå§‹tokençš„æ¦‚ç‡è¾ƒä½ï¼Œä¹Ÿèƒ½é€‰æ‹©æ•´ä½“æ¦‚ç‡æ›´é«˜çš„åºåˆ—ã€‚å®ƒæœ€é€‚åˆåŸºäºè¾“å…¥çš„ä»»åŠ¡ï¼Œå¦‚æè¿°å›¾åƒæˆ–è¯­éŸ³è¯†åˆ«ã€‚ä½ ä¹Ÿå¯ä»¥åœ¨æŸæœç´¢ä¸­ä½¿ç”¨ `do_sample=True` åœ¨æ¯ä¸€æ­¥è¿›è¡Œé‡‡æ ·ï¼Œä½†æŸæœç´¢ä»ä¼šåœ¨æ­¥éª¤ä¹‹é—´è´ªå©ªåœ°å‰ªé™¤ä½æ¦‚ç‡åºåˆ—ã€‚

> [!TIP]
> æŸ¥çœ‹ [æŸæœç´¢å¯è§†åŒ–å·¥å…·](https://huggingface.co/spaces/m-ric/beam_search_visualizer) æ¥äº†è§£æŸæœç´¢çš„å·¥ä½œåŸç†ã€‚

é€šè¿‡ `num_beams` å‚æ•°å¯ç”¨æŸæœç´¢ï¼ˆåº”å¤§äº1ï¼Œå¦åˆ™ç­‰åŒäºè´ªå©ªæœç´¢ï¼‰ã€‚

```py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from accelerate import Accelerator

device = Accelerator().device

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
inputs = tokenizer("Hugging Face is an open-source company", return_tensors="pt").to(device)

model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf", dtype=torch.float16).to(device)
# æ˜¾å¼è®¾ç½®ä¸º100ï¼Œå› ä¸ºLlama2çš„ç”Ÿæˆé•¿åº¦æ˜¯4096
outputs = model.generate(**inputs, max_new_tokens=50, num_beams=2)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
"['Hugging Face is an open-source company that develops and maintains the Hugging Face platform, which is a collection of tools and libraries for building and deploying natural language processing (NLP) models. Hugging Face was founded in 2018 by Thomas Wolf']"
```

## è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•

è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•å¯ä»¥å®ç°ç‰¹æ®Šçš„è¡Œä¸ºï¼Œä¾‹å¦‚ï¼š

- å¦‚æœæ¨¡å‹ä¸ç¡®å®šï¼Œè®©å®ƒç»§ç»­æ€è€ƒï¼›
- å¦‚æœæ¨¡å‹å¡ä½äº†ï¼Œå›æ»šç”Ÿæˆï¼›
- ä½¿ç”¨è‡ªå®šä¹‰é€»è¾‘å¤„ç†ç‰¹æ®Štokenï¼›
- ä½¿ç”¨ä¸“é—¨çš„ KV ç¼“å­˜ï¼›

æˆ‘ä»¬é€šè¿‡æ¨¡å‹ä»“åº“å¯ç”¨è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•ï¼Œå‡è®¾æœ‰ç‰¹å®šçš„æ¨¡å‹æ ‡ç­¾å’Œæ–‡ä»¶ç»“æ„ï¼ˆè§ä¸‹é¢çš„å°èŠ‚ï¼‰ã€‚æ­¤åŠŸèƒ½æ˜¯ [è‡ªå®šä¹‰å»ºæ¨¡ä»£ç ](./models.md#custom-models) çš„æ‰©å±•ï¼Œå› æ­¤åŒæ ·éœ€è¦è®¾ç½® `trust_remote_code=True`ã€‚

å¦‚æœæ¨¡å‹ä»“åº“åŒ…å«è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•ï¼Œæœ€ç®€å•çš„å°è¯•æ–¹æ³•æ˜¯åŠ è½½æ¨¡å‹å¹¶ç”¨å®ƒç”Ÿæˆï¼š

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

# `transformers-community/custom_generate_example` åŒ…å« `Qwen/Qwen2.5-0.5B-Instruct` çš„å‰¯æœ¬ï¼Œä½†
# å¸¦æœ‰è‡ªå®šä¹‰ç”Ÿæˆä»£ç  -> è°ƒç”¨ `generate` ä¼šä½¿ç”¨è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•ï¼
tokenizer = AutoTokenizer.from_pretrained("transformers-community/custom_generate_example")
model = AutoModelForCausalLM.from_pretrained(
    "transformers-community/custom_generate_example", device_map="auto", trust_remote_code=True
)

inputs = tokenizer(["The quick brown"], return_tensors="pt").to(model.device)
# è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•æ˜¯ä¸€ä¸ªæœ€å°çš„è´ªå©ªè§£ç å®ç°ã€‚å®ƒè¿˜ä¼šåœ¨è¿è¡Œæ—¶æ‰“å°è‡ªå®šä¹‰æ¶ˆæ¯ã€‚
gen_out = model.generate(**inputs)
# ä½ ç°åœ¨åº”è¯¥èƒ½çœ‹åˆ°å®ƒçš„è‡ªå®šä¹‰æ¶ˆæ¯ï¼Œ"âœ¨ using a custom generation method âœ¨"
print(tokenizer.batch_decode(gen_out, skip_special_tokens=True))
'The quick brown fox jumps over a lazy dog, and the dog is a type of animal. Is'
```

å…·æœ‰è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•çš„æ¨¡å‹ä»“åº“æœ‰ä¸€ä¸ªç‰¹æ®Šå±æ€§ï¼šå®ƒä»¬çš„ç”Ÿæˆæ–¹æ³•å¯ä»¥é€šè¿‡ [`~GenerationMixin.generate`] çš„ `custom_generate` å‚æ•°ä»**ä»»ä½•**æ¨¡å‹åŠ è½½ã€‚è¿™æ„å‘³ç€ä»»ä½•äººéƒ½å¯ä»¥åˆ›å»ºå’Œå…±äº«ä»–ä»¬çš„è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•ï¼Œä½¿å…¶èƒ½ä¸ä»»ä½• Transformers æ¨¡å‹ä¸€èµ·å·¥ä½œï¼Œè€Œæ— éœ€ç”¨æˆ·å®‰è£…é¢å¤–çš„ Python åŒ…ã€‚

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct")
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct", device_map="auto")

inputs = tokenizer(["The quick brown"], return_tensors="pt").to(model.device)
# `custom_generate` ç”¨ `transformers-community/custom_generate_example` ä¸­å®šä¹‰çš„
# è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•æ›¿æ¢åŸå§‹çš„ `generate`
gen_out = model.generate(**inputs, custom_generate="transformers-community/custom_generate_example", trust_remote_code=True)
print(tokenizer.batch_decode(gen_out, skip_special_tokens=True)[0])
'The quick brown fox jumps over a lazy dog, and the dog is a type of animal. Is'
```

ä½ åº”è¯¥é˜…è¯»åŒ…å«è‡ªå®šä¹‰ç”Ÿæˆç­–ç•¥çš„ä»“åº“çš„ `README.md` æ–‡ä»¶ï¼Œä»¥äº†è§£æ–°å‚æ•°å’Œè¾“å‡ºç±»å‹çš„å·®å¼‚ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰ã€‚å¦åˆ™ï¼Œä½ å¯ä»¥å‡è®¾å®ƒçš„å·¥ä½œæ–¹å¼ä¸åŸºç¡€ [`~GenerationMixin.generate`] æ–¹æ³•ç›¸åŒã€‚

> [!TIP]
> ä½ å¯ä»¥é€šè¿‡ [æœç´¢è‡ªå®šä¹‰æ ‡ç­¾](https://huggingface.co/models?other=custom_generate) `custom_generate` æ¥æ‰¾åˆ°æ‰€æœ‰è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•ã€‚

ä»¥ Hub ä»“åº“ [transformers-community/custom_generate_example](https://huggingface.co/transformers-community/custom_generate_example) ä¸ºä¾‹ã€‚`README.md` è¯´æ˜å®ƒæœ‰ä¸€ä¸ªé¢å¤–çš„è¾“å…¥å‚æ•° `left_padding`ï¼Œå¯ä»¥åœ¨æç¤ºè¯å‰æ·»åŠ è‹¥å¹²ä¸ªå¡«å……tokenã€‚

```py
gen_out = model.generate(
    **inputs, custom_generate="transformers-community/custom_generate_example", trust_remote_code=True, left_padding=5
)
print(tokenizer.batch_decode(gen_out)[0])
'<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>The quick brown fox jumps over the lazy dog.\n\nThe sentence "The quick'
```

å¦‚æœè‡ªå®šä¹‰æ–¹æ³•æœ‰ä½ çš„ç¯å¢ƒä¸æ»¡è¶³çš„å›ºå®š Python ä¾èµ–è¦æ±‚ï¼Œä½ ä¼šæ”¶åˆ°å…³äºç¼ºå°‘ä¾èµ–çš„å¼‚å¸¸ã€‚ä¾‹å¦‚ï¼Œ[transformers-community/custom_generate_bad_requirements](https://huggingface.co/transformers-community/custom_generate_bad_requirements) åœ¨å…¶ `custom_generate/requirements.txt` æ–‡ä»¶ä¸­å®šä¹‰äº†ä¸€ç»„ä¸å¯èƒ½çš„ä¾èµ–è¦æ±‚ï¼Œå¦‚æœä½ å°è¯•è¿è¡Œå®ƒï¼Œä¼šçœ‹åˆ°ä»¥ä¸‹é”™è¯¯æ¶ˆæ¯ã€‚

```text
ImportError: Missing requirements in your local environment for `transformers-community/custom_generate_bad_requirements`:
foo (installed: None)
bar==0.0.0 (installed: None)
torch>=99.0 (installed: 2.6.0)
```

ç›¸åº”åœ°æ›´æ–°ä½ çš„ Python ä¾èµ–å°†æ¶ˆé™¤æ­¤é”™è¯¯æ¶ˆæ¯ã€‚

### åˆ›å»ºè‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•

è¦åˆ›å»ºæ–°çš„ç”Ÿæˆæ–¹æ³•ï¼Œä½ éœ€è¦åˆ›å»ºä¸€ä¸ªæ–°çš„ [**æ¨¡å‹**](https://huggingface.co/new) ä»“åº“å¹¶å‘å…¶ä¸­æ¨é€ä¸€äº›æ–‡ä»¶ã€‚

1. ä½ ä¸ºå…¶è®¾è®¡ç”Ÿæˆæ–¹æ³•çš„æ¨¡å‹ã€‚
2. `custom_generate/generate.py`ï¼ŒåŒ…å«ä½ è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•çš„æ‰€æœ‰é€»è¾‘ã€‚
3. `custom_generate/requirements.txt`ï¼Œç”¨äºå¯é€‰åœ°æ·»åŠ æ–°çš„ Python ä¾èµ–å’Œ/æˆ–é”å®šç‰¹å®šç‰ˆæœ¬ä»¥æ­£ç¡®ä½¿ç”¨ä½ çš„æ–¹æ³•ã€‚
4. `README.md`ï¼Œä½ åº”è¯¥åœ¨è¿™é‡Œæ·»åŠ  `custom_generate` æ ‡ç­¾ï¼Œå¹¶è®°å½•ä½ è‡ªå®šä¹‰æ–¹æ³•çš„ä»»ä½•æ–°å‚æ•°æˆ–è¾“å‡ºç±»å‹å·®å¼‚ã€‚

æ·»åŠ æ‰€æœ‰å¿…éœ€æ–‡ä»¶åï¼Œä½ çš„ä»“åº“åº”è¯¥å¦‚ä¸‹æ‰€ç¤ºï¼š

```text
your_repo/
â”œâ”€â”€ README.md          # åŒ…å« 'custom_generate' æ ‡ç­¾
â”œâ”€â”€ config.json
â”œâ”€â”€ ...
â””â”€â”€ custom_generate/
    â”œâ”€â”€ generate.py
    â””â”€â”€ requirements.txt
```

#### æ·»åŠ åŸºç¡€æ¨¡å‹

ä½ è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•çš„èµ·ç‚¹æ˜¯ä¸€ä¸ªä¸å…¶ä»–ä»»ä½•æ¨¡å‹ä»“åº“ä¸€æ ·çš„æ¨¡å‹ä»“åº“ã€‚è¦æ·»åŠ åˆ°æ­¤ä»“åº“çš„æ¨¡å‹åº”è¯¥æ˜¯ä½ ä¸ºå…¶è®¾è®¡æ–¹æ³•çš„æ¨¡å‹ï¼Œå®ƒæ—¨åœ¨æˆä¸ºä¸€ä¸ªå¯å·¥ä½œçš„è‡ªåŒ…å«æ¨¡å‹-ç”Ÿæˆå¯¹çš„ä¸€éƒ¨åˆ†ã€‚å½“åŠ è½½æ­¤ä»“åº“ä¸­çš„æ¨¡å‹æ—¶ï¼Œä½ çš„è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•å°†è¦†ç›– `generate`ã€‚ä¸ç”¨æ‹…å¿ƒâ€”â€”å¦‚ä¸Šä¸€èŠ‚æ‰€è¿°ï¼Œä½ çš„ç”Ÿæˆæ–¹æ³•ä»ç„¶å¯ä»¥ä¸ä»»ä½•å…¶ä»– Transformers æ¨¡å‹ä¸€èµ·åŠ è½½ã€‚

å¦‚æœä½ åªæ˜¯æƒ³å¤åˆ¶ç°æœ‰æ¨¡å‹ï¼Œå¯ä»¥è¿™æ ·åšï¼š

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("source/model_repo")
model = AutoModelForCausalLM.from_pretrained("source/model_repo")
tokenizer.save_pretrained("your/generation_method", push_to_hub=True)
model.save_pretrained("your/generation_method", push_to_hub=True)
```

#### generate.py

è¿™æ˜¯ä½ ç”Ÿæˆæ–¹æ³•çš„æ ¸å¿ƒã€‚å®ƒ*å¿…é¡»*åŒ…å«ä¸€ä¸ªåä¸º `generate` çš„æ–¹æ³•ï¼Œä¸”è¯¥æ–¹æ³•*å¿…é¡»*ä»¥ `model` å‚æ•°ä½œä¸ºå…¶ç¬¬ä¸€ä¸ªå‚æ•°ã€‚`model` æ˜¯æ¨¡å‹å®ä¾‹ï¼Œè¿™æ„å‘³ç€ä½ å¯ä»¥è®¿é—®æ¨¡å‹ä¸­çš„æ‰€æœ‰å±æ€§å’Œæ–¹æ³•ï¼ŒåŒ…æ‹¬ [`GenerationMixin`] ä¸­å®šä¹‰çš„æ–¹æ³•ï¼ˆå¦‚åŸºç¡€ `generate` æ–¹æ³•ï¼‰ã€‚

> [!WARNING]
> `generate.py` å¿…é¡»æ”¾åœ¨åä¸º `custom_generate` çš„æ–‡ä»¶å¤¹ä¸­ï¼Œè€Œä¸æ˜¯ä»“åº“çš„æ ¹ç›®å½•ã€‚æ­¤åŠŸèƒ½çš„æ–‡ä»¶è·¯å¾„æ˜¯ç¡¬ç¼–ç çš„ã€‚

åœ¨åº•å±‚ï¼Œå½“åŸºç¡€ [`~GenerationMixin.generate`] æ–¹æ³•è¢«è°ƒç”¨å¹¶å¸¦æœ‰ `custom_generate` å‚æ•°æ—¶ï¼Œå®ƒé¦–å…ˆæ£€æŸ¥å…¶ Python ä¾èµ–è¦æ±‚ï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œç„¶ååœ¨ `generate.py` ä¸­å®šä½è‡ªå®šä¹‰ `generate` æ–¹æ³•ï¼Œæœ€åè°ƒç”¨è‡ªå®šä¹‰ `generate`ã€‚æ‰€æœ‰æ¥æ”¶åˆ°çš„å‚æ•°å’Œ `model` éƒ½ä¼šè½¬å‘åˆ°ä½ çš„è‡ªå®šä¹‰ `generate` æ–¹æ³•ï¼Œä½†ç”¨äºè§¦å‘è‡ªå®šä¹‰ç”Ÿæˆçš„å‚æ•°ï¼ˆ`trust_remote_code` å’Œ `custom_generate`ï¼‰é™¤å¤–ã€‚

è¿™æ„å‘³ç€ä½ çš„ `generate` å¯ä»¥æ··åˆä½¿ç”¨åŸå§‹å‚æ•°å’Œè‡ªå®šä¹‰å‚æ•°ï¼ˆä»¥åŠä¸åŒçš„è¾“å‡ºç±»å‹ï¼‰ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚

```py
import torch

def generate(model, input_ids, generation_config=None, left_padding=None, **kwargs):
    generation_config = generation_config or model.generation_config  # é»˜è®¤ä½¿ç”¨æ¨¡å‹ç”Ÿæˆé…ç½®
    cur_length = input_ids.shape[1]
    max_length = generation_config.max_length or cur_length + generation_config.max_new_tokens

    # è‡ªå®šä¹‰å‚æ•°ç¤ºä¾‹ï¼šåœ¨æç¤ºè¯å‰æ·»åŠ  `left_padding`ï¼ˆæ•´æ•°ï¼‰ä¸ªå¡«å……token
    if left_padding is not None:
        if not isinstance(left_padding, int) or left_padding < 0:
            raise ValueError(f"left_padding å¿…é¡»æ˜¯å¤§äº0çš„æ•´æ•°ï¼Œä½†å¾—åˆ°çš„æ˜¯ {left_padding}")

        pad_token = kwargs.pop("pad_token", None) or generation_config.pad_token_id or model.config.pad_token_id
        if pad_token is None:
            raise ValueError("pad_token æœªå®šä¹‰")
        batch_size = input_ids.shape[0]
        pad_tensor = torch.full(size=(batch_size, left_padding), fill_value=pad_token).to(input_ids.device)
        input_ids = torch.cat((pad_tensor, input_ids), dim=1)
        cur_length = input_ids.shape[1]

    # ç®€å•çš„è´ªå©ªè§£ç å¾ªç¯
    while cur_length < max_length:
        logits = model(input_ids).logits
        next_token_logits = logits[:, -1, :]
        next_tokens = torch.argmax(next_token_logits, dim=-1)
        input_ids = torch.cat((input_ids, next_tokens[:, None]), dim=-1)
        cur_length += 1

    return input_ids
```

éµå¾ªä»¥ä¸‹æ¨èåšæ³•ä»¥ç¡®ä¿ä½ çš„è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•æŒ‰é¢„æœŸå·¥ä½œã€‚

- éšæ„é‡ç”¨åŸå§‹ [`~GenerationMixin.generate`] ä¸­çš„éªŒè¯å’Œè¾“å…¥å‡†å¤‡é€»è¾‘ã€‚
- å¦‚æœåœ¨ `model` ä¸­ä½¿ç”¨ä»»ä½•ç§æœ‰æ–¹æ³•/å±æ€§ï¼Œè¯·åœ¨ requirements ä¸­å›ºå®š `transformers` ç‰ˆæœ¬ã€‚
- è€ƒè™‘æ·»åŠ æ¨¡å‹éªŒè¯ã€è¾“å…¥éªŒè¯ï¼Œç”šè‡³å•ç‹¬çš„æµ‹è¯•æ–‡ä»¶ï¼Œä»¥å¸®åŠ©ç”¨æˆ·åœ¨å…¶ç¯å¢ƒä¸­å¯¹ä½ çš„ä»£ç è¿›è¡Œå¥å…¨æ€§æ£€æŸ¥ã€‚

ä½ çš„è‡ªå®šä¹‰ `generate` æ–¹æ³•å¯ä»¥ä» `custom_generate` æ–‡ä»¶å¤¹ç›¸å¯¹å¯¼å…¥ä»£ç ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æœ‰ä¸€ä¸ª `utils.py` æ–‡ä»¶ï¼Œå¯ä»¥è¿™æ ·å¯¼å…¥ï¼š

```py
from .utils import some_function
```

ä»…æ”¯æŒä»åŒçº§ `custom_generate` æ–‡ä»¶å¤¹çš„ç›¸å¯¹å¯¼å…¥ã€‚çˆ¶çº§/å…„å¼Ÿæ–‡ä»¶å¤¹å¯¼å…¥æ— æ•ˆã€‚`custom_generate` å‚æ•°ä¹Ÿå¯ä»¥åœ¨æœ¬åœ°ä¸åŒ…å« `custom_generate` ç»“æ„çš„ä»»ä½•ç›®å½•ä¸€èµ·ä½¿ç”¨ã€‚è¿™æ˜¯å¼€å‘è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•çš„æ¨èå·¥ä½œæµç¨‹ã€‚

#### requirements.txt

ä½ å¯ä»¥åœ¨ `custom_generate` æ–‡ä»¶å¤¹å†…çš„ `requirements.txt` æ–‡ä»¶ä¸­å¯é€‰åœ°æŒ‡å®šé¢å¤–çš„ Python ä¾èµ–è¦æ±‚ã€‚è¿™äº›ä¼šåœ¨è¿è¡Œæ—¶æ£€æŸ¥ï¼Œå¦‚æœç¼ºå°‘ä¼šæŠ›å‡ºå¼‚å¸¸ï¼Œæç¤ºç”¨æˆ·ç›¸åº”åœ°æ›´æ–°å…¶ç¯å¢ƒã€‚

#### README.md

æ¨¡å‹ä»“åº“æ ¹ç›®å½•çš„ `README.md` é€šå¸¸æè¿°å…¶ä¸­çš„æ¨¡å‹ã€‚ç„¶è€Œï¼Œç”±äºä»“åº“çš„é‡ç‚¹æ˜¯è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•ï¼Œæˆ‘ä»¬å¼ºçƒˆå»ºè®®å°†å…¶é‡ç‚¹è½¬å‘æè¿°è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•ã€‚é™¤äº†æ–¹æ³•çš„æè¿°å¤–ï¼Œæˆ‘ä»¬å»ºè®®è®°å½•ä¸åŸå§‹ [`~GenerationMixin.generate`] çš„ä»»ä½•è¾“å…¥å’Œ/æˆ–è¾“å‡ºå·®å¼‚ã€‚è¿™æ ·ï¼Œç”¨æˆ·å¯ä»¥ä¸“æ³¨äºæ–°å†…å®¹ï¼Œå¹¶ä¾èµ– Transformers æ–‡æ¡£äº†è§£é€šç”¨å®ç°ç»†èŠ‚ã€‚

ä¸ºäº†ä¾¿äºå‘ç°ï¼Œæˆ‘ä»¬å¼ºçƒˆå»ºè®®ä½ ä¸ºä»“åº“æ·»åŠ  `custom_generate` æ ‡ç­¾ã€‚ä¸ºæ­¤ï¼Œä½ çš„ `README.md` æ–‡ä»¶é¡¶éƒ¨åº”å¦‚ä¸‹ä¾‹æ‰€ç¤ºã€‚æ¨é€æ–‡ä»¶åï¼Œä½ åº”è¯¥èƒ½åœ¨ä»“åº“ä¸­çœ‹åˆ°è¯¥æ ‡ç­¾ï¼

```text
---
library_name: transformers
tags:
  - custom_generate
---

(ä½ çš„ markdown å†…å®¹åœ¨è¿™é‡Œ)
```

æ¨èåšæ³•ï¼š

- è®°å½• [`~GenerationMixin.generate`] çš„è¾“å…¥å’Œè¾“å‡ºå·®å¼‚ã€‚
- æ·»åŠ è‡ªåŒ…å«çš„ç¤ºä¾‹ä»¥ä¾¿å¿«é€Ÿå®éªŒã€‚
- æè¿°è½¯æ€§è¦æ±‚ï¼Œä¾‹å¦‚è¯¥æ–¹æ³•æ˜¯å¦ä»…é€‚ç”¨äºç‰¹å®šæ¨¡å‹ç³»åˆ—ã€‚

### é‡ç”¨ `generate` çš„è¾“å…¥å‡†å¤‡

å¦‚æœä½ æ­£åœ¨æ·»åŠ æ–°çš„è§£ç å¾ªç¯ï¼Œä½ å¯èƒ½å¸Œæœ›ä¿ç•™ `generate` ä¸­çš„è¾“å…¥å‡†å¤‡ï¼ˆæ‰¹æ¬¡æ‰©å±•ã€æ³¨æ„åŠ›æ©ç ã€logits å¤„ç†å™¨ã€åœæ­¢æ¡ä»¶ç­‰ï¼‰ã€‚ä½ ä¹Ÿå¯ä»¥ä¼ é€’ä¸€ä¸ª**å¯è°ƒç”¨å¯¹è±¡**ç»™ `custom_generate`ï¼Œä»¥é‡ç”¨ [`~GenerationMixin.generate`] çš„å®Œæ•´å‡†å¤‡æµç¨‹ï¼ŒåŒæ—¶ä»…è¦†ç›–è§£ç å¾ªç¯ã€‚

```py
def custom_loop(model, input_ids, attention_mask, logits_processor, stopping_criteria, generation_config, **model_kwargs):
    next_tokens = input_ids
    while input_ids.shape[1] < stopping_criteria[0].max_length:
        logits = model(next_tokens, attention_mask=attention_mask, **model_kwargs).logits
        next_token_logits = logits_processor(input_ids, logits[:, -1, :])
        next_tokens = torch.argmax(next_token_logits, dim=-1)[:, None]
        input_ids = torch.cat((input_ids, next_tokens), dim=-1)
        attention_mask = torch.cat((attention_mask, torch.ones_like(next_tokens)), dim=-1)
    return input_ids

output = model.generate(
    **inputs,
    custom_generate=custom_loop,
    max_new_tokens=10,
)
```

> [!TIP]
> å¦‚æœä½ å‘å¸ƒ `custom_generate` ä»“åº“ï¼Œä½ çš„ `generate` å®ç°æœ¬èº«å¯ä»¥å®šä¹‰ä¸€ä¸ªå¯è°ƒç”¨å¯¹è±¡å¹¶å°†å…¶ä¼ é€’ç»™ `model.generate()`ã€‚è¿™è®©ä½ å¯ä»¥è‡ªå®šä¹‰è§£ç å¾ªç¯ï¼ŒåŒæ—¶ä»ç„¶å—ç›Šäº Transformers å†…ç½®çš„è¾“å…¥å‡†å¤‡é€»è¾‘ã€‚

### æŸ¥æ‰¾è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•

ä½ å¯ä»¥é€šè¿‡ [æœç´¢è‡ªå®šä¹‰æ ‡ç­¾](https://huggingface.co/models?other=custom_generate) `custom_generate` æ¥æ‰¾åˆ°æ‰€æœ‰è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³•ã€‚é™¤äº†æ ‡ç­¾å¤–ï¼Œæˆ‘ä»¬è¿˜ç­–åˆ’äº†ä¸¤ä¸ª `custom_generate` æ–¹æ³•é›†åˆï¼š

- [è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³• - ç¤¾åŒº](https://huggingface.co/collections/transformers-community/custom-generation-methods-community-6888fb1da0efbc592d3a8ab6) -- ç¤¾åŒºè´¡çŒ®çš„å¼ºå¤§æ–¹æ³•é›†åˆï¼›
- [è‡ªå®šä¹‰ç”Ÿæˆæ–¹æ³• - æ•™ç¨‹](https://huggingface.co/collections/transformers-community/custom-generation-methods-tutorials-6823589657a94940ea02cfec) -- ä»¥å‰æ˜¯ `transformers` ä¸€éƒ¨åˆ†çš„æ–¹æ³•çš„å‚è€ƒå®ç°é›†åˆï¼Œä»¥åŠ `custom_generate` çš„æ•™ç¨‹ã€‚

## èµ„æº

é˜…è¯» [å¦‚ä½•ç”Ÿæˆæ–‡æœ¬ï¼šä½¿ç”¨ä¸åŒçš„è§£ç æ–¹æ³•è¿›è¡Œ Transformers è¯­è¨€ç”Ÿæˆ](https://huggingface.co/blog/how-to-generate) åšå®¢æ–‡ç« ï¼Œäº†è§£å¸¸è§è§£ç ç­–ç•¥çš„å·¥ä½œåŸç†è§£é‡Šã€‚

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\gguf.md
============================================================

# GGUF å’Œ Transformers çš„äº¤äº’

GGUFæ–‡ä»¶æ ¼å¼ç”¨äºå­˜å‚¨æ¨¡å‹ï¼Œä»¥ä¾¿é€šè¿‡[GGML](https://github.com/ggerganov/ggml)å’Œå…¶ä»–ä¾èµ–å®ƒçš„åº“è¿›è¡Œæ¨ç†ï¼Œä¾‹å¦‚éå¸¸æµè¡Œçš„[llama.cpp](https://github.com/ggerganov/llama.cpp)æˆ–[whisper.cpp](https://github.com/ggerganov/whisper.cpp)ã€‚

è¯¥æ–‡ä»¶æ ¼å¼[ç”±æŠ±æŠ±è„¸æ”¯æŒ](https://huggingface.co/docs/hub/en/gguf)ï¼Œå¯ç”¨äºå¿«é€Ÿæ£€æŸ¥æ–‡ä»¶ä¸­å¼ é‡å’Œå…ƒæ•°æ®ã€‚

è¯¥æ–‡ä»¶æ ¼å¼æ˜¯ä¸€ç§â€œå•æ–‡ä»¶æ ¼å¼â€ï¼Œé€šå¸¸å•ä¸ªæ–‡ä»¶å°±åŒ…å«äº†é…ç½®å±æ€§ã€åˆ†è¯å™¨è¯æ±‡è¡¨å’Œå…¶ä»–å±æ€§ï¼ŒåŒæ—¶è¿˜æœ‰æ¨¡å‹ä¸­è¦åŠ è½½çš„æ‰€æœ‰å¼ é‡ã€‚è¿™äº›æ–‡ä»¶æ ¹æ®æ–‡ä»¶çš„é‡åŒ–ç±»å‹æœ‰ä¸åŒçš„æ ¼å¼ã€‚æˆ‘ä»¬åœ¨[è¿™é‡Œ](https://huggingface.co/docs/hub/en/gguf#quantization-types)è¿›è¡Œäº†ç®€è¦ä»‹ç»ã€‚

## åœ¨ Transformers ä¸­çš„æ”¯æŒ

æˆ‘ä»¬åœ¨ transformers ä¸­æ·»åŠ äº†åŠ è½½ gguf æ–‡ä»¶çš„åŠŸèƒ½ï¼Œè¿™æ ·å¯ä»¥å¯¹ GGUF æ¨¡å‹è¿›è¡Œè¿›ä¸€æ­¥çš„è®­ç»ƒæˆ–å¾®è°ƒï¼Œç„¶åå†å°†æ¨¡å‹è½¬æ¢å› GGUF æ ¼å¼ï¼Œä»¥ä¾¿åœ¨ ggml ç”Ÿæ€ç³»ç»Ÿä¸­ä½¿ç”¨ã€‚åŠ è½½æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬é¦–å…ˆå°†å…¶åé‡åŒ–ä¸º FP32ï¼Œç„¶åå†åŠ è½½æƒé‡ä»¥åœ¨ PyTorch ä¸­ä½¿ç”¨ã€‚

>    [!æ³¨æ„]
>    ç›®å‰è¿™ä¸ªåŠŸèƒ½è¿˜å¤„äºæ¢ç´¢é˜¶æ®µï¼Œæ¬¢è¿å¤§å®¶è´¡çŒ®åŠ›é‡ï¼Œä»¥ä¾¿åœ¨ä¸åŒé‡åŒ–ç±»å‹å’Œæ¨¡å‹æ¶æ„ä¹‹é—´æ›´å¥½åœ°å®Œå–„è¿™ä¸€åŠŸèƒ½ã€‚

ç›®å‰ï¼Œæ”¯æŒçš„æ¨¡å‹æ¶æ„å’Œé‡åŒ–ç±»å‹å¦‚ä¸‹ï¼š

### æ”¯æŒçš„é‡åŒ–ç±»å‹

æ ¹æ®åˆ†äº«åœ¨ Hub ä¸Šçš„è¾ƒä¸ºçƒ­é—¨çš„é‡åŒ–æ–‡ä»¶ï¼Œåˆæ­¥æ”¯æŒä»¥ä¸‹é‡åŒ–ç±»å‹ï¼š

- F32
- F16
- BF16
- Q4_0
- Q4_1
- Q5_0
- Q5_1
- Q8_0
- Q2_K
- Q3_K
- Q4_K
- Q5_K
- Q6_K
- IQ1_S
- IQ1_M
- IQ2_XXS
- IQ2_XS
- IQ2_S
- IQ3_XXS
- IQ3_S
- IQ4_XS
- IQ4_NL

>    [!æ³¨æ„]
>    ä¸ºäº†æ”¯æŒ gguf åé‡åŒ–ï¼Œéœ€è¦å®‰è£… `gguf>=0.10.0`ã€‚

### æ”¯æŒçš„æ¨¡å‹æ¶æ„

ç›®å‰æ”¯æŒä»¥ä¸‹åœ¨ Hub ä¸Šéå¸¸çƒ­é—¨çš„æ¨¡å‹æ¶æ„ï¼š

- LLaMa
- Mistral
- Qwen2
- Qwen2Moe
- Phi3
- Bloom
- Falcon
- StableLM
- GPT2
- Starcoder2

## ä½¿ç”¨ç¤ºä¾‹

ä¸ºäº†åœ¨`transformers`ä¸­åŠ è½½`gguf`æ–‡ä»¶ï¼Œä½ éœ€è¦åœ¨ `from_pretrained`æ–¹æ³•ä¸­ä¸ºåˆ†è¯å™¨å’Œæ¨¡å‹æŒ‡å®š `gguf_file`å‚æ•°ã€‚ä¸‹é¢æ˜¯ä»åŒä¸€ä¸ªæ–‡ä»¶ä¸­åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹çš„ç¤ºä¾‹ï¼š

```py
from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF"
filename = "tinyllama-1.1b-chat-v1.0.Q6_K.gguf"

tokenizer = AutoTokenizer.from_pretrained(model_id, gguf_file=filename)
model = AutoModelForCausalLM.from_pretrained(model_id, gguf_file=filename)
```

ç°åœ¨ï¼Œä½ å°±å·²ç»å¯ä»¥ç»“åˆ PyTorch ç”Ÿæ€ç³»ç»Ÿä¸­çš„ä¸€ç³»åˆ—å…¶ä»–å·¥å…·ï¼Œæ¥ä½¿ç”¨å®Œæ•´çš„ã€æœªé‡åŒ–çš„æ¨¡å‹äº†ã€‚

ä¸ºäº†å°†æ¨¡å‹è½¬æ¢å›`gguf`æ–‡ä»¶ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨`llama.cpp`ä¸­çš„[`convert-hf-to-gguf.py`æ–‡ä»¶](https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py)ã€‚

ä»¥ä¸‹æ˜¯å¦‚ä½•è¡¥å……ä¸Šé¢çš„è„šæœ¬ï¼Œä»¥ä¿å­˜æ¨¡å‹å¹¶å°†å…¶å¯¼å‡ºå› `gguf`çš„ç¤ºä¾‹ï¼š

```py
tokenizer.save_pretrained('directory')
model.save_pretrained('directory')

!python ${path_to_llama_cpp}/convert-hf-to-gguf.py ${directory}
```

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\hpo_train.md
============================================================



# ä½¿ç”¨Trainer APIè¿›è¡Œè¶…å‚æ•°æœç´¢

ğŸ¤— Transformersåº“æä¾›äº†ä¸€ä¸ªä¼˜åŒ–è¿‡çš„[`Trainer`]ç±»ï¼Œç”¨äºè®­ç»ƒğŸ¤— Transformersæ¨¡å‹ï¼Œç›¸æ¯”äºæ‰‹åŠ¨ç¼–å†™è‡ªå·±çš„è®­ç»ƒå¾ªç¯ï¼Œè¿™æ›´å®¹æ˜“å¼€å§‹è®­ç»ƒã€‚[`Trainer`]æä¾›äº†è¶…å‚æ•°æœç´¢çš„APIã€‚æœ¬æ–‡æ¡£å±•ç¤ºäº†å¦‚ä½•åœ¨ç¤ºä¾‹ä¸­å¯ç”¨å®ƒã€‚


## è¶…å‚æ•°æœç´¢åç«¯

[`Trainer`] ç›®å‰æ”¯æŒå››ç§è¶…å‚æ•°æœç´¢åç«¯ï¼š[optuna](https://optuna.org/)ï¼Œ[raytune](https://docs.ray.io/en/latest/tune/index.html)ï¼Œ[wandb](https://wandb.ai/site/sweeps)

åœ¨ä½¿ç”¨å®ƒä»¬ä¹‹å‰ï¼Œæ‚¨åº”è¯¥å…ˆå®‰è£…å®ƒä»¬ä½œä¸ºè¶…å‚æ•°æœç´¢åç«¯ã€‚

```bash
pip install optuna/wandb/ray[tune]
```

## å¦‚ä½•åœ¨ç¤ºä¾‹ä¸­å¯ç”¨è¶…å‚æ•°æœç´¢

å®šä¹‰è¶…å‚æ•°æœç´¢ç©ºé—´ï¼Œä¸åŒçš„åç«¯éœ€è¦ä¸åŒçš„æ ¼å¼ã€‚

å¯¹äºoptunaï¼Œè¯·å‚é˜…optuna [object_parameter](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/002_configurations.html#sphx-glr-tutorial-10-key-features-002-configurations-py)ï¼Œå®ƒç±»ä¼¼äºä»¥ä¸‹å†…å®¹ï¼š

```py
>>> def optuna_hp_space(trial):
...     return {
...         "learning_rate": trial.suggest_float("learning_rate", 1e-6, 1e-4, log=True),
...         "per_device_train_batch_size": trial.suggest_categorical("per_device_train_batch_size", [16, 32, 64, 128]),
...     }
```

Optunaæä¾›äº†å¤šç›®æ ‡HPOã€‚æ‚¨å¯ä»¥åœ¨`hyperparameter_search`ä¸­ä¼ é€’`direction`å‚æ•°ï¼Œå¹¶å®šä¹‰è‡ªå·±çš„`compute_objective`ä»¥è¿”å›å¤šä¸ªç›®æ ‡å€¼ã€‚åœ¨`hyperparameter_search`ä¸­å°†è¿”å›Pareto Frontï¼ˆ`list[BestRun]`ï¼‰ï¼Œæ‚¨åº”è¯¥å‚è€ƒ[test_trainer](https://github.com/huggingface/transformers/blob/main/tests/trainer/test_trainer.py)ä¸­çš„æµ‹è¯•ç”¨ä¾‹`TrainerHyperParameterMultiObjectOptunaIntegrationTest`ã€‚å®ƒç±»ä¼¼äºä»¥ä¸‹å†…å®¹ï¼š

```py
>>> best_trials = trainer.hyperparameter_search(
...     direction=["minimize", "maximize"],
...     backend="optuna",
...     hp_space=optuna_hp_space,
...     n_trials=20,
...     compute_objective=compute_objective,
... )
```

å¯¹äºraytuneï¼Œå¯ä»¥å‚è€ƒraytuneçš„[object_parameter](https://docs.ray.io/en/latest/tune/api/search_space.html)ï¼Œå®ƒç±»ä¼¼äºä»¥ä¸‹å†…å®¹ï¼š

```py
>>> def ray_hp_space(trial):
...     return {
...         "learning_rate": tune.loguniform(1e-6, 1e-4),
...         "per_device_train_batch_size": tune.choice([16, 32, 64, 128]),
...     }
```

å¯¹äºwandbï¼Œå¯ä»¥å‚è€ƒwandbçš„[object_parameter](https://docs.wandb.ai/guides/sweeps/configuration)ï¼Œå®ƒç±»ä¼¼äºä»¥ä¸‹å†…å®¹ï¼š

```py
>>> def wandb_hp_space(trial):
...     return {
...         "method": "random",
...         "metric": {"name": "objective", "goal": "minimize"},
...         "parameters": {
...             "learning_rate": {"distribution": "uniform", "min": 1e-6, "max": 1e-4},
...             "per_device_train_batch_size": {"values": [16, 32, 64, 128]},
...         },
...     }
```

å®šä¹‰ä¸€ä¸ª`model_init`å‡½æ•°å¹¶å°†å…¶ä¼ é€’ç»™[Trainer]ï¼Œä½œä¸ºç¤ºä¾‹ï¼š

```py
>>> def model_init(trial):
...     return AutoModelForSequenceClassification.from_pretrained(
...         model_args.model_name_or_path,
...         config=config,
...         cache_dir=model_args.cache_dir,
...         revision=model_args.model_revision,
...     )
```

ä½¿ç”¨ä½ çš„`model_init`å‡½æ•°ã€è®­ç»ƒå‚æ•°ã€è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†ä»¥åŠè¯„ä¼°å‡½æ•°åˆ›å»ºä¸€ä¸ª[`Trainer`]ã€‚

```py
>>> trainer = Trainer(
...     model=None,
...     args=training_args,
...     train_dataset=small_train_dataset,
...     eval_dataset=small_eval_dataset,
...     compute_metrics=compute_metrics,
...     processing_class=tokenizer,
...     model_init=model_init,
...     data_collator=data_collator,
... )
```

è°ƒç”¨è¶…å‚æ•°æœç´¢ï¼Œè·å–æœ€ä½³è¯•éªŒå‚æ•°ï¼Œåç«¯å¯ä»¥æ˜¯`"optuna"`/`"wandb"`/`"ray"`ã€‚æ–¹å‘å¯ä»¥æ˜¯`"minimize"`æˆ–`"maximize"`ï¼Œè¡¨ç¤ºæ˜¯å¦ä¼˜åŒ–æ›´å¤§æˆ–æ›´ä½çš„ç›®æ ‡ã€‚

æ‚¨å¯ä»¥å®šä¹‰è‡ªå·±çš„compute_objectiveå‡½æ•°ï¼Œå¦‚æœæ²¡æœ‰å®šä¹‰ï¼Œå°†è°ƒç”¨é»˜è®¤çš„compute_objectiveï¼Œå¹¶å°†è¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚f1ï¼‰ä¹‹å’Œä½œä¸ºç›®æ ‡å€¼è¿”å›ã€‚

```py
>>> best_trial = trainer.hyperparameter_search(
...     direction="maximize",
...     backend="optuna",
...     hp_space=optuna_hp_space,
...     n_trials=20,
...     compute_objective=compute_objective,
... )
```

## é’ˆå¯¹DDPå¾®è°ƒçš„è¶…å‚æ•°æœç´¢
ç›®å‰ï¼ŒOptunaå·²å¯ç”¨é’ˆå¯¹DDPçš„è¶…å‚æ•°æœç´¢ã€‚åªæœ‰rank-zeroè¿›ç¨‹ä¼šè¿›è¡Œè¶…å‚æ•°æœç´¢å¹¶å°†å‚æ•°ä¼ é€’ç»™å…¶ä»–è¿›ç¨‹ã€‚

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\index.md
============================================================



# ğŸ¤— Transformersç®€ä»‹

ä¸º [PyTorch](https://pytorch.org/) æ‰“é€ çš„å…ˆè¿›çš„æœºå™¨å­¦ä¹ å·¥å…·.

ğŸ¤— Transformers æä¾›äº†å¯ä»¥è½»æ¾åœ°ä¸‹è½½å¹¶ä¸”è®­ç»ƒå…ˆè¿›çš„é¢„è®­ç»ƒæ¨¡å‹çš„ API å’Œå·¥å…·ã€‚ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹å¯ä»¥å‡å°‘è®¡ç®—æ¶ˆè€—å’Œç¢³æ’æ”¾ï¼Œå¹¶ä¸”èŠ‚çœä»å¤´è®­ç»ƒæ‰€éœ€è¦çš„æ—¶é—´å’Œèµ„æºã€‚è¿™äº›æ¨¡å‹æ”¯æŒä¸åŒæ¨¡æ€ä¸­çš„å¸¸è§ä»»åŠ¡ï¼Œæ¯”å¦‚ï¼š

ğŸ“ **è‡ªç„¶è¯­è¨€å¤„ç†**ï¼šæ–‡æœ¬åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«ã€é—®ç­”ã€è¯­è¨€å»ºæ¨¡ã€æ‘˜è¦ã€ç¿»è¯‘ã€å¤šé¡¹é€‰æ‹©å’Œæ–‡æœ¬ç”Ÿæˆã€‚<br>
ğŸ–¼ï¸ **æœºå™¨è§†è§‰**ï¼šå›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œè¯­ä¹‰åˆ†å‰²ã€‚<br>
ğŸ—£ï¸ **éŸ³é¢‘**ï¼šè‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’ŒéŸ³é¢‘åˆ†ç±»ã€‚<br>
ğŸ™ **å¤šæ¨¡æ€**ï¼šè¡¨æ ¼é—®ç­”ã€å…‰å­¦å­—ç¬¦è¯†åˆ«ã€ä»æ‰«ææ–‡æ¡£æå–ä¿¡æ¯ã€è§†é¢‘åˆ†ç±»å’Œè§†è§‰é—®ç­”ã€‚

ğŸ¤— Transformers æ¨¡å‹å¯ä»¥è¢«å¯¼å‡ºä¸º ONNX å’Œ TorchScript æ ¼å¼ï¼Œç”¨äºåœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²ã€‚

é©¬ä¸ŠåŠ å…¥åœ¨ [Hub](https://huggingface.co/models)ã€[è®ºå›](https://discuss.huggingface.co/) æˆ–è€… [Discord](https://discord.com/invite/JfAtkvEtRb) ä¸Šæ­£åœ¨å¿«é€Ÿå‘å±•çš„ç¤¾åŒºå§ï¼

## å¦‚æœä½ éœ€è¦æ¥è‡ª Hugging Face å›¢é˜Ÿçš„ä¸ªæ€§åŒ–æ”¯æŒ

<a target="_blank" href="https://huggingface.co/support">
    <img alt="HuggingFace Expert Acceleration Program" src="https://cdn-media.huggingface.co/marketing/transformers/new-support-improved.png" style="width: 100%; max-width: 600px; border: 1px solid #eee; border-radius: 4px; box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);">
</a>

## ç›®å½•

è¿™ç¯‡æ–‡æ¡£ç”±ä»¥ä¸‹ 5 ä¸ªç« èŠ‚ç»„æˆï¼š

- **å¼€å§‹ä½¿ç”¨** åŒ…å«äº†åº“çš„å¿«é€Ÿä¸Šæ‰‹å’Œå®‰è£…è¯´æ˜ï¼Œä¾¿äºé…ç½®å’Œè¿è¡Œã€‚
- **æ•™ç¨‹** æ˜¯ä¸€ä¸ªåˆå­¦è€…å¼€å§‹çš„å¥½åœ°æ–¹ã€‚æœ¬ç« èŠ‚å°†å¸®åŠ©ä½ è·å¾—ä½ ä¼šç”¨åˆ°çš„ä½¿ç”¨è¿™ä¸ªåº“çš„åŸºæœ¬æŠ€èƒ½ã€‚
- **æ“ä½œæŒ‡å—** å‘ä½ å±•ç¤ºå¦‚ä½•å®ç°ä¸€ä¸ªç‰¹å®šç›®æ ‡ï¼Œæ¯”å¦‚ä¸ºè¯­è¨€å»ºæ¨¡å¾®è°ƒä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹æˆ–è€…å¦‚ä½•åˆ›é€ å¹¶åˆ†äº«ä¸ªæ€§åŒ–æ¨¡å‹ã€‚
- **æ¦‚å¿µæŒ‡å—** å¯¹ ğŸ¤— Transformers çš„æ¨¡å‹ï¼Œä»»åŠ¡å’Œè®¾è®¡ç†å¿µèƒŒåçš„åŸºæœ¬æ¦‚å¿µå’Œæ€æƒ³åšäº†æ›´å¤šçš„è®¨è®ºå’Œè§£é‡Šã€‚
- **API ä»‹ç»** æè¿°äº†æ‰€æœ‰çš„ç±»å’Œå‡½æ•°ï¼š

  - **ä¸»è¦ç±»åˆ«** è¯¦è¿°äº†é…ç½®ï¼ˆconfigurationï¼‰ã€æ¨¡å‹ï¼ˆmodelï¼‰ã€åˆ†è¯å™¨ï¼ˆtokenizerï¼‰å’Œæµæ°´çº¿ï¼ˆpipelineï¼‰è¿™å‡ ä¸ªæœ€é‡è¦çš„ç±»ã€‚
  - **æ¨¡å‹** è¯¦è¿°äº†åœ¨è¿™ä¸ªåº“ä¸­å’Œæ¯ä¸ªæ¨¡å‹å®ç°æœ‰å…³çš„ç±»å’Œå‡½æ•°ã€‚
  - **å†…éƒ¨å¸®åŠ©** è¯¦è¿°äº†å†…éƒ¨ä½¿ç”¨çš„å·¥å…·ç±»å’Œå‡½æ•°ã€‚

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\installation.md
============================================================

# å®‰è£…

ä¸ºä½ æ­£åœ¨ä½¿ç”¨çš„æ·±åº¦å­¦ä¹ æ¡†æ¶å®‰è£… ğŸ¤— Transformersã€è®¾ç½®ç¼“å­˜ï¼Œå¹¶é€‰æ‹©æ€§é…ç½® ğŸ¤— Transformers ä»¥ç¦»çº¿è¿è¡Œã€‚

ğŸ¤— Transformers å·²åœ¨ Python 3.9+ ä»¥åŠ PyTorch 2.2.0+ ä¸Šè¿›è¡Œæµ‹è¯•ã€‚é’ˆå¯¹ä½ ä½¿ç”¨çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œè¯·å‚ç…§ä»¥ä¸‹å®‰è£…è¯´æ˜è¿›è¡Œå®‰è£…ï¼š

* [PyTorch](https://pytorch.org/get-started/locally/) å®‰è£…è¯´æ˜ã€‚

## ä½¿ç”¨ pip å®‰è£…

ä½ åº”è¯¥ä½¿ç”¨ [è™šæ‹Ÿç¯å¢ƒ](https://docs.python.org/3/library/venv.html) å®‰è£… ğŸ¤— Transformersã€‚å¦‚æœä½ ä¸ç†Ÿæ‚‰ Python è™šæ‹Ÿç¯å¢ƒï¼Œè¯·æŸ¥çœ‹æ­¤ [æ•™ç¨‹](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/)ã€‚ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒï¼Œä½ å¯ä»¥è½»æ¾ç®¡ç†ä¸åŒé¡¹ç›®ï¼Œé¿å…ä¸åŒä¾èµ–é¡¹ä¹‹é—´çš„å…¼å®¹æ€§é—®é¢˜ã€‚

é¦–å…ˆï¼Œåœ¨é¡¹ç›®ç›®å½•ä¸­åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼š

```bash
python -m venv .env
```

åœ¨ Linux å’Œ MacOs ç³»ç»Ÿä¸­æ¿€æ´»è™šæ‹Ÿç¯å¢ƒï¼š

```bash
source .env/bin/activate
```
åœ¨ Windows ç³»ç»Ÿä¸­æ¿€æ´»è™šæ‹Ÿç¯å¢ƒï¼š

```bash
.env/Scripts/activate
```

ç°åœ¨ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£… ğŸ¤— Transformersï¼š

```bash
pip install transformers
```

è‹¥ä»…éœ€ CPU æ”¯æŒï¼Œå¯ä»¥ä½¿ç”¨å•è¡Œå‘½ä»¤æ–¹ä¾¿åœ°å®‰è£… ğŸ¤— Transformers å’Œæ·±åº¦å­¦ä¹ åº“ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£… ğŸ¤— Transformers å’Œ PyTorchï¼š

```bash
pip install 'transformers[torch]'
```

æœ€åï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤ä»¥æ£€æŸ¥ ğŸ¤— Transformers æ˜¯å¦å·²è¢«æ­£ç¡®å®‰è£…ã€‚è¯¥å‘½ä»¤å°†ä¸‹è½½ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼š

```bash
python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))"
```

ç„¶åæ‰“å°æ ‡ç­¾ä»¥åŠåˆ†æ•°ï¼š

```bash
[{'label': 'POSITIVE', 'score': 0.9998704791069031}]
```

## æºç å®‰è£…

ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ä»æºç å®‰è£… ğŸ¤— Transformersï¼š

```bash
pip install git+https://github.com/huggingface/transformers
```

æ­¤å‘½ä»¤ä¸‹è½½çš„æ˜¯æœ€æ–°çš„å‰æ²¿ `main` ç‰ˆæœ¬è€Œä¸æ˜¯æœ€æ–°çš„ `stable` ç‰ˆæœ¬ã€‚`main` ç‰ˆæœ¬é€‚ç”¨äºè·Ÿæœ€æ–°å¼€å‘ä¿æŒä¸€è‡´ã€‚ä¾‹å¦‚ï¼Œä¸Šæ¬¡æ­£å¼ç‰ˆå‘å¸ƒå¸¦æ¥çš„ bug è¢«ä¿®å¤äº†ï¼Œä½†æ–°ç‰ˆæœ¬å°šæœªè¢«æ¨å‡ºã€‚ä½†æ˜¯ï¼Œè¿™ä¹Ÿè¯´æ˜ `main` ç‰ˆæœ¬å¹¶ä¸ä¸€å®šæ€»æ˜¯ç¨³å®šçš„ã€‚æˆ‘ä»¬åŠªåŠ›ä¿æŒ `main` ç‰ˆæœ¬çš„å¯æ“ä½œæ€§ï¼Œå¤§å¤šæ•°é—®é¢˜é€šå¸¸åœ¨å‡ ä¸ªå°æ—¶æˆ–ä¸€å¤©ä»¥å†…å°±èƒ½è¢«è§£å†³ã€‚å¦‚æœä½ é‡åˆ°é—®é¢˜ï¼Œè¯·æä¸ª [Issue](https://github.com/huggingface/transformers/issues) ä»¥ä¾¿æˆ‘ä»¬èƒ½æ›´å¿«ä¿®å¤ã€‚

è¿è¡Œä»¥ä¸‹å‘½ä»¤ä»¥æ£€æŸ¥ ğŸ¤— Transformers æ˜¯å¦å·²è¢«æ­£ç¡®å®‰è£…ï¼š

```bash
python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))"
```

## å¯ç¼–è¾‘å®‰è£…

å¦‚æœä½ æœ‰ä¸‹åˆ—éœ€æ±‚ï¼Œéœ€è¦è¿›è¡Œå¯ç¼–è¾‘å®‰è£…ï¼š

* ä½¿ç”¨æºç çš„ `main` ç‰ˆæœ¬ã€‚
* ä¸º ğŸ¤— Transformers è´¡çŒ®ä»£ç ï¼Œéœ€è¦æµ‹è¯•ä»£ç ä¸­çš„æ›´æ”¹ã€‚

ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å…‹éš†ä»“åº“å¹¶å®‰è£… ğŸ¤— Transformersï¼š

```bash
git clone https://github.com/huggingface/transformers.git
cd transformers
pip install -e .
```

è¿™äº›å‘½ä»¤å°†ä¼šé“¾æ¥ä½ å…‹éš†çš„ä»“åº“ä»¥åŠä½ çš„ Python åº“è·¯å¾„ã€‚ç°åœ¨ï¼ŒPython ä¸ä»…ä¼šåœ¨æ­£å¸¸çš„åº“è·¯å¾„ä¸­æœç´¢åº“ï¼Œä¹Ÿä¼šåœ¨ä½ å…‹éš†åˆ°çš„æ–‡ä»¶å¤¹ä¸­è¿›è¡ŒæŸ¥æ‰¾ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ çš„ Python åŒ…é€šå¸¸æœ¬åº”å®‰è£…åœ¨ `~/anaconda3/envs/main/lib/python3.7/site-packages/` ç›®å½•ä¸­ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ Python ä¹Ÿä¼šæœç´¢ä½ å…‹éš†åˆ°çš„æ–‡ä»¶å¤¹ï¼š`~/transformers/`ã€‚

<Tip warning={true}>

å¦‚æœä½ æƒ³ç»§ç»­ä½¿ç”¨è¿™ä¸ªåº“ï¼Œå¿…é¡»ä¿ç•™ `transformers` æ–‡ä»¶å¤¹ã€‚

</Tip>

ç°åœ¨ï¼Œä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼Œå°†ä½ å…‹éš†çš„ ğŸ¤— Transformers åº“è½»æ¾æ›´æ–°è‡³æœ€æ–°ç‰ˆæœ¬ï¼š

```bash
cd ~/transformers/
git pull
```

ä½ çš„ Python ç¯å¢ƒå°†åœ¨ä¸‹æ¬¡è¿è¡Œæ—¶æ‰¾åˆ° `main` ç‰ˆæœ¬çš„ ğŸ¤— Transformersã€‚

## ä½¿ç”¨ conda å®‰è£…

ä» conda çš„ `conda-forge` é¢‘é“å®‰è£…ï¼š

```bash
conda install conda-forge::transformers
```

## ç¼“å­˜è®¾ç½®

é¢„è®­ç»ƒæ¨¡å‹ä¼šè¢«ä¸‹è½½å¹¶æœ¬åœ°ç¼“å­˜åˆ° `~/.cache/huggingface/hub`ã€‚è¿™æ˜¯ç”±ç¯å¢ƒå˜é‡ `HF_HUB_CACHE` æŒ‡å®šçš„é»˜è®¤ç›®å½•ã€‚åœ¨ Windows ä¸Šï¼Œé»˜è®¤ç›®å½•ä¸º `C:\Users\username\.cache\huggingface\hub`ã€‚ä½ å¯ä»¥æŒ‰ç…§ä¸åŒä¼˜å…ˆçº§æ”¹å˜ä¸‹è¿°ç¯å¢ƒå˜é‡ï¼Œä»¥æŒ‡å®šä¸åŒçš„ç¼“å­˜ç›®å½•ã€‚

1. ç¯å¢ƒå˜é‡ï¼ˆé»˜è®¤ï¼‰: `HF_HUB_CACHE`ã€‚
2. ç¯å¢ƒå˜é‡ `HF_HOME`ã€‚
3. ç¯å¢ƒå˜é‡ `XDG_CACHE_HOME` + `/huggingface`ã€‚

## ç¦»çº¿æ¨¡å¼

ğŸ¤— Transformers å¯ä»¥ä»…ä½¿ç”¨æœ¬åœ°æ–‡ä»¶åœ¨é˜²ç«å¢™æˆ–ç¦»çº¿ç¯å¢ƒä¸­è¿è¡Œã€‚è®¾ç½®ç¯å¢ƒå˜é‡ `HF_HUB_OFFLINE=1` ä»¥å¯ç”¨è¯¥è¡Œä¸ºã€‚

<Tip>

é€šè¿‡è®¾ç½®ç¯å¢ƒå˜é‡ `HF_DATASETS_OFFLINE=1` å°† [ğŸ¤— Datasets](https://huggingface.co/docs/datasets/) æ·»åŠ è‡³ä½ çš„ç¦»çº¿è®­ç»ƒå·¥ä½œæµç¨‹ä¸­ã€‚

</Tip>

ä¾‹å¦‚ï¼Œä½ é€šå¸¸ä¼šä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯¹å¤–éƒ¨å®ä¾‹è¿›è¡Œé˜²ç«å¢™ä¿æŠ¤çš„çš„æ™®é€šç½‘ç»œä¸Šè¿è¡Œç¨‹åºï¼š

```bash
python examples/pytorch/translation/run_translation.py --model_name_or_path google-t5/t5-small --dataset_name wmt16 --dataset_config ro-en ...
```

åœ¨ç¦»çº¿ç¯å¢ƒä¸­è¿è¡Œç›¸åŒçš„ç¨‹åºï¼š

```bash
HF_DATASETS_OFFLINE=1 HF_HUB_OFFLINE=1 \
python examples/pytorch/translation/run_translation.py --model_name_or_path google-t5/t5-small --dataset_name wmt16 --dataset_config ro-en ...
```

ç°åœ¨è„šæœ¬å¯ä»¥åº”è¯¥æ­£å¸¸è¿è¡Œï¼Œè€Œæ— éœ€æŒ‚èµ·æˆ–ç­‰å¾…è¶…æ—¶ï¼Œå› ä¸ºå®ƒçŸ¥é“åªåº”æŸ¥æ‰¾æœ¬åœ°æ–‡ä»¶ã€‚

### è·å–ç¦»çº¿æ—¶ä½¿ç”¨çš„æ¨¡å‹å’Œåˆ†è¯å™¨

å¦ä¸€ç§ç¦»çº¿æ—¶ä½¿ç”¨ ğŸ¤— Transformers çš„æ–¹æ³•æ˜¯é¢„å…ˆä¸‹è½½å¥½æ–‡ä»¶ï¼Œç„¶ååœ¨éœ€è¦ç¦»çº¿ä½¿ç”¨æ—¶æŒ‡å‘å®ƒä»¬çš„ç¦»çº¿è·¯å¾„ã€‚æœ‰ä¸‰ç§å®ç°çš„æ–¹æ³•ï¼š

* å•å‡» [Model Hub](https://huggingface.co/models) ç”¨æˆ·ç•Œé¢ä¸Šçš„ â†“ å›¾æ ‡ä¸‹è½½æ–‡ä»¶ã€‚

    ![ä¸‹è½½å›¾æ ‡](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/download-icon.png)

* ä½¿ç”¨ [`PreTrainedModel.from_pretrained`] å’Œ [`PreTrainedModel.save_pretrained`] å·¥ä½œæµç¨‹ï¼š

    1. é¢„å…ˆä½¿ç”¨ [`PreTrainedModel.from_pretrained`] ä¸‹è½½æ–‡ä»¶ï¼š

    ```py
    >>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

    >>> tokenizer = AutoTokenizer.from_pretrained("bigscience/T0_3B")
    >>> model = AutoModelForSeq2SeqLM.from_pretrained("bigscience/T0_3B")
    ```

    2. ä½¿ç”¨ [`PreTrainedModel.save_pretrained`] å°†æ–‡ä»¶ä¿å­˜è‡³æŒ‡å®šç›®å½•ï¼š

    ```py
    >>> tokenizer.save_pretrained("./your/path/bigscience_t0")
    >>> model.save_pretrained("./your/path/bigscience_t0")
    ```

    3. ç°åœ¨ï¼Œä½ å¯ä»¥åœ¨ç¦»çº¿æ—¶ä»æŒ‡å®šç›®å½•ä½¿ç”¨ [`PreTrainedModel.from_pretrained`] é‡æ–°åŠ è½½ä½ çš„æ–‡ä»¶ï¼š

    ```py
    >>> tokenizer = AutoTokenizer.from_pretrained("./your/path/bigscience_t0")
    >>> model = AutoModel.from_pretrained("./your/path/bigscience_t0")
    ```

* ä½¿ç”¨ä»£ç ç”¨ [huggingface_hub](https://github.com/huggingface/huggingface_hub/tree/main/src/huggingface_hub) åº“ä¸‹è½½æ–‡ä»¶ï¼š

    1. åœ¨ä½ çš„è™šæ‹Ÿç¯å¢ƒä¸­å®‰è£… `huggingface_hub` åº“ï¼š

    ```bash
    python -m pip install huggingface_hub
    ```

    2. ä½¿ç”¨ [`hf_hub_download`](https://huggingface.co/docs/hub/adding-a-library#download-files-from-the-hub) å‡½æ•°å°†æ–‡ä»¶ä¸‹è½½åˆ°æŒ‡å®šè·¯å¾„ã€‚ä¾‹å¦‚ï¼Œä»¥ä¸‹å‘½ä»¤å°† `config.json` æ–‡ä»¶ä» [T0](https://huggingface.co/bigscience/T0_3B) æ¨¡å‹ä¸‹è½½è‡³ä½ æƒ³è¦çš„è·¯å¾„ï¼š

    ```py
    >>> from huggingface_hub import hf_hub_download

    >>> hf_hub_download(repo_id="bigscience/T0_3B", filename="config.json", cache_dir="./your/path/bigscience_t0")
    ```

ä¸‹è½½å®Œæ–‡ä»¶å¹¶åœ¨æœ¬åœ°ç¼“å­˜åï¼ŒæŒ‡å®šå…¶æœ¬åœ°è·¯å¾„ä»¥åŠ è½½å’Œä½¿ç”¨è¯¥æ¨¡å‹ï¼š

```py
>>> from transformers import AutoConfig

>>> config = AutoConfig.from_pretrained("./your/path/bigscience_t0/config.json")
```

<Tip>

è¯·å‚é˜… [å¦‚ä½•ä» Hub ä¸‹è½½æ–‡ä»¶](https://huggingface.co/docs/hub/how-to-downstream) éƒ¨åˆ†ï¼Œè·å–æœ‰å…³ä¸‹è½½å­˜å‚¨åœ¨ Hub ä¸Šæ–‡ä»¶çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚

</Tip>

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\llm_tutorial.md
============================================================




## ä½¿ç”¨LLMsè¿›è¡Œç”Ÿæˆ

[[open-in-colab]]

LLMsï¼Œå³å¤§è¯­è¨€æ¨¡å‹ï¼Œæ˜¯æ–‡æœ¬ç”ŸæˆèƒŒåçš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚ç®€å•æ¥è¯´ï¼Œå®ƒä»¬åŒ…å«ç»è¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒçš„transformeræ¨¡å‹ï¼Œç”¨äºæ ¹æ®ç»™å®šçš„è¾“å…¥æ–‡æœ¬é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼ˆæˆ–æ›´å‡†ç¡®åœ°è¯´ï¼Œä¸‹ä¸€ä¸ª`token`ï¼‰ã€‚ç”±äºå®ƒä»¬ä¸€æ¬¡åªé¢„æµ‹ä¸€ä¸ª`token`ï¼Œå› æ­¤é™¤äº†è°ƒç”¨æ¨¡å‹ä¹‹å¤–ï¼Œæ‚¨éœ€è¦æ‰§è¡Œæ›´å¤æ‚çš„æ“ä½œæ¥ç”Ÿæˆæ–°çš„å¥å­â€”â€”æ‚¨éœ€è¦è¿›è¡Œè‡ªå›å½’ç”Ÿæˆã€‚

è‡ªå›å½’ç”Ÿæˆæ˜¯åœ¨ç»™å®šä¸€äº›åˆå§‹è¾“å…¥ï¼Œé€šè¿‡è¿­ä»£è°ƒç”¨æ¨¡å‹åŠå…¶è‡ªèº«çš„ç”Ÿæˆè¾“å‡ºæ¥ç”Ÿæˆæ–‡æœ¬çš„æ¨ç†è¿‡ç¨‹ã€‚åœ¨ğŸ¤— Transformersä¸­ï¼Œè¿™ç”±[`~generation.GenerationMixin.generate`]æ–¹æ³•å¤„ç†ï¼Œæ‰€æœ‰å…·æœ‰ç”Ÿæˆèƒ½åŠ›çš„æ¨¡å‹éƒ½å¯ä»¥ä½¿ç”¨è¯¥æ–¹æ³•ã€‚

æœ¬æ•™ç¨‹å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ï¼š

* ä½¿ç”¨LLMç”Ÿæˆæ–‡æœ¬
* é¿å…å¸¸è§çš„é™·é˜±
* å¸®åŠ©æ‚¨å……åˆ†åˆ©ç”¨LLMä¸‹ä¸€æ­¥æŒ‡å¯¼

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š


```bash
pip install transformers bitsandbytes>=0.39.0 -q
```


## ç”Ÿæˆæ–‡æœ¬

ä¸€ä¸ªç”¨äº[å› æœè¯­è¨€å»ºæ¨¡](tasks/language_modeling)è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼Œå°†æ–‡æœ¬`tokens`åºåˆ—ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›ä¸‹ä¸€ä¸ª`token`çš„æ¦‚ç‡åˆ†å¸ƒã€‚


<!-- [GIF 1 -- FWD PASS] -->
<figure class="image table text-center m-0 w-full">
    <video
        style="max-width: 90%; margin: auto;"
        autoplay loop muted playsinline
        src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_1_1080p.mov"
    ></video>
    <figcaption>"LLMçš„å‰å‘ä¼ é€’"</figcaption>
</figure>

ä½¿ç”¨LLMè¿›è¡Œè‡ªå›å½’ç”Ÿæˆçš„ä¸€ä¸ªå…³é”®æ–¹é¢æ˜¯å¦‚ä½•ä»è¿™ä¸ªæ¦‚ç‡åˆ†å¸ƒä¸­é€‰æ‹©ä¸‹ä¸€ä¸ª`token`ã€‚è¿™ä¸ªæ­¥éª¤å¯ä»¥éšæ„è¿›è¡Œï¼Œåªè¦æœ€ç»ˆå¾—åˆ°ä¸‹ä¸€ä¸ªè¿­ä»£çš„`token`ã€‚è¿™æ„å‘³ç€å¯ä»¥ç®€å•çš„ä»æ¦‚ç‡åˆ†å¸ƒä¸­é€‰æ‹©æœ€å¯èƒ½çš„`token`ï¼Œä¹Ÿå¯ä»¥å¤æ‚çš„åœ¨å¯¹ç»“æœåˆ†å¸ƒè¿›è¡Œé‡‡æ ·ä¹‹å‰åº”ç”¨å¤šç§å˜æ¢ï¼Œè¿™å–å†³äºä½ çš„éœ€æ±‚ã€‚

<!-- [GIF 2 -- TEXT GENERATION] -->
<figure class="image table text-center m-0 w-full">
    <video
        style="max-width: 90%; margin: auto;"
        autoplay loop muted playsinline
        src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_2_1080p.mov"
    ></video>
    <figcaption>"è‡ªå›å½’ç”Ÿæˆè¿­ä»£åœ°ä»æ¦‚ç‡åˆ†å¸ƒä¸­é€‰æ‹©ä¸‹ä¸€ä¸ªtokenä»¥ç”Ÿæˆæ–‡æœ¬"</figcaption>
</figure>

ä¸Šè¿°è¿‡ç¨‹æ˜¯è¿­ä»£é‡å¤çš„ï¼Œç›´åˆ°è¾¾åˆ°æŸä¸ªåœæ­¢æ¡ä»¶ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œåœæ­¢æ¡ä»¶ç”±æ¨¡å‹å†³å®šï¼Œè¯¥æ¨¡å‹åº”å­¦ä¼šåœ¨ä½•æ—¶è¾“å‡ºä¸€ä¸ªç»“æŸåºåˆ—ï¼ˆ`EOS`ï¼‰æ ‡è®°ã€‚å¦‚æœä¸æ˜¯è¿™ç§æƒ…å†µï¼Œç”Ÿæˆå°†åœ¨è¾¾åˆ°æŸä¸ªé¢„å®šä¹‰çš„æœ€å¤§é•¿åº¦æ—¶åœæ­¢ã€‚

æ­£ç¡®è®¾ç½®`token`é€‰æ‹©æ­¥éª¤å’Œåœæ­¢æ¡ä»¶å¯¹äºè®©ä½ çš„æ¨¡å‹æŒ‰ç…§é¢„æœŸçš„æ–¹å¼æ‰§è¡Œä»»åŠ¡è‡³å…³é‡è¦ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬ä¸ºæ¯ä¸ªæ¨¡å‹éƒ½æœ‰ä¸€ä¸ª[~generation.GenerationConfig]æ–‡ä»¶ï¼Œå®ƒåŒ…å«ä¸€ä¸ªæ•ˆæœä¸é”™çš„é»˜è®¤ç”Ÿæˆå‚æ•°é…ç½®ï¼Œå¹¶ä¸æ‚¨æ¨¡å‹ä¸€èµ·åŠ è½½ã€‚

è®©æˆ‘ä»¬è°ˆè°ˆä»£ç ï¼

<Tip>

å¦‚æœæ‚¨å¯¹åŸºæœ¬çš„LLMä½¿ç”¨æ„Ÿå…´è¶£ï¼Œæˆ‘ä»¬é«˜çº§çš„[`Pipeline`](pipeline_tutorial)æ¥å£æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ã€‚ç„¶è€Œï¼ŒLLMsé€šå¸¸éœ€è¦åƒ`quantization`å’Œ`tokené€‰æ‹©æ­¥éª¤çš„ç²¾ç»†æ§åˆ¶`ç­‰é«˜çº§åŠŸèƒ½ï¼Œè¿™æœ€å¥½é€šè¿‡[`~generation.GenerationMixin.generate`]æ¥å®Œæˆã€‚ä½¿ç”¨LLMè¿›è¡Œè‡ªå›å½’ç”Ÿæˆä¹Ÿæ˜¯èµ„æºå¯†é›†å‹çš„æ“ä½œï¼Œåº”è¯¥åœ¨GPUä¸Šæ‰§è¡Œä»¥è·å¾—è¶³å¤Ÿçš„ååé‡ã€‚

</Tip>

é¦–å…ˆï¼Œæ‚¨éœ€è¦åŠ è½½æ¨¡å‹ã€‚

```py
>>> from transformers import AutoModelForCausalLM, BitsAndBytesConfig

>>> model = AutoModelForCausalLM.from_pretrained(
...     "mistralai/Mistral-7B-v0.1", device_map="auto", quantization_config=BitsAndBytesConfig(load_in_4bit=True)
... )
```

æ‚¨å°†ä¼šæ³¨æ„åˆ°åœ¨`from_pretrained`è°ƒç”¨ä¸­çš„ä¸¤ä¸ªæ ‡å¿—ï¼š

- `device_map`ç¡®ä¿æ¨¡å‹è¢«ç§»åŠ¨åˆ°æ‚¨çš„GPU(s)ä¸Š
- `load_in_4bit`åº”ç”¨[4ä½åŠ¨æ€é‡åŒ–](main_classes/quantization)æ¥æå¤§åœ°å‡å°‘èµ„æºéœ€æ±‚

è¿˜æœ‰å…¶ä»–æ–¹å¼æ¥åˆå§‹åŒ–ä¸€ä¸ªæ¨¡å‹ï¼Œä½†è¿™æ˜¯ä¸€ä¸ªå¼€å§‹ä½¿ç”¨LLMå¾ˆå¥½çš„èµ·ç‚¹ã€‚

æ¥ä¸‹æ¥ï¼Œä½ éœ€è¦ä½¿ç”¨ä¸€ä¸ª[tokenizer](tokenizer_summary)æ¥é¢„å¤„ç†ä½ çš„æ–‡æœ¬è¾“å…¥ã€‚

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1", padding_side="left")
>>> model_inputs = tokenizer(["A list of colors: red, blue"], return_tensors="pt").to(model.device)
```

`model_inputs`å˜é‡ä¿å­˜ç€åˆ†è¯åçš„æ–‡æœ¬è¾“å…¥ä»¥åŠæ³¨æ„åŠ›æ©ç ã€‚å°½ç®¡[`~generation.GenerationMixin.generate`]åœ¨æœªä¼ é€’æ³¨æ„åŠ›æ©ç æ—¶ä¼šå°½å…¶æ‰€èƒ½æ¨æ–­å‡ºæ³¨æ„åŠ›æ©ç ï¼Œä½†å»ºè®®å°½å¯èƒ½ä¼ é€’å®ƒä»¥è·å¾—æœ€ä½³ç»“æœã€‚

åœ¨å¯¹è¾“å…¥è¿›è¡Œåˆ†è¯åï¼Œå¯ä»¥è°ƒç”¨[`~generation.GenerationMixin.generate`]æ–¹æ³•æ¥è¿”å›ç”Ÿæˆçš„`tokens`ã€‚ç”Ÿæˆçš„`tokens`åº”è¯¥åœ¨æ‰“å°ä¹‹å‰è½¬æ¢ä¸ºæ–‡æœ¬ã€‚

```py
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'A list of colors: red, blue, green, yellow, orange, purple, pink,'
```

æœ€åï¼Œæ‚¨ä¸éœ€è¦ä¸€æ¬¡å¤„ç†ä¸€ä¸ªåºåˆ—ï¼æ‚¨å¯ä»¥æ‰¹é‡è¾“å…¥ï¼Œè¿™å°†åœ¨å°å»¶è¿Ÿå’Œä½å†…å­˜æˆæœ¬ä¸‹æ˜¾è‘—æé«˜ååé‡ã€‚æ‚¨åªéœ€è¦ç¡®ä¿æ­£ç¡®åœ°å¡«å……æ‚¨çš„è¾“å…¥ï¼ˆè¯¦è§ä¸‹æ–‡ï¼‰ã€‚

```py
>>> tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default
>>> model_inputs = tokenizer(
...     ["A list of colors: red, blue", "Portugal is"], return_tensors="pt", padding=True
... ).to(model.device)
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
['A list of colors: red, blue, green, yellow, orange, purple, pink,',
'Portugal is a country in southwestern Europe, on the Iber']
```

å°±æ˜¯è¿™æ ·ï¼åœ¨å‡ è¡Œä»£ç ä¸­ï¼Œæ‚¨å°±å¯ä»¥åˆ©ç”¨LLMçš„å¼ºå¤§åŠŸèƒ½ã€‚


## å¸¸è§é™·é˜±

æœ‰è®¸å¤š[ç”Ÿæˆç­–ç•¥](generation_strategies)ï¼Œæœ‰æ—¶é»˜è®¤å€¼å¯èƒ½ä¸é€‚åˆæ‚¨çš„ç”¨ä¾‹ã€‚å¦‚æœæ‚¨çš„è¾“å‡ºä¸æ‚¨æœŸæœ›çš„ç»“æœä¸åŒ¹é…ï¼Œæˆ‘ä»¬å·²ç»åˆ›å»ºäº†ä¸€ä¸ªæœ€å¸¸è§çš„é™·é˜±åˆ—è¡¨ä»¥åŠå¦‚ä½•é¿å…å®ƒä»¬ã€‚

```py
>>> from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")
>>> tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default
>>> model = AutoModelForCausalLM.from_pretrained(
...     "mistralai/Mistral-7B-v0.1", device_map="auto", quantization_config=BitsAndBytesConfig(load_in_4bit=True)
... )
```

### ç”Ÿæˆçš„è¾“å‡ºå¤ªçŸ­/å¤ªé•¿

å¦‚æœåœ¨[`~generation.GenerationConfig`]æ–‡ä»¶ä¸­æ²¡æœ‰æŒ‡å®šï¼Œ`generate`é»˜è®¤è¿”å›20ä¸ªtokensã€‚æˆ‘ä»¬å¼ºçƒˆå»ºè®®åœ¨æ‚¨çš„`generate`è°ƒç”¨ä¸­æ‰‹åŠ¨è®¾ç½®`max_new_tokens`ä»¥æ§åˆ¶å®ƒå¯ä»¥è¿”å›çš„æœ€å¤§æ–°tokensæ•°é‡ã€‚è¯·æ³¨æ„ï¼ŒLLMsï¼ˆæ›´å‡†ç¡®åœ°è¯´ï¼Œä»…[è§£ç å™¨æ¨¡å‹](https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt)ï¼‰ä¹Ÿå°†è¾“å…¥æç¤ºä½œä¸ºè¾“å‡ºçš„ä¸€éƒ¨åˆ†è¿”å›ã€‚

```py
>>> model_inputs = tokenizer(["A sequence of numbers: 1, 2"], return_tensors="pt").to(model.device)

>>> # By default, the output will contain up to 20 tokens
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'A sequence of numbers: 1, 2, 3, 4, 5'

>>> # Setting `max_new_tokens` allows you to control the maximum length
>>> generated_ids = model.generate(**model_inputs, max_new_tokens=50)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'A sequence of numbers: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,'
```

### é”™è¯¯çš„ç”Ÿæˆæ¨¡å¼

é»˜è®¤æƒ…å†µä¸‹ï¼Œé™¤éåœ¨[`~generation.GenerationConfig`]æ–‡ä»¶ä¸­æŒ‡å®šï¼Œå¦åˆ™`generate`ä¼šåœ¨æ¯ä¸ªè¿­ä»£ä¸­é€‰æ‹©æœ€å¯èƒ½çš„tokenï¼ˆè´ªå©ªè§£ç ï¼‰ã€‚å¯¹äºæ‚¨çš„ä»»åŠ¡ï¼Œè¿™å¯èƒ½æ˜¯ä¸ç†æƒ³çš„ï¼›åƒèŠå¤©æœºå™¨äººæˆ–å†™ä½œæ–‡ç« è¿™æ ·çš„åˆ›é€ æ€§ä»»åŠ¡å—ç›Šäºé‡‡æ ·ã€‚å¦ä¸€æ–¹é¢ï¼ŒåƒéŸ³é¢‘è½¬å½•æˆ–ç¿»è¯‘è¿™æ ·çš„åŸºäºè¾“å…¥çš„ä»»åŠ¡å—ç›Šäºè´ªå©ªè§£ç ã€‚é€šè¿‡å°†`do_sample=True`å¯ç”¨é‡‡æ ·ï¼Œæ‚¨å¯ä»¥åœ¨è¿™ç¯‡[åšå®¢æ–‡ç« ](https://huggingface.co/blog/how-to-generate)ä¸­äº†è§£æ›´å¤šå…³äºè¿™ä¸ªè¯é¢˜çš„ä¿¡æ¯ã€‚

```py
>>> # Set seed or reproducibility -- you don't need this unless you want full reproducibility
>>> from transformers import set_seed
>>> set_seed(42)

>>> model_inputs = tokenizer(["I am a cat."], return_tensors="pt").to(model.device)

>>> # LLM + greedy decoding = repetitive, boring output
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'I am a cat. I am a cat. I am a cat. I am a cat'

>>> # With sampling, the output becomes more creative!
>>> generated_ids = model.generate(**model_inputs, do_sample=True)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'I am a cat.  Specifically, I am an indoor-only cat.  I'
```

### é”™è¯¯çš„å¡«å……ä½ç½®

LLMsæ˜¯[ä»…è§£ç å™¨](https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt)æ¶æ„ï¼Œæ„å‘³ç€å®ƒä»¬ä¼šæŒç»­è¿­ä»£æ‚¨çš„è¾“å…¥æç¤ºã€‚å¦‚æœæ‚¨çš„è¾“å…¥é•¿åº¦ä¸ç›¸åŒï¼Œåˆ™éœ€è¦å¯¹å®ƒä»¬è¿›è¡Œå¡«å……ã€‚ç”±äºLLMsæ²¡æœ‰æ¥å—è¿‡ä»`pad tokens`ç»§ç»­è®­ç»ƒï¼Œå› æ­¤æ‚¨çš„è¾“å…¥éœ€è¦å·¦å¡«å……ã€‚ç¡®ä¿åœ¨ç”Ÿæˆæ—¶ä¸è¦å¿˜è®°ä¼ é€’æ³¨æ„åŠ›æ©ç ï¼

```py
>>> # The tokenizer initialized above has right-padding active by default: the 1st sequence,
>>> # which is shorter, has padding on the right side. Generation fails to capture the logic.
>>> model_inputs = tokenizer(
...     ["1, 2, 3", "A, B, C, D, E"], padding=True, return_tensors="pt"
... ).to(model.device)
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'1, 2, 33333333333'

>>> # With left-padding, it works as expected!
>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1", padding_side="left")
>>> tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default
>>> model_inputs = tokenizer(
...     ["1, 2, 3", "A, B, C, D, E"], padding=True, return_tensors="pt"
... ).to(model.device)
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'1, 2, 3, 4, 5, 6,'
```

### é”™è¯¯çš„æç¤º

ä¸€äº›æ¨¡å‹å’Œä»»åŠ¡æœŸæœ›æŸç§è¾“å…¥æç¤ºæ ¼å¼æ‰èƒ½æ­£å¸¸å·¥ä½œã€‚å½“æœªåº”ç”¨æ­¤æ ¼å¼æ—¶ï¼Œæ‚¨å°†è·å¾—æ‚„ç„¶çš„æ€§èƒ½ä¸‹é™ï¼šæ¨¡å‹èƒ½å·¥ä½œï¼Œä½†ä¸å¦‚é¢„æœŸæç¤ºé‚£æ ·å¥½ã€‚æœ‰å…³æç¤ºçš„æ›´å¤šä¿¡æ¯ï¼ŒåŒ…æ‹¬å“ªäº›æ¨¡å‹å’Œä»»åŠ¡éœ€è¦å°å¿ƒï¼Œå¯åœ¨[æŒ‡å—](tasks/prompting)ä¸­æ‰¾åˆ°ã€‚è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªä½¿ç”¨[èŠå¤©æ¨¡æ¿](chat_templating)çš„èŠå¤©LLMç¤ºä¾‹ï¼š

```python
>>> tokenizer = AutoTokenizer.from_pretrained("HuggingFaceH4/zephyr-7b-alpha")
>>> model = AutoModelForCausalLM.from_pretrained(
...     "HuggingFaceH4/zephyr-7b-alpha", device_map="auto", quantization_config=BitsAndBytesConfig(load_in_4bit=True)
... )
>>> set_seed(0)
>>> prompt = """How many helicopters can a human eat in one sitting? Reply as a thug."""
>>> model_inputs = tokenizer([prompt], return_tensors="pt").to(model.device)
>>> input_length = model_inputs.input_ids.shape[1]
>>> generated_ids = model.generate(**model_inputs, max_new_tokens=20)
>>> print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])
"I'm not a thug, but i can tell you that a human cannot eat"
>>> # Oh no, it did not follow our instruction to reply as a thug! Let's see what happens when we write
>>> # a better prompt and use the right template for this model (through `tokenizer.apply_chat_template`)

>>> set_seed(0)
>>> messages = [
...     {
...         "role": "system",
...         "content": "You are a friendly chatbot who always responds in the style of a thug",
...     },
...     {"role": "user", "content": "How many helicopters can a human eat in one sitting?"},
... ]
>>> model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt").to(model.device)
>>> input_length = model_inputs.shape[1]
>>> generated_ids = model.generate(model_inputs, do_sample=True, max_new_tokens=20)
>>> print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])
'None, you thug. How bout you try to focus on more useful questions?'
>>> # As we can see, it followed a proper thug style ğŸ˜
```

## æ›´å¤šèµ„æº

è™½ç„¶è‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹ç›¸å¯¹ç®€å•ï¼Œä½†è¦å……åˆ†åˆ©ç”¨LLMå¯èƒ½æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºå¾ˆå¤šç»„ä»¶å¤æ‚ä¸”å¯†åˆ‡å…³è”ã€‚ä»¥ä¸‹æ˜¯å¸®åŠ©æ‚¨æ·±å…¥äº†è§£LLMä½¿ç”¨å’Œç†è§£çš„ä¸‹ä¸€æ­¥ï¼š

### é«˜çº§ç”Ÿæˆç”¨æ³•

1. [æŒ‡å—](generation_strategies)ï¼Œä»‹ç»å¦‚ä½•æ§åˆ¶ä¸åŒçš„ç”Ÿæˆæ–¹æ³•ã€å¦‚ä½•è®¾ç½®ç”Ÿæˆé…ç½®æ–‡ä»¶ä»¥åŠå¦‚ä½•è¿›è¡Œè¾“å‡ºæµå¼ä¼ è¾“ï¼›
2. [æŒ‡å—](chat_templating)ï¼Œä»‹ç»èŠå¤©LLMsçš„æç¤ºæ¨¡æ¿ï¼›
3. [æŒ‡å—](tasks/prompting)ï¼Œä»‹ç»å¦‚ä½•å……åˆ†åˆ©ç”¨æç¤ºè®¾è®¡ï¼›
4. APIå‚è€ƒæ–‡æ¡£ï¼ŒåŒ…æ‹¬[`~generation.GenerationConfig`]ã€[`~generation.GenerationMixin.generate`]å’Œ[ä¸ç”Ÿæˆç›¸å…³çš„ç±»](internal/generation_utils)ã€‚

### LLMæ’è¡Œæ¦œ

1. [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard), ä¾§é‡äºå¼€æºæ¨¡å‹çš„è´¨é‡;
2. [Open LLM-Perf Leaderboard](https://huggingface.co/spaces/optimum/llm-perf-leaderboard), ä¾§é‡äºLLMçš„ååé‡.

### å»¶è¿Ÿã€ååé‡å’Œå†…å­˜åˆ©ç”¨ç‡

1. [æŒ‡å—](llm_tutorial_optimization),å¦‚ä½•ä¼˜åŒ–LLMsä»¥æé«˜é€Ÿåº¦å’Œå†…å­˜åˆ©ç”¨ï¼›
2. [æŒ‡å—](main_classes/quantization), å…³äº`quantization`ï¼Œå¦‚bitsandbyteså’ŒGPT-QModelçš„æŒ‡å—ï¼Œæ•™æ‚¨å¦‚ä½•å¤§å¹…é™ä½å†…å­˜éœ€æ±‚ã€‚

### ç›¸å…³åº“

1. [`text-generation-inference`](https://github.com/huggingface/text-generation-inference), ä¸€ä¸ªé¢å‘ç”Ÿäº§çš„LLMæœåŠ¡å™¨ï¼›
2. [`optimum`](https://github.com/huggingface/optimum), ä¸€ä¸ªğŸ¤— Transformersçš„æ‰©å±•ï¼Œä¼˜åŒ–ç‰¹å®šç¡¬ä»¶è®¾å¤‡çš„æ€§èƒ½

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\model_sharing.md
============================================================



# åˆ†äº«æ¨¡å‹

æœ€åä¸¤ä¸ªæ•™ç¨‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨PyTorchã€Keraså’Œ ğŸ¤— Accelerateè¿›è¡Œåˆ†å¸ƒå¼è®¾ç½®æ¥å¾®è°ƒæ¨¡å‹ã€‚ä¸‹ä¸€æ­¥æ˜¯å°†æ‚¨çš„æ¨¡å‹ä¸ç¤¾åŒºåˆ†äº«ï¼åœ¨Hugging Faceï¼Œæˆ‘ä»¬ç›¸ä¿¡å…¬å¼€åˆ†äº«çŸ¥è¯†å’Œèµ„æºï¼Œèƒ½å®ç°äººå·¥æ™ºèƒ½çš„æ™®åŠåŒ–ï¼Œè®©æ¯ä¸ªäººéƒ½èƒ½å—ç›Šã€‚æˆ‘ä»¬é¼“åŠ±æ‚¨å°†æ‚¨çš„æ¨¡å‹ä¸ç¤¾åŒºåˆ†äº«ï¼Œä»¥å¸®åŠ©ä»–äººèŠ‚çœæ—¶é—´å’Œç²¾åŠ›ã€‚

åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæ‚¨å°†å­¦ä¹ ä¸¤ç§åœ¨[Model Hub](https://huggingface.co/models)ä¸Šå…±äº«è®­ç»ƒå¥½çš„æˆ–å¾®è°ƒçš„æ¨¡å‹çš„æ–¹æ³•ï¼š

- é€šè¿‡ç¼–ç¨‹å°†æ–‡ä»¶æ¨é€åˆ°Hubã€‚
- ä½¿ç”¨Webç•Œé¢å°†æ–‡ä»¶æ‹–æ”¾åˆ°Hubã€‚

<iframe width="560" height="315" src="https://www.youtube.com/embed/XvSGPZFEjDY" title="YouTube video player"
frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope;
picture-in-picture" allowfullscreen></iframe>

<Tip>

è¦ä¸ç¤¾åŒºå…±äº«æ¨¡å‹ï¼Œæ‚¨éœ€è¦åœ¨[huggingface.co](https://huggingface.co/join)ä¸Šæ‹¥æœ‰ä¸€ä¸ªå¸æˆ·ã€‚æ‚¨è¿˜å¯ä»¥åŠ å…¥ç°æœ‰çš„ç»„ç»‡æˆ–åˆ›å»ºä¸€ä¸ªæ–°çš„ç»„ç»‡ã€‚

</Tip>

## ä»“åº“åŠŸèƒ½

Model Hubä¸Šçš„æ¯ä¸ªä»“åº“éƒ½åƒæ˜¯ä¸€ä¸ªå…¸å‹çš„GitHubä»“åº“ã€‚æˆ‘ä»¬çš„ä»“åº“æä¾›ç‰ˆæœ¬æ§åˆ¶ã€æäº¤å†å²è®°å½•ä»¥åŠå¯è§†åŒ–å·®å¼‚çš„èƒ½åŠ›ã€‚

Model Hubçš„å†…ç½®ç‰ˆæœ¬æ§åˆ¶åŸºäºgitå’Œ[git-lfs](https://git-lfs.github.com/)ã€‚æ¢å¥è¯è¯´ï¼Œæ‚¨å¯ä»¥å°†ä¸€ä¸ªæ¨¡å‹è§†ä¸ºä¸€ä¸ªä»“åº“ï¼Œä»è€Œå®ç°æ›´å¥½çš„è®¿é—®æ§åˆ¶å’Œå¯æ‰©å±•æ€§ã€‚ç‰ˆæœ¬æ§åˆ¶å…è®¸ä½¿ç”¨*ä¿®è®¢*æ–¹æ³•æ¥å›ºå®šç‰¹å®šç‰ˆæœ¬çš„æ¨¡å‹ï¼Œå¯ä»¥ä½¿ç”¨æäº¤å“ˆå¸Œå€¼ã€æ ‡ç­¾æˆ–åˆ†æ”¯æ¥æ ‡è®°ã€‚

å› æ­¤ï¼Œæ‚¨å¯ä»¥é€šè¿‡`revision`å‚æ•°åŠ è½½ç‰¹å®šçš„æ¨¡å‹ç‰ˆæœ¬ï¼š

```py
>>> model = AutoModel.from_pretrained(
...     "julien-c/EsperBERTo-small", revision="4c77982"  # tag name, or branch name, or commit hash
... )
```

æ–‡ä»¶ä¹Ÿå¯ä»¥è½»æ¾åœ°åœ¨ä»“åº“ä¸­ç¼–è¾‘ï¼Œæ‚¨å¯ä»¥æŸ¥çœ‹æäº¤å†å²è®°å½•ä»¥åŠå·®å¼‚ï¼š
![vis_diff](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/vis_diff.png)

## è®¾ç½®

åœ¨å°†æ¨¡å‹å…±äº«åˆ°Hubä¹‹å‰ï¼Œæ‚¨éœ€è¦æ‹¥æœ‰Hugging Faceçš„å‡­è¯ã€‚å¦‚æœæ‚¨æœ‰è®¿é—®ç»ˆç«¯çš„æƒé™ï¼Œè¯·åœ¨å®‰è£…ğŸ¤— Transformersçš„è™šæ‹Ÿç¯å¢ƒä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤ã€‚è¿™å°†åœ¨æ‚¨çš„Hugging Faceç¼“å­˜æ–‡ä»¶å¤¹ï¼ˆé»˜è®¤ä¸º`~/.cache/`ï¼‰ä¸­å­˜å‚¨æ‚¨çš„`access token`ï¼š


```bash
hf auth login
```

å¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨åƒJupyteræˆ–Colaboratoryè¿™æ ·çš„`notebook`ï¼Œè¯·ç¡®ä¿æ‚¨å·²å®‰è£…äº†[`huggingface_hub`](https://huggingface.co/docs/hub/adding-a-library)åº“ã€‚è¯¥åº“å…è®¸æ‚¨ä»¥ç¼–ç¨‹æ–¹å¼ä¸Hubè¿›è¡Œäº¤äº’ã€‚

```bash
pip install huggingface_hub
```
ç„¶åä½¿ç”¨`notebook_login`ç™»å½•åˆ°Hubï¼Œå¹¶æŒ‰ç…§[è¿™é‡Œ](https://huggingface.co/settings/token)çš„é“¾æ¥ç”Ÿæˆä¸€ä¸ªtokenè¿›è¡Œç™»å½•ï¼š


```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¨é€æ¨¡å‹

<Youtube id="Z1-XMy-GNLQ"/>

å°†æ¨¡å‹åˆ†äº«åˆ°Hubå°±åƒæ·»åŠ ä¸€ä¸ªé¢å¤–çš„å‚æ•°æˆ–å›è°ƒå‡½æ•°ä¸€æ ·ç®€å•ã€‚è¯·è®°ä½ï¼Œåœ¨[å¾®è°ƒæ•™ç¨‹](training)ä¸­ï¼Œ`TrainingArguments`ç±»æ˜¯æ‚¨æŒ‡å®šè¶…å‚æ•°å’Œé™„åŠ è®­ç»ƒé€‰é¡¹çš„åœ°æ–¹ã€‚å…¶ä¸­ä¸€é¡¹è®­ç»ƒé€‰é¡¹åŒ…æ‹¬ç›´æ¥å°†æ¨¡å‹æ¨é€åˆ°Hubçš„èƒ½åŠ›ã€‚åœ¨æ‚¨çš„`TrainingArguments`ä¸­è®¾ç½®`push_to_hub=True`ï¼š


```py
>>> training_args = TrainingArguments(output_dir="my-awesome-model", push_to_hub=True)
```

åƒå¾€å¸¸ä¸€æ ·å°†æ‚¨çš„è®­ç»ƒå‚æ•°ä¼ é€’ç»™[`Trainer`]ï¼š

```py
>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=small_train_dataset,
...     eval_dataset=small_eval_dataset,
...     compute_metrics=compute_metrics,
... )
```

åœ¨æ‚¨å¾®è°ƒå®Œæ¨¡å‹åï¼Œåœ¨[`Trainer`]ä¸Šè°ƒç”¨[`~transformers.Trainer.push_to_hub`]å°†è®­ç»ƒå¥½çš„æ¨¡å‹æ¨é€åˆ°Hubã€‚ğŸ¤— Transformersç”šè‡³ä¼šè‡ªåŠ¨å°†è®­ç»ƒè¶…å‚æ•°ã€è®­ç»ƒç»“æœå’Œæ¡†æ¶ç‰ˆæœ¬æ·»åŠ åˆ°ä½ çš„æ¨¡å‹å¡ç‰‡ä¸­ï¼

```py
>>> trainer.push_to_hub()
```

## ä½¿ç”¨`push_to_hub`åŠŸèƒ½

æ‚¨å¯ä»¥ç›´æ¥åœ¨æ‚¨çš„æ¨¡å‹ä¸Šè°ƒç”¨`push_to_hub`æ¥å°†å…¶ä¸Šä¼ åˆ°Hubã€‚

åœ¨`push_to_hub`ä¸­æŒ‡å®šä½ çš„æ¨¡å‹åç§°ï¼š

```py
>>> pt_model.push_to_hub("my-awesome-model")
```

è¿™ä¼šåœ¨æ‚¨çš„ç”¨æˆ·åä¸‹åˆ›å»ºä¸€ä¸ªåä¸º`my-awesome-model`çš„ä»“åº“ã€‚ç”¨æˆ·ç°åœ¨å¯ä»¥ä½¿ç”¨`from_pretrained`å‡½æ•°åŠ è½½æ‚¨çš„æ¨¡å‹ï¼š

```py
>>> from transformers import AutoModel

>>> model = AutoModel.from_pretrained("your_username/my-awesome-model")
```

å¦‚æœæ‚¨å±äºä¸€ä¸ªç»„ç»‡ï¼Œå¹¶å¸Œæœ›å°†æ‚¨çš„æ¨¡å‹æ¨é€åˆ°ç»„ç»‡åç§°ä¸‹ï¼Œåªéœ€å°†å…¶æ·»åŠ åˆ°`repo_id`ä¸­ï¼š

```py
>>> pt_model.push_to_hub("my-awesome-org/my-awesome-model")
```

`push_to_hub`å‡½æ•°è¿˜å¯ä»¥ç”¨äºå‘æ¨¡å‹ä»“åº“æ·»åŠ å…¶ä»–æ–‡ä»¶ã€‚ä¾‹å¦‚ï¼Œå‘æ¨¡å‹ä»“åº“ä¸­æ·»åŠ ä¸€ä¸ª`tokenizer`ï¼š

```py
>>> tokenizer.push_to_hub("my-awesome-model")
```

ç°åœ¨ï¼Œå½“æ‚¨å¯¼èˆªåˆ°æ‚¨çš„Hugging Faceä¸ªäººèµ„æ–™æ—¶ï¼Œæ‚¨åº”è¯¥çœ‹åˆ°æ‚¨æ–°åˆ›å»ºçš„æ¨¡å‹ä»“åº“ã€‚ç‚¹å‡»**æ–‡ä»¶**é€‰é¡¹å¡å°†æ˜¾ç¤ºæ‚¨å·²ä¸Šä¼ åˆ°ä»“åº“çš„æ‰€æœ‰æ–‡ä»¶ã€‚

æœ‰å…³å¦‚ä½•åˆ›å»ºå’Œä¸Šä¼ æ–‡ä»¶åˆ°ä»“åº“çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è€ƒHubæ–‡æ¡£[è¿™é‡Œ](https://huggingface.co/docs/hub/how-to-upstream)ã€‚


## ä½¿ç”¨Webç•Œé¢ä¸Šä¼ 

å–œæ¬¢æ— ä»£ç æ–¹æ³•çš„ç”¨æˆ·å¯ä»¥é€šè¿‡Hugging Faceçš„Webç•Œé¢ä¸Šä¼ æ¨¡å‹ã€‚è®¿é—®[huggingface.co/new](https://huggingface.co/new)åˆ›å»ºä¸€ä¸ªæ–°çš„ä»“åº“ï¼š

![new_model_repo](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/new_model_repo.png)

ä»è¿™é‡Œå¼€å§‹ï¼Œæ·»åŠ ä¸€äº›å…³äºæ‚¨çš„æ¨¡å‹çš„ä¿¡æ¯ï¼š

- é€‰æ‹©ä»“åº“çš„**æ‰€æœ‰è€…**ã€‚è¿™å¯ä»¥æ˜¯æ‚¨æœ¬äººæˆ–è€…æ‚¨æ‰€å±çš„ä»»ä½•ç»„ç»‡ã€‚
- ä¸ºæ‚¨çš„é¡¹ç›®é€‰æ‹©ä¸€ä¸ªåç§°ï¼Œè¯¥åç§°ä¹Ÿå°†æˆä¸ºä»“åº“çš„åç§°ã€‚
- é€‰æ‹©æ‚¨çš„æ¨¡å‹æ˜¯å…¬å¼€è¿˜æ˜¯ç§æœ‰ã€‚
- æŒ‡å®šæ‚¨çš„æ¨¡å‹çš„è®¸å¯è¯ä½¿ç”¨æƒ…å†µã€‚

ç°åœ¨ç‚¹å‡»**æ–‡ä»¶**é€‰é¡¹å¡ï¼Œç„¶åç‚¹å‡»**æ·»åŠ æ–‡ä»¶**æŒ‰é’®å°†ä¸€ä¸ªæ–°æ–‡ä»¶ä¸Šä¼ åˆ°ä½ çš„ä»“åº“ã€‚æ¥ç€æ‹–æ”¾ä¸€ä¸ªæ–‡ä»¶è¿›è¡Œä¸Šä¼ ï¼Œå¹¶æ·»åŠ æäº¤ä¿¡æ¯ã€‚

![upload_file](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/upload_file.png)

## æ·»åŠ æ¨¡å‹å¡ç‰‡

ä¸ºäº†ç¡®ä¿ç”¨æˆ·äº†è§£æ‚¨çš„æ¨¡å‹çš„èƒ½åŠ›ã€é™åˆ¶ã€æ½œåœ¨åå·®å’Œä¼¦ç†è€ƒè™‘ï¼Œè¯·åœ¨ä»“åº“ä¸­æ·»åŠ ä¸€ä¸ªæ¨¡å‹å¡ç‰‡ã€‚æ¨¡å‹å¡ç‰‡åœ¨`README.md`æ–‡ä»¶ä¸­å®šä¹‰ã€‚ä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼æ·»åŠ æ¨¡å‹å¡ç‰‡ï¼š

* æ‰‹åŠ¨åˆ›å»ºå¹¶ä¸Šä¼ ä¸€ä¸ª`README.md`æ–‡ä»¶ã€‚
* åœ¨ä½ çš„æ¨¡å‹ä»“åº“ä¸­ç‚¹å‡»**ç¼–è¾‘æ¨¡å‹å¡ç‰‡**æŒ‰é’®ã€‚

å¯ä»¥å‚è€ƒDistilBertçš„[æ¨¡å‹å¡ç‰‡](https://huggingface.co/distilbert/distilbert-base-uncased)æ¥äº†è§£æ¨¡å‹å¡ç‰‡åº”è¯¥åŒ…å«çš„ä¿¡æ¯ç±»å‹ã€‚æœ‰å…³æ‚¨å¯ä»¥åœ¨`README.md`æ–‡ä»¶ä¸­æ§åˆ¶çš„æ›´å¤šé€‰é¡¹çš„ç»†èŠ‚ï¼Œä¾‹å¦‚æ¨¡å‹çš„ç¢³è¶³è¿¹æˆ–å°éƒ¨ä»¶ç¤ºä¾‹ï¼Œè¯·å‚è€ƒæ–‡æ¡£[è¿™é‡Œ](https://huggingface.co/docs/hub/models-cards)ã€‚

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\multilingual.md
============================================================



# ç”¨äºæ¨ç†çš„å¤šè¯­è¨€æ¨¡å‹

[[open-in-colab]]

ğŸ¤— Transformers ä¸­æœ‰å¤šç§å¤šè¯­è¨€æ¨¡å‹ï¼Œå®ƒä»¬çš„æ¨ç†ç”¨æ³•ä¸å•è¯­è¨€æ¨¡å‹ä¸åŒã€‚ä½†æ˜¯ï¼Œå¹¶é*æ‰€æœ‰*çš„å¤šè¯­è¨€æ¨¡å‹ç”¨æ³•éƒ½ä¸åŒã€‚ä¸€äº›æ¨¡å‹ï¼Œä¾‹å¦‚ [google-bert/bert-base-multilingual-uncased](https://huggingface.co/google-bert/bert-base-multilingual-uncased) å°±å¯ä»¥åƒå•è¯­è¨€æ¨¡å‹ä¸€æ ·ä½¿ç”¨ã€‚æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨ä¸åŒç”¨é€”çš„å¤šè¯­è¨€æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚

## XLM

XLM æœ‰åä¸ªä¸åŒçš„æ£€æŸ¥ç‚¹ï¼Œå…¶ä¸­åªæœ‰ä¸€ä¸ªæ˜¯å•è¯­è¨€çš„ã€‚å‰©ä¸‹çš„ä¹ä¸ªæ£€æŸ¥ç‚¹å¯ä»¥å½’ä¸ºä¸¤ç±»ï¼šä½¿ç”¨è¯­è¨€åµŒå…¥çš„æ£€æŸ¥ç‚¹å’Œä¸ä½¿ç”¨è¯­è¨€åµŒå…¥çš„æ£€æŸ¥ç‚¹ã€‚

### å¸¦æœ‰è¯­è¨€åµŒå…¥çš„ XLM

ä»¥ä¸‹ XLM æ¨¡å‹ä½¿ç”¨è¯­è¨€åµŒå…¥æ¥æŒ‡å®šæ¨ç†ä¸­ä½¿ç”¨çš„è¯­è¨€ï¼š

- `FacebookAI/xlm-mlm-ende-1024` ï¼ˆæ©ç è¯­è¨€å»ºæ¨¡ï¼Œè‹±è¯­-å¾·è¯­ï¼‰
- `FacebookAI/xlm-mlm-enfr-1024` ï¼ˆæ©ç è¯­è¨€å»ºæ¨¡ï¼Œè‹±è¯­-æ³•è¯­ï¼‰
- `FacebookAI/xlm-mlm-enro-1024` ï¼ˆæ©ç è¯­è¨€å»ºæ¨¡ï¼Œè‹±è¯­-ç½—é©¬å°¼äºšè¯­ï¼‰
- `FacebookAI/xlm-mlm-xnli15-1024` ï¼ˆæ©ç è¯­è¨€å»ºæ¨¡ï¼ŒXNLI æ•°æ®é›†è¯­è¨€ï¼‰
- `FacebookAI/xlm-mlm-tlm-xnli15-1024` ï¼ˆæ©ç è¯­è¨€å»ºæ¨¡+ç¿»è¯‘ï¼ŒXNLI æ•°æ®é›†è¯­è¨€ï¼‰
- `FacebookAI/xlm-clm-enfr-1024` ï¼ˆå› æœè¯­è¨€å»ºæ¨¡ï¼Œè‹±è¯­-æ³•è¯­ï¼‰
- `FacebookAI/xlm-clm-ende-1024` ï¼ˆå› æœè¯­è¨€å»ºæ¨¡ï¼Œè‹±è¯­-å¾·è¯­ï¼‰

è¯­è¨€åµŒå…¥è¢«è¡¨ç¤ºä¸€ä¸ªå¼ é‡ï¼Œå…¶å½¢çŠ¶ä¸ä¼ é€’ç»™æ¨¡å‹çš„ `input_ids` ç›¸åŒã€‚è¿™äº›å¼ é‡ä¸­çš„å€¼å–å†³äºæ‰€ä½¿ç”¨çš„è¯­è¨€ï¼Œå¹¶ç”±åˆ†è¯å™¨çš„ `lang2id` å’Œ `id2lang`  å±æ€§è¯†åˆ«ã€‚

åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼ŒåŠ è½½ `FacebookAI/xlm-clm-enfr-1024` æ£€æŸ¥ç‚¹ï¼ˆå› æœè¯­è¨€å»ºæ¨¡ï¼Œè‹±è¯­-æ³•è¯­ï¼‰ï¼š

```py
>>> import torch
>>> from transformers import XLMTokenizer, XLMWithLMHeadModel

>>> tokenizer = XLMTokenizer.from_pretrained("FacebookAI/xlm-clm-enfr-1024")
>>> model = XLMWithLMHeadModel.from_pretrained("FacebookAI/xlm-clm-enfr-1024")
```

åˆ†è¯å™¨çš„ `lang2id` å±æ€§æ˜¾ç¤ºäº†è¯¥æ¨¡å‹çš„è¯­è¨€åŠå…¶å¯¹åº”çš„idï¼š

```py
>>> print(tokenizer.lang2id)
{'en': 0, 'fr': 1}
```

æ¥ä¸‹æ¥ï¼Œåˆ›å»ºä¸€ä¸ªç¤ºä¾‹è¾“å…¥ï¼š

```py
>>> input_ids = torch.tensor([tokenizer.encode("Wikipedia was used to")])  # batch size ä¸º 1
```

å°†è¯­è¨€ id è®¾ç½®ä¸º `"en"` å¹¶ç”¨å…¶å®šä¹‰è¯­è¨€åµŒå…¥ã€‚è¯­è¨€åµŒå…¥æ˜¯ä¸€ä¸ªç”¨ `0` å¡«å……çš„å¼ é‡ï¼Œè¿™ä¸ªå¼ é‡åº”è¯¥ä¸ `input_ids` å¤§å°ç›¸åŒã€‚

```py
>>> language_id = tokenizer.lang2id["en"]  # 0
>>> langs = torch.tensor([language_id] * input_ids.shape[1])  # torch.tensor([0, 0, 0, ..., 0])

>>> # æˆ‘ä»¬å°†å…¶ reshape ä¸º (batch_size, sequence_length) å¤§å°
>>> langs = langs.view(1, -1)  # ç°åœ¨çš„å½¢çŠ¶æ˜¯ [1, sequence_length] (æˆ‘ä»¬çš„ batch size ä¸º 1)
```

ç°åœ¨ï¼Œä½ å¯ä»¥å°† `input_ids` å’Œè¯­è¨€åµŒå…¥ä¼ é€’ç»™æ¨¡å‹ï¼š

```py
>>> outputs = model(input_ids, langs=langs)
```

[run_generation.py](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-generation/run_generation.py) è„šæœ¬å¯ä»¥ä½¿ç”¨ `xlm-clm` æ£€æŸ¥ç‚¹ç”Ÿæˆå¸¦æœ‰è¯­è¨€åµŒå…¥çš„æ–‡æœ¬ã€‚

### ä¸å¸¦è¯­è¨€åµŒå…¥çš„ XLM

ä»¥ä¸‹ XLM æ¨¡å‹åœ¨æ¨ç†æ—¶ä¸éœ€è¦è¯­è¨€åµŒå…¥ï¼š

- `FacebookAI/xlm-mlm-17-1280` ï¼ˆæ©ç è¯­è¨€å»ºæ¨¡ï¼Œæ”¯æŒ 17 ç§è¯­è¨€ï¼‰
- `FacebookAI/xlm-mlm-100-1280` ï¼ˆæ©ç è¯­è¨€å»ºæ¨¡ï¼Œæ”¯æŒ 100 ç§è¯­è¨€ï¼‰

ä¸ä¹‹å‰çš„ XLM æ£€æŸ¥ç‚¹ä¸åŒï¼Œè¿™äº›æ¨¡å‹ç”¨äºé€šç”¨å¥å­è¡¨ç¤ºã€‚

## BERT

ä»¥ä¸‹ BERT æ¨¡å‹å¯ç”¨äºå¤šè¯­è¨€ä»»åŠ¡ï¼š

- `google-bert/bert-base-multilingual-uncased` ï¼ˆæ©ç è¯­è¨€å»ºæ¨¡ + ä¸‹ä¸€å¥é¢„æµ‹ï¼Œæ”¯æŒ 102 ç§è¯­è¨€ï¼‰
- `google-bert/bert-base-multilingual-cased` ï¼ˆæ©ç è¯­è¨€å»ºæ¨¡ + ä¸‹ä¸€å¥é¢„æµ‹ï¼Œæ”¯æŒ 104 ç§è¯­è¨€ï¼‰

è¿™äº›æ¨¡å‹åœ¨æ¨ç†æ—¶ä¸éœ€è¦è¯­è¨€åµŒå…¥ã€‚å®ƒä»¬åº”è¯¥èƒ½å¤Ÿä»ä¸Šä¸‹æ–‡ä¸­è¯†åˆ«è¯­è¨€å¹¶è¿›è¡Œç›¸åº”çš„æ¨ç†ã€‚

## XLM-RoBERTa

ä»¥ä¸‹ XLM-RoBERTa æ¨¡å‹å¯ç”¨äºå¤šè¯­è¨€ä»»åŠ¡ï¼š

- `FacebookAI/xlm-roberta-base` ï¼ˆæ©ç è¯­è¨€å»ºæ¨¡ï¼Œæ”¯æŒ 100 ç§è¯­è¨€ï¼‰
- `FacebookAI/xlm-roberta-large` ï¼ˆæ©ç è¯­è¨€å»ºæ¨¡ï¼Œæ”¯æŒ 100 ç§è¯­è¨€ï¼‰

XLM-RoBERTa ä½¿ç”¨ 100 ç§è¯­è¨€çš„ 2.5TB æ–°åˆ›å»ºå’Œæ¸…ç†çš„ CommonCrawl æ•°æ®è¿›è¡Œäº†è®­ç»ƒã€‚ä¸ä¹‹å‰å‘å¸ƒçš„ mBERT æˆ– XLM ç­‰å¤šè¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼Œå®ƒåœ¨åˆ†ç±»ã€åºåˆ—æ ‡è®°å’Œé—®ç­”ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸Šæä¾›äº†æ›´å¼ºå¤§çš„ä¼˜åŠ¿ã€‚

## M2M100

ä»¥ä¸‹ M2M100 æ¨¡å‹å¯ç”¨äºå¤šè¯­è¨€ç¿»è¯‘ï¼š

- `facebook/m2m100_418M` ï¼ˆç¿»è¯‘ï¼‰
- `facebook/m2m100_1.2B` ï¼ˆç¿»è¯‘ï¼‰

åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼ŒåŠ è½½ `facebook/m2m100_418M` æ£€æŸ¥ç‚¹ä»¥å°†ä¸­æ–‡ç¿»è¯‘ä¸ºè‹±æ–‡ã€‚ä½ å¯ä»¥åœ¨åˆ†è¯å™¨ä¸­è®¾ç½®æºè¯­è¨€ï¼š

```py
>>> from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer

>>> en_text = "Do not meddle in the affairs of wizards, for they are subtle and quick to anger."
>>> chinese_text = "ä¸è¦æ’æ‰‹å·«å¸«çš„äº‹å‹™, å› ç‚ºä»–å€‘æ˜¯å¾®å¦™çš„, å¾ˆå¿«å°±æœƒç™¼æ€’."

>>> tokenizer = M2M100Tokenizer.from_pretrained("facebook/m2m100_418M", src_lang="zh")
>>> model = M2M100ForConditionalGeneration.from_pretrained("facebook/m2m100_418M")
```

å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼š

```py
>>> encoded_zh = tokenizer(chinese_text, return_tensors="pt")
```

M2M100 å¼ºåˆ¶å°†ç›®æ ‡è¯­è¨€ id ä½œä¸ºç¬¬ä¸€ä¸ªç”Ÿæˆçš„æ ‡è®°ï¼Œä»¥è¿›è¡Œåˆ°ç›®æ ‡è¯­è¨€çš„ç¿»è¯‘ã€‚åœ¨ `generate` æ–¹æ³•ä¸­å°† `forced_bos_token_id` è®¾ç½®ä¸º `en` ä»¥ç¿»è¯‘æˆè‹±è¯­ï¼š

```py
>>> generated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id("en"))
>>> tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
'Do not interfere with the matters of the witches, because they are delicate and will soon be angry.'
```

## MBart

ä»¥ä¸‹ MBart æ¨¡å‹å¯ç”¨äºå¤šè¯­è¨€ç¿»è¯‘ï¼š

- `facebook/mbart-large-50-one-to-many-mmt` ï¼ˆä¸€å¯¹å¤šå¤šè¯­è¨€æœºå™¨ç¿»è¯‘ï¼Œæ”¯æŒ 50 ç§è¯­è¨€ï¼‰
- `facebook/mbart-large-50-many-to-many-mmt` ï¼ˆå¤šå¯¹å¤šå¤šè¯­è¨€æœºå™¨ç¿»è¯‘ï¼Œæ”¯æŒ 50 ç§è¯­è¨€ï¼‰
- `facebook/mbart-large-50-many-to-one-mmt` ï¼ˆå¤šå¯¹ä¸€å¤šè¯­è¨€æœºå™¨ç¿»è¯‘ï¼Œæ”¯æŒ 50 ç§è¯­è¨€ï¼‰
- `facebook/mbart-large-50` ï¼ˆå¤šè¯­è¨€ç¿»è¯‘ï¼Œæ”¯æŒ 50 ç§è¯­è¨€ï¼‰
- `facebook/mbart-large-cc25`

åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼ŒåŠ è½½  `facebook/mbart-large-50-many-to-many-mmt` æ£€æŸ¥ç‚¹ä»¥å°†èŠ¬å…°è¯­ç¿»è¯‘ä¸ºè‹±è¯­ã€‚ ä½ å¯ä»¥åœ¨åˆ†è¯å™¨ä¸­è®¾ç½®æºè¯­è¨€ï¼š

```py
>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

>>> en_text = "Do not meddle in the affairs of wizards, for they are subtle and quick to anger."
>>> fi_text = "Ã„lÃ¤ sekaannu velhojen asioihin, sillÃ¤ ne ovat hienovaraisia ja nopeasti vihaisia."

>>> tokenizer = AutoTokenizer.from_pretrained("facebook/mbart-large-50-many-to-many-mmt", src_lang="fi_FI")
>>> model = AutoModelForSeq2SeqLM.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")
```

å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼š

```py
>>> encoded_en = tokenizer(en_text, return_tensors="pt")
```

MBart å¼ºåˆ¶å°†ç›®æ ‡è¯­è¨€ id ä½œä¸ºç¬¬ä¸€ä¸ªç”Ÿæˆçš„æ ‡è®°ï¼Œä»¥è¿›è¡Œåˆ°ç›®æ ‡è¯­è¨€çš„ç¿»è¯‘ã€‚åœ¨ `generate` æ–¹æ³•ä¸­å°† `forced_bos_token_id` è®¾ç½®ä¸º `en` ä»¥ç¿»è¯‘æˆè‹±è¯­ï¼š

```py
>>> generated_tokens = model.generate(**encoded_en, forced_bos_token_id=tokenizer.lang_code_to_id["en_XX"])
>>> tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
"Don't interfere with the wizard's affairs, because they are subtle, will soon get angry."
```

å¦‚æœä½ ä½¿ç”¨çš„æ˜¯ `facebook/mbart-large-50-many-to-one-mmt` æ£€æŸ¥ç‚¹ï¼Œåˆ™æ— éœ€å¼ºåˆ¶ç›®æ ‡è¯­è¨€ id ä½œä¸ºç¬¬ä¸€ä¸ªç”Ÿæˆçš„ä»¤ç‰Œï¼Œå¦åˆ™ç”¨æ³•æ˜¯ç›¸åŒçš„ã€‚

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\peft.md
============================================================

# ä½¿ç”¨ ğŸ¤— PEFT åŠ è½½adapters

[[open-in-colab]]

[å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•](https://huggingface.co/blog/peft)åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å†»ç»“é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°ï¼Œå¹¶åœ¨å…¶é¡¶éƒ¨æ·»åŠ å°‘é‡å¯è®­ç»ƒå‚æ•°ï¼ˆadaptersï¼‰ã€‚adaptersè¢«è®­ç»ƒä»¥å­¦ä¹ ç‰¹å®šä»»åŠ¡çš„ä¿¡æ¯ã€‚è¿™ç§æ–¹æ³•å·²è¢«è¯æ˜éå¸¸èŠ‚çœå†…å­˜ï¼ŒåŒæ—¶å…·æœ‰è¾ƒä½çš„è®¡ç®—ä½¿ç”¨é‡ï¼ŒåŒæ—¶äº§ç”Ÿä¸å®Œå…¨å¾®è°ƒæ¨¡å‹ç›¸å½“çš„ç»“æœã€‚

ä½¿ç”¨PEFTè®­ç»ƒçš„adaptersé€šå¸¸æ¯”å®Œæ•´æ¨¡å‹å°ä¸€ä¸ªæ•°é‡çº§ï¼Œä½¿å…¶æ–¹ä¾¿å…±äº«ã€å­˜å‚¨å’ŒåŠ è½½ã€‚

<div class="flex flex-col justify-center">
  <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/PEFT-hub-screenshot.png"/>
  <figcaption class="text-center">ä¸å®Œæ•´å°ºå¯¸çš„æ¨¡å‹æƒé‡ï¼ˆçº¦ä¸º700MBï¼‰ç›¸æ¯”ï¼Œå­˜å‚¨åœ¨Hubä¸Šçš„OPTForCausalLMæ¨¡å‹çš„adapteræƒé‡ä»…ä¸º~6MBã€‚</figcaption>
</div>

å¦‚æœæ‚¨å¯¹å­¦ä¹ æ›´å¤šå…³äºğŸ¤— PEFTåº“æ„Ÿå…´è¶£ï¼Œè¯·æŸ¥çœ‹[æ–‡æ¡£](https://huggingface.co/docs/peft/index)ã€‚


## è®¾ç½®

é¦–å…ˆå®‰è£… ğŸ¤— PEFTï¼š

```bash
pip install peft
```

å¦‚æœä½ æƒ³å°è¯•å…¨æ–°çš„ç‰¹æ€§ï¼Œä½ å¯èƒ½ä¼šæœ‰å…´è¶£ä»æºä»£ç å®‰è£…è¿™ä¸ªåº“ï¼š

```bash
pip install git+https://github.com/huggingface/peft.git
```
## æ”¯æŒçš„ PEFT æ¨¡å‹

TransformersåŸç”Ÿæ”¯æŒä¸€äº›PEFTæ–¹æ³•ï¼Œè¿™æ„å‘³ç€ä½ å¯ä»¥åŠ è½½æœ¬åœ°å­˜å‚¨æˆ–åœ¨Hubä¸Šçš„adapteræƒé‡ï¼Œå¹¶ä½¿ç”¨å‡ è¡Œä»£ç è½»æ¾è¿è¡Œæˆ–è®­ç»ƒå®ƒä»¬ã€‚ä»¥ä¸‹æ˜¯å—æ”¯æŒçš„æ–¹æ³•ï¼š

- [Low Rank Adapters](https://huggingface.co/docs/peft/conceptual_guides/lora)
- [IA3](https://huggingface.co/docs/peft/conceptual_guides/ia3)
- [AdaLoRA](https://huggingface.co/papers/2303.10512)

å¦‚æœä½ æƒ³ä½¿ç”¨å…¶ä»–PEFTæ–¹æ³•ï¼Œä¾‹å¦‚æç¤ºå­¦ä¹ æˆ–æç¤ºå¾®è°ƒï¼Œæˆ–è€…å…³äºé€šç”¨çš„ ğŸ¤— PEFTåº“ï¼Œè¯·å‚é˜…[æ–‡æ¡£](https://huggingface.co/docs/peft/index)ã€‚

## åŠ è½½ PEFT adapter

è¦ä»huggingfaceçš„Transformersåº“ä¸­åŠ è½½å¹¶ä½¿ç”¨PEFTadapteræ¨¡å‹ï¼Œè¯·ç¡®ä¿Hubä»“åº“æˆ–æœ¬åœ°ç›®å½•åŒ…å«ä¸€ä¸ª`adapter_config.json`æ–‡ä»¶å’Œadapteræƒé‡ï¼Œå¦‚ä¸Šä¾‹æ‰€ç¤ºã€‚ç„¶åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨`AutoModelFor`ç±»åŠ è½½PEFT adapteræ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œè¦ä¸ºå› æœè¯­è¨€å»ºæ¨¡åŠ è½½ä¸€ä¸ªPEFT adapteræ¨¡å‹ï¼š

1. æŒ‡å®šPEFTæ¨¡å‹id
2. å°†å…¶ä¼ é€’ç»™[`AutoModelForCausalLM`]ç±»

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

peft_model_id = "ybelkada/opt-350m-lora"
model = AutoModelForCausalLM.from_pretrained(peft_model_id)
```

<Tip>

ä½ å¯ä»¥ä½¿ç”¨`AutoModelFor`ç±»æˆ–åŸºç¡€æ¨¡å‹ç±»ï¼ˆå¦‚`OPTForCausalLM`æˆ–`LlamaForCausalLM`ï¼‰æ¥åŠ è½½ä¸€ä¸ªPEFT adapterã€‚


</Tip>

æ‚¨ä¹Ÿå¯ä»¥é€šè¿‡`load_adapter`æ–¹æ³•æ¥åŠ è½½ PEFT adapterã€‚

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "facebook/opt-350m"
peft_model_id = "ybelkada/opt-350m-lora"

model = AutoModelForCausalLM.from_pretrained(model_id)
model.load_adapter(peft_model_id)
```

## åŸºäº8bitæˆ–4bitè¿›è¡ŒåŠ è½½

`bitsandbytes`é›†æˆæ”¯æŒ8bitå’Œ4bitç²¾åº¦æ•°æ®ç±»å‹ï¼Œè¿™å¯¹äºåŠ è½½å¤§æ¨¡å‹éå¸¸æœ‰ç”¨ï¼Œå› ä¸ºå®ƒå¯ä»¥èŠ‚çœå†…å­˜ï¼ˆè¯·å‚é˜…`bitsandbytes`[æŒ‡å—](./quantization#bitsandbytes-integration)ä»¥äº†è§£æ›´å¤šä¿¡æ¯ï¼‰ã€‚è¦æœ‰æ•ˆåœ°å°†æ¨¡å‹åˆ†é…åˆ°æ‚¨çš„ç¡¬ä»¶ï¼Œè¯·åœ¨[`~PreTrainedModel.from_pretrained`]ä¸­æ·»åŠ `load_in_8bit`æˆ–`load_in_4bit`å‚æ•°ï¼Œå¹¶å°†`device_map="auto"`è®¾ç½®ä¸ºï¼š

```py
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

peft_model_id = "ybelkada/opt-350m-lora"
model = AutoModelForCausalLM.from_pretrained(peft_model_id, quantization_config=BitsAndBytesConfig(load_in_8bit=True))
```

## æ·»åŠ æ–°çš„adapter

ä½ å¯ä»¥ä½¿ç”¨[`~peft.PeftModel.add_adapter`]æ–¹æ³•ä¸ºä¸€ä¸ªå·²æœ‰adapterçš„æ¨¡å‹æ·»åŠ ä¸€ä¸ªæ–°çš„adapterï¼Œåªè¦æ–°adapterçš„ç±»å‹ä¸å½“å‰adapterç›¸åŒå³å¯ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æœ‰ä¸€ä¸ªé™„åŠ åˆ°æ¨¡å‹ä¸Šçš„LoRA adapterï¼š

```py
from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
from peft import PeftConfig

model_id = "facebook/opt-350m"
model = AutoModelForCausalLM.from_pretrained(model_id)

lora_config = LoraConfig(
    target_modules=["q_proj", "k_proj"],
    init_lora_weights=False
)

model.add_adapter(lora_config, adapter_name="adapter_1")
```


æ·»åŠ ä¸€ä¸ªæ–°çš„adapterï¼š

```py
# attach new adapter with same config
model.add_adapter(lora_config, adapter_name="adapter_2")
```
ç°åœ¨æ‚¨å¯ä»¥ä½¿ç”¨[`~peft.PeftModel.set_adapter`]æ¥è®¾ç½®è¦ä½¿ç”¨çš„adapterã€‚

```py
# use adapter_1
model.set_adapter("adapter_1")
output = model.generate(**inputs)
print(tokenizer.decode(output_disabled[0], skip_special_tokens=True))

# use adapter_2
model.set_adapter("adapter_2")
output_enabled = model.generate(**inputs)
print(tokenizer.decode(output_enabled[0], skip_special_tokens=True))
```

## å¯ç”¨å’Œç¦ç”¨adapters
ä¸€æ—¦æ‚¨å°†adapteræ·»åŠ åˆ°æ¨¡å‹ä¸­ï¼Œæ‚¨å¯ä»¥å¯ç”¨æˆ–ç¦ç”¨adapteræ¨¡å—ã€‚è¦å¯ç”¨adapteræ¨¡å—ï¼š


```py
from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer
from peft import PeftConfig

model_id = "facebook/opt-350m"
adapter_model_id = "ybelkada/opt-350m-lora"
tokenizer = AutoTokenizer.from_pretrained(model_id)
text = "Hello"
inputs = tokenizer(text, return_tensors="pt")

model = AutoModelForCausalLM.from_pretrained(model_id)
peft_config = PeftConfig.from_pretrained(adapter_model_id)

# to initiate with random weights
peft_config.init_lora_weights = False

model.add_adapter(peft_config)
model.enable_adapters()
output = model.generate(**inputs)
```
è¦ç¦ç”¨adapteræ¨¡å—ï¼š

```py
model.disable_adapters()
output = model.generate(**inputs)
```
## è®­ç»ƒä¸€ä¸ª PEFT adapter

PEFTé€‚é…å™¨å—[`Trainer`]ç±»æ”¯æŒï¼Œå› æ­¤æ‚¨å¯ä»¥ä¸ºæ‚¨çš„ç‰¹å®šç”¨ä¾‹è®­ç»ƒé€‚é…å™¨ã€‚å®ƒåªéœ€è¦æ·»åŠ å‡ è¡Œä»£ç å³å¯ã€‚ä¾‹å¦‚ï¼Œè¦è®­ç»ƒä¸€ä¸ªLoRA adapterï¼š


<Tip>

å¦‚æœä½ ä¸ç†Ÿæ‚‰å¦‚ä½•ä½¿ç”¨[`Trainer`]å¾®è°ƒæ¨¡å‹ï¼Œè¯·æŸ¥çœ‹[å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹](training)æ•™ç¨‹ã€‚

</Tip>

1. ä½¿ç”¨ä»»åŠ¡ç±»å‹å’Œè¶…å‚æ•°å®šä¹‰adapteré…ç½®ï¼ˆå‚è§[`~peft.LoraConfig`]ä»¥äº†è§£è¶…å‚æ•°çš„è¯¦ç»†ä¿¡æ¯ï¼‰ã€‚

```py
from peft import LoraConfig

peft_config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias="none",
    task_type="CAUSAL_LM",
)
```

2. å°†adapteræ·»åŠ åˆ°æ¨¡å‹ä¸­ã€‚

```py
model.add_adapter(peft_config)
```

3. ç°åœ¨å¯ä»¥å°†æ¨¡å‹ä¼ é€’ç»™[`Trainer`]äº†ï¼

```py
trainer = Trainer(model=model, ...)
trainer.train()
```

è¦ä¿å­˜è®­ç»ƒå¥½çš„adapterå¹¶é‡æ–°åŠ è½½å®ƒï¼š

```py
model.save_pretrained(save_dir)
model = AutoModelForCausalLM.from_pretrained(save_dir)
```

<!--
TODO: (@younesbelkada @stevhliu)
-   Link to PEFT docs for further details
-   Trainer  
-   8-bit / 4-bit examples ?
-->

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\performance.md
============================================================

# æ€§èƒ½ä¸å¯æ‰©å±•æ€§

è®­ç»ƒå¤§å‹transformeræ¨¡å‹å¹¶å°†å…¶éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒä¼šé¢ä¸´å„ç§æŒ‘æˆ˜ã€‚
åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹å¯èƒ½éœ€è¦æ¯”å¯ç”¨çš„GPUå†…å­˜æ›´å¤šçš„èµ„æºï¼Œæˆ–è€…è¡¨ç°å‡ºè¾ƒæ…¢çš„è®­ç»ƒé€Ÿåº¦ã€‚åœ¨éƒ¨ç½²é˜¶æ®µï¼Œæ¨¡å‹å¯èƒ½åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éš¾ä»¥å¤„ç†æ‰€éœ€çš„ååé‡ã€‚

æœ¬æ–‡æ¡£æ—¨åœ¨å¸®åŠ©æ‚¨å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œå¹¶æ‰¾åˆ°é€‚åˆæ‚¨ä½¿ç”¨åœºæ™¯çš„æœ€ä½³è®¾ç½®ã€‚æ•™ç¨‹åˆ†ä¸ºè®­ç»ƒå’Œæ¨ç†éƒ¨åˆ†ï¼Œå› ä¸ºæ¯ä¸ªéƒ¨åˆ†éƒ½æœ‰ä¸åŒçš„æŒ‘æˆ˜å’Œè§£å†³æ–¹æ¡ˆã€‚åœ¨æ¯ä¸ªéƒ¨åˆ†ä¸­ï¼Œæ‚¨å°†æ‰¾åˆ°é’ˆå¯¹ä¸åŒç¡¬ä»¶é…ç½®çš„å•ç‹¬æŒ‡å—ï¼Œä¾‹å¦‚å•GPUä¸å¤šGPUç”¨äºè®­ç»ƒæˆ–CPUä¸GPUç”¨äºæ¨ç†ã€‚

å°†æ­¤æ–‡æ¡£ä½œä¸ºæ‚¨çš„èµ·ç‚¹ï¼Œè¿›ä¸€æ­¥å¯¼èˆªåˆ°ä¸æ‚¨çš„æƒ…å†µåŒ¹é…çš„æ–¹æ³•ã€‚

## è®­ç»ƒ

é«˜æ•ˆè®­ç»ƒå¤§å‹transformeræ¨¡å‹éœ€è¦ä½¿ç”¨åŠ é€Ÿå™¨ç¡¬ä»¶ï¼Œå¦‚GPUæˆ–TPUã€‚æœ€å¸¸è§çš„æƒ…å†µæ˜¯æ‚¨åªæœ‰ä¸€ä¸ªGPUã€‚æ‚¨åº”ç”¨äºå•ä¸ªGPUä¸Šæé«˜è®­ç»ƒæ•ˆç‡çš„æ–¹æ³•å¯ä»¥æ‰©å±•åˆ°å…¶ä»–è®¾ç½®ï¼Œå¦‚å¤šä¸ªGPUã€‚ç„¶è€Œï¼Œä¹Ÿæœ‰ä¸€äº›ç‰¹å®šäºå¤šGPUæˆ–CPUè®­ç»ƒçš„æŠ€æœ¯ã€‚æˆ‘ä»¬åœ¨å•ç‹¬çš„éƒ¨åˆ†ä¸­ä»‹ç»å®ƒä»¬ã€‚

* [åœ¨å•ä¸ªGPUä¸Šè¿›è¡Œé«˜æ•ˆè®­ç»ƒçš„æ–¹æ³•å’Œå·¥å…·](perf_train_gpu_one)ï¼šä»è¿™é‡Œå¼€å§‹å­¦ä¹ å¸¸è§çš„æ–¹æ³•ï¼Œå¯ä»¥å¸®åŠ©ä¼˜åŒ–GPUå†…å­˜åˆ©ç”¨ç‡ã€åŠ å¿«è®­ç»ƒé€Ÿåº¦æˆ–ä¸¤è€…å…¼å¤‡ã€‚
* [å¤šGPUè®­ç»ƒéƒ¨åˆ†](perf_train_gpu_many)ï¼šæ¢ç´¢æ­¤éƒ¨åˆ†ä»¥äº†è§£é€‚ç”¨äºå¤šGPUè®¾ç½®çš„è¿›ä¸€æ­¥ä¼˜åŒ–æ–¹æ³•ï¼Œä¾‹å¦‚æ•°æ®å¹¶è¡Œã€å¼ é‡å¹¶è¡Œå’Œæµæ°´çº¿å¹¶è¡Œã€‚
* [CPUè®­ç»ƒéƒ¨åˆ†](perf_train_cpu)ï¼šäº†è§£åœ¨CPUä¸Šçš„æ··åˆç²¾åº¦è®­ç»ƒã€‚
* [åœ¨å¤šä¸ªCPUä¸Šè¿›è¡Œé«˜æ•ˆè®­ç»ƒ](perf_train_cpu_many)ï¼šäº†è§£åˆ†å¸ƒå¼CPUè®­ç»ƒã€‚
* [ä½¿ç”¨TensorFlowåœ¨TPUä¸Šè¿›è¡Œè®­ç»ƒ](perf_train_tpu_tf)ï¼šå¦‚æœæ‚¨å¯¹TPUè¿˜ä¸ç†Ÿæ‚‰ï¼Œè¯·å‚è€ƒæ­¤éƒ¨åˆ†ï¼Œäº†è§£æœ‰å…³åœ¨TPUä¸Šè¿›è¡Œè®­ç»ƒå’Œä½¿ç”¨XLAçš„å»ºè®®æ€§ä»‹ç»ã€‚
* [è‡ªå®šä¹‰ç¡¬ä»¶è¿›è¡Œè®­ç»ƒ](perf_hardware)ï¼šåœ¨æ„å»ºè‡ªå·±çš„æ·±åº¦å­¦ä¹ æœºå™¨æ—¶æŸ¥æ‰¾æŠ€å·§å’Œçªé—¨ã€‚
* [ä½¿ç”¨Trainer APIè¿›è¡Œè¶…å‚æ•°æœç´¢](hpo_train)


## æ¨ç†

åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å¯¹å¤§å‹æ¨¡å‹è¿›è¡Œé«˜æ•ˆæ¨ç†å¯èƒ½ä¸è®­ç»ƒå®ƒä»¬ä¸€æ ·å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†è¯¦ç»†ä»‹ç»å¦‚ä½•åœ¨CPUå’Œå•/å¤šGPUè®¾ç½®ä¸Šè¿›è¡Œæ¨ç†çš„æ­¥éª¤ã€‚

* [åœ¨å•ä¸ªCPUä¸Šè¿›è¡Œæ¨ç†](perf_infer_cpu)
* [åœ¨å•ä¸ªGPUä¸Šè¿›è¡Œæ¨ç†](perf_infer_gpu_one)
* [å¤šGPUæ¨ç†](perf_infer_gpu_one)
* [TensorFlowæ¨¡å‹çš„XLAé›†æˆ](tf_xla)

## è®­ç»ƒå’Œæ¨ç†

åœ¨è¿™é‡Œï¼Œæ‚¨å°†æ‰¾åˆ°é€‚ç”¨äºè®­ç»ƒæ¨¡å‹æˆ–ä½¿ç”¨å®ƒè¿›è¡Œæ¨ç†çš„æŠ€å·§ã€çªé—¨å’ŒæŠ€å·§ã€‚

* [å®ä¾‹åŒ–å¤§å‹æ¨¡å‹](big_models)
* [è§£å†³æ€§èƒ½é—®é¢˜](debugging)

## è´¡çŒ®

è¿™ä»½æ–‡æ¡£è¿˜è¿œè¿œæ²¡æœ‰å®Œæˆï¼Œè¿˜æœ‰å¾ˆå¤šéœ€è¦æ·»åŠ çš„å†…å®¹ï¼Œæ‰€ä»¥å¦‚æœä½ æœ‰è¡¥å……æˆ–æ›´æ­£çš„å†…å®¹ï¼Œè¯·æ¯«ä¸çŠ¹è±«åœ°æäº¤ä¸€ä¸ªPRï¼ˆPull Requestï¼‰ï¼Œæˆ–è€…å¦‚æœä½ ä¸ç¡®å®šï¼Œå¯ä»¥åˆ›å»ºä¸€ä¸ªIssueï¼Œæˆ‘ä»¬å¯ä»¥åœ¨é‚£é‡Œè®¨è®ºç»†èŠ‚ã€‚

åœ¨åšå‡ºè´¡çŒ®æ—¶ï¼Œå¦‚æœAæ¯”Bæ›´å¥½ï¼Œè¯·å°½é‡åŒ…å«å¯é‡å¤çš„åŸºå‡†æµ‹è¯•å’Œ(æˆ–)è¯¥ä¿¡æ¯æ¥æºçš„é“¾æ¥ï¼ˆé™¤éå®ƒç›´æ¥æ¥è‡ªæ‚¨ï¼‰ã€‚

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\perf_hardware.md
============================================================


# è®­ç»ƒç”¨çš„å®šåˆ¶ç¡¬ä»¶

æ‚¨ç”¨æ¥è¿è¡Œæ¨¡å‹è®­ç»ƒå’Œæ¨æ–­çš„ç¡¬ä»¶å¯èƒ½ä¼šå¯¹æ€§èƒ½äº§ç”Ÿé‡å¤§å½±å“ã€‚è¦æ·±å…¥äº†è§£ GPUï¼ŒåŠ¡å¿…æŸ¥çœ‹ Tim Dettmer å‡ºè‰²çš„[åšæ–‡](https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/)ã€‚

è®©æˆ‘ä»¬æ¥çœ‹ä¸€äº›å…³äº GPU é…ç½®çš„å®ç”¨å»ºè®®ã€‚

## GPU
å½“ä½ è®­ç»ƒæ›´å¤§çš„æ¨¡å‹æ—¶ï¼ŒåŸºæœ¬ä¸Šæœ‰ä¸‰ç§é€‰æ‹©ï¼š

- æ›´å¤§çš„ GPU
- æ›´å¤šçš„ GPU
- æ›´å¤šçš„ CPU å’Œ NVMeï¼ˆé€šè¿‡[DeepSpeed-Infinity](main_classes/deepspeed#nvme-support)å®ç°ï¼‰

è®©æˆ‘ä»¬ä»åªæœ‰ä¸€å—GPUçš„æƒ…å†µå¼€å§‹ã€‚

### ä¾›ç”µå’Œæ•£çƒ­

å¦‚æœæ‚¨è´­ä¹°äº†æ˜‚è´µçš„é«˜ç«¯GPUï¼Œè¯·ç¡®ä¿ä¸ºå…¶æä¾›æ­£ç¡®çš„ä¾›ç”µå’Œè¶³å¤Ÿçš„æ•£çƒ­ã€‚

**ä¾›ç”µ**ï¼š

ä¸€äº›é«˜ç«¯æ¶ˆè´¹è€…çº§GPUå¡å…·æœ‰2ä¸ªï¼Œæœ‰æ—¶ç”šè‡³3ä¸ªPCI-E-8é’ˆç”µæºæ’å£ã€‚è¯·ç¡®ä¿å°†ä¸æ’å£æ•°é‡ç›¸åŒçš„ç‹¬ç«‹12V PCI-E-8é’ˆçº¿ç¼†æ’å…¥å¡ä¸­ã€‚ä¸è¦ä½¿ç”¨åŒä¸€æ ¹çº¿ç¼†ä¸¤ç«¯çš„2ä¸ªåˆ†å‰ï¼ˆä¹Ÿç§°ä¸ºpigtail cableï¼‰ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå¦‚æœæ‚¨çš„GPUä¸Šæœ‰2ä¸ªæ’å£ï¼Œæ‚¨éœ€è¦ä½¿ç”¨2æ¡PCI-E-8é’ˆçº¿ç¼†è¿æ¥ç”µæºå’Œå¡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨ä¸€æ¡æœ«ç«¯æœ‰2ä¸ªPCI-E-8é’ˆè¿æ¥å™¨çš„çº¿ç¼†ï¼å¦åˆ™ï¼Œæ‚¨æ— æ³•å……åˆ†å‘æŒ¥å¡çš„æ€§èƒ½ã€‚

æ¯ä¸ªPCI-E-8é’ˆç”µæºçº¿ç¼†éœ€è¦æ’å…¥ç”µæºä¾§çš„12Vè½¨ä¸Šï¼Œå¹¶ä¸”å¯ä»¥æä¾›æœ€å¤š150Wçš„åŠŸç‡ã€‚

å…¶ä»–ä¸€äº›å¡å¯èƒ½ä½¿ç”¨PCI-E-12é’ˆè¿æ¥å™¨ï¼Œè¿™äº›è¿æ¥å™¨å¯ä»¥æä¾›æœ€å¤š500-600Wçš„åŠŸç‡ã€‚

ä½ç«¯å¡å¯èƒ½ä½¿ç”¨6é’ˆè¿æ¥å™¨ï¼Œè¿™äº›è¿æ¥å™¨å¯æä¾›æœ€å¤š75Wçš„åŠŸç‡ã€‚

æ­¤å¤–ï¼Œæ‚¨éœ€è¦é€‰æ‹©å…·æœ‰ç¨³å®šç”µå‹çš„é«˜ç«¯ç”µæºã€‚ä¸€äº›è´¨é‡è¾ƒä½çš„ç”µæºå¯èƒ½æ— æ³•ä¸ºå¡æä¾›æ‰€éœ€çš„ç¨³å®šç”µå‹ä»¥å‘æŒ¥å…¶æœ€å¤§æ€§èƒ½ã€‚

å½“ç„¶ï¼Œç”µæºè¿˜éœ€è¦æœ‰è¶³å¤Ÿçš„æœªä½¿ç”¨çš„ç“¦æ•°æ¥ä¸ºå¡ä¾›ç”µã€‚

**æ•£çƒ­**ï¼š

å½“GPUè¿‡çƒ­æ—¶ï¼Œå®ƒå°†å¼€å§‹é™é¢‘ï¼Œä¸ä¼šæä¾›å®Œæ•´çš„æ€§èƒ½ã€‚å¦‚æœæ¸©åº¦è¿‡é«˜ï¼Œå¯èƒ½ä¼šç¼©çŸ­GPUçš„ä½¿ç”¨å¯¿å‘½ã€‚

å½“GPUè´Ÿè½½å¾ˆé‡æ—¶ï¼Œå¾ˆéš¾ç¡®å®šæœ€ä½³æ¸©åº¦æ˜¯å¤šå°‘ï¼Œä½†ä»»ä½•ä½äº+80åº¦çš„æ¸©åº¦éƒ½æ˜¯å¥½çš„ï¼Œè¶Šä½è¶Šå¥½ï¼Œä¹Ÿè®¸åœ¨70-75åº¦ä¹‹é—´æ˜¯ä¸€ä¸ªéå¸¸å¥½çš„èŒƒå›´ã€‚é™é¢‘å¯èƒ½ä»å¤§çº¦84-90åº¦å¼€å§‹ã€‚ä½†æ˜¯é™¤äº†é™é¢‘å¤–ï¼ŒæŒç»­çš„é«˜æ¸©å¯èƒ½ä¼šç¼©çŸ­GPUçš„ä½¿ç”¨å¯¿å‘½ã€‚

æ¥ä¸‹æ¥è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹æ‹¥æœ‰å¤šä¸ªGPUæ—¶æœ€é‡è¦çš„æ–¹é¢ä¹‹ä¸€ï¼šè¿æ¥ã€‚

### å¤šGPUè¿æ¥

å¦‚æœæ‚¨ä½¿ç”¨å¤šä¸ªGPUï¼Œåˆ™å¡ä¹‹é—´çš„äº’è¿æ–¹å¼å¯èƒ½ä¼šå¯¹æ€»è®­ç»ƒæ—¶é—´äº§ç”Ÿå·¨å¤§å½±å“ã€‚å¦‚æœGPUä½äºåŒä¸€ç‰©ç†èŠ‚ç‚¹ä¸Šï¼Œæ‚¨å¯ä»¥è¿è¡Œä»¥ä¸‹ä»£ç ï¼š

```bash
nvidia-smi topo -m
```

å®ƒå°†å‘Šè¯‰æ‚¨GPUå¦‚ä½•äº’è¿ã€‚åœ¨å…·æœ‰åŒGPUå¹¶é€šè¿‡NVLinkè¿æ¥çš„æœºå™¨ä¸Šï¼Œæ‚¨æœ€æœ‰å¯èƒ½çœ‹åˆ°ç±»ä¼¼ä»¥ä¸‹å†…å®¹ï¼š

```
        GPU0    GPU1    CPU Affinity    NUMA Affinity
GPU0     X      NV2     0-23            N/A
GPU1    NV2      X      0-23            N/A
```

åœ¨ä¸åŒçš„æœºå™¨ä¸Šï¼Œå¦‚æœæ²¡æœ‰NVLinkï¼Œæˆ‘ä»¬å¯èƒ½ä¼šçœ‹åˆ°ï¼š
```
        GPU0    GPU1    CPU Affinity    NUMA Affinity
GPU0     X      PHB     0-11            N/A
GPU1    PHB      X      0-11            N/A
```

è¿™ä¸ªæŠ¥å‘ŠåŒ…æ‹¬äº†è¿™ä¸ªè¾“å‡ºï¼š

```
  X    = Self
  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)
  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node
  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)
  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)
  PIX  = Connection traversing at most a single PCIe bridge
  NV#  = Connection traversing a bonded set of # NVLinks
```

å› æ­¤ï¼Œç¬¬ä¸€ä¸ªæŠ¥å‘Š`NV2`å‘Šè¯‰æˆ‘ä»¬GPUé€šè¿‡2ä¸ªNVLinkäº’è¿ï¼Œè€Œç¬¬äºŒä¸ªæŠ¥å‘Š`PHB`å±•ç¤ºäº†å…¸å‹çš„æ¶ˆè´¹è€…çº§PCIe+Bridgeè®¾ç½®ã€‚

æ£€æŸ¥ä½ çš„è®¾ç½®ä¸­å…·æœ‰å“ªç§è¿æ¥ç±»å‹ã€‚å…¶ä¸­ä¸€äº›ä¼šä½¿å¡ä¹‹é—´çš„é€šä¿¡æ›´å¿«ï¼ˆä¾‹å¦‚NVLinkï¼‰ï¼Œè€Œå…¶ä»–åˆ™è¾ƒæ…¢ï¼ˆä¾‹å¦‚PHBï¼‰ã€‚

æ ¹æ®ä½¿ç”¨çš„æ‰©å±•è§£å†³æ–¹æ¡ˆçš„ç±»å‹ï¼Œè¿æ¥é€Ÿåº¦å¯èƒ½ä¼šäº§ç”Ÿé‡å¤§æˆ–è¾ƒå°çš„å½±å“ã€‚å¦‚æœGPUå¾ˆå°‘éœ€è¦åŒæ­¥ï¼Œå°±åƒåœ¨DDPä¸­ä¸€æ ·ï¼Œé‚£ä¹ˆè¾ƒæ…¢çš„è¿æ¥çš„å½±å“å°†ä¸é‚£ä¹ˆæ˜¾è‘—ã€‚å¦‚æœGPUç»å¸¸éœ€è¦ç›¸äº’å‘é€æ¶ˆæ¯ï¼Œå°±åƒåœ¨ZeRO-DPä¸­ä¸€æ ·ï¼Œé‚£ä¹ˆæ›´å¿«çš„è¿æ¥å¯¹äºå®ç°æ›´å¿«çš„è®­ç»ƒå˜å¾—éå¸¸é‡è¦ã€‚


#### NVlink

[NVLink](https://en.wikipedia.org/wiki/NVLink)æ˜¯ç”±Nvidiaå¼€å‘çš„ä¸€ç§åŸºäºçº¿ç¼†çš„ä¸²è¡Œå¤šé€šé“è¿‘ç¨‹é€šä¿¡é“¾æ¥ã€‚

æ¯ä¸ªæ–°ä¸€ä»£æä¾›æ›´å¿«çš„å¸¦å®½ï¼Œä¾‹å¦‚åœ¨[Nvidia Ampere GA102 GPUæ¶æ„](https://www.nvidia.com/content/dam/en-zz/Solutions/geforce/ampere/pdf/NVIDIA-ampere-GA102-GPU-Architecture-Whitepaper-V1.pdf)ä¸­æœ‰è¿™æ ·çš„å¼•è¿°ï¼š

> Third-Generation NVLinkÂ®
> GA102 GPUs utilize NVIDIAâ€™s third-generation NVLink interface, which includes four x4 links,
> with each link providing 14.0625 GB/sec bandwidth in each direction between two GPUs. Four
> links provide 56.25 GB/sec bandwidth in each direction, and 112.5 GB/sec total bandwidth
> between two GPUs. Two RTX 3090 GPUs can be connected together for SLI using NVLink.
> (Note that 3-Way and 4-Way SLI configurations are not supported.)

æ‰€ä»¥ï¼Œåœ¨`nvidia-smi topo -m`è¾“å‡ºçš„`NVX`æŠ¥å‘Šä¸­è·å–åˆ°çš„æ›´é«˜çš„`X`å€¼æ„å‘³ç€æ›´å¥½çš„æ€§èƒ½ã€‚ç”Ÿæˆçš„ç»“æœå°†å–å†³äºæ‚¨çš„GPUæ¶æ„ã€‚

è®©æˆ‘ä»¬æ¯”è¾ƒåœ¨å°æ ·æœ¬wikitextä¸Šè®­ç»ƒgpt2è¯­è¨€æ¨¡å‹çš„æ‰§è¡Œç»“æœã€‚

ç»“æœæ˜¯ï¼š


| NVlink | Time |
| -----  | ---: |
| Y      | 101s |
| N      | 131s |


å¯ä»¥çœ‹åˆ°ï¼ŒNVLinkä½¿è®­ç»ƒé€Ÿåº¦æé«˜äº†çº¦23%ã€‚åœ¨ç¬¬äºŒä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨`NCCL_P2P_DISABLE=1`å‘Šè¯‰GPUä¸è¦ä½¿ç”¨NVLinkã€‚

è¿™é‡Œæ˜¯å®Œæ•´çš„åŸºå‡†æµ‹è¯•ä»£ç å’Œè¾“å‡ºï¼š

```bash
# DDP w/ NVLink

rm -r /tmp/test-clm; CUDA_VISIBLE_DEVICES=0,1 torchrun \
--nproc_per_node 2 examples/pytorch/language-modeling/run_clm.py --model_name_or_path openai-community/gpt2 \
--dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 --do_train \
--output_dir /tmp/test-clm --per_device_train_batch_size 4 --max_steps 200

{'train_runtime': 101.9003, 'train_samples_per_second': 1.963, 'epoch': 0.69}

# DDP w/o NVLink

rm -r /tmp/test-clm; CUDA_VISIBLE_DEVICES=0,1 NCCL_P2P_DISABLE=1 torchrun \
--nproc_per_node 2 examples/pytorch/language-modeling/run_clm.py --model_name_or_path openai-community/gpt2 \
--dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 --do_train
--output_dir /tmp/test-clm --per_device_train_batch_size 4 --max_steps 200

{'train_runtime': 131.4367, 'train_samples_per_second': 1.522, 'epoch': 0.69}
```

ç¡¬ä»¶: 2x TITAN RTX 24GB each + NVlink with 2 NVLinks (`NV2` in `nvidia-smi topo -m`)
è½¯ä»¶: `pytorch-1.8-to-be` + `cuda-11.0` / `transformers==4.3.0.dev0`

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\perf_infer_gpu_multi.md
============================================================



# å¤šGPUæ¨ç†

æŸäº›æ¨¡å‹ç°å·²æ”¯æŒå†…ç½®çš„**å¼ é‡å¹¶è¡Œ**ï¼ˆTensor Parallelism, TPï¼‰ï¼Œå¹¶é€šè¿‡ PyTorch å®ç°ã€‚å¼ é‡å¹¶è¡ŒæŠ€æœ¯å°†æ¨¡å‹åˆ‡åˆ†åˆ°å¤šä¸ª GPU ä¸Šï¼Œä»è€Œæ”¯æŒæ›´å¤§çš„æ¨¡å‹å°ºå¯¸ï¼Œå¹¶å¯¹è¯¸å¦‚çŸ©é˜µä¹˜æ³•ç­‰è®¡ç®—ä»»åŠ¡è¿›è¡Œå¹¶è¡ŒåŒ–ã€‚

è¦å¯ç”¨å¼ é‡å¹¶è¡Œï¼Œåªéœ€åœ¨è°ƒç”¨ [`~AutoModelForCausalLM.from_pretrained`] æ—¶ä¼ é€’å‚æ•° `tp_plan="auto"`ï¼š

```python
import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "meta-llama/Meta-Llama-3-8B-Instruct"

# åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
rank = int(os.environ["RANK"])
device = torch.device(f"cuda:{rank}")
torch.cuda.set_device(device)
torch.distributed.init_process_group("nccl", device_id=device)

# è·å–æ”¯æŒå¼ é‡å¹¶è¡Œçš„æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    tp_plan="auto",
)

# å‡†å¤‡è¾“å…¥tokens
tokenizer = AutoTokenizer.from_pretrained(model_id)
prompt = "Can I help"
inputs = tokenizer(prompt, return_tensors="pt").input_ids.to(device)

# åˆ†å¸ƒå¼è¿è¡Œ
outputs = model(inputs)
```

æ‚¨å¯ä»¥ä½¿ç”¨ `torchrun` å‘½ä»¤å¯åŠ¨ä¸Šè¿°è„šæœ¬ï¼Œå¤šè¿›ç¨‹æ¨¡å¼ä¼šè‡ªåŠ¨å°†æ¯ä¸ªè¿›ç¨‹æ˜ å°„åˆ°ä¸€å¼  GPUï¼š

```
torchrun --nproc-per-node 4 demo.py
```

ç›®å‰ï¼ŒPyTorch å¼ é‡å¹¶è¡Œæ”¯æŒä»¥ä¸‹æ¨¡å‹ï¼š
* [Llama](https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaModel)

å¦‚æœæ‚¨å¸Œæœ›å¯¹å…¶ä»–æ¨¡å‹æ·»åŠ å¼ é‡å¹¶è¡Œæ”¯æŒï¼Œå¯ä»¥é€šè¿‡æäº¤ GitHub Issue æˆ– Pull Request æ¥æå‡ºè¯·æ±‚ã€‚

### é¢„æœŸæ€§èƒ½æå‡

å¯¹äºæ¨ç†åœºæ™¯ï¼ˆå°¤å…¶æ˜¯å¤„ç†å¤§æ‰¹é‡æˆ–é•¿åºåˆ—çš„è¾“å…¥ï¼‰ï¼Œå¼ é‡å¹¶è¡Œå¯ä»¥æ˜¾è‘—æå‡è®¡ç®—é€Ÿåº¦ã€‚

ä»¥ä¸‹æ˜¯ [Llama](https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaModel) æ¨¡å‹åœ¨åºåˆ—é•¿åº¦ä¸º 512 ä¸”ä¸åŒæ‰¹é‡å¤§å°æƒ…å†µä¸‹çš„å•æ¬¡å‰å‘æ¨ç†çš„é¢„æœŸåŠ é€Ÿæ•ˆæœï¼š

<div style="text-align: center">
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/Meta-Llama-3-8B-Instruct%2C%20seqlen%20%3D%20512%2C%20python%2C%20w_%20compile.png">
</div>

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\perf_torch_compile.md
============================================================



# ä½¿ç”¨ torch.compile() ä¼˜åŒ–æ¨ç†

æœ¬æŒ‡å—æ—¨åœ¨ä¸ºä½¿ç”¨[`torch.compile()`](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)åœ¨[ğŸ¤— Transformersä¸­çš„è®¡ç®—æœºè§†è§‰æ¨¡å‹](https://huggingface.co/models?pipeline_tag=image-classification&library=transformers&sort=trending)ä¸­å¼•å…¥çš„æ¨ç†é€Ÿåº¦æå‡æä¾›ä¸€ä¸ªåŸºå‡†ã€‚


## torch.compile çš„ä¼˜åŠ¿
   
æ ¹æ®æ¨¡å‹å’ŒGPUçš„ä¸åŒï¼Œ`torch.compile()`åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯ä»¥æé«˜å¤šè¾¾30%çš„é€Ÿåº¦ã€‚è¦ä½¿ç”¨`torch.compile()`ï¼Œåªéœ€å®‰è£…2.0åŠä»¥ä¸Šç‰ˆæœ¬çš„`torch`å³å¯ã€‚

ç¼–è¯‘æ¨¡å‹éœ€è¦æ—¶é—´ï¼Œå› æ­¤å¦‚æœæ‚¨åªéœ€è¦ç¼–è¯‘ä¸€æ¬¡æ¨¡å‹è€Œä¸æ˜¯æ¯æ¬¡æ¨ç†éƒ½ç¼–è¯‘ï¼Œé‚£ä¹ˆå®ƒéå¸¸æœ‰ç”¨ã€‚
è¦ç¼–è¯‘æ‚¨é€‰æ‹©çš„ä»»ä½•è®¡ç®—æœºè§†è§‰æ¨¡å‹ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ–¹å¼è°ƒç”¨`torch.compile()`ï¼š


```diff
from transformers import AutoModelForImageClassification

model = AutoModelForImageClassification.from_pretrained(MODEL_ID).to("cuda")
+ model = torch.compile(model)
```

`compile()` æä¾›äº†å¤šç§ç¼–è¯‘æ¨¡å¼ï¼Œå®ƒä»¬åœ¨ç¼–è¯‘æ—¶é—´å’Œæ¨ç†å¼€é”€ä¸Šæœ‰æ‰€ä¸åŒã€‚`max-autotune` æ¯” `reduce-overhead` éœ€è¦æ›´é•¿çš„æ—¶é—´ï¼Œä½†ä¼šå¾—åˆ°æ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚é»˜è®¤æ¨¡å¼åœ¨ç¼–è¯‘æ—¶æœ€å¿«ï¼Œä½†åœ¨æ¨ç†æ—¶é—´ä¸Šä¸ `reduce-overhead` ç›¸æ¯”æ•ˆç‡è¾ƒä½ã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†é»˜è®¤æ¨¡å¼ã€‚æ‚¨å¯ä»¥åœ¨[è¿™é‡Œ](https://pytorch.org/get-started/pytorch-2.0/#user-experience)äº†è§£æ›´å¤šä¿¡æ¯ã€‚

æˆ‘ä»¬åœ¨ PyTorch 2.0.1 ç‰ˆæœ¬ä¸Šä½¿ç”¨ä¸åŒçš„è®¡ç®—æœºè§†è§‰æ¨¡å‹ã€ä»»åŠ¡ã€ç¡¬ä»¶ç±»å‹å’Œæ•°æ®æ‰¹é‡å¤§å°å¯¹ `torch.compile` è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚

## åŸºå‡†æµ‹è¯•ä»£ç 

ä»¥ä¸‹æ˜¯æ¯ä¸ªä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ä»£ç ã€‚æˆ‘ä»¬åœ¨æ¨ç†ä¹‹å‰â€é¢„çƒ­â€œGPUï¼Œå¹¶å–300æ¬¡æ¨ç†çš„å¹³å‡å€¼ï¼Œæ¯æ¬¡ä½¿ç”¨ç›¸åŒçš„å›¾åƒã€‚

### ä½¿ç”¨ ViT è¿›è¡Œå›¾åƒåˆ†ç±»

```python 
import torch
from PIL import Image
import requests
import numpy as np
from transformers import AutoImageProcessor, AutoModelForImageClassification

url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)

processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")
model = AutoModelForImageClassification.from_pretrained("google/vit-base-patch16-224").to("cuda")
model = torch.compile(model)

processed_input = processor(image, return_tensors='pt').to(device="cuda")

with torch.no_grad():
    _ = model(**processed_input)

```

#### ä½¿ç”¨ DETR è¿›è¡Œç›®æ ‡æ£€æµ‹

```python 
from transformers import AutoImageProcessor, AutoModelForObjectDetection

processor = AutoImageProcessor.from_pretrained("facebook/detr-resnet-50")
model = AutoModelForObjectDetection.from_pretrained("facebook/detr-resnet-50").to("cuda")
model = torch.compile(model)

texts = ["a photo of a cat", "a photo of a dog"]
inputs = processor(text=texts, images=image, return_tensors="pt").to("cuda")

with torch.no_grad():
    _ = model(**inputs)
```

#### ä½¿ç”¨ Segformer è¿›è¡Œå›¾åƒåˆ†å‰²

```python 
from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation

processor = SegformerImageProcessor.from_pretrained("nvidia/segformer-b0-finetuned-ade-512-512")
model = SegformerForSemanticSegmentation.from_pretrained("nvidia/segformer-b0-finetuned-ade-512-512").to("cuda")
model = torch.compile(model)
seg_inputs = processor(images=image, return_tensors="pt").to("cuda")

with torch.no_grad():
    _ = model(**seg_inputs)
```

ä»¥ä¸‹æ˜¯æˆ‘ä»¬è¿›è¡ŒåŸºå‡†æµ‹è¯•çš„æ¨¡å‹åˆ—è¡¨ã€‚

**å›¾åƒåˆ†ç±»** 
- [google/vit-base-patch16-224](https://huggingface.co/google/vit-base-patch16-224)
- [microsoft/beit-base-patch16-224-pt22k-ft22k](https://huggingface.co/microsoft/beit-base-patch16-224-pt22k-ft22k)
- [facebook/convnext-large-224](https://huggingface.co/facebook/convnext-large-224)
- [microsoft/resnet-50](https://huggingface.co/)

**å›¾åƒåˆ†å‰²** 
- [nvidia/segformer-b0-finetuned-ade-512-512](https://huggingface.co/nvidia/segformer-b0-finetuned-ade-512-512)
- [facebook/mask2former-swin-tiny-coco-panoptic](https://huggingface.co/facebook/mask2former-swin-tiny-coco-panoptic)
- [facebook/maskformer-swin-base-ade](https://huggingface.co/facebook/maskformer-swin-base-ade)
- [google/deeplabv3_mobilenet_v2_1.0_513](https://huggingface.co/google/deeplabv3_mobilenet_v2_1.0_513)

**ç›®æ ‡æ£€æµ‹** 
- [google/owlvit-base-patch32](https://huggingface.co/google/owlvit-base-patch32)
- [facebook/detr-resnet-101](https://huggingface.co/facebook/detr-resnet-101)
- [microsoft/conditional-detr-resnet-50](https://huggingface.co/microsoft/conditional-detr-resnet-50)

 ä¸‹é¢æ˜¯ä½¿ç”¨å’Œä¸ä½¿ç”¨`torch.compile()`çš„æ¨ç†æŒç»­æ—¶é—´å¯è§†åŒ–ï¼Œä»¥åŠæ¯ä¸ªæ¨¡å‹åœ¨ä¸åŒç¡¬ä»¶å’Œæ•°æ®æ‰¹é‡å¤§å°ä¸‹çš„æ”¹è¿›ç™¾åˆ†æ¯”ã€‚


<div class="flex">
  <div>
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/a100_batch_comp.png" />
  </div>
  <div>
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/v100_batch_comp.png" />
  </div>
   <div>
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/t4_batch_comp.png" />
  </div>
</div>

<div class="flex">
  <div>
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/A100_1_duration.png" />
  </div>
  <div>
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/A100_1_percentage.png" />
  </div>
</div>


![Duration Comparison on V100 with Batch Size of 1](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/v100_1_duration.png)

![Percentage Improvement on T4 with Batch Size of 4](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/T4_4_percentage.png)

ä¸‹é¢å¯ä»¥æ‰¾åˆ°æ¯ä¸ªæ¨¡å‹ä½¿ç”¨å’Œä¸ä½¿ç”¨`compile()`çš„æ¨ç†æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ã€‚è¯·æ³¨æ„ï¼ŒOwlViTåœ¨å¤§æ‰¹é‡å¤§å°ä¸‹ä¼šå¯¼è‡´å†…å­˜æº¢å‡ºã€‚

### A100 (batch size: 1)

| **Task/Model** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compile** |
|:---:|:---:|:---:|
| Image Classification/ViT | 9.325 | 7.584 | 
| Image Segmentation/Segformer | 11.759 | 10.500 |
| Object Detection/OwlViT | 24.978 | 18.420 |
| Image Classification/BeiT | 11.282 | 8.448 | 
| Object Detection/DETR | 34.619 | 19.040 |
| Image Classification/ConvNeXT | 10.410 | 10.208 | 
| Image Classification/ResNet | 6.531 | 4.124 |
| Image Segmentation/Mask2former | 60.188 | 49.117 |
| Image Segmentation/Maskformer | 75.764 | 59.487 | 
| Image Segmentation/MobileNet | 8.583 | 3.974 |
| Object Detection/Resnet-101 | 36.276 | 18.197 |
| Object Detection/Conditional-DETR | 31.219 | 17.993 |


### A100 (batch size: 4)

| **Task/Model** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compile** |
|:---:|:---:|:---:|
| Image Classification/ViT | 14.832 | 14.499 | 
| Image Segmentation/Segformer | 18.838 | 16.476 |
| Image Classification/BeiT | 13.205 | 13.048 | 
| Object Detection/DETR | 48.657 | 32.418|
| Image Classification/ConvNeXT | 22.940 | 21.631 | 
| Image Classification/ResNet | 6.657 | 4.268 |
| Image Segmentation/Mask2former | 74.277 | 61.781 |
| Image Segmentation/Maskformer | 180.700 | 159.116 | 
| Image Segmentation/MobileNet | 14.174 | 8.515 |
| Object Detection/Resnet-101 | 68.101 | 44.998 |
| Object Detection/Conditional-DETR | 56.470 | 35.552 |

### A100 (batch size: 16)

| **Task/Model** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compile** |
|:---:|:---:|:---:|
| Image Classification/ViT | 40.944 | 40.010 | 
| Image Segmentation/Segformer | 37.005 | 31.144 |
| Image Classification/BeiT | 41.854 | 41.048 | 
| Object Detection/DETR | 164.382 | 161.902 |
| Image Classification/ConvNeXT | 82.258 | 75.561 | 
| Image Classification/ResNet | 7.018 | 5.024 |
| Image Segmentation/Mask2former | 178.945 | 154.814 |
| Image Segmentation/Maskformer | 638.570 | 579.826 | 
| Image Segmentation/MobileNet | 51.693 | 30.310 |
| Object Detection/Resnet-101 | 232.887 | 155.021 |
| Object Detection/Conditional-DETR | 180.491 | 124.032 |

### V100 (batch size: 1)

| **Task/Model** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compile** |
|:---:|:---:|:---:|
| Image Classification/ViT | 10.495 | 6.00 | 
| Image Segmentation/Segformer | 13.321 | 5.862 | 
| Object Detection/OwlViT | 25.769 | 22.395 | 
| Image Classification/BeiT | 11.347 | 7.234 | 
| Object Detection/DETR | 33.951 | 19.388 |
| Image Classification/ConvNeXT | 11.623 | 10.412 | 
| Image Classification/ResNet | 6.484 | 3.820 |
| Image Segmentation/Mask2former | 64.640 | 49.873 |
| Image Segmentation/Maskformer | 95.532 | 72.207 | 
| Image Segmentation/MobileNet | 9.217 | 4.753 |
| Object Detection/Resnet-101 | 52.818 | 28.367 |
| Object Detection/Conditional-DETR | 39.512 | 20.816 |

### V100 (batch size: 4)

| **Task/Model** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compile** |
|:---:|:---:|:---:|
| Image Classification/ViT | 15.181 | 14.501 | 
| Image Segmentation/Segformer | 16.787 | 16.188 |
| Image Classification/BeiT | 15.171 | 14.753 | 
| Object Detection/DETR | 88.529 | 64.195 |
| Image Classification/ConvNeXT | 29.574 | 27.085 | 
| Image Classification/ResNet | 6.109 | 4.731 |
| Image Segmentation/Mask2former | 90.402 | 76.926 |
| Image Segmentation/Maskformer | 234.261 | 205.456 | 
| Image Segmentation/MobileNet | 24.623 | 14.816 |
| Object Detection/Resnet-101 | 134.672 | 101.304 |
| Object Detection/Conditional-DETR | 97.464 | 69.739 |

### V100 (batch size: 16)

| **Task/Model** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compile** |
|:---:|:---:|:---:|
| Image Classification/ViT | 52.209 | 51.633 | 
| Image Segmentation/Segformer | 61.013 | 55.499 |
| Image Classification/BeiT | 53.938 | 53.581  |
| Object Detection/DETR | OOM | OOM |
| Image Classification/ConvNeXT | 109.682 | 100.771 | 
| Image Classification/ResNet | 14.857 | 12.089 |
| Image Segmentation/Mask2former | 249.605 | 222.801 |
| Image Segmentation/Maskformer | 831.142 | 743.645 | 
| Image Segmentation/MobileNet | 93.129 | 55.365 |
| Object Detection/Resnet-101 | 482.425 | 361.843 |
| Object Detection/Conditional-DETR | 344.661 | 255.298 |

### T4 (batch size: 1)

| **Task/Model** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compile** |
|:---:|:---:|:---:|
| Image Classification/ViT | 16.520 | 15.786 | 
| Image Segmentation/Segformer | 16.116 | 14.205 |
| Object Detection/OwlViT | 53.634 | 51.105 |
| Image Classification/BeiT | 16.464 | 15.710 | 
| Object Detection/DETR | 73.100 | 53.99 |
| Image Classification/ConvNeXT | 32.932 | 30.845 | 
| Image Classification/ResNet | 6.031 | 4.321 |
| Image Segmentation/Mask2former | 79.192 | 66.815 |
| Image Segmentation/Maskformer | 200.026 | 188.268 | 
| Image Segmentation/MobileNet | 18.908 | 11.997 |
| Object Detection/Resnet-101 | 106.622 | 82.566 |
| Object Detection/Conditional-DETR | 77.594 | 56.984 |

### T4 (batch size: 4)

| **Task/Model** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compile** |
|:---:|:---:|:---:|
| Image Classification/ViT | 43.653 | 43.626 | 
| Image Segmentation/Segformer | 45.327 | 42.445 |
| Image Classification/BeiT | 52.007 | 51.354 | 
| Object Detection/DETR | 277.850 | 268.003 |
| Image Classification/ConvNeXT | 119.259 | 105.580 | 
| Image Classification/ResNet | 13.039 | 11.388 |
| Image Segmentation/Mask2former | 201.540 | 184.670 |
| Image Segmentation/Maskformer | 764.052 | 711.280 | 
| Image Segmentation/MobileNet | 74.289 | 48.677 |
| Object Detection/Resnet-101 | 421.859 | 357.614 |
| Object Detection/Conditional-DETR | 289.002 | 226.945 |

### T4 (batch size: 16)

| **Task/Model** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compile** |
|:---:|:---:|:---:|
| Image Classification/ViT | 163.914 | 160.907 | 
| Image Segmentation/Segformer | 192.412 | 163.620 |
| Image Classification/BeiT | 188.978 | 187.976 | 
| Object Detection/DETR | OOM | OOM |
| Image Classification/ConvNeXT | 422.886 | 388.078 | 
| Image Classification/ResNet | 44.114 | 37.604 |
| Image Segmentation/Mask2former | 756.337 | 695.291 |
| Image Segmentation/Maskformer | 2842.940 | 2656.88 | 
| Image Segmentation/MobileNet | 299.003 | 201.942 |
| Object Detection/Resnet-101 |  1619.505 | 1262.758 | 
| Object Detection/Conditional-DETR | 1137.513 | 897.390|

## PyTorch Nightly
æˆ‘ä»¬è¿˜åœ¨ PyTorch Nightly ç‰ˆæœ¬ï¼ˆ2.1.0devï¼‰ä¸Šè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå¯ä»¥åœ¨[è¿™é‡Œ](https://download.pytorch.org/whl/nightly/cu118)æ‰¾åˆ° Nightly ç‰ˆæœ¬çš„å®‰è£…åŒ…ï¼Œå¹¶è§‚å¯Ÿåˆ°äº†æœªç¼–è¯‘å’Œç¼–è¯‘æ¨¡å‹çš„å»¶è¿Ÿæ€§èƒ½æ”¹å–„ã€‚

### A100

| **Task/Model** | **Batch Size** | **torch 2.0 - no compile** | **torch 2.0 -<br> compile** |
|:---:|:---:|:---:|:---:|
| Image Classification/BeiT | Unbatched | 12.462 | 6.954 | 
| Image Classification/BeiT | 4 | 14.109 | 12.851 | 
| Image Classification/BeiT | 16 | 42.179 | 42.147 | 
| Object Detection/DETR | Unbatched | 30.484 | 15.221 |
| Object Detection/DETR | 4 | 46.816 | 30.942 |
| Object Detection/DETR | 16 | 163.749 | 163.706  |

### T4

| **Task/Model** | **Batch Size** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compile** |
|:---:|:---:|:---:|:---:|
| Image Classification/BeiT | Unbatched | 14.408 | 14.052 | 
| Image Classification/BeiT | 4 | 47.381 | 46.604 | 
| Image Classification/BeiT | 16 | 42.179 | 42.147  | 
| Object Detection/DETR | Unbatched | 68.382 | 53.481 |
| Object Detection/DETR | 4 | 269.615 | 204.785 |
| Object Detection/DETR | 16 | OOM | OOM   |

### V100

| **Task/Model** | **Batch Size** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compile** |
|:---:|:---:|:---:|:---:|
| Image Classification/BeiT | Unbatched | 13.477 | 7.926 | 
| Image Classification/BeiT | 4 | 15.103 | 14.378 | 
| Image Classification/BeiT | 16 | 52.517 | 51.691  | 
| Object Detection/DETR | Unbatched | 28.706 | 19.077 |
| Object Detection/DETR | 4 | 88.402 | 62.949|
| Object Detection/DETR | 16 | OOM | OOM  |


## é™ä½å¼€é”€
æˆ‘ä»¬åœ¨ PyTorch Nightly ç‰ˆæœ¬ä¸­ä¸º A100 å’Œ T4 è¿›è¡Œäº† `reduce-overhead` ç¼–è¯‘æ¨¡å¼çš„æ€§èƒ½åŸºå‡†æµ‹è¯•ã€‚

### A100

| **Task/Model** | **Batch Size** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compile** |
|:---:|:---:|:---:|:---:|
| Image Classification/ConvNeXT | Unbatched | 11.758 | 7.335 | 
| Image Classification/ConvNeXT | 4 | 23.171 | 21.490 | 
| Image Classification/ResNet | Unbatched | 7.435 | 3.801 | 
| Image Classification/ResNet | 4 | 7.261 | 2.187 | 
| Object Detection/Conditional-DETR | Unbatched | 32.823 | 11.627  | 
| Object Detection/Conditional-DETR | 4 | 50.622 | 33.831  | 
| Image Segmentation/MobileNet | Unbatched | 9.869 | 4.244 |
| Image Segmentation/MobileNet | 4 | 14.385 | 7.946 |


### T4

| **Task/Model** | **Batch Size** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compile** | 
|:---:|:---:|:---:|:---:|
| Image Classification/ConvNeXT | Unbatched | 32.137 | 31.84 | 
| Image Classification/ConvNeXT | 4 | 120.944 | 110.209 | 
| Image Classification/ResNet | Unbatched | 9.761 | 7.698 | 
| Image Classification/ResNet | 4 | 15.215 | 13.871 | 
| Object Detection/Conditional-DETR | Unbatched | 72.150 | 57.660  | 
| Object Detection/Conditional-DETR | 4 | 301.494 | 247.543  | 
| Image Segmentation/MobileNet | Unbatched | 22.266 | 19.339  |
| Image Segmentation/MobileNet | 4 | 78.311 | 50.983 |



============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\perf_train_cpu.md
============================================================



# åœ¨CPUä¸Šè¿›è¡Œé«˜æ•ˆè®­ç»ƒ

æœ¬æŒ‡å—å°†é‡ç‚¹ä»‹ç»å¦‚ä½•åœ¨CPUä¸Šé«˜æ•ˆè®­ç»ƒå¤§å‹æ¨¡å‹ã€‚

## ä½¿ç”¨IPEXè¿›è¡Œæ··åˆç²¾åº¦è®­ç»ƒ
æ··åˆç²¾åº¦è®­ç»ƒåœ¨æ¨¡å‹ä¸­å¯ä»¥åŒæ—¶ä½¿ç”¨å•ç²¾åº¦ï¼ˆfp32ï¼‰å’ŒåŠç²¾åº¦ï¼ˆbf16/fp16ï¼‰çš„æ•°æ®ç±»å‹æ¥åŠ é€Ÿè®­ç»ƒæˆ–æ¨ç†è¿‡ç¨‹ï¼Œå¹¶ä¸”ä»ç„¶èƒ½ä¿ç•™å¤§éƒ¨åˆ†å•ç²¾åº¦çš„å‡†ç¡®æ€§ã€‚ç°ä»£çš„CPUï¼Œä¾‹å¦‚ç¬¬ä¸‰ä»£ã€ç¬¬å››ä»£å’Œç¬¬äº”ä»£IntelÂ® XeonÂ® Scalableå¤„ç†å™¨ï¼ŒåŸç”Ÿæ”¯æŒbf16ï¼Œè€Œç¬¬å…­ä»£IntelÂ® XeonÂ® Scalableå¤„ç†å™¨åŸç”Ÿæ”¯æŒbf16å’Œfp16ã€‚æ‚¨åœ¨è®­ç»ƒæ—¶å¯ç”¨bf16æˆ–fp16çš„æ··åˆç²¾åº¦è®­ç»ƒå¯ä»¥ç›´æ¥æé«˜å¤„ç†æ€§èƒ½ã€‚

ä¸ºäº†è¿›ä¸€æ­¥æœ€å¤§åŒ–è®­ç»ƒæ€§èƒ½ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨IntelÂ® PyTorchæ‰©å±•ï¼ˆIPEXï¼‰ã€‚IPEXæ˜¯ä¸€ä¸ªåŸºäºPyTorchæ„å»ºçš„åº“ï¼Œå¢åŠ äº†é¢å¤–çš„CPUæŒ‡ä»¤é›†æ¶æ„ï¼ˆISAï¼‰çº§åˆ«çš„æ”¯æŒï¼Œæ¯”å¦‚IntelÂ®é«˜çº§å‘é‡æ‰©å±•512ï¼ˆIntelÂ® AVX512-VNNIï¼‰å’ŒIntelÂ®é«˜çº§çŸ©é˜µæ‰©å±•ï¼ˆIntelÂ® AMXï¼‰ã€‚è¿™ä¸ºIntel CPUæä¾›é¢å¤–çš„æ€§èƒ½æå‡ã€‚ç„¶è€Œï¼Œä»…æ”¯æŒAVX2çš„CPUï¼ˆä¾‹å¦‚AMDæˆ–è¾ƒæ—§çš„Intel CPUï¼‰åœ¨ä½¿ç”¨IPEXæ—¶å¹¶ä¸ä¿è¯èƒ½æé«˜æ€§èƒ½ã€‚

ä»PyTorch 1.10ç‰ˆæœ¬èµ·ï¼ŒCPUåç«¯å·²ç»å¯ç”¨äº†è‡ªåŠ¨æ··åˆç²¾åº¦ï¼ˆAMPï¼‰ã€‚IPEXè¿˜æ”¯æŒbf16/fp16çš„AMPå’Œbf16/fp16ç®—å­ä¼˜åŒ–ï¼Œå¹¶ä¸”éƒ¨åˆ†åŠŸèƒ½å·²ç»ä¸Šæ¸¸åˆ°PyTorchä¸»åˆ†æ”¯ã€‚é€šè¿‡IPEX AMPï¼Œæ‚¨å¯ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½å’Œç”¨æˆ·ä½“éªŒã€‚

ç‚¹å‡»[è¿™é‡Œ](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/features/amp.html)æŸ¥çœ‹**è‡ªåŠ¨æ··åˆç²¾åº¦**çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚


### IPEX å®‰è£…:

IPEX çš„å‘å¸ƒä¸ PyTorch ä¸€è‡´ï¼Œæ‚¨å¯ä»¥é€šè¿‡ pip å®‰è£…ï¼š

| PyTorch Version   | IPEX version   |
| :---------------: | :----------:   |
| 2.5.0             |  2.5.0+cpu     |
| 2.4.0             |  2.4.0+cpu     |
| 2.3.0             |  2.3.0+cpu     |
| 2.2.0             |  2.2.0+cpu     |

è¯·è¿è¡Œ `pip list | grep torch` ä»¥è·å–æ‚¨çš„ `pytorch_version`ï¼Œç„¶åæ ¹æ®è¯¥ç‰ˆæœ¬å®‰è£…ç›¸åº”çš„ `IPEX version_name`ã€‚
```bash
pip install intel_extension_for_pytorch==<version_name> -f https://developer.intel.com/ipex-whl-stable-cpu
```

å¦‚æœéœ€è¦çš„è¯ï¼Œæ‚¨å¯ä»¥åœ¨ [ipex-whl-stable-cpu](https://developer.intel.com/ipex-whl-stable-cpu) æŸ¥çœ‹æœ€æ–°ç‰ˆæœ¬ã€‚

æŸ¥çœ‹æ›´å¤š [å®‰è£…IPEX](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/installation.html) çš„æ–¹æ³•ã€‚


### åœ¨ Trainer ä¸­ä½¿ç”¨ IPEX
åœ¨ Trainer ä¸­ä½¿ç”¨ IPEX æ—¶ï¼Œæ‚¨åº”åœ¨è®­ç»ƒå‘½ä»¤å‚æ•°ä¸­æ·»åŠ  `use_ipex`ã€`bf16` æˆ– `fp16` ä»¥åŠ `no_cuda` æ¥å¯ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦ã€‚

ä»¥ [Transformers é—®ç­”ä»»åŠ¡](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering)ä¸ºä¾‹ï¼š

- åœ¨ CPU ä¸Šä½¿ç”¨ BF16 è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒ IPEX çš„ç¤ºä¾‹å¦‚ä¸‹ï¼š
<pre> python examples/pytorch/question-answering/run_qa.py \
--model_name_or_path google-bert/bert-base-uncased \
--dataset_name squad \
--do_train \
--do_eval \
--per_device_train_batch_size 12 \
--learning_rate 3e-5 \
--num_train_epochs 2 \
--max_seq_length 384 \
--doc_stride 128 \
--output_dir /tmp/debug_squad/ \
<b>--use_ipex</b> \
<b>--bf16</b> \
<b>--use_cpu</b></pre> 

å¦‚æœæ‚¨æƒ³åœ¨è„šæœ¬ä¸­å¯ç”¨ `use_ipex` å’Œ `bf16`ï¼Œè¯·åƒä¸‹é¢è¿™æ ·å°†è¿™äº›å‚æ•°æ·»åŠ åˆ° `TrainingArguments` ä¸­ï¼š
```diff
training_args = TrainingArguments(
    output_dir=args.output_path,
+   bf16=True,
+   use_ipex=True,
+   use_cpu=True,
    **kwargs
)
```

### å®è·µç¤ºä¾‹

åšå®¢: [ä½¿ç”¨ Intel Sapphire Rapids åŠ é€Ÿ PyTorch Transformers](https://huggingface.co/blog/intel-sapphire-rapids)

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\perf_train_special.md
============================================================

# åœ¨ Apple Silicon èŠ¯ç‰‡ä¸Šè¿›è¡Œ PyTorch è®­ç»ƒ

ä¹‹å‰ï¼Œåœ¨ Mac ä¸Šè®­ç»ƒæ¨¡å‹ä»…é™äºä½¿ç”¨ CPU è®­ç»ƒã€‚ä¸è¿‡éšç€PyTorch v1.12çš„å‘å¸ƒï¼Œæ‚¨å¯ä»¥é€šè¿‡åœ¨ Apple Silicon èŠ¯ç‰‡çš„ GPU ä¸Šè®­ç»ƒæ¨¡å‹æ¥æ˜¾è‘—æé«˜æ€§èƒ½å’Œè®­ç»ƒé€Ÿåº¦ã€‚è¿™æ˜¯é€šè¿‡å°† Apple çš„ Metal æ€§èƒ½ç€è‰²å™¨ (Metal Performance Shaders, MPS) ä½œä¸ºåç«¯é›†æˆåˆ°PyTorchä¸­å®ç°çš„ã€‚[MPSåç«¯](https://pytorch.org/docs/stable/notes/mps.html) å°† PyTorch æ“ä½œè§†ä¸ºè‡ªå®šä¹‰çš„ Metal ç€è‰²å™¨æ¥å®ç°ï¼Œå¹¶å°†å¯¹åº”æ¨¡å—éƒ¨ç½²åˆ°`mps`è®¾å¤‡ä¸Šã€‚

<Tip warning={true}>

æŸäº› PyTorch æ“ä½œç›®å‰è¿˜æœªåœ¨ MPS ä¸Šå®ç°ï¼Œå¯èƒ½ä¼šæŠ›å‡ºé”™è¯¯æç¤ºã€‚å¯ä»¥é€šè¿‡è®¾ç½®ç¯å¢ƒå˜é‡`PYTORCH_ENABLE_MPS_FALLBACK=1`æ¥ä½¿ç”¨CPUå†…æ ¸ä»¥é¿å…è¿™ç§æƒ…å†µå‘ç”Ÿï¼ˆæ‚¨ä»ç„¶ä¼šçœ‹åˆ°ä¸€ä¸ª`UserWarning`ï¼‰ã€‚

<br>

å¦‚æœæ‚¨é‡åˆ°ä»»ä½•å…¶ä»–é”™è¯¯ï¼Œè¯·åœ¨[PyTorchåº“](https://github.com/pytorch/pytorch/issues)ä¸­åˆ›å»ºä¸€ä¸ª issueï¼Œå› ä¸º[`Trainer`]ç±»ä¸­åªé›†æˆäº† MPS åç«¯.

</Tip>

é…ç½®å¥½`mps`è®¾å¤‡åï¼Œæ‚¨å¯ä»¥ï¼š

* åœ¨æœ¬åœ°è®­ç»ƒæ›´å¤§çš„ç½‘ç»œæˆ–æ›´å¤§çš„æ‰¹é‡å¤§å°
* é™ä½æ•°æ®è·å–å»¶è¿Ÿï¼Œå› ä¸º GPU çš„ç»Ÿä¸€å†…å­˜æ¶æ„å…è®¸ç›´æ¥è®¿é—®æ•´ä¸ªå†…å­˜å­˜å‚¨
* é™ä½æˆæœ¬ï¼Œå› ä¸ºæ‚¨ä¸éœ€è¦å†åœ¨äº‘ç«¯ GPU ä¸Šè®­ç»ƒæˆ–å¢åŠ é¢å¤–çš„æœ¬åœ° GPU

åœ¨ç¡®ä¿å·²å®‰è£…PyTorchåå°±å¯ä»¥å¼€å§‹ä½¿ç”¨äº†ã€‚ MPS åŠ é€Ÿæ”¯æŒmacOS 12.3åŠä»¥ä¸Šç‰ˆæœ¬ã€‚

```bash
pip install torch torchvision torchaudio
```

[`TrainingArguments`]ç±»é»˜è®¤ä½¿ç”¨`mps`è®¾å¤‡(å¦‚æœå¯ç”¨)å› æ­¤æ— éœ€æ˜¾å¼è®¾ç½®è®¾å¤‡ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥ç›´æ¥è¿è¡Œ[run_glue.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py)è„šæœ¬ï¼Œåœ¨æ— éœ€è¿›è¡Œä»»ä½•ä¿®æ”¹çš„æƒ…å†µä¸‹è‡ªåŠ¨å¯ç”¨ MPS åç«¯ã€‚

```diff
export TASK_NAME=mrpc

python examples/pytorch/text-classification/run_glue.py \
  --model_name_or_path google-bert/bert-base-cased \
  --task_name $TASK_NAME \
- --use_mps_device \
  --do_train \
  --do_eval \
  --max_seq_length 128 \
  --per_device_train_batch_size 32 \
  --learning_rate 2e-5 \
  --num_train_epochs 3 \
  --output_dir /tmp/$TASK_NAME/ \
```

ç”¨äº[åˆ†å¸ƒå¼è®¾ç½®](https://pytorch.org/docs/stable/distributed.html#backends)çš„åç«¯(å¦‚`gloo`å’Œ`nccl`)ä¸æ”¯æŒ`mps`è®¾å¤‡ï¼Œè¿™ä¹Ÿæ„å‘³ç€ä½¿ç”¨ MPS åç«¯æ—¶åªèƒ½åœ¨å•ä¸ª GPU ä¸Šè¿›è¡Œè®­ç»ƒã€‚

æ‚¨å¯ä»¥åœ¨[Introducing Accelerated PyTorch Training on Mac](https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/)åšå®¢æ–‡ç« ä¸­äº†è§£æœ‰å…³ MPS åç«¯çš„æ›´å¤šä¿¡æ¯ã€‚

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\philosophy.md
============================================================





# Transformers çš„è®¾è®¡ç†å¿µ

ğŸ¤— Transformers æ˜¯ä¸€ä¸ªä¸“ä¸ºä»¥ä¸‹ç”¨æˆ·ç¾¤ä½“æ„å»ºçš„åº“ï¼š

- å¯»æ±‚ä½¿ç”¨ã€ç ”ç©¶æˆ–æ‰©å±•å¤§è§„æ¨¡ Transformers æ¨¡å‹çš„æœºå™¨å­¦ä¹ ç ”ç©¶äººå‘˜å’Œæ•™è‚²è€…ã€‚
- å¸Œæœ›å¾®è°ƒè¿™äº›æ¨¡å‹æˆ–åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨å®ƒä»¬ï¼ˆæˆ–ä¸¤è€…å…¼è€Œæœ‰ä¹‹ï¼‰çš„å®é™…æ“ä½œè€…ã€‚
- åªæƒ³ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹å¹¶å°†å…¶ç”¨äºè§£å†³ç»™å®šæœºå™¨å­¦ä¹ ä»»åŠ¡çš„å·¥ç¨‹å¸ˆã€‚

Transformers è®¾è®¡æ—¶æœ‰ä¸¤ä¸ªä¸»è¦ç›®æ ‡ï¼š

1. å°½å¯èƒ½ç®€å•å¿«é€Ÿåœ°ä½¿ç”¨ï¼š

   - æˆ‘ä»¬å°½å¯èƒ½åœ°é™åˆ¶ç”¨æˆ·èƒ½æ¥è§¦çš„æŠ½è±¡å±‚ï¼Œå®é™…ä¸Šå‡ ä¹æ²¡æœ‰æŠ½è±¡ã€‚ç”¨æˆ·åªéœ€å­¦ä¹ ä¸‰ä¸ªæ ‡å‡†ç±»å³å¯ä½¿ç”¨æ¯ä¸ªæ¨¡å‹ï¼š[configuration](main_classes/configuration)ã€[models](main_classes/model) å’Œä¸€ä¸ªé¢„å¤„ç†ç±»ï¼ˆç”¨äº NLP çš„ [tokenizer](main_classes/tokenizer)ï¼Œç”¨äºè§†è§‰çš„ [image processor](main_classes/image_processor)ï¼Œç”¨äºéŸ³é¢‘çš„ [feature extractor](main_classes/feature_extractor)ï¼Œä»¥åŠç”¨äºå¤šæ¨¡æ€è¾“å…¥çš„ [processor](main_classes/processors)ï¼‰ã€‚
   - æ‰€æœ‰è¿™äº›ç±»éƒ½å¯ä»¥é€šè¿‡ä¸€ä¸ªé€šç”¨çš„ `from_pretrained()` æ–¹æ³•ä»é¢„è®­ç»ƒå®ä¾‹ä¸­ç®€å•ç»Ÿä¸€åœ°åˆå§‹åŒ–ï¼Œè¯¥æ–¹æ³•ä¼šä»æä¾›åœ¨ [Hugging Face Hub](https://huggingface.co/models) ä¸Šçš„é¢„è®­ç»ƒæ£€æŸ¥ç‚¹ï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰ä¸‹è½½ã€ç¼“å­˜å’ŒåŠ è½½ç›¸å…³ç±»å®ä¾‹åŠç›¸å…³æ•°æ®ï¼ˆé…ç½®çš„è¶…å‚æ•°ã€åˆ†è¯å™¨çš„è¯æ±‡è¡¨å’Œæ¨¡å‹çš„æƒé‡ï¼‰ã€‚
   - åœ¨è¿™ä¸‰ä¸ªåŸºæœ¬ç±»ä¹‹ä¸Šï¼Œè¯¥åº“æä¾›äº†ä¸¤ç§ APIï¼š[`pipeline`] ç”¨äºå¿«é€Ÿåœ¨ç»™å®šä»»åŠ¡ä¸Šä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨æ–­ï¼Œä»¥åŠ [`Trainer`] ç”¨äºå¿«é€Ÿè®­ç»ƒæˆ–å¾®è°ƒ PyTorch æ¨¡å‹ã€‚
   - å› æ­¤ï¼ŒTransformers ä¸æ˜¯ç¥ç»ç½‘ç»œçš„æ¨¡å—åŒ–å·¥å…·ç®±ã€‚å¦‚æœè¦åŸºäº Transformers æ‰©å±•æˆ–æ­å»ºæ–°é¡¹ç›®ï¼Œè¯·ä½¿ç”¨å¸¸è§„çš„ Python æˆ–è€… PyTorch æ¨¡å—ï¼Œå¹¶ä» Transformers çš„åŸºç±»ç»§æ‰¿ä»¥é‡ç”¨æ¨¡å‹åŠ è½½å’Œä¿å­˜ç­‰åŠŸèƒ½ã€‚å¦‚æœæƒ³äº†è§£æ›´å¤šæœ‰å…³æˆ‘ä»¬çš„æ¨¡å‹ä»£ç çš„è®¾è®¡ç†å¿µï¼Œè¯·æŸ¥çœ‹æˆ‘ä»¬çš„[é‡å¤è‡ªå·±](https://huggingface.co/blog/transformers-design-philosophy)åšæ–‡ã€‚

2. æä¾›ä¸åŸå§‹æ¨¡å‹æ€§èƒ½å°½å¯èƒ½æ¥è¿‘çš„æœ€æ–°æ¨¡å‹ï¼š

   - æˆ‘ä»¬ä¸ºæ¯ç§æ¶æ„æä¾›è‡³å°‘ä¸€ä¸ªç¤ºä¾‹ï¼Œå¤ç°äº†è¯¥æ¶æ„å®˜æ–¹ä½œè€…æä¾›çš„ç»“æœã€‚
   - ä»£ç é€šå¸¸å°½å¯èƒ½æ¥è¿‘åŸå§‹ä»£ç åº“ï¼Œè¿™æ„å‘³ç€æŸäº› PyTorch ä»£ç å¯èƒ½ä¸å¤Ÿ*pytorchic*ï¼Œå› ä¸ºå®ƒå¯èƒ½æ˜¯ä»å…¶å®ƒçš„æ·±åº¦å­¦ä¹ æ¡†æ¶è½¬æ¢è¿‡æ¥çš„ä»£ç ã€‚

å…¶ä»–å‡ ä¸ªç›®æ ‡ï¼š

- å°½å¯èƒ½ä¸€è‡´åœ°å…¬å¼€æ¨¡å‹çš„å†…éƒ¨ï¼š

   - æˆ‘ä»¬ä½¿ç”¨å•ä¸€ API æä¾›å¯¹å®Œæ•´éšè—çŠ¶æ€å’Œæ³¨æ„åŠ›æƒé‡çš„è®¿é—®ã€‚
   - é¢„å¤„ç†ç±»å’ŒåŸºæœ¬æ¨¡å‹ API æ ‡å‡†åŒ–ï¼Œä¾¿äºåœ¨ä¸åŒæ¨¡å‹ä¹‹é—´è½»æ¾åˆ‡æ¢ã€‚

- ç»“åˆä¸»è§‚é€‰æ‹©çš„æœ‰å‰é€”çš„å·¥å…·è¿›è¡Œæ¨¡å‹å¾®è°ƒå’Œè°ƒæŸ¥ï¼š

   - ç®€å•ä¸€è‡´çš„æ–¹æ³•æ¥å‘è¯æ±‡è¡¨å’ŒåµŒå…¥ä¸­æ·»åŠ æ–°æ ‡è®°ä»¥è¿›è¡Œå¾®è°ƒã€‚
   - ç®€å•çš„æ–¹æ³•æ¥å±è”½å’Œä¿®å‰ª Transformer å¤´éƒ¨ã€‚

## ä¸»è¦æ¦‚å¿µ

è¯¥åº“å›´ç»•æ¯ä¸ªæ¨¡å‹çš„ä¸‰ç±»ç±»æ„å»ºï¼š

- **æ¨¡å‹ç±»** æ˜¯ PyTorch æ¨¡å‹ï¼ˆ[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)ï¼‰ï¼Œè¿™äº›æ¨¡å‹å¯ä»¥ä½¿ç”¨åº“ä¸­æä¾›çš„é¢„è®­ç»ƒæƒé‡ã€‚
- **é…ç½®ç±»** å­˜å‚¨æ„å»ºæ¨¡å‹æ‰€éœ€çš„è¶…å‚æ•°ï¼ˆå¦‚å±‚æ•°å’Œéšè—å¤§å°ï¼‰ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œå¦‚æœæ‚¨ä½¿ç”¨ä¸è¿›è¡Œä»»ä½•ä¿®æ”¹çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œåˆ™åˆ›å»ºæ¨¡å‹å°†è‡ªåŠ¨å¤„ç†é…ç½®çš„å®ä¾‹åŒ–ï¼ˆé…ç½®æ˜¯æ¨¡å‹çš„ä¸€éƒ¨åˆ†ï¼‰ã€‚
- **é¢„å¤„ç†ç±»** å°†åŸå§‹æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹å¯æ¥å—çš„æ ¼å¼ã€‚ä¸€ä¸ª [tokenizer](main_classes/tokenizer) å­˜å‚¨æ¯ä¸ªæ¨¡å‹çš„è¯æ±‡è¡¨ï¼Œå¹¶æä¾›ç¼–ç å’Œè§£ç å­—ç¬¦ä¸²ä¸ºè¦é¦ˆé€åˆ°æ¨¡å‹çš„ä»¤ç‰ŒåµŒå…¥ç´¢å¼•åˆ—è¡¨çš„æ–¹æ³•ã€‚[Image processors](main_classes/image_processor) é¢„å¤„ç†è§†è§‰è¾“å…¥ï¼Œ[feature extractors](main_classes/feature_extractor) é¢„å¤„ç†éŸ³é¢‘è¾“å…¥ï¼Œè€Œ [processor](main_classes/processors) åˆ™å¤„ç†å¤šæ¨¡æ€è¾“å…¥ã€‚

æ‰€æœ‰è¿™äº›ç±»éƒ½å¯ä»¥ä»é¢„è®­ç»ƒå®ä¾‹ä¸­å®ä¾‹åŒ–ã€æœ¬åœ°ä¿å­˜ï¼Œå¹¶é€šè¿‡ä»¥ä¸‹ä¸‰ç§æ–¹æ³•ä¸ Hub å…±äº«ï¼š

- `from_pretrained()` å…è®¸æ‚¨ä»åº“è‡ªèº«æä¾›çš„é¢„è®­ç»ƒç‰ˆæœ¬ï¼ˆæ”¯æŒçš„æ¨¡å‹å¯åœ¨ [Model Hub](https://huggingface.co/models) ä¸Šæ‰¾åˆ°ï¼‰æˆ–ç”¨æˆ·æœ¬åœ°ï¼ˆæˆ–æœåŠ¡å™¨ä¸Šï¼‰å­˜å‚¨çš„ç‰ˆæœ¬å®ä¾‹åŒ–æ¨¡å‹ã€é…ç½®å’Œé¢„å¤„ç†ç±»ã€‚
- `save_pretrained()` å…è®¸æ‚¨æœ¬åœ°ä¿å­˜æ¨¡å‹ã€é…ç½®å’Œé¢„å¤„ç†ç±»ï¼Œä»¥ä¾¿å¯ä»¥ä½¿ç”¨ `from_pretrained()` é‡æ–°åŠ è½½ã€‚
- `push_to_hub()` å…è®¸æ‚¨å°†æ¨¡å‹ã€é…ç½®å’Œé¢„å¤„ç†ç±»å…±äº«åˆ° Hubï¼Œä»¥ä¾¿æ‰€æœ‰äººéƒ½å¯ä»¥è½»æ¾è®¿é—®ã€‚

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\pipeline_tutorial.md
============================================================



# æ¨ç†pipeline

[`pipeline`] è®©ä½¿ç”¨[Hub](https://huggingface.co/models)ä¸Šçš„ä»»ä½•æ¨¡å‹è¿›è¡Œä»»ä½•è¯­è¨€ã€è®¡ç®—æœºè§†è§‰ã€è¯­éŸ³ä»¥åŠå¤šæ¨¡æ€ä»»åŠ¡çš„æ¨ç†å˜å¾—éå¸¸ç®€å•ã€‚å³ä½¿æ‚¨å¯¹ç‰¹å®šçš„æ¨¡æ€æ²¡æœ‰ç»éªŒï¼Œæˆ–è€…ä¸ç†Ÿæ‚‰æ¨¡å‹çš„æºç ï¼Œæ‚¨ä»ç„¶å¯ä»¥ä½¿ç”¨[`pipeline`]è¿›è¡Œæ¨ç†ï¼æœ¬æ•™ç¨‹å°†æ•™æ‚¨ï¼š

- å¦‚ä½•ä½¿ç”¨[`pipeline`] è¿›è¡Œæ¨ç†ã€‚
- å¦‚ä½•ä½¿ç”¨ç‰¹å®šçš„`tokenizer`(åˆ†è¯å™¨)æˆ–æ¨¡å‹ã€‚
- å¦‚ä½•ä½¿ç”¨[`pipeline`] è¿›è¡ŒéŸ³é¢‘ã€è§†è§‰å’Œå¤šæ¨¡æ€ä»»åŠ¡çš„æ¨ç†ã€‚

<Tip>

è¯·æŸ¥çœ‹[`pipeline`]æ–‡æ¡£ä»¥è·å–å·²æ”¯æŒçš„ä»»åŠ¡å’Œå¯ç”¨å‚æ•°çš„å®Œæ•´åˆ—è¡¨ã€‚

</Tip>

## Pipelineä½¿ç”¨

è™½ç„¶æ¯ä¸ªä»»åŠ¡éƒ½æœ‰ä¸€ä¸ªå…³è”çš„[`pipeline`]ï¼Œä½†ä½¿ç”¨é€šç”¨çš„æŠ½è±¡çš„[`pipeline`]æ›´åŠ ç®€å•ï¼Œå…¶ä¸­åŒ…å«æ‰€æœ‰ç‰¹å®šä»»åŠ¡çš„`pipelines`ã€‚[`pipeline`]ä¼šè‡ªåŠ¨åŠ è½½ä¸€ä¸ªé»˜è®¤æ¨¡å‹å’Œä¸€ä¸ªèƒ½å¤Ÿè¿›è¡Œä»»åŠ¡æ¨ç†çš„é¢„å¤„ç†ç±»ã€‚è®©æˆ‘ä»¬ä»¥ä½¿ç”¨[`pipeline`]è¿›è¡Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æˆ–è¯­éŸ³è½¬æ–‡æœ¬ä¸ºä¾‹ã€‚

1. é¦–å…ˆï¼Œåˆ›å»ºä¸€ä¸ª[`pipeline`]å¹¶æŒ‡å®šæ¨ç†ä»»åŠ¡ï¼š

```py
>>> from transformers import pipeline

>>> transcriber = pipeline(task="automatic-speech-recognition")
```

2. å°†æ‚¨çš„è¾“å…¥ä¼ é€’ç»™[`pipeline`]ã€‚å¯¹äºè¯­éŸ³è¯†åˆ«ï¼Œè¿™é€šå¸¸æ˜¯ä¸€ä¸ªéŸ³é¢‘è¾“å…¥æ–‡ä»¶ï¼š


```py
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': 'I HAVE A DREAM BUT ONE DAY THIS NATION WILL RISE UP LIVE UP THE TRUE MEANING OF ITS TREES'}
```

æ‚¨æ²¡æœ‰å¾—åˆ°æ‚¨æœŸæœ›çš„ç»“æœï¼Ÿå¯ä»¥åœ¨Hubä¸ŠæŸ¥çœ‹ä¸€äº›[æœ€å—æ¬¢è¿çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹](https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&sort=trending) 
ï¼Œçœ‹çœ‹æ˜¯å¦å¯ä»¥è·å¾—æ›´å¥½çš„è½¬å½•ã€‚

è®©æˆ‘ä»¬å°è¯•æ¥è‡ª OpenAI çš„[Whisper large-v2](https://huggingface.co/openai/whisper-large) æ¨¡å‹ã€‚Whisperbæ¯”Wav2Vec2æ™š2å¹´å‘å¸ƒï¼Œä½¿ç”¨æ¥è¿‘10å€çš„æ•°æ®è¿›è¡Œäº†è®­ç»ƒã€‚å› æ­¤ï¼Œå®ƒåœ¨å¤§å¤šæ•°ä¸‹æ¸¸åŸºå‡†æµ‹è¯•ä¸Šå‡»è´¥äº†Wav2Vec2ã€‚
å®ƒè¿˜å…·æœ‰é¢„æµ‹æ ‡ç‚¹å’Œå¤§å°å†™çš„é™„åŠ ä¼˜åŠ¿ï¼Œè€ŒWav2Vec2åˆ™æ— æ³•å®ç°è¿™äº›åŠŸèƒ½ã€‚

è®©æˆ‘ä»¬åœ¨è¿™é‡Œå°è¯•ä¸€ä¸‹ï¼Œçœ‹çœ‹å®ƒçš„è¡¨ç°å¦‚ä½•ï¼š


```py
>>> transcriber = pipeline(model="openai/whisper-large-v2")
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}
```

ç°åœ¨è¿™ä¸ªç»“æœçœ‹èµ·æ¥æ›´å‡†ç¡®äº†ï¼è¦è¿›è¡Œæ·±å…¥çš„Wav2Vec2ä¸Whisperæ¯”è¾ƒï¼Œè¯·å‚é˜…[éŸ³é¢‘å˜æ¢å™¨è¯¾ç¨‹](https://huggingface.co/learn/audio-course/chapter5/asr_models)ã€‚
æˆ‘ä»¬é¼“åŠ±æ‚¨åœ¨ Hub ä¸ŠæŸ¥çœ‹ä¸åŒè¯­è¨€çš„æ¨¡å‹ï¼Œä»¥åŠä¸“ä¸šé¢†åŸŸçš„æ¨¡å‹ç­‰ã€‚æ‚¨å¯ä»¥åœ¨Hubä¸Šç›´æ¥æŸ¥çœ‹å¹¶æ¯”è¾ƒæ¨¡å‹çš„ç»“æœï¼Œä»¥ç¡®å®šæ˜¯å¦é€‚åˆæˆ–å¤„ç†è¾¹ç¼˜æƒ…å†µæ˜¯å¦æ¯”å…¶ä»–æ¨¡å‹æ›´å¥½ã€‚å¦‚æœæ‚¨æ²¡æœ‰æ‰¾åˆ°é€‚ç”¨äºæ‚¨çš„ç”¨ä¾‹çš„æ¨¡å‹ï¼Œæ‚¨å§‹ç»ˆå¯ä»¥[è®­ç»ƒ](training)è‡ªå·±çš„æ¨¡å‹ï¼

å¦‚æœæ‚¨æœ‰å¤šä¸ªè¾“å…¥ï¼Œæ‚¨å¯ä»¥å°†è¾“å…¥ä½œä¸ºåˆ—è¡¨ä¼ é€’ï¼š


```py
transcriber(
    [
        "https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac",
        "https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac",
    ]
)
```

`Pipelines`éå¸¸é€‚åˆç”¨äºæµ‹è¯•ï¼Œå› ä¸ºä»ä¸€ä¸ªæ¨¡å‹åˆ‡æ¢åˆ°å¦ä¸€ä¸ªæ¨¡å‹éå¸¸çç¢ï¼›ä½†æ˜¯ï¼Œè¿˜æœ‰ä¸€äº›æ–¹æ³•å¯ä»¥å°†å®ƒä»¬ä¼˜åŒ–åç”¨äºå¤§å‹å·¥ä½œè´Ÿè½½è€Œä¸ä»…ä»…æ˜¯æµ‹è¯•ã€‚è¯·æŸ¥çœ‹ä»¥ä¸‹æŒ‡å—ï¼Œæ·±å…¥æ¢è®¨å¦‚ä½•è¿­ä»£æ•´ä¸ªæ•°æ®é›†æˆ–åœ¨WebæœåŠ¡å™¨ä¸­ä½¿ç”¨`Pipelines`ï¼š
* [åœ¨æ•°æ®é›†ä¸Šä½¿ç”¨æµæ°´çº¿](#using-pipelines-on-a-dataset)
* [åœ¨WebæœåŠ¡å™¨ä¸­ä½¿ç”¨æµæ°´çº¿](./pipeline_webserver)


## å‚æ•°

[`pipeline`] æ”¯æŒè®¸å¤šå‚æ•°ï¼›æœ‰äº›æ˜¯é€‚ç”¨äºç‰¹å®šä»»åŠ¡çš„ï¼Œè€Œæœ‰äº›é€‚ç”¨äºæ‰€æœ‰`pipeline`ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œæ‚¨å¯ä»¥åœ¨ä»»ä½•åœ°æ–¹æŒ‡å®šå¯¹åº”å‚æ•°ï¼š


```py
transcriber = pipeline(model="openai/whisper-large-v2", my_parameter=1)

out = transcriber(...)  # This will use `my_parameter=1`.
out = transcriber(..., my_parameter=2)  # This will override and use `my_parameter=2`.
out = transcriber(...)  # This will go back to using `my_parameter=1`.
```

è®©æˆ‘ä»¬æŸ¥çœ‹å…¶ä¸­çš„ä¸‰ä¸ªé‡è¦å‚æ•°ï¼š


### è®¾å¤‡

å¦‚æœæ‚¨ä½¿ç”¨ `device=n`ï¼Œ`pipeline`ä¼šè‡ªåŠ¨å°†æ¨¡å‹æ”¾åœ¨æŒ‡å®šçš„è®¾å¤‡ä¸Šã€‚æ— è®ºæ‚¨ä½¿ç”¨PyTorchè¿˜æ˜¯Tensorflowï¼Œè¿™éƒ½å¯ä»¥å·¥ä½œã€‚


```py
transcriber = pipeline(model="openai/whisper-large-v2", device=0)
```

å¦‚æœæ¨¡å‹å¯¹äºå•ä¸ªGPUæ¥è¯´è¿‡äºåºå¤§ï¼Œå¹¶ä¸”æ‚¨æ­£åœ¨ä½¿ç”¨PyTorchï¼Œæ‚¨å¯ä»¥è®¾ç½® `device_map="auto"` ä»¥è‡ªåŠ¨ç¡®å®šå¦‚ä½•åŠ è½½å’Œå­˜å‚¨æ¨¡å‹æƒé‡ã€‚ä½¿ç”¨ `device_map` å‚æ•°éœ€è¦å®‰è£…ğŸ¤— [Accelerate](https://huggingface.co/docs/accelerate) è½¯ä»¶åŒ…ï¼š


```bash
pip install --upgrade accelerate
```

ä»¥ä¸‹ä»£ç ä¼šè‡ªåŠ¨åœ¨å„ä¸ªè®¾å¤‡ä¸ŠåŠ è½½å’Œå­˜å‚¨æ¨¡å‹æƒé‡ï¼š


```py
transcriber = pipeline(model="openai/whisper-large-v2", device_map="auto")
```

è¯·æ³¨æ„ï¼Œå¦‚æœä¼ é€’äº† `device_map="auto"`ï¼Œåœ¨å®ä¾‹åŒ–æ‚¨çš„ `pipeline` æ—¶ä¸éœ€è¦æ·»åŠ  `device=device` å‚æ•°ï¼Œå¦åˆ™å¯èƒ½ä¼šé‡åˆ°ä¸€äº›æ„å¤–çš„çŠ¶å†µï¼

### æ‰¹é‡å¤§å°

é»˜è®¤æƒ…å†µä¸‹ï¼Œ`pipelines`ä¸ä¼šè¿›è¡Œæ‰¹é‡æ¨ç†ï¼ŒåŸå› åœ¨[è¿™é‡Œ](https://huggingface.co/docs/transformers/main_classes/pipelines#pipeline-batching)è¯¦ç»†è§£é‡Šã€‚å› ä¸ºæ‰¹å¤„ç†ä¸ä¸€å®šæ›´å¿«ï¼Œå®é™…ä¸Šåœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½ä¼šæ›´æ…¢ã€‚

ä½†å¦‚æœåœ¨æ‚¨çš„ç”¨ä¾‹ä¸­èµ·ä½œç”¨ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ï¼š


```py
transcriber = pipeline(model="openai/whisper-large-v2", device=0, batch_size=2)
audio_filenames = [f"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/{i}.flac" for i in range(1, 5)]
texts = transcriber(audio_filenames)
```

ä»¥ä¸Šä»£ç ä¼šåœ¨æä¾›çš„4ä¸ªéŸ³é¢‘æ–‡ä»¶ä¸Šè¿è¡Œ`pipeline`ï¼Œå®ƒä¼šå°†å®ƒä»¬ä»¥2ä¸ªä¸€ç»„çš„æ‰¹æ¬¡ä¼ é€’ç»™æ¨¡å‹ï¼ˆæ¨¡å‹åœ¨GPUä¸Šï¼Œæ­¤æ—¶æ‰¹å¤„ç†æ›´æœ‰å¯èƒ½æœ‰æ‰€å¸®åŠ©ï¼‰ï¼Œè€Œæ‚¨æ— éœ€ç¼–å†™é¢å¤–çš„ä»£ç ã€‚è¾“å‡ºåº”å§‹ç»ˆä¸æ²¡æœ‰æ‰¹å¤„ç†æ—¶æ”¶åˆ°çš„ç»“æœç›¸ä¸€è‡´ã€‚å®ƒåªæ˜¯ä¸€ç§å¸®åŠ©æ‚¨æ›´å¿«åœ°ä½¿ç”¨`pipeline`çš„æ–¹å¼ã€‚

`pipeline`ä¹Ÿå¯ä»¥å‡è½»ä¸€äº›æ‰¹å¤„ç†çš„å¤æ‚æ€§ï¼Œå› ä¸ºå¯¹äºæŸäº›`pipeline`ï¼Œéœ€è¦å°†å•ä¸ªé¡¹ç›®ï¼ˆå¦‚é•¿éŸ³é¢‘æ–‡ä»¶ï¼‰åˆ†æˆå¤šä¸ªéƒ¨åˆ†ä»¥ä¾›æ¨¡å‹å¤„ç†ã€‚`pipeline`ä¸ºæ‚¨æ‰§è¡Œè¿™ç§[*chunk batching*](./main_classes/pipelines#pipeline-chunk-batching)ã€‚

### ä»»åŠ¡ç‰¹å®šå‚æ•°

æ‰€æœ‰ä»»åŠ¡éƒ½æä¾›äº†ç‰¹å®šäºä»»åŠ¡çš„å‚æ•°ï¼Œè¿™äº›å‚æ•°æä¾›é¢å¤–çš„çµæ´»æ€§å’Œé€‰æ‹©ï¼Œä»¥å¸®åŠ©æ‚¨å®Œæˆå·¥ä½œã€‚
ä¾‹å¦‚ï¼Œ[`transformers.AutomaticSpeechRecognitionPipeline.__call__`] æ–¹æ³•å…·æœ‰ä¸€ä¸ª `return_timestamps` å‚æ•°ï¼Œå¯¹äºå­—å¹•è§†é¢‘ä¼¼ä¹å¾ˆæœ‰å¸®åŠ©ï¼š

```py
>>> transcriber = pipeline(model="openai/whisper-large-v2", return_timestamps=True)
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.', 'chunks': [{'timestamp': (0.0, 11.88), 'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its'}, {'timestamp': (11.88, 12.38), 'text': ' creed.'}]}
```

æ­£å¦‚æ‚¨æ‰€çœ‹åˆ°çš„ï¼Œæ¨¡å‹æ¨æ–­å‡ºäº†æ–‡æœ¬ï¼Œè¿˜è¾“å‡ºäº†å„ä¸ªå¥å­å‘éŸ³çš„**æ—¶é—´**ã€‚

æ¯ä¸ªä»»åŠ¡éƒ½æœ‰è®¸å¤šå¯ç”¨çš„å‚æ•°ï¼Œå› æ­¤è¯·æŸ¥çœ‹æ¯ä¸ªä»»åŠ¡çš„APIå‚è€ƒï¼Œä»¥äº†è§£æ‚¨å¯ä»¥è¿›è¡Œå“ªäº›è°ƒæ•´ï¼ä¾‹å¦‚ï¼Œ[`~transformers.AutomaticSpeechRecognitionPipeline`] å…·æœ‰ `chunk_length_s` å‚æ•°ï¼Œå¯¹äºå¤„ç†éå¸¸é•¿çš„éŸ³é¢‘æ–‡ä»¶ï¼ˆä¾‹å¦‚ï¼Œä¸ºæ•´éƒ¨ç”µå½±æˆ–é•¿è¾¾ä¸€å°æ—¶çš„è§†é¢‘é…å­—å¹•ï¼‰éå¸¸æœ‰å¸®åŠ©ï¼Œè¿™é€šå¸¸æ˜¯æ¨¡å‹æ— æ³•å•ç‹¬å¤„ç†çš„ï¼š

```python
>>> transcriber = pipeline(model="openai/whisper-large-v2", chunk_length_s=30, return_timestamps=True)
>>> transcriber("https://huggingface.co/datasets/sanchit-gandhi/librispeech_long/resolve/main/audio.wav")
{'text': " Chapter 16. I might have told you of the beginning of this liaison in a few lines, but I wanted you to see every step by which we came.  I, too, agree to whatever Marguerite wished, Marguerite to be unable to live apart from me. It was the day after the evening...
```

å¦‚æœæ‚¨æ‰¾ä¸åˆ°ä¸€ä¸ªçœŸæ­£æœ‰å¸®åŠ©çš„å‚æ•°ï¼Œæ¬¢è¿[æå‡ºè¯·æ±‚](https://github.com/huggingface/transformers/issues/new?assignees=&labels=feature&template=feature-request.yml)ï¼

## åœ¨æ•°æ®é›†ä¸Šä½¿ç”¨pipelines

`pipelines`ä¹Ÿå¯ä»¥å¯¹å¤§å‹æ•°æ®é›†è¿›è¡Œæ¨ç†ã€‚æˆ‘ä»¬å»ºè®®ä½¿ç”¨è¿­ä»£å™¨æ¥å®Œæˆè¿™ä¸€ä»»åŠ¡ï¼Œè¿™æ˜¯æœ€ç®€å•çš„æ–¹æ³•ï¼š


```py
def data():
    for i in range(1000):
        yield f"My example {i}"


pipe = pipeline(model="openai-community/gpt2", device=0)
generated_characters = 0
for out in pipe(data()):
    generated_characters += len(out[0]["generated_text"])
```

è¿­ä»£å™¨ `data()` ä¼šäº§ç”Ÿæ¯ä¸ªç»“æœï¼Œ`pipelines`ä¼šè‡ªåŠ¨è¯†åˆ«è¾“å…¥ä¸ºå¯è¿­ä»£å¯¹è±¡ï¼Œå¹¶åœ¨GPUä¸Šå¤„ç†æ•°æ®çš„åŒæ—¶å¼€å§‹è·å–æ•°æ®ï¼ˆåœ¨åº•å±‚ä½¿ç”¨[DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)ï¼‰ã€‚è¿™ä¸€ç‚¹éå¸¸é‡è¦ï¼Œå› ä¸ºæ‚¨ä¸å¿…ä¸ºæ•´ä¸ªæ•°æ®é›†åˆ†é…å†…å­˜ï¼Œå¯ä»¥å°½å¯èƒ½å¿«åœ°å°†æ•°æ®ä¼ é€åˆ°GPUã€‚

ç”±äºæ‰¹å¤„ç†å¯ä»¥åŠ é€Ÿå¤„ç†ï¼Œå› æ­¤åœ¨è¿™é‡Œå°è¯•è°ƒæ•´ `batch_size` å‚æ•°å¯èƒ½ä¼šå¾ˆæœ‰ç”¨ã€‚

è¿­ä»£æ•°æ®é›†çš„æœ€ç®€å•æ–¹æ³•å°±æ˜¯ä»ğŸ¤— [Datasets](https://github.com/huggingface/datasets/) ä¸­åŠ è½½æ•°æ®é›†ï¼š


```py
# KeyDataset is a util that will just output the item we're interested in.
from transformers.pipelines.pt_utils import KeyDataset
from datasets import load_dataset

pipe = pipeline(model="hf-internal-testing/tiny-random-wav2vec2", device=0)
dataset = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation[:10]")

for out in pipe(KeyDataset(dataset, "audio")):
    print(out)
```


## åœ¨WebæœåŠ¡å™¨ä¸Šä½¿ç”¨pipelines

<Tip>
åˆ›å»ºæ¨ç†å¼•æ“æ˜¯ä¸€ä¸ªå¤æ‚çš„ä¸»é¢˜ï¼Œå€¼å¾—æœ‰è‡ªå·±çš„é¡µé¢ã€‚
</Tip>

[é“¾æ¥](./pipeline_webserver)

## è§†è§‰æµæ°´çº¿

å¯¹äºè§†è§‰ä»»åŠ¡ï¼Œä½¿ç”¨[`pipeline`] å‡ ä¹æ˜¯ç›¸åŒçš„ã€‚

æŒ‡å®šæ‚¨çš„ä»»åŠ¡å¹¶å°†å›¾åƒä¼ é€’ç»™åˆ†ç±»å™¨ã€‚å›¾åƒå¯ä»¥æ˜¯é“¾æ¥ã€æœ¬åœ°è·¯å¾„æˆ–base64ç¼–ç çš„å›¾åƒã€‚ä¾‹å¦‚ï¼Œä¸‹é¢æ˜¾ç¤ºçš„æ˜¯å“ªç§å“ç§çš„çŒ«ï¼Ÿ

![pipeline-cat-chonk](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg)
```py
>>> from transformers import pipeline

>>> vision_classifier = pipeline(model="google/vit-base-patch16-224")
>>> preds = vision_classifier(
...     images="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> preds
[{'score': 0.4335, 'label': 'lynx, catamount'}, {'score': 0.0348, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.0324, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.0239, 'label': 'Egyptian cat'}, {'score': 0.0229, 'label': 'tiger cat'}]
```

## æ–‡æœ¬æµæ°´çº¿

å¯¹äºNLPä»»åŠ¡ï¼Œä½¿ç”¨[`pipeline`] å‡ ä¹æ˜¯ç›¸åŒçš„ã€‚


```py
>>> from transformers import pipeline

>>> # This model is a `zero-shot-classification` model.
>>> # It will classify text, except you are free to choose any label you might imagine
>>> classifier = pipeline(model="facebook/bart-large-mnli")
>>> classifier(
...     "I have a problem with my iphone that needs to be resolved asap!!",
...     candidate_labels=["urgent", "not urgent", "phone", "tablet", "computer"],
... )
{'sequence': 'I have a problem with my iphone that needs to be resolved asap!!', 'labels': ['urgent', 'phone', 'computer', 'not urgent', 'tablet'], 'scores': [0.504, 0.479, 0.013, 0.003, 0.002]}
```

## å¤šæ¨¡æ€æµæ°´çº¿

[`pipeline`] æ”¯æŒå¤šä¸ªæ¨¡æ€ã€‚ä¾‹å¦‚ï¼Œè§†è§‰é—®é¢˜å›ç­”ï¼ˆVQAï¼‰ä»»åŠ¡ç»“åˆäº†æ–‡æœ¬å’Œå›¾åƒã€‚è¯·éšæ„ä½¿ç”¨æ‚¨å–œæ¬¢çš„ä»»ä½•å›¾åƒé“¾æ¥å’Œæ‚¨æƒ³è¦é—®å…³äºè¯¥å›¾åƒçš„é—®é¢˜ã€‚å›¾åƒå¯ä»¥æ˜¯URLæˆ–å›¾åƒçš„æœ¬åœ°è·¯å¾„ã€‚

ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨ä½¿ç”¨è¿™ä¸ª[invoice image](https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png)ï¼š


```py
>>> from transformers import pipeline

>>> vqa = pipeline(model="impira/layoutlm-document-qa")
>>> output = vqa(
...     image="https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png",
...     question="What is the invoice number?",
... )
>>> output[0]["score"] = round(output[0]["score"], 3)
>>> output
[{'score': 0.425, 'answer': 'us-001', 'start': 16, 'end': 16}]
```

<Tip>

è¦è¿è¡Œä¸Šé¢çš„ç¤ºä¾‹ï¼Œé™¤äº†ğŸ¤— Transformersä¹‹å¤–ï¼Œæ‚¨éœ€è¦å®‰è£…[`pytesseract`](https://pypi.org/project/pytesseract/)ã€‚


```bash
sudo apt install -y tesseract-ocr
pip install pytesseract
```

</Tip>

## åœ¨å¤§æ¨¡å‹ä¸Šä½¿ç”¨ğŸ¤— `accelerate`å’Œ`pipeline`ï¼š

æ‚¨å¯ä»¥è½»æ¾åœ°ä½¿ç”¨ğŸ¤— `accelerate`åœ¨å¤§æ¨¡å‹ä¸Šè¿è¡Œ `pipeline`ï¼é¦–å…ˆç¡®ä¿æ‚¨å·²ç»ä½¿ç”¨ `pip install accelerate` å®‰è£…äº† `accelerate`ã€‚

é¦–å…ˆä½¿ç”¨ `device_map="auto"` åŠ è½½æ‚¨çš„æ¨¡å‹ï¼æˆ‘ä»¬å°†åœ¨ç¤ºä¾‹ä¸­ä½¿ç”¨ `facebook/opt-1.3b`ã€‚


```py
# pip install accelerate
import torch
from transformers import pipeline

pipe = pipeline(model="facebook/opt-1.3b", dtype=torch.bfloat16, device_map="auto")
output = pipe("This is a cool example!", do_sample=True, top_p=0.95)
```

å¦‚æœå®‰è£… `bitsandbytes` å¹¶æ·»åŠ å‚æ•° `load_in_8bit=True`ï¼Œæ‚¨è¿˜å¯ä»¥ä¼ é€’8ä½åŠ è½½çš„æ¨¡å‹ã€‚


```py
# pip install accelerate bitsandbytes
import torch
from transformers import pipeline


pipe = pipeline(model="facebook/opt-1.3b", device_map="auto", model_kwargs={"quantization_config": BitsAndBytesConfig(load_in_8bit=True)})
output = pipe("This is a cool example!", do_sample=True, top_p=0.95)
```

è¯·æ³¨æ„ï¼Œæ‚¨å¯ä»¥å°†`checkpoint`æ›¿æ¢ä¸ºä»»ä½•æ”¯æŒå¤§æ¨¡å‹åŠ è½½çš„Hugging Faceæ¨¡å‹ï¼Œæ¯”å¦‚BLOOMï¼


============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\preprocessing.md
============================================================



# é¢„å¤„ç†

[[open-in-colab]]

åœ¨æ‚¨å¯ä»¥åœ¨æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œæ•°æ®éœ€è¦è¢«é¢„å¤„ç†ä¸ºæœŸæœ›çš„æ¨¡å‹è¾“å…¥æ ¼å¼ã€‚æ— è®ºæ‚¨çš„æ•°æ®æ˜¯æ–‡æœ¬ã€å›¾åƒè¿˜æ˜¯éŸ³é¢‘ï¼Œå®ƒä»¬éƒ½éœ€è¦è¢«è½¬æ¢å¹¶ç»„åˆæˆæ‰¹é‡çš„å¼ é‡ã€‚ğŸ¤— Transformers æä¾›äº†ä¸€ç»„é¢„å¤„ç†ç±»æ¥å¸®åŠ©å‡†å¤‡æ•°æ®ä»¥ä¾›æ¨¡å‹ä½¿ç”¨ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæ‚¨å°†äº†è§£ä»¥ä¸‹å†…å®¹ï¼š

* å¯¹äºæ–‡æœ¬ï¼Œä½¿ç”¨[åˆ†è¯å™¨](./main_classes/tokenizer)(`Tokenizer`)å°†æ–‡æœ¬è½¬æ¢ä¸ºä¸€ç³»åˆ—æ ‡è®°(`tokens`)ï¼Œå¹¶åˆ›å»º`tokens`çš„æ•°å­—è¡¨ç¤ºï¼Œå°†å®ƒä»¬ç»„åˆæˆå¼ é‡ã€‚
* å¯¹äºè¯­éŸ³å’ŒéŸ³é¢‘ï¼Œä½¿ç”¨[ç‰¹å¾æå–å™¨](./main_classes/feature_extractor)(`Feature extractor`)ä»éŸ³é¢‘æ³¢å½¢ä¸­æå–é¡ºåºç‰¹å¾å¹¶å°†å…¶è½¬æ¢ä¸ºå¼ é‡ã€‚
* å›¾åƒè¾“å…¥ä½¿ç”¨[å›¾åƒå¤„ç†å™¨](./main_classes/image)(`ImageProcessor`)å°†å›¾åƒè½¬æ¢ä¸ºå¼ é‡ã€‚
* å¤šæ¨¡æ€è¾“å…¥ï¼Œä½¿ç”¨[å¤„ç†å™¨](./main_classes/processors)(`Processor`)ç»“åˆäº†`Tokenizer`å’Œ`ImageProcessor`æˆ–`Processor`ã€‚

<Tip>

`AutoProcessor` **å§‹ç»ˆ**æœ‰æ•ˆçš„è‡ªåŠ¨é€‰æ‹©é€‚ç”¨äºæ‚¨ä½¿ç”¨çš„æ¨¡å‹çš„æ­£ç¡®`class`ï¼Œæ— è®ºæ‚¨ä½¿ç”¨çš„æ˜¯`Tokenizer`ã€`ImageProcessor`ã€`Feature extractor`è¿˜æ˜¯`Processor`ã€‚

</Tip>

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·å®‰è£…ğŸ¤— Datasetsï¼Œä»¥ä¾¿æ‚¨å¯ä»¥åŠ è½½ä¸€äº›æ•°æ®é›†æ¥è¿›è¡Œå®éªŒï¼š


```bash
pip install datasets
```

## è‡ªç„¶è¯­è¨€å¤„ç†

<Youtube id="Yffk5aydLzg"/>

å¤„ç†æ–‡æœ¬æ•°æ®çš„ä¸»è¦å·¥å…·æ˜¯[Tokenizer](main_classes/tokenizer)ã€‚`Tokenizer`æ ¹æ®ä¸€ç»„è§„åˆ™å°†æ–‡æœ¬æ‹†åˆ†ä¸º`tokens`ã€‚ç„¶åå°†è¿™äº›`tokens`è½¬æ¢ä¸ºæ•°å­—ï¼Œç„¶åè½¬æ¢ä¸ºå¼ é‡ï¼Œæˆä¸ºæ¨¡å‹çš„è¾“å…¥ã€‚æ¨¡å‹æ‰€éœ€çš„ä»»ä½•é™„åŠ è¾“å…¥éƒ½ç”±`Tokenizer`æ·»åŠ ã€‚

<Tip>

å¦‚æœæ‚¨è®¡åˆ’ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œé‡è¦çš„æ˜¯ä½¿ç”¨ä¸ä¹‹å…³è”çš„é¢„è®­ç»ƒ`Tokenizer`ã€‚è¿™ç¡®ä¿æ–‡æœ¬çš„æ‹†åˆ†æ–¹å¼ä¸é¢„è®­ç»ƒè¯­æ–™åº“ç›¸åŒï¼Œå¹¶åœ¨é¢„è®­ç»ƒæœŸé—´ä½¿ç”¨ç›¸åŒçš„æ ‡è®°-ç´¢å¼•çš„å¯¹åº”å…³ç³»ï¼ˆé€šå¸¸ç§°ä¸º*è¯æ±‡è¡¨*-`vocab`ï¼‰ã€‚

</Tip>

å¼€å§‹ä½¿ç”¨[`AutoTokenizer.from_pretrained`]æ–¹æ³•åŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒ`tokenizer`ã€‚è¿™å°†ä¸‹è½½æ¨¡å‹é¢„è®­ç»ƒçš„`vocab`ï¼š


```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-cased")
```

ç„¶åå°†æ‚¨çš„æ–‡æœ¬ä¼ é€’ç»™`tokenizer`ï¼š


```py
>>> encoded_input = tokenizer("Do not meddle in the affairs of wizards, for they are subtle and quick to anger.")
>>> print(encoded_input)
{'input_ids': [101, 2079, 2025, 19960, 10362, 1999, 1996, 3821, 1997, 16657, 1010, 2005, 2027, 2024, 11259, 1998, 4248, 2000, 4963, 1012, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

`tokenizer`è¿”å›ä¸€ä¸ªåŒ…å«ä¸‰ä¸ªé‡è¦å¯¹è±¡çš„å­—å…¸ï¼š

* [input_ids](glossary#input-ids) æ˜¯ä¸å¥å­ä¸­æ¯ä¸ª`token`å¯¹åº”çš„ç´¢å¼•ã€‚
* [attention_mask](glossary#attention-mask) æŒ‡ç¤ºæ˜¯å¦åº”è¯¥å…³æ³¨ä¸€ä¸ª`token`ã€‚
* [token_type_ids](glossary#token-type-ids) åœ¨å­˜åœ¨å¤šä¸ªåºåˆ—æ—¶æ ‡è¯†ä¸€ä¸ª`token`å±äºå“ªä¸ªåºåˆ—ã€‚

é€šè¿‡è§£ç  `input_ids` æ¥è¿”å›æ‚¨çš„è¾“å…¥ï¼š


```py
>>> tokenizer.decode(encoded_input["input_ids"])
'[CLS] Do not meddle in the affairs of wizards, for they are subtle and quick to anger. [SEP]'
```

å¦‚æ‚¨æ‰€è§ï¼Œ`tokenizer`å‘å¥å­ä¸­æ·»åŠ äº†ä¸¤ä¸ªç‰¹æ®Š`token` - `CLS` å’Œ `SEP`ï¼ˆåˆ†ç±»å™¨å’Œåˆ†éš”ç¬¦ï¼‰ã€‚å¹¶éæ‰€æœ‰æ¨¡å‹éƒ½éœ€è¦ç‰¹æ®Š`token`ï¼Œä½†å¦‚æœéœ€è¦ï¼Œ`tokenizer`ä¼šè‡ªåŠ¨ä¸ºæ‚¨æ·»åŠ ã€‚

å¦‚æœæœ‰å¤šä¸ªå¥å­éœ€è¦é¢„å¤„ç†ï¼Œå°†å®ƒä»¬ä½œä¸ºåˆ—è¡¨ä¼ é€’ç»™`tokenizer`ï¼š


```py
>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast, Pip.",
...     "What about elevensies?",
... ]
>>> encoded_inputs = tokenizer(batch_sentences)
>>> print(encoded_inputs)
{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102],
               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],
               [101, 1327, 1164, 5450, 23434, 136, 102]],
 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 0]],
 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1],
                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                    [1, 1, 1, 1, 1, 1, 1]]}
```

### å¡«å……

å¥å­çš„é•¿åº¦å¹¶ä¸æ€»æ˜¯ç›¸åŒï¼Œè¿™å¯èƒ½ä¼šæˆä¸ºä¸€ä¸ªé—®é¢˜ï¼Œå› ä¸ºæ¨¡å‹è¾“å…¥çš„å¼ é‡éœ€è¦å…·æœ‰ç»Ÿä¸€çš„å½¢çŠ¶ã€‚å¡«å……æ˜¯ä¸€ç§ç­–ç•¥ï¼Œé€šè¿‡åœ¨è¾ƒçŸ­çš„å¥å­ä¸­æ·»åŠ ä¸€ä¸ªç‰¹æ®Šçš„`padding token`ï¼Œä»¥ç¡®ä¿å¼ é‡æ˜¯çŸ©å½¢çš„ã€‚

å°† `padding` å‚æ•°è®¾ç½®ä¸º `True`ï¼Œä»¥ä½¿æ‰¹æ¬¡ä¸­è¾ƒçŸ­çš„åºåˆ—å¡«å……åˆ°ä¸æœ€é•¿åºåˆ—ç›¸åŒ¹é…çš„é•¿åº¦ï¼š

```py
>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast, Pip.",
...     "What about elevensies?",
... ]
>>> encoded_input = tokenizer(batch_sentences, padding=True)
>>> print(encoded_input)
{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],
               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],
               [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]],
 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],
 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                    [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}
```

ç¬¬ä¸€å¥å’Œç¬¬ä¸‰å¥å› ä¸ºè¾ƒçŸ­ï¼Œé€šè¿‡`0`è¿›è¡Œå¡«å……ï¼Œã€‚

### æˆªæ–­

å¦ä¸€æ–¹é¢ï¼Œæœ‰æ—¶å€™ä¸€ä¸ªåºåˆ—å¯èƒ½å¯¹æ¨¡å‹æ¥è¯´å¤ªé•¿äº†ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨éœ€è¦å°†åºåˆ—æˆªæ–­ä¸ºæ›´çŸ­çš„é•¿åº¦ã€‚

å°† `truncation` å‚æ•°è®¾ç½®ä¸º `True`ï¼Œä»¥å°†åºåˆ—æˆªæ–­ä¸ºæ¨¡å‹æ¥å—çš„æœ€å¤§é•¿åº¦ï¼š


```py
>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast, Pip.",
...     "What about elevensies?",
... ]
>>> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True)
>>> print(encoded_input)
{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],
               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],
               [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]],
 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],
 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                    [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}
```

<Tip>

æŸ¥çœ‹[å¡«å……å’Œæˆªæ–­](./pad_truncation)æ¦‚å¿µæŒ‡å—ï¼Œäº†è§£æ›´å¤šæœ‰å…³å¡«å……å’Œæˆªæ–­å‚æ•°çš„ä¿¡æ¯ã€‚

</Tip>

### æ„å»ºå¼ é‡

æœ€åï¼Œ`tokenizer`å¯ä»¥è¿”å›å®é™…è¾“å…¥åˆ°æ¨¡å‹çš„å¼ é‡ã€‚

å°† `return_tensors` å‚æ•°è®¾ç½®ä¸º `pt`ï¼ˆå¯¹äºPyTorchï¼‰æˆ– `tf`ï¼ˆå¯¹äºTensorFlowï¼‰ï¼š



```py
>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast, Pip.",
...     "What about elevensies?",
... ]
>>> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors="pt")
>>> print(encoded_input)
{'input_ids': tensor([[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],
                      [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],
                      [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]]),
 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),
 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
                           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                           [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}
```

## éŸ³é¢‘

å¯¹äºéŸ³é¢‘ä»»åŠ¡ï¼Œæ‚¨éœ€è¦[feature extractor](main_classes/feature_extractor)æ¥å‡†å¤‡æ‚¨çš„æ•°æ®é›†ä»¥ä¾›æ¨¡å‹ä½¿ç”¨ã€‚`feature extractor`æ—¨åœ¨ä»åŸå§‹éŸ³é¢‘æ•°æ®ä¸­æå–ç‰¹å¾ï¼Œå¹¶å°†å®ƒä»¬è½¬æ¢ä¸ºå¼ é‡ã€‚

åŠ è½½[MInDS-14](https://huggingface.co/datasets/PolyAI/minds14)æ•°æ®é›†ï¼ˆæœ‰å…³å¦‚ä½•åŠ è½½æ•°æ®é›†çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…ğŸ¤— [Datasetsæ•™ç¨‹](https://huggingface.co/docs/datasets/load_hub)ï¼‰ä»¥äº†è§£å¦‚ä½•åœ¨éŸ³é¢‘æ•°æ®é›†ä¸­ä½¿ç”¨`feature extractor`ï¼š


```py
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")
```

è®¿é—® `audio` åˆ—çš„ç¬¬ä¸€ä¸ªå…ƒç´ ä»¥æŸ¥çœ‹è¾“å…¥ã€‚è°ƒç”¨ `audio` åˆ—ä¼šè‡ªåŠ¨åŠ è½½å’Œé‡æ–°é‡‡æ ·éŸ³é¢‘æ–‡ä»¶ï¼š

```py
>>> dataset[0]["audio"]
{'array': array([ 0.        ,  0.00024414, -0.00024414, ..., -0.00024414,
         0.        ,  0.        ], dtype=float32),
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',
 'sampling_rate': 8000}
```

è¿™ä¼šè¿”å›ä¸‰ä¸ªå¯¹è±¡ï¼š

* `array` æ˜¯åŠ è½½çš„è¯­éŸ³ä¿¡å· - å¹¶åœ¨å¿…è¦æ—¶é‡æ–°é‡‡ä¸º`1D array`ã€‚
* `path` æŒ‡å‘éŸ³é¢‘æ–‡ä»¶çš„ä½ç½®ã€‚
* `sampling_rate` æ˜¯æ¯ç§’æµ‹é‡çš„è¯­éŸ³ä¿¡å·æ•°æ®ç‚¹æ•°é‡ã€‚

å¯¹äºæœ¬æ•™ç¨‹ï¼Œæ‚¨å°†ä½¿ç”¨[Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base)æ¨¡å‹ã€‚æŸ¥çœ‹æ¨¡å‹å¡ç‰‡ï¼Œæ‚¨å°†äº†è§£åˆ°Wav2Vec2æ˜¯åœ¨16kHzé‡‡æ ·çš„è¯­éŸ³éŸ³é¢‘æ•°æ®ä¸Šé¢„è®­ç»ƒçš„ã€‚é‡è¦çš„æ˜¯ï¼Œæ‚¨çš„éŸ³é¢‘æ•°æ®çš„é‡‡æ ·ç‡è¦ä¸ç”¨äºé¢„è®­ç»ƒæ¨¡å‹çš„æ•°æ®é›†çš„é‡‡æ ·ç‡åŒ¹é…ã€‚å¦‚æœæ‚¨çš„æ•°æ®çš„é‡‡æ ·ç‡ä¸åŒï¼Œé‚£ä¹ˆæ‚¨éœ€è¦å¯¹æ•°æ®è¿›è¡Œé‡æ–°é‡‡æ ·ã€‚

1. ä½¿ç”¨ğŸ¤— Datasetsçš„[`~datasets.Dataset.cast_column`]æ–¹æ³•å°†é‡‡æ ·ç‡æå‡åˆ°16kHzï¼š

```py
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=16_000))
```

2. å†æ¬¡è°ƒç”¨ `audio` åˆ—ä»¥é‡æ–°é‡‡æ ·éŸ³é¢‘æ–‡ä»¶ï¼š


```py
>>> dataset[0]["audio"]
{'array': array([ 2.3443763e-05,  2.1729663e-04,  2.2145823e-04, ...,
         3.8356509e-05, -7.3497440e-06, -2.1754686e-05], dtype=float32),
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',
 'sampling_rate': 16000}
```

æ¥ä¸‹æ¥ï¼ŒåŠ è½½ä¸€ä¸ª`feature extractor`ä»¥å¯¹è¾“å…¥è¿›è¡Œæ ‡å‡†åŒ–å’Œå¡«å……ã€‚å½“å¡«å……æ–‡æœ¬æ•°æ®æ—¶ï¼Œä¼šä¸ºè¾ƒçŸ­çš„åºåˆ—æ·»åŠ  `0`ã€‚ç›¸åŒçš„ç†å¿µé€‚ç”¨äºéŸ³é¢‘æ•°æ®ã€‚`feature extractor`æ·»åŠ  `0` - è¢«è§£é‡Šä¸ºé™éŸ³ - åˆ°`array` ã€‚

ä½¿ç”¨ [`AutoFeatureExtractor.from_pretrained`] åŠ è½½`feature extractor`ï¼š


```py
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base")
```

å°†éŸ³é¢‘ `array` ä¼ é€’ç»™`feature extractor`ã€‚æˆ‘ä»¬è¿˜å»ºè®®åœ¨`feature extractor`ä¸­æ·»åŠ  `sampling_rate` å‚æ•°ï¼Œä»¥æ›´å¥½åœ°è°ƒè¯•å¯èƒ½å‘ç”Ÿçš„é™éŸ³é”™è¯¯ï¼š


```py
>>> audio_input = [dataset[0]["audio"]["array"]]
>>> feature_extractor(audio_input, sampling_rate=16000)
{'input_values': [array([ 3.8106556e-04,  2.7506407e-03,  2.8015103e-03, ...,
        5.6335266e-04,  4.6588284e-06, -1.7142107e-04], dtype=float32)]}
```

å°±åƒ`tokenizer`ä¸€æ ·ï¼Œæ‚¨å¯ä»¥åº”ç”¨å¡«å……æˆ–æˆªæ–­æ¥å¤„ç†æ‰¹æ¬¡ä¸­çš„å¯å˜åºåˆ—ã€‚è¯·æŸ¥çœ‹è¿™ä¸¤ä¸ªéŸ³é¢‘æ ·æœ¬çš„åºåˆ—é•¿åº¦ï¼š


```py
>>> dataset[0]["audio"]["array"].shape
(173398,)

>>> dataset[1]["audio"]["array"].shape
(106496,)
```

åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥é¢„å¤„ç†æ•°æ®é›†ï¼Œä»¥ä½¿éŸ³é¢‘æ ·æœ¬å…·æœ‰ç›¸åŒçš„é•¿åº¦ã€‚é€šè¿‡æŒ‡å®šæœ€å¤§æ ·æœ¬é•¿åº¦ï¼Œ`feature extractor`å°†å¡«å……æˆ–æˆªæ–­åºåˆ—ä»¥ä½¿å…¶åŒ¹é…ï¼š


```py
>>> def preprocess_function(examples):
...     audio_arrays = [x["array"] for x in examples["audio"]]
...     inputs = feature_extractor(
...         audio_arrays,
...         sampling_rate=16000,
...         padding=True,
...         max_length=100000,
...         truncation=True,
...     )
...     return inputs
```

å°†`preprocess_function`åº”ç”¨äºæ•°æ®é›†ä¸­çš„å‰å‡ ä¸ªç¤ºä¾‹ï¼š


```py
>>> processed_dataset = preprocess_function(dataset[:5])
```

ç°åœ¨æ ·æœ¬é•¿åº¦æ˜¯ç›¸åŒçš„ï¼Œå¹¶ä¸”ä¸æŒ‡å®šçš„æœ€å¤§é•¿åº¦åŒ¹é…ã€‚æ‚¨ç°åœ¨å¯ä»¥å°†ç»è¿‡å¤„ç†çš„æ•°æ®é›†ä¼ é€’ç»™æ¨¡å‹äº†ï¼


```py
>>> processed_dataset["input_values"][0].shape
(100000,)

>>> processed_dataset["input_values"][1].shape
(100000,)
```

## è®¡ç®—æœºè§†è§‰

å¯¹äºè®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼Œæ‚¨éœ€è¦ä¸€ä¸ª[ image processor](main_classes/image_processor)æ¥å‡†å¤‡æ•°æ®é›†ä»¥ä¾›æ¨¡å‹ä½¿ç”¨ã€‚å›¾åƒé¢„å¤„ç†åŒ…æ‹¬å¤šä¸ªæ­¥éª¤å°†å›¾åƒè½¬æ¢ä¸ºæ¨¡å‹æœŸæœ›è¾“å…¥çš„æ ¼å¼ã€‚è¿™äº›æ­¥éª¤åŒ…æ‹¬ä½†ä¸é™äºè°ƒæ•´å¤§å°ã€æ ‡å‡†åŒ–ã€é¢œè‰²é€šé“æ ¡æ­£ä»¥åŠå°†å›¾åƒè½¬æ¢ä¸ºå¼ é‡ã€‚

<Tip>

å›¾åƒé¢„å¤„ç†é€šå¸¸éµå¾ªæŸç§å½¢å¼çš„å›¾åƒå¢å¼ºã€‚å›¾åƒé¢„å¤„ç†å’Œå›¾åƒå¢å¼ºéƒ½ä¼šæ”¹å˜å›¾åƒæ•°æ®ï¼Œä½†å®ƒä»¬æœ‰ä¸åŒçš„ç›®çš„ï¼š

* å›¾åƒå¢å¼ºå¯ä»¥å¸®åŠ©é˜²æ­¢è¿‡æ‹Ÿåˆå¹¶å¢åŠ æ¨¡å‹çš„é²æ£’æ€§ã€‚æ‚¨å¯ä»¥åœ¨æ•°æ®å¢å¼ºæ–¹é¢å……åˆ†å‘æŒ¥åˆ›é€ æ€§ - è°ƒæ•´äº®åº¦å’Œé¢œè‰²ã€è£å‰ªã€æ—‹è½¬ã€è°ƒæ•´å¤§å°ã€ç¼©æ”¾ç­‰ã€‚ä½†è¦æ³¨æ„ä¸è¦æ”¹å˜å›¾åƒçš„å«ä¹‰ã€‚
* å›¾åƒé¢„å¤„ç†ç¡®ä¿å›¾åƒä¸æ¨¡å‹é¢„æœŸçš„è¾“å…¥æ ¼å¼åŒ¹é…ã€‚åœ¨å¾®è°ƒè®¡ç®—æœºè§†è§‰æ¨¡å‹æ—¶ï¼Œå¿…é¡»å¯¹å›¾åƒè¿›è¡Œä¸æ¨¡å‹è®­ç»ƒæ—¶ç›¸åŒçš„é¢„å¤„ç†ã€‚

æ‚¨å¯ä»¥ä½¿ç”¨ä»»ä½•æ‚¨å–œæ¬¢çš„å›¾åƒå¢å¼ºåº“ã€‚å¯¹äºå›¾åƒé¢„å¤„ç†ï¼Œè¯·ä½¿ç”¨ä¸æ¨¡å‹ç›¸å…³è”çš„`ImageProcessor`ã€‚

</Tip>

åŠ è½½[food101](https://huggingface.co/datasets/ethz/food101)æ•°æ®é›†ï¼ˆæœ‰å…³å¦‚ä½•åŠ è½½æ•°æ®é›†çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…ğŸ¤— [Datasetsæ•™ç¨‹](https://huggingface.co/docs/datasets/load_hub)ï¼‰ä»¥äº†è§£å¦‚ä½•åœ¨è®¡ç®—æœºè§†è§‰æ•°æ®é›†ä¸­ä½¿ç”¨å›¾åƒå¤„ç†å™¨ï¼š

<Tip>

å› ä¸ºæ•°æ®é›†ç›¸å½“å¤§ï¼Œè¯·ä½¿ç”¨ğŸ¤— Datasetsçš„`split`å‚æ•°åŠ è½½è®­ç»ƒé›†ä¸­çš„å°‘é‡æ ·æœ¬ï¼

</Tip>


```py
>>> from datasets import load_dataset

>>> dataset = load_dataset("ethz/food101", split="train[:100]")
```

æ¥ä¸‹æ¥ï¼Œä½¿ç”¨ğŸ¤— Datasetsçš„[`Image`](https://huggingface.co/docs/datasets/package_reference/main_classes?highlight=image#datasets.Image)åŠŸèƒ½æŸ¥çœ‹å›¾åƒï¼š


```py
>>> dataset[0]["image"]
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/vision-preprocess-tutorial.png"/>
</div>

ä½¿ç”¨ [`AutoImageProcessor.from_pretrained`] åŠ è½½`image processor`ï¼š

```py
>>> from transformers import AutoImageProcessor

>>> image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")
```

é¦–å…ˆï¼Œè®©æˆ‘ä»¬è¿›è¡Œå›¾åƒå¢å¼ºã€‚æ‚¨å¯ä»¥ä½¿ç”¨ä»»ä½•æ‚¨å–œæ¬¢çš„åº“ï¼Œä½†åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨torchvisionçš„[`transforms`](https://pytorch.org/vision/stable/transforms.html)æ¨¡å—ã€‚å¦‚æœæ‚¨æœ‰å…´è¶£ä½¿ç”¨å…¶ä»–æ•°æ®å¢å¼ºåº“ï¼Œè¯·å‚é˜…[Albumentations](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb)æˆ–[Kornia notebooks](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_kornia.ipynb)ä¸­çš„ç¤ºä¾‹ã€‚

1. åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨[`Compose`](https://pytorch.org/vision/master/generated/torchvision.transforms.Compose.html)å°†[`RandomResizedCrop`](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html)å’Œ [`ColorJitter`](https://pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html)å˜æ¢è¿æ¥åœ¨ä¸€èµ·ã€‚è¯·æ³¨æ„ï¼Œå¯¹äºè°ƒæ•´å¤§å°ï¼Œæˆ‘ä»¬å¯ä»¥ä»`image_processor`ä¸­è·å–å›¾åƒå°ºå¯¸è¦æ±‚ã€‚å¯¹äºä¸€äº›æ¨¡å‹ï¼Œç²¾ç¡®çš„é«˜åº¦å’Œå®½åº¦éœ€è¦è¢«å®šä¹‰ï¼Œå¯¹äºå…¶ä»–æ¨¡å‹åªéœ€å®šä¹‰`shortest_edge`ã€‚


```py
>>> from torchvision.transforms import RandomResizedCrop, ColorJitter, Compose

>>> size = (
...     image_processor.size["shortest_edge"]
...     if "shortest_edge" in image_processor.size
...     else (image_processor.size["height"], image_processor.size["width"])
... )

>>> _transforms = Compose([RandomResizedCrop(size), ColorJitter(brightness=0.5, hue=0.5)])
```

2. æ¨¡å‹æ¥å— [`pixel_values`](model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderModel.forward.pixel_values) ä½œä¸ºè¾“å…¥ã€‚`ImageProcessor` å¯ä»¥è¿›è¡Œå›¾åƒçš„æ ‡å‡†åŒ–ï¼Œå¹¶ç”Ÿæˆé€‚å½“çš„å¼ é‡ã€‚åˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œå°†å›¾åƒå¢å¼ºå’Œå›¾åƒé¢„å¤„ç†æ­¥éª¤ç»„åˆèµ·æ¥å¤„ç†æ‰¹é‡å›¾åƒï¼Œå¹¶ç”Ÿæˆ `pixel_values`ï¼š


```py
>>> def transforms(examples):
...     images = [_transforms(img.convert("RGB")) for img in examples["image"]]
...     examples["pixel_values"] = image_processor(images, do_resize=False, return_tensors="pt")["pixel_values"]
...     return examples
```

<Tip>

åœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬è®¾ç½®`do_resize=False`ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»åœ¨å›¾åƒå¢å¼ºè½¬æ¢ä¸­è°ƒæ•´äº†å›¾åƒçš„å¤§å°ï¼Œå¹¶åˆ©ç”¨äº†é€‚å½“çš„`image_processor`çš„`size`å±æ€§ã€‚å¦‚æœæ‚¨åœ¨å›¾åƒå¢å¼ºæœŸé—´ä¸è°ƒæ•´å›¾åƒçš„å¤§å°ï¼Œè¯·å°†æ­¤å‚æ•°æ’é™¤åœ¨å¤–ã€‚é»˜è®¤æƒ…å†µä¸‹`ImageProcessor`å°†å¤„ç†è°ƒæ•´å¤§å°ã€‚

å¦‚æœå¸Œæœ›å°†å›¾åƒæ ‡å‡†åŒ–æ­¥éª¤ä¸ºå›¾åƒå¢å¼ºçš„ä¸€éƒ¨åˆ†ï¼Œè¯·ä½¿ç”¨`image_processor.image_mean`å’Œ`image_processor.image_std`ã€‚

</Tip>

3. ç„¶åä½¿ç”¨ğŸ¤— Datasetsçš„[`set_transform`](https://huggingface.co/docs/datasets/process#format-transform)åœ¨è¿è¡Œæ—¶åº”ç”¨è¿™äº›å˜æ¢ï¼š


```py
>>> dataset.set_transform(transforms)
```

4. ç°åœ¨ï¼Œå½“æ‚¨è®¿é—®å›¾åƒæ—¶ï¼Œæ‚¨å°†æ³¨æ„åˆ°`image processor`å·²æ·»åŠ äº† `pixel_values`ã€‚æ‚¨ç°åœ¨å¯ä»¥å°†ç»è¿‡å¤„ç†çš„æ•°æ®é›†ä¼ é€’ç»™æ¨¡å‹äº†ï¼


```py
>>> dataset[0].keys()
```

è¿™æ˜¯åœ¨åº”ç”¨å˜æ¢åçš„å›¾åƒæ ·å­ã€‚å›¾åƒå·²è¢«éšæœºè£å‰ªï¼Œå¹¶å…¶é¢œè‰²å±æ€§å‘ç”Ÿäº†å˜åŒ–ã€‚


```py
>>> import numpy as np
>>> import matplotlib.pyplot as plt

>>> img = dataset[0]["pixel_values"]
>>> plt.imshow(img.permute(1, 2, 0))
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/preprocessed_image.png"/>
</div>

<Tip>

å¯¹äºè¯¸å¦‚ç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²ã€å®ä¾‹åˆ†å‰²å’Œå…¨æ™¯åˆ†å‰²ç­‰ä»»åŠ¡ï¼Œ`ImageProcessor`æä¾›äº†è®­ç»ƒåå¤„ç†æ–¹æ³•ã€‚è¿™äº›æ–¹æ³•å°†æ¨¡å‹çš„åŸå§‹è¾“å‡ºè½¬æ¢ä¸ºæœ‰æ„ä¹‰çš„é¢„æµ‹ï¼Œå¦‚è¾¹ç•Œæ¡†æˆ–åˆ†å‰²åœ°å›¾ã€‚

</Tip>

### å¡«å……

åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä¾‹å¦‚ï¼Œåœ¨å¾®è°ƒ[DETR](./model_doc/detr)æ—¶ï¼Œæ¨¡å‹åœ¨è®­ç»ƒæ—¶åº”ç”¨äº†å°ºåº¦å¢å¼ºã€‚è¿™å¯èƒ½å¯¼è‡´æ‰¹å¤„ç†ä¸­çš„å›¾åƒå¤§å°ä¸åŒã€‚æ‚¨å¯ä»¥ä½¿ç”¨[`DetrImageProcessor.pad`]æ¥æŒ‡å®šè‡ªå®šä¹‰çš„`collate_fn`å°†å›¾åƒæ‰¹å¤„ç†åœ¨ä¸€èµ·ã€‚

```py
>>> def collate_fn(batch):
...     pixel_values = [item["pixel_values"] for item in batch]
...     encoding = image_processor.pad(pixel_values, return_tensors="pt")
...     labels = [item["labels"] for item in batch]
...     batch = {}
...     batch["pixel_values"] = encoding["pixel_values"]
...     batch["pixel_mask"] = encoding["pixel_mask"]
...     batch["labels"] = labels
...     return batch
```

## å¤šæ¨¡æ€

å¯¹äºæ¶‰åŠå¤šæ¨¡æ€è¾“å…¥çš„ä»»åŠ¡ï¼Œæ‚¨éœ€è¦[processor](main_classes/processors)æ¥ä¸ºæ¨¡å‹å‡†å¤‡æ•°æ®é›†ã€‚`processor`å°†ä¸¤ä¸ªå¤„ç†å¯¹è±¡-ä¾‹å¦‚`tokenizer`å’Œ`feature extractor`-ç»„åˆåœ¨ä¸€èµ·ã€‚

åŠ è½½[LJ Speech](https://huggingface.co/datasets/lj_speech)æ•°æ®é›†ï¼ˆæœ‰å…³å¦‚ä½•åŠ è½½æ•°æ®é›†çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…ğŸ¤— [Datasets æ•™ç¨‹](https://huggingface.co/docs/datasets/load_hub)ï¼‰ä»¥äº†è§£å¦‚ä½•ä½¿ç”¨`processor`è¿›è¡Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ï¼š


```py
>>> from datasets import load_dataset

>>> lj_speech = load_dataset("lj_speech", split="train")
```

å¯¹äºASRï¼ˆè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼‰ï¼Œä¸»è¦å…³æ³¨`audio`å’Œ`text`ï¼Œå› æ­¤å¯ä»¥åˆ é™¤å…¶ä»–åˆ—ï¼š


```py
>>> lj_speech = lj_speech.map(remove_columns=["file", "id", "normalized_text"])
```

ç°åœ¨æŸ¥çœ‹`audio`å’Œ`text`åˆ—ï¼š

```py
>>> lj_speech[0]["audio"]
{'array': array([-7.3242188e-04, -7.6293945e-04, -6.4086914e-04, ...,
         7.3242188e-04,  2.1362305e-04,  6.1035156e-05], dtype=float32),
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/917ece08c95cf0c4115e45294e3cd0dee724a1165b7fc11798369308a465bd26/LJSpeech-1.1/wavs/LJ001-0001.wav',
 'sampling_rate': 22050}

>>> lj_speech[0]["text"]
'Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition'
```

è¯·è®°ä½ï¼Œæ‚¨åº”å§‹ç»ˆ[é‡æ–°é‡‡æ ·](preprocessing#audio)éŸ³é¢‘æ•°æ®é›†çš„é‡‡æ ·ç‡ï¼Œä»¥åŒ¹é…ç”¨äºé¢„è®­ç»ƒæ¨¡å‹æ•°æ®é›†çš„é‡‡æ ·ç‡ï¼


```py
>>> lj_speech = lj_speech.cast_column("audio", Audio(sampling_rate=16_000))
```

ä½¿ç”¨[`AutoProcessor.from_pretrained`]åŠ è½½ä¸€ä¸ª`processor`ï¼š


```py
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")
```

1. åˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºå°†åŒ…å«åœ¨ `array` ä¸­çš„éŸ³é¢‘æ•°æ®å¤„ç†ä¸º `input_values`ï¼Œå¹¶å°† `text` æ ‡è®°ä¸º `labels`ã€‚è¿™äº›å°†æ˜¯è¾“å…¥æ¨¡å‹çš„æ•°æ®ï¼š

```py
>>> def prepare_dataset(example):
...     audio = example["audio"]

...     example.update(processor(audio=audio["array"], text=example["text"], sampling_rate=16000))

...     return example
```

2. å°† `prepare_dataset` å‡½æ•°åº”ç”¨äºä¸€ä¸ªç¤ºä¾‹ï¼š

```py
>>> prepare_dataset(lj_speech[0])
```

`processor`ç°åœ¨å·²ç»æ·»åŠ äº† `input_values` å’Œ `labels`ï¼Œå¹¶ä¸”é‡‡æ ·ç‡ä¹Ÿæ­£ç¡®é™ä½ä¸ºä¸º16kHzã€‚ç°åœ¨å¯ä»¥å°†å¤„ç†åçš„æ•°æ®é›†ä¼ é€’ç»™æ¨¡å‹ï¼

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\quicktour.md
============================================================



# å¿«é€Ÿä¸Šæ‰‹

[[open-in-colab]]

å¿«æ¥ä½¿ç”¨ ğŸ¤— Transformers å§ï¼æ— è®ºä½ æ˜¯å¼€å‘äººå‘˜è¿˜æ˜¯æ—¥å¸¸ç”¨æˆ·ï¼Œè¿™ç¯‡å¿«é€Ÿä¸Šæ‰‹æ•™ç¨‹éƒ½å°†å¸®åŠ©ä½ å…¥é—¨å¹¶ä¸”å‘ä½ å±•ç¤ºå¦‚ä½•ä½¿ç”¨ [`pipeline`] è¿›è¡Œæ¨ç†ï¼Œä½¿ç”¨ [AutoClass](./model_doc/auto) åŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹å’Œé¢„å¤„ç†å™¨ï¼Œä»¥åŠä½¿ç”¨ PyTorch æˆ– TensorFlow å¿«é€Ÿè®­ç»ƒä¸€ä¸ªæ¨¡å‹ã€‚å¦‚æœä½ æ˜¯ä¸€ä¸ªåˆå­¦è€…ï¼Œæˆ‘ä»¬å»ºè®®ä½ æ¥ä¸‹æ¥æŸ¥çœ‹æˆ‘ä»¬çš„æ•™ç¨‹æˆ–è€…[è¯¾ç¨‹](https://huggingface.co/course/chapter1/1)ï¼Œæ¥æ›´æ·±å…¥åœ°äº†è§£åœ¨è¿™é‡Œä»‹ç»åˆ°çš„æ¦‚å¿µã€‚

åœ¨å¼€å§‹ä¹‹å‰ï¼Œç¡®ä¿ä½ å·²ç»å®‰è£…äº†æ‰€æœ‰å¿…è¦çš„åº“ï¼š

```bash
!pip install transformers datasets evaluate accelerate
```

ä½ è¿˜éœ€è¦å®‰è£…å–œæ¬¢çš„æœºå™¨å­¦ä¹ æ¡†æ¶ï¼š


```bash
pip install torch
```

## Pipeline

<Youtube id="tiZFewofSLM"/>

ä½¿ç”¨ [`pipeline`] æ˜¯åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ¨ç†çš„æœ€ç®€å•çš„æ–¹å¼ã€‚ä½ èƒ½å¤Ÿå°† [`pipeline`] å¼€ç®±å³ç”¨åœ°ç”¨äºè·¨ä¸åŒæ¨¡æ€çš„å¤šç§ä»»åŠ¡ã€‚æ¥çœ‹çœ‹å®ƒæ”¯æŒçš„ä»»åŠ¡åˆ—è¡¨ï¼š

| **ä»»åŠ¡**                     | **æè¿°**                            | **æ¨¡æ€**        | **Pipeline**                       |
|------------------------------|-----------------------------------|-----------------|-----------------------------------------------|
| æ–‡æœ¬åˆ†ç±»                      | ä¸ºç»™å®šçš„æ–‡æœ¬åºåˆ—åˆ†é…ä¸€ä¸ªæ ‡ç­¾                    | NLP             | pipeline(task="sentiment-analysis")           |
| æ–‡æœ¬ç”Ÿæˆ                      | æ ¹æ®ç»™å®šçš„æç¤ºç”Ÿæˆæ–‡æœ¬                       | NLP             | pipeline(task="text-generation")              |
| å‘½åå®ä½“è¯†åˆ«                  | ä¸ºåºåˆ—é‡Œçš„æ¯ä¸ª token åˆ†é…ä¸€ä¸ªæ ‡ç­¾ï¼ˆäºº, ç»„ç»‡, åœ°å€ç­‰ç­‰ï¼‰ | NLP             | pipeline(task="ner")                          |
| é—®ç­”ç³»ç»Ÿ                      | é€šè¿‡ç»™å®šçš„ä¸Šä¸‹æ–‡å’Œé—®é¢˜, åœ¨æ–‡æœ¬ä¸­æå–ç­”æ¡ˆ             | NLP             | pipeline(task="question-answering")           |
| æ©ç›–å¡«å……                      | é¢„æµ‹å‡ºæ­£ç¡®çš„åœ¨åºåˆ—ä¸­è¢«æ©ç›–çš„token               | NLP             | pipeline(task="fill-mask")                    |
| æ–‡æœ¬æ‘˜è¦                      | ä¸ºæ–‡æœ¬åºåˆ—æˆ–æ–‡æ¡£ç”Ÿæˆæ€»ç»“                      | NLP             | pipeline(task="summarization")                |
| æ–‡æœ¬ç¿»è¯‘                      | å°†æ–‡æœ¬ä»ä¸€ç§è¯­è¨€ç¿»è¯‘ä¸ºå¦ä¸€ç§è¯­è¨€                  | NLP             | pipeline(task="translation")                  |
| å›¾åƒåˆ†ç±»                      | ä¸ºå›¾åƒåˆ†é…ä¸€ä¸ªæ ‡ç­¾                         | Computer vision | pipeline(task="image-classification")         |
| å›¾åƒåˆ†å‰²                      | ä¸ºå›¾åƒä¸­æ¯ä¸ªç‹¬ç«‹çš„åƒç´ åˆ†é…æ ‡ç­¾ï¼ˆæ”¯æŒè¯­ä¹‰ã€å…¨æ™¯å’Œå®ä¾‹åˆ†å‰²ï¼‰     | Computer vision | pipeline(task="image-segmentation")           |
| ç›®æ ‡æ£€æµ‹                      | é¢„æµ‹å›¾åƒä¸­ç›®æ ‡å¯¹è±¡çš„è¾¹ç•Œæ¡†å’Œç±»åˆ«                  | Computer vision | pipeline(task="object-detection")             |
| éŸ³é¢‘åˆ†ç±»                      | ç»™éŸ³é¢‘æ–‡ä»¶åˆ†é…ä¸€ä¸ªæ ‡ç­¾                       | Audio           | pipeline(task="audio-classification")         |
| è‡ªåŠ¨è¯­éŸ³è¯†åˆ«                   | å°†éŸ³é¢‘æ–‡ä»¶ä¸­çš„è¯­éŸ³æå–ä¸ºæ–‡æœ¬                    | Audio           | pipeline(task="automatic-speech-recognition") |
| è§†è§‰é—®ç­”                      | ç»™å®šä¸€ä¸ªå›¾åƒå’Œä¸€ä¸ªé—®é¢˜ï¼Œæ­£ç¡®åœ°å›ç­”æœ‰å…³å›¾åƒçš„é—®é¢˜          | Multimodal      | pipeline(task="vqa")                          |

åˆ›å»ºä¸€ä¸ª [`pipeline`] å®ä¾‹å¹¶ä¸”æŒ‡å®šä½ æƒ³è¦å°†å®ƒç”¨äºçš„ä»»åŠ¡ï¼Œå°±å¯ä»¥å¼€å§‹äº†ã€‚ä½ å¯ä»¥å°† [`pipeline`] ç”¨äºä»»ä½•ä¸€ä¸ªä¸Šé¢æåˆ°çš„ä»»åŠ¡ï¼Œå¦‚æœæƒ³çŸ¥é“æ”¯æŒçš„ä»»åŠ¡çš„å®Œæ•´åˆ—è¡¨ï¼Œå¯ä»¥æŸ¥é˜… [pipeline API å‚è€ƒ](./main_classes/pipelines)ã€‚ä¸è¿‡, åœ¨è¿™ç¯‡æ•™ç¨‹ä¸­ï¼Œä½ å°†æŠŠ [`pipeline`] ç”¨åœ¨ä¸€ä¸ªæƒ…æ„Ÿåˆ†æç¤ºä¾‹ä¸Šï¼š

```py
>>> from transformers import pipeline

>>> classifier = pipeline("sentiment-analysis")
```

[`pipeline`] ä¼šä¸‹è½½å¹¶ç¼“å­˜ä¸€ä¸ªç”¨äºæƒ…æ„Ÿåˆ†æçš„é»˜è®¤çš„[é¢„è®­ç»ƒæ¨¡å‹](https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english)å’Œåˆ†è¯å™¨ã€‚ç°åœ¨ä½ å¯ä»¥åœ¨ç›®æ ‡æ–‡æœ¬ä¸Šä½¿ç”¨ `classifier` äº†ï¼š

```py
>>> classifier("We are very happy to show you the ğŸ¤— Transformers library.")
[{'label': 'POSITIVE', 'score': 0.9998}]
```

å¦‚æœä½ æœ‰ä¸æ­¢ä¸€ä¸ªè¾“å…¥ï¼Œå¯ä»¥æŠŠæ‰€æœ‰è¾“å…¥æ”¾å…¥ä¸€ä¸ªåˆ—è¡¨ç„¶åä¼ ç»™[`pipeline`]ï¼Œå®ƒå°†ä¼šè¿”å›ä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼š

```py
>>> results = classifier(["We are very happy to show you the ğŸ¤— Transformers library.", "We hope you don't hate it."])
>>> for result in results:
...     print(f"label: {result['label']}, with score: {round(result['score'], 4)}")
label: POSITIVE, with score: 0.9998
label: NEGATIVE, with score: 0.5309
```

[`pipeline`] ä¹Ÿå¯ä»¥ä¸ºä»»ä½•ä½ å–œæ¬¢çš„ä»»åŠ¡éå†æ•´ä¸ªæ•°æ®é›†ã€‚åœ¨ä¸‹é¢è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œè®©æˆ‘ä»¬é€‰æ‹©è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä½œä¸ºæˆ‘ä»¬çš„ä»»åŠ¡ï¼š

```py
>>> import torch
>>> from transformers import pipeline

>>> speech_recognizer = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h")
```

åŠ è½½ä¸€ä¸ªä½ æƒ³éå†çš„éŸ³é¢‘æ•°æ®é›†ï¼ˆæŸ¥é˜… ğŸ¤— Datasets [å¿«é€Ÿå¼€å§‹](https://huggingface.co/docs/datasets/quickstart#audio) è·å¾—æ›´å¤šä¿¡æ¯ï¼‰ã€‚æ¯”å¦‚ï¼ŒåŠ è½½ [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) æ•°æ®é›†ï¼š

```py
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")  # doctest: +IGNORE_RESULT
```

ä½ éœ€è¦ç¡®ä¿æ•°æ®é›†ä¸­çš„éŸ³é¢‘çš„é‡‡æ ·ç‡ä¸ [`facebook/wav2vec2-base-960h`](https://huggingface.co/facebook/wav2vec2-base-960h) è®­ç»ƒç”¨åˆ°çš„éŸ³é¢‘çš„é‡‡æ ·ç‡ä¸€è‡´ï¼š

```py
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))
```

å½“è°ƒç”¨ `"audio"` åˆ—æ—¶, éŸ³é¢‘æ–‡ä»¶å°†ä¼šè‡ªåŠ¨åŠ è½½å¹¶é‡é‡‡æ ·ã€‚
ä»å‰å››ä¸ªæ ·æœ¬ä¸­æå–åŸå§‹æ³¢å½¢æ•°ç»„ï¼Œå°†å®ƒä½œä¸ºåˆ—è¡¨ä¼ ç»™ pipelineï¼š

```py
>>> result = speech_recognizer(dataset[:4]["audio"])
>>> print([d["text"] for d in result])
['I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT', "FODING HOW I'D SET UP A JOIN TO HET WITH MY WIFE AND WHERE THE AP MIGHT BE", "I I'D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I'M NOT SEEING THE OPTION TO DO IT ON THE AP SO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AND I'M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS", 'HOW DO I THURN A JOIN A COUNT']
```

å¯¹äºè¾“å…¥éå¸¸åºå¤§çš„å¤§å‹æ•°æ®é›†ï¼ˆæ¯”å¦‚è¯­éŸ³æˆ–è§†è§‰ï¼‰ï¼Œä½ ä¼šæƒ³åˆ°ä½¿ç”¨ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå°†æ‰€æœ‰è¾“å…¥éƒ½åŠ è½½è¿›å†…å­˜çš„åˆ—è¡¨ã€‚æŸ¥é˜… [pipeline API å‚è€ƒ](./main_classes/pipelines) æ¥è·å–æ›´å¤šä¿¡æ¯ã€‚

### åœ¨ pipeline ä¸­ä½¿ç”¨å¦ä¸€ä¸ªæ¨¡å‹å’Œåˆ†è¯å™¨

[`pipeline`] å¯ä»¥å®¹çº³ [Hub](https://huggingface.co/models) ä¸­çš„ä»»ä½•æ¨¡å‹ï¼Œè¿™è®© [`pipeline`] æ›´å®¹æ˜“é€‚ç”¨äºå…¶ä»–ç”¨ä¾‹ã€‚æ¯”å¦‚ï¼Œä½ æƒ³è¦ä¸€ä¸ªèƒ½å¤Ÿå¤„ç†æ³•è¯­æ–‡æœ¬çš„æ¨¡å‹ï¼Œå°±å¯ä»¥ä½¿ç”¨ Hub ä¸Šçš„æ ‡è®°æ¥ç­›é€‰å‡ºåˆé€‚çš„æ¨¡å‹ã€‚é å‰çš„ç­›é€‰ç»“æœä¼šè¿”å›ä¸€ä¸ªä¸ºæƒ…æ„Ÿåˆ†æå¾®è°ƒçš„å¤šè¯­è¨€çš„ [BERT æ¨¡å‹](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment)ï¼Œä½ å¯ä»¥å°†å®ƒç”¨äºæ³•è¯­æ–‡æœ¬ï¼š

```py
>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
```

ä½¿ç”¨ [`AutoModelForSequenceClassification`] å’Œ [`AutoTokenizer`] æ¥åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œå®ƒå…³è”çš„åˆ†è¯å™¨ï¼ˆæ›´å¤šä¿¡æ¯å¯ä»¥å‚è€ƒä¸‹ä¸€èŠ‚çš„ `AutoClass`ï¼‰ï¼š

```py
>>> from transformers import AutoTokenizer, AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained(model_name)
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
```

åœ¨ [`pipeline`] ä¸­æŒ‡å®šæ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œç°åœ¨ä½ å°±å¯ä»¥åœ¨æ³•è¯­æ–‡æœ¬ä¸Šä½¿ç”¨ `classifier` äº†ï¼š

```py
>>> classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
>>> classifier("Nous sommes trÃ¨s heureux de vous prÃ©senter la bibliothÃ¨que ğŸ¤— Transformers.")
[{'label': '5 stars', 'score': 0.7273}]
```

å¦‚æœä½ æ²¡æœ‰æ‰¾åˆ°é€‚åˆä½ çš„æ¨¡å‹ï¼Œå°±éœ€è¦åœ¨ä½ çš„æ•°æ®ä¸Šå¾®è°ƒä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹äº†ã€‚æŸ¥çœ‹ [å¾®è°ƒæ•™ç¨‹](./training) æ¥å­¦ä¹ æ€æ ·è¿›è¡Œå¾®è°ƒã€‚æœ€åï¼Œå¾®è°ƒå®Œæ¨¡å‹åï¼Œè€ƒè™‘ä¸€ä¸‹åœ¨ Hub ä¸Šä¸ç¤¾åŒº [åˆ†äº«](./model_sharing) è¿™ä¸ªæ¨¡å‹ï¼ŒæŠŠæœºå™¨å­¦ä¹ æ™®åŠåˆ°æ¯ä¸€ä¸ªäºº! ğŸ¤—

## AutoClass

<Youtube id="AhChOFRegn4"/>

åœ¨å¹•åï¼Œæ˜¯ç”± [`AutoModelForSequenceClassification`] å’Œ [`AutoTokenizer`] ä¸€èµ·æ”¯æŒä½ åœ¨ä¸Šé¢ç”¨åˆ°çš„ [`pipeline`]ã€‚[AutoClass](./model_doc/auto) æ˜¯ä¸€ä¸ªèƒ½å¤Ÿé€šè¿‡é¢„è®­ç»ƒæ¨¡å‹çš„åç§°æˆ–è·¯å¾„è‡ªåŠ¨æŸ¥æ‰¾å…¶æ¶æ„çš„å¿«æ·æ–¹å¼ã€‚ä½ åªéœ€è¦ä¸ºä½ çš„ä»»åŠ¡é€‰æ‹©åˆé€‚çš„ `AutoClass` å’Œå®ƒå…³è”çš„é¢„å¤„ç†ç±»ã€‚

è®©æˆ‘ä»¬å›è¿‡å¤´æ¥çœ‹ä¸Šä¸€èŠ‚çš„ç¤ºä¾‹ï¼Œçœ‹çœ‹æ€æ ·ä½¿ç”¨ `AutoClass` æ¥é‡ç°ä½¿ç”¨ [`pipeline`] çš„ç»“æœã€‚

### AutoTokenizer

åˆ†è¯å™¨è´Ÿè´£é¢„å¤„ç†æ–‡æœ¬ï¼Œå°†æ–‡æœ¬è½¬æ¢ä¸ºç”¨äºè¾“å…¥æ¨¡å‹çš„æ•°å­—æ•°ç»„ã€‚æœ‰å¤šä¸ªç”¨æ¥ç®¡ç†åˆ†è¯è¿‡ç¨‹çš„è§„åˆ™ï¼ŒåŒ…æ‹¬å¦‚ä½•æ‹†åˆ†å•è¯å’Œåœ¨ä»€ä¹ˆæ ·çš„çº§åˆ«ä¸Šæ‹†åˆ†å•è¯ï¼ˆåœ¨ [åˆ†è¯å™¨æ€»ç»“](./tokenizer_summary) å­¦ä¹ æ›´å¤šå…³äºåˆ†è¯çš„ä¿¡æ¯ï¼‰ã€‚è¦è®°ä½æœ€é‡è¦çš„æ˜¯ä½ éœ€è¦å®ä¾‹åŒ–çš„åˆ†è¯å™¨è¦ä¸æ¨¡å‹çš„åç§°ç›¸åŒ, æ¥ç¡®ä¿å’Œæ¨¡å‹è®­ç»ƒæ—¶ä½¿ç”¨ç›¸åŒçš„åˆ†è¯è§„åˆ™ã€‚

ä½¿ç”¨ [`AutoTokenizer`] åŠ è½½ä¸€ä¸ªåˆ†è¯å™¨:

```py
>>> from transformers import AutoTokenizer

>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
```

å°†æ–‡æœ¬ä¼ å…¥åˆ†è¯å™¨ï¼š

```py
>>> encoding = tokenizer("We are very happy to show you the ğŸ¤— Transformers library.")
>>> print(encoding)
{'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

åˆ†è¯å™¨è¿”å›äº†å«æœ‰å¦‚ä¸‹å†…å®¹çš„å­—å…¸:

* [input_ids](./glossary#input-ids)ï¼šç”¨æ•°å­—è¡¨ç¤ºçš„ tokenã€‚
* [attention_mask](.glossary#attention-mask)ï¼šåº”è¯¥å…³æ³¨å“ªäº› token çš„æŒ‡ç¤ºã€‚

åˆ†è¯å™¨ä¹Ÿå¯ä»¥æ¥å—åˆ—è¡¨ä½œä¸ºè¾“å…¥ï¼Œå¹¶å¡«å……å’Œæˆªæ–­æ–‡æœ¬ï¼Œè¿”å›å…·æœ‰ç»Ÿä¸€é•¿åº¦çš„æ‰¹æ¬¡ï¼š


```py
>>> pt_batch = tokenizer(
...     ["We are very happy to show you the ğŸ¤— Transformers library.", "We hope you don't hate it."],
...     padding=True,
...     truncation=True,
...     max_length=512,
...     return_tensors="pt",
... )
```

<Tip>

æŸ¥é˜…[é¢„å¤„ç†](./preprocessing)æ•™ç¨‹æ¥è·å¾—æœ‰å…³åˆ†è¯çš„æ›´è¯¦ç»†çš„ä¿¡æ¯ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨ [`AutoFeatureExtractor`] å’Œ [`AutoProcessor`] æ¥å¤„ç†å›¾åƒï¼ŒéŸ³é¢‘ï¼Œè¿˜æœ‰å¤šæ¨¡å¼è¾“å…¥ã€‚

</Tip>

### AutoModel

ğŸ¤— Transformers æä¾›äº†ä¸€ç§ç®€å•ç»Ÿä¸€çš„æ–¹å¼æ¥åŠ è½½é¢„è®­ç»ƒçš„å®ä¾‹. è¿™è¡¨ç¤ºä½ å¯ä»¥åƒåŠ è½½ [`AutoTokenizer`] ä¸€æ ·åŠ è½½ [`AutoModel`]ã€‚å”¯ä¸€ä¸åŒçš„åœ°æ–¹æ˜¯ä¸ºä½ çš„ä»»åŠ¡é€‰æ‹©æ­£ç¡®çš„[`AutoModel`]ã€‚å¯¹äºæ–‡æœ¬ï¼ˆæˆ–åºåˆ—ï¼‰åˆ†ç±»ï¼Œä½ åº”è¯¥åŠ è½½[`AutoModelForSequenceClassification`]ï¼š

```py
>>> from transformers import AutoModelForSequenceClassification

>>> model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
>>> pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
```

<Tip>

é€šè¿‡ [ä»»åŠ¡æ‘˜è¦](./task_summary) æŸ¥æ‰¾ [`AutoModel`] æ”¯æŒçš„ä»»åŠ¡.

</Tip>

ç°åœ¨å¯ä»¥æŠŠé¢„å¤„ç†å¥½çš„è¾“å…¥æ‰¹æ¬¡ç›´æ¥é€è¿›æ¨¡å‹ã€‚ä½ åªéœ€è¦é€šè¿‡ `**` æ¥è§£åŒ…å­—å…¸:

```py
>>> pt_outputs = pt_model(**pt_batch)
```

æ¨¡å‹åœ¨ `logits` å±æ€§è¾“å‡ºæœ€ç»ˆçš„æ¿€æ´»ç»“æœ. åœ¨ `logits` ä¸Šåº”ç”¨ softmax å‡½æ•°æ¥æŸ¥è¯¢æ¦‚ç‡:

```py
>>> from torch import nn

>>> pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
>>> print(pt_predictions)
tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],
        [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=<SoftmaxBackward0>)
```

<Tip>

æ‰€æœ‰ ğŸ¤— Transformers æ¨¡å‹ï¼ˆPyTorch æˆ– TensorFlowï¼‰åœ¨æœ€ç»ˆçš„æ¿€æ´»å‡½æ•°ï¼ˆæ¯”å¦‚ softmaxï¼‰*ä¹‹å‰* è¾“å‡ºå¼ é‡ï¼Œ
å› ä¸ºæœ€ç»ˆçš„æ¿€æ´»å‡½æ•°å¸¸å¸¸ä¸ loss èåˆã€‚æ¨¡å‹çš„è¾“å‡ºæ˜¯ç‰¹æ®Šçš„æ•°æ®ç±»ï¼Œæ‰€ä»¥å®ƒä»¬çš„å±æ€§å¯ä»¥åœ¨ IDE ä¸­è¢«è‡ªåŠ¨è¡¥å…¨ã€‚æ¨¡å‹çš„è¾“å‡ºå°±åƒä¸€ä¸ªå…ƒç»„æˆ–å­—å…¸ï¼ˆä½ å¯ä»¥é€šè¿‡æ•´æ•°ã€åˆ‡ç‰‡æˆ–å­—ç¬¦ä¸²æ¥ç´¢å¼•å®ƒï¼‰ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¸º None çš„å±æ€§ä¼šè¢«å¿½ç•¥ã€‚

</Tip>

### ä¿å­˜æ¨¡å‹

å½“ä½ çš„æ¨¡å‹å¾®è°ƒå®Œæˆï¼Œä½ å°±å¯ä»¥ä½¿ç”¨ [`PreTrainedModel.save_pretrained`] æŠŠå®ƒå’Œå®ƒçš„åˆ†è¯å™¨ä¿å­˜ä¸‹æ¥ï¼š

```py
>>> pt_save_directory = "./pt_save_pretrained"
>>> tokenizer.save_pretrained(pt_save_directory)  # doctest: +IGNORE_RESULT
>>> pt_model.save_pretrained(pt_save_directory)
```

å½“ä½ å‡†å¤‡å†æ¬¡ä½¿ç”¨è¿™ä¸ªæ¨¡å‹æ—¶ï¼Œå°±å¯ä»¥ä½¿ç”¨ [`PreTrainedModel.from_pretrained`] åŠ è½½å®ƒäº†ï¼š

```py
>>> pt_model = AutoModelForSequenceClassification.from_pretrained("./pt_save_pretrained")
```

## è‡ªå®šä¹‰æ¨¡å‹æ„å»º

ä½ å¯ä»¥ä¿®æ”¹æ¨¡å‹çš„é…ç½®ç±»æ¥æ”¹å˜æ¨¡å‹çš„æ„å»ºæ–¹å¼ã€‚é…ç½®æŒ‡æ˜äº†æ¨¡å‹çš„å±æ€§ï¼Œæ¯”å¦‚éšè—å±‚æˆ–è€…æ³¨æ„åŠ›å¤´çš„æ•°é‡ã€‚å½“ä½ ä»è‡ªå®šä¹‰çš„é…ç½®ç±»åˆå§‹åŒ–æ¨¡å‹æ—¶ï¼Œä½ å°±å¼€å§‹è‡ªå®šä¹‰æ¨¡å‹æ„å»ºäº†ã€‚æ¨¡å‹å±æ€§æ˜¯éšæœºåˆå§‹åŒ–çš„ï¼Œä½ éœ€è¦å…ˆè®­ç»ƒæ¨¡å‹ï¼Œç„¶åæ‰èƒ½å¾—åˆ°æœ‰æ„ä¹‰çš„ç»“æœã€‚

é€šè¿‡å¯¼å…¥ [`AutoConfig`] æ¥å¼€å§‹ï¼Œä¹‹ååŠ è½½ä½ æƒ³ä¿®æ”¹çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚åœ¨ [`AutoConfig.from_pretrained`] ä¸­ï¼Œä½ èƒ½å¤ŸæŒ‡å®šæƒ³è¦ä¿®æ”¹çš„å±æ€§ï¼Œæ¯”å¦‚æ³¨æ„åŠ›å¤´çš„æ•°é‡ï¼š

```py
>>> from transformers import AutoConfig

>>> my_config = AutoConfig.from_pretrained("distilbert/distilbert-base-uncased", n_heads=12)
```

ä½¿ç”¨ [`AutoModel.from_config`] æ ¹æ®ä½ çš„è‡ªå®šä¹‰é…ç½®åˆ›å»ºä¸€ä¸ªæ¨¡å‹ï¼š

```py
>>> from transformers import AutoModel

>>> my_model = AutoModel.from_config(my_config)
```

æŸ¥é˜… [åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰ç»“æ„](./create_a_model) æŒ‡å—è·å–æ›´å¤šå…³äºæ„å»ºè‡ªå®šä¹‰é…ç½®çš„ä¿¡æ¯ã€‚

## Trainer - PyTorch ä¼˜åŒ–è®­ç»ƒå¾ªç¯

æ‰€æœ‰çš„æ¨¡å‹éƒ½æ˜¯æ ‡å‡†çš„ [`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)ï¼Œæ‰€ä»¥ä½ å¯ä»¥åœ¨ä»»ä½•å…¸å‹çš„è®­ç»ƒæ¨¡å‹ä¸­ä½¿ç”¨å®ƒä»¬ã€‚å½“ä½ ç¼–å†™è‡ªå·±çš„è®­ç»ƒå¾ªç¯æ—¶ï¼ŒğŸ¤— Transformers ä¸º PyTorch æä¾›äº†ä¸€ä¸ª [`Trainer`] ç±»ï¼Œå®ƒåŒ…å«äº†åŸºç¡€çš„è®­ç»ƒå¾ªç¯å¹¶ä¸”ä¸ºè¯¸å¦‚åˆ†å¸ƒå¼è®­ç»ƒï¼Œæ··åˆç²¾åº¦ç­‰ç‰¹æ€§å¢åŠ äº†é¢å¤–çš„åŠŸèƒ½ã€‚

å–å†³äºä½ çš„ä»»åŠ¡, ä½ é€šå¸¸å¯ä»¥ä¼ é€’ä»¥ä¸‹çš„å‚æ•°ç»™ [`Trainer`]ï¼š

1. [`PreTrainedModel`] æˆ–è€… [`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)ï¼š

   ```py
   >>> from transformers import AutoModelForSequenceClassification

   >>> model = AutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
   ```

2. [`TrainingArguments`] å«æœ‰ä½ å¯ä»¥ä¿®æ”¹çš„æ¨¡å‹è¶…å‚æ•°ï¼Œæ¯”å¦‚å­¦ä¹ ç‡ï¼Œæ‰¹æ¬¡å¤§å°å’Œè®­ç»ƒæ—¶çš„è¿­ä»£æ¬¡æ•°ã€‚å¦‚æœä½ æ²¡æœ‰æŒ‡å®šè®­ç»ƒå‚æ•°ï¼Œé‚£ä¹ˆå®ƒä¼šä½¿ç”¨é»˜è®¤å€¼ï¼š

   ```py
   >>> from transformers import TrainingArguments

   >>> training_args = TrainingArguments(
   ...     output_dir="path/to/save/folder/",
   ...     learning_rate=2e-5,
   ...     per_device_train_batch_size=8,
   ...     per_device_eval_batch_size=8,
   ...     num_train_epochs=2,
   ... )
   ```

3. ä¸€ä¸ªé¢„å¤„ç†ç±»ï¼Œæ¯”å¦‚åˆ†è¯å™¨ï¼Œç‰¹å¾æå–å™¨æˆ–è€…å¤„ç†å™¨ï¼š

   ```py
   >>> from transformers import AutoTokenizer

   >>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
   ```

4. åŠ è½½ä¸€ä¸ªæ•°æ®é›†ï¼š

   ```py
   >>> from datasets import load_dataset

   >>> dataset = load_dataset("rotten_tomatoes")  # doctest: +IGNORE_RESULT
   ```

5. åˆ›å»ºä¸€ä¸ªç»™æ•°æ®é›†åˆ†è¯çš„å‡½æ•°ï¼Œå¹¶ä¸”ä½¿ç”¨ [`~datasets.Dataset.map`] åº”ç”¨åˆ°æ•´ä¸ªæ•°æ®é›†ï¼š

   ```py
   >>> def tokenize_dataset(dataset):
   ...     return tokenizer(dataset["text"])

   >>> dataset = dataset.map(tokenize_dataset, batched=True)
   ```

6. ç”¨æ¥ä»æ•°æ®é›†ä¸­åˆ›å»ºæ‰¹æ¬¡çš„ [`DataCollatorWithPadding`]ï¼š

   ```py
   >>> from transformers import DataCollatorWithPadding

   >>> data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
   ```

ç°åœ¨æŠŠæ‰€æœ‰çš„ç±»ä¼ ç»™ [`Trainer`]ï¼š

```py
>>> from transformers import Trainer

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=dataset["train"],
...     eval_dataset=dataset["test"],
...     processing_class=tokenizer,
...     data_collator=data_collator,
... )  # doctest: +SKIP
```

ä¸€åˆ‡å‡†å¤‡å°±ç»ªåï¼Œè°ƒç”¨ [`~Trainer.train`] è¿›è¡Œè®­ç»ƒï¼š

```py
>>> trainer.train()  # doctest: +SKIP
```

<Tip>

å¯¹äºåƒç¿»è¯‘æˆ–æ‘˜è¦è¿™äº›ä½¿ç”¨åºåˆ—åˆ°åºåˆ—æ¨¡å‹çš„ä»»åŠ¡ï¼Œç”¨ [`Seq2SeqTrainer`] å’Œ [`Seq2SeqTrainingArguments`] æ¥æ›¿ä»£ã€‚

</Tip>

ä½ å¯ä»¥é€šè¿‡å­ç±»åŒ– [`Trainer`] ä¸­çš„æ–¹æ³•æ¥è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ã€‚è¿™æ ·ä½ å°±å¯ä»¥è‡ªå®šä¹‰åƒæŸå¤±å‡½æ•°ï¼Œä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨è¿™æ ·çš„ç‰¹æ€§ã€‚æŸ¥é˜… [`Trainer`] å‚è€ƒæ‰‹å†Œäº†è§£å“ªäº›æ–¹æ³•èƒ½å¤Ÿè¢«å­ç±»åŒ–ã€‚

å¦ä¸€ä¸ªè‡ªå®šä¹‰è®­ç»ƒå¾ªç¯çš„æ–¹å¼æ˜¯é€šè¿‡[å›è°ƒ](./main_classes/callback)ã€‚ä½ å¯ä»¥ä½¿ç”¨å›è°ƒæ¥ä¸å…¶ä»–åº“é›†æˆï¼ŒæŸ¥çœ‹è®­ç»ƒå¾ªç¯æ¥æŠ¥å‘Šè¿›åº¦æˆ–æå‰ç»“æŸè®­ç»ƒã€‚å›è°ƒä¸ä¼šä¿®æ”¹è®­ç»ƒå¾ªç¯ã€‚å¦‚æœæƒ³è‡ªå®šä¹‰æŸå¤±å‡½æ•°ç­‰ï¼Œå°±éœ€è¦å­ç±»åŒ– [`Trainer`] äº†ã€‚

## ä½¿ç”¨ Tensorflow è®­ç»ƒ

æ‰€æœ‰æ¨¡å‹éƒ½æ˜¯æ ‡å‡†çš„ [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model)ï¼Œæ‰€ä»¥ä½ å¯ä»¥é€šè¿‡ [Keras](https://keras.io/) API å®ç°åœ¨ Tensorflow ä¸­è®­ç»ƒã€‚ğŸ¤— Transformers æä¾›äº† [`~TFPreTrainedModel.prepare_tf_dataset`] æ–¹æ³•æ¥è½»æ¾åœ°å°†æ•°æ®é›†åŠ è½½ä¸º `tf.data.Dataset`ï¼Œè¿™æ ·ä½ å°±å¯ä»¥ä½¿ç”¨ Keras çš„ [`compile`](https://keras.io/api/models/model_training_apis/#compile-method) å’Œ [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) æ–¹æ³•é©¬ä¸Šå¼€å§‹è®­ç»ƒã€‚

1. ä½¿ç”¨ [`TFPreTrainedModel`] æˆ–è€… [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) æ¥å¼€å§‹ï¼š

   ```py
   >>> from transformers import TFAutoModelForSequenceClassification

   >>> model = TFAutoModelForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")
   ```

2. ä¸€ä¸ªé¢„å¤„ç†ç±»ï¼Œæ¯”å¦‚åˆ†è¯å™¨ï¼Œç‰¹å¾æå–å™¨æˆ–è€…å¤„ç†å™¨ï¼š

   ```py
   >>> from transformers import AutoTokenizer

   >>> tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
   ```

3. åˆ›å»ºä¸€ä¸ªç»™æ•°æ®é›†åˆ†è¯çš„å‡½æ•°

   ```py
   >>> def tokenize_dataset(dataset):
   ...     return tokenizer(dataset["text"])  # doctest: +SKIP
   ```

4. ä½¿ç”¨ [`~datasets.Dataset.map`] å°†åˆ†è¯å™¨åº”ç”¨åˆ°æ•´ä¸ªæ•°æ®é›†ï¼Œä¹‹åå°†æ•°æ®é›†å’Œåˆ†è¯å™¨ä¼ ç»™ [`~TFPreTrainedModel.prepare_tf_dataset`]ã€‚å¦‚æœä½ éœ€è¦çš„è¯ï¼Œä¹Ÿå¯ä»¥åœ¨è¿™é‡Œæ”¹å˜æ‰¹æ¬¡å¤§å°å’Œæ˜¯å¦æ‰“ä¹±æ•°æ®é›†ï¼š

   ```py
   >>> dataset = dataset.map(tokenize_dataset)  # doctest: +SKIP
   >>> tf_dataset = model.prepare_tf_dataset(
   ...     dataset, batch_size=16, shuffle=True, tokenizer=tokenizer
   ... )  # doctest: +SKIP
   ```

5. ä¸€åˆ‡å‡†å¤‡å°±ç»ªåï¼Œè°ƒç”¨ `compile` å’Œ `fit` å¼€å§‹è®­ç»ƒï¼š

   ```py
   >>> from tensorflow.keras.optimizers import Adam

   >>> model.compile(optimizer=Adam(3e-5))
   >>> model.fit(dataset)  # doctest: +SKIP
   ```

## æ¥ä¸‹æ¥åšä»€ä¹ˆ?

ç°åœ¨ä½ å·²ç»å®Œæˆäº† ğŸ¤— Transformers çš„å¿«é€Ÿä¸Šæ‰‹æ•™ç¨‹ï¼Œæ¥çœ‹çœ‹æˆ‘ä»¬çš„æŒ‡å—å¹¶ä¸”å­¦ä¹ å¦‚ä½•åšä¸€äº›æ›´å…·ä½“çš„äº‹æƒ…ï¼Œæ¯”å¦‚å†™ä¸€ä¸ªè‡ªå®šä¹‰æ¨¡å‹ï¼Œä¸ºæŸä¸ªä»»åŠ¡å¾®è°ƒä¸€ä¸ªæ¨¡å‹ä»¥åŠå¦‚ä½•ä½¿ç”¨è„šæœ¬æ¥è®­ç»ƒæ¨¡å‹ã€‚å¦‚æœä½ æœ‰å…´è¶£äº†è§£æ›´å¤š ğŸ¤— Transformers çš„æ ¸å¿ƒç« èŠ‚ï¼Œé‚£å°±å–æ¯å’–å•¡ç„¶åæ¥çœ‹çœ‹æˆ‘ä»¬çš„æ¦‚å¿µæŒ‡å—å§ï¼

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\run_scripts.md
============================================================



# ä½¿ç”¨è„šæœ¬è¿›è¡Œè®­ç»ƒ

é™¤äº† ğŸ¤— Transformers [notebooks](./notebooks)ï¼Œè¿˜æœ‰ç¤ºä¾‹è„šæœ¬æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨[PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch)è®­ç»ƒæ¨¡å‹ä»¥è§£å†³ç‰¹å®šä»»åŠ¡ã€‚

æ‚¨è¿˜å¯ä»¥åœ¨è¿™äº›ç¤ºä¾‹ä¸­æ‰¾åˆ°æˆ‘ä»¬åœ¨[ç ”ç©¶é¡¹ç›®](https://github.com/huggingface/transformers-research-projects/)å’Œ[é—ç•™ç¤ºä¾‹](https://github.com/huggingface/transformers/tree/main/examples/legacy)ä¸­ä½¿ç”¨è¿‡çš„è„šæœ¬ï¼Œè¿™äº›è„šæœ¬ä¸»è¦æ˜¯ç”±ç¤¾åŒºè´¡çŒ®çš„ã€‚è¿™äº›è„šæœ¬å·²ä¸å†è¢«ç§¯æç»´æŠ¤ï¼Œéœ€è¦ä½¿ç”¨ç‰¹å®šç‰ˆæœ¬çš„ğŸ¤— Transformersï¼Œ å¯èƒ½ä¸åº“çš„æœ€æ–°ç‰ˆæœ¬ä¸å…¼å®¹ã€‚

ç¤ºä¾‹è„šæœ¬å¯èƒ½æ— æ³•åœ¨åˆå§‹é…ç½®ä¸‹ç›´æ¥è§£å†³æ¯ä¸ªé—®é¢˜ï¼Œæ‚¨å¯èƒ½éœ€è¦æ ¹æ®è¦è§£å†³çš„é—®é¢˜è°ƒæ•´è„šæœ¬ã€‚ä¸ºäº†å¸®åŠ©æ‚¨ï¼Œå¤§å¤šæ•°è„šæœ¬éƒ½å®Œå…¨æš´éœ²äº†æ•°æ®é¢„å¤„ç†çš„æ–¹å¼ï¼Œå…è®¸æ‚¨æ ¹æ®éœ€è¦å¯¹å…¶è¿›è¡Œç¼–è¾‘ã€‚

å¦‚æœæ‚¨æƒ³åœ¨ç¤ºä¾‹è„šæœ¬ä¸­å®ç°ä»»ä½•åŠŸèƒ½ï¼Œè¯·åœ¨[è®ºå›](https://discuss.huggingface.co/)æˆ–[issue](https://github.com/huggingface/transformers/issues)ä¸Šè®¨è®ºï¼Œç„¶åå†æäº¤Pull Requestã€‚è™½ç„¶æˆ‘ä»¬æ¬¢è¿ä¿®å¤é”™è¯¯ï¼Œä½†ä¸å¤ªå¯èƒ½åˆå¹¶æ·»åŠ æ›´å¤šåŠŸèƒ½çš„Pull Requestï¼Œå› ä¸ºè¿™ä¼šé™ä½å¯è¯»æ€§ã€‚

æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•åœ¨[PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization)ä¸­è¿è¡Œç¤ºä¾‹æ‘˜è¦è®­ç»ƒè„šæœ¬ã€‚

## è®¾ç½®

è¦æˆåŠŸè¿è¡Œç¤ºä¾‹è„šæœ¬çš„æœ€æ–°ç‰ˆæœ¬ï¼Œæ‚¨å¿…é¡»åœ¨æ–°è™šæ‹Ÿç¯å¢ƒä¸­**ä»æºä»£ç å®‰è£… ğŸ¤— Transformers**ï¼š

```bash
git clone https://github.com/huggingface/transformers
cd transformers
pip install .
```

å¯¹äºæ—§ç‰ˆæœ¬çš„ç¤ºä¾‹è„šæœ¬ï¼Œè¯·ç‚¹å‡»ä¸‹é¢çš„åˆ‡æ¢æŒ‰é’®ï¼š

<details>
  <summary>è€ç‰ˆæœ¬ğŸ¤— Transformersç¤ºä¾‹ </summary>
	<ul>
		<li><a href="https://github.com/huggingface/transformers/tree/v4.5.1/examples">v4.5.1</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v4.4.2/examples">v4.4.2</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v4.3.3/examples">v4.3.3</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v4.2.2/examples">v4.2.2</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v4.1.1/examples">v4.1.1</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v4.0.1/examples">v4.0.1</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v3.5.1/examples">v3.5.1</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v3.4.0/examples">v3.4.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v3.3.1/examples">v3.3.1</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v3.2.0/examples">v3.2.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v3.1.0/examples">v3.1.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v3.0.2/examples">v3.0.2</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.11.0/examples">v2.11.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.10.0/examples">v2.10.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.9.1/examples">v2.9.1</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.8.0/examples">v2.8.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.7.0/examples">v2.7.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.6.0/examples">v2.6.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.5.1/examples">v2.5.1</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.4.0/examples">v2.4.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.3.0/examples">v2.3.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.2.0/examples">v2.2.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.1.0/examples">v2.1.1</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v2.0.0/examples">v2.0.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v1.2.0/examples">v1.2.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v1.1.0/examples">v1.1.0</a></li>
		<li><a href="https://github.com/huggingface/transformers/tree/v1.0.0/examples">v1.0.0</a></li>
	</ul>
</details>

ç„¶ååˆ‡æ¢æ‚¨cloneçš„ ğŸ¤— Transformers ä»“åˆ°ç‰¹å®šçš„ç‰ˆæœ¬ï¼Œä¾‹å¦‚v3.5.1ï¼š

```bash
git checkout tags/v3.5.1
```

åœ¨å®‰è£…äº†æ­£ç¡®çš„åº“ç‰ˆæœ¬åï¼Œè¿›å…¥æ‚¨é€‰æ‹©çš„ç‰ˆæœ¬çš„`example`æ–‡ä»¶å¤¹å¹¶å®‰è£…ä¾‹å­è¦æ±‚çš„ç¯å¢ƒï¼š

```bash
pip install -r requirements.txt
```

## è¿è¡Œè„šæœ¬


ç¤ºä¾‹è„šæœ¬ä»ğŸ¤— [Datasets](https://huggingface.co/docs/datasets/)åº“ä¸‹è½½å¹¶é¢„å¤„ç†æ•°æ®é›†ã€‚ç„¶åï¼Œè„šæœ¬é€šè¿‡[Trainer](https://huggingface.co/docs/transformers/main_classes/trainer)ä½¿ç”¨æ”¯æŒæ‘˜è¦ä»»åŠ¡çš„æ¶æ„å¯¹æ•°æ®é›†è¿›è¡Œå¾®è°ƒã€‚ä»¥ä¸‹ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•åœ¨[CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail)æ•°æ®é›†ä¸Šå¾®è°ƒ[T5-small](https://huggingface.co/google-t5/t5-small)ã€‚ç”±äºT5æ¨¡å‹çš„è®­ç»ƒæ–¹å¼ï¼Œå®ƒéœ€è¦ä¸€ä¸ªé¢å¤–çš„`source_prefix`å‚æ•°ã€‚è¿™ä¸ªæç¤ºè®©T5çŸ¥é“è¿™æ˜¯ä¸€ä¸ªæ‘˜è¦ä»»åŠ¡ã€‚

```bash
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --predict_with_generate
```

## åˆ†å¸ƒå¼è®­ç»ƒå’Œæ··åˆç²¾åº¦

[Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒå’Œæ··åˆç²¾åº¦ï¼Œè¿™æ„å‘³ç€ä½ ä¹Ÿå¯ä»¥åœ¨è„šæœ¬ä¸­ä½¿ç”¨å®ƒã€‚è¦å¯ç”¨è¿™ä¸¤ä¸ªåŠŸèƒ½ï¼Œå¯ä»¥åšå¦‚ä¸‹è®¾ç½®ï¼š

- æ·»åŠ  `fp16` å‚æ•°ä»¥å¯ç”¨æ··åˆç²¾åº¦ã€‚
- ä½¿ç”¨ `nproc_per_node` å‚æ•°è®¾ç½®ä½¿ç”¨çš„GPUæ•°é‡ã€‚


```bash
torchrun \
    --nproc_per_node 8 pytorch/summarization/run_summarization.py \
    --fp16 \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --predict_with_generate
```

## åœ¨TPUä¸Šè¿è¡Œè„šæœ¬


å¼ é‡å¤„ç†å•å…ƒï¼ˆTPUsï¼‰æ˜¯ä¸“é—¨è®¾è®¡ç”¨äºåŠ é€Ÿæ€§èƒ½çš„ã€‚PyTorchä½¿ç”¨ [PyTorch/XLA](https://github.com/pytorch/xla/blob/master/README.md) æ”¯æŒTPUã€‚è¦ä½¿ç”¨TPUï¼Œè¯·å¯åŠ¨`xla_spawn.py`è„šæœ¬å¹¶ä½¿ç”¨`num_cores`å‚æ•°è®¾ç½®è¦ä½¿ç”¨çš„TPUæ ¸å¿ƒæ•°é‡ã€‚

```bash
python xla_spawn.py --num_cores 8 \
    summarization/run_summarization.py \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --predict_with_generate
```

## åŸºäºğŸ¤— Accelerateè¿è¡Œè„šæœ¬

ğŸ¤— [Accelerate](https://huggingface.co/docs/accelerate) æ˜¯ä¸€ä¸ªä»…æ”¯æŒ PyTorch çš„åº“ï¼Œå®ƒæä¾›äº†ä¸€ç§ç»Ÿä¸€çš„æ–¹æ³•æ¥åœ¨ä¸åŒç±»å‹çš„è®¾ç½®ï¼ˆä»… CPUã€å¤šä¸ª GPUã€å¤šä¸ªTPUï¼‰ä¸Šè®­ç»ƒæ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒå¯¹ PyTorch è®­ç»ƒå¾ªç¯çš„å®Œå…¨å¯è§æ€§ã€‚å¦‚æœä½ è¿˜æ²¡æœ‰å®‰è£… ğŸ¤— Accelerateï¼Œè¯·ç¡®ä¿ä½ å·²ç»å®‰è£…äº†å®ƒï¼š

> æ³¨æ„ï¼šç”±äº Accelerate æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œå› æ­¤å¿…é¡»å®‰è£… git ç‰ˆæœ¬çš„ accelerate æ¥è¿è¡Œè„šæœ¬ã€‚

```bash
pip install git+https://github.com/huggingface/accelerate
```

ä½ éœ€è¦ä½¿ç”¨`run_summarization_no_trainer.py`è„šæœ¬ï¼Œè€Œä¸æ˜¯`run_summarization.py`è„šæœ¬ã€‚ğŸ¤— Accelerateæ”¯æŒçš„è„šæœ¬éœ€è¦åœ¨æ–‡ä»¶å¤¹ä¸­æœ‰ä¸€ä¸ª`task_no_trainer.py`æ–‡ä»¶ã€‚é¦–å…ˆè¿è¡Œä»¥ä¸‹å‘½ä»¤ä»¥åˆ›å»ºå¹¶ä¿å­˜é…ç½®æ–‡ä»¶ï¼š

```bash
accelerate config
```
æ£€æµ‹æ‚¨çš„è®¾ç½®ä»¥ç¡®ä¿é…ç½®æ­£ç¡®ï¼š

```bash
accelerate test
```

ç°åœ¨æ‚¨å¯ä»¥å¼€å§‹è®­ç»ƒæ¨¡å‹äº†ï¼š

```bash
accelerate launch run_summarization_no_trainer.py \
    --model_name_or_path google-t5/t5-small \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir ~/tmp/tst-summarization
```

## ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†

æ‘˜è¦è„šæœ¬æ”¯æŒè‡ªå®šä¹‰æ•°æ®é›†ï¼Œåªè¦å®ƒä»¬æ˜¯CSVæˆ–JSON Lineæ–‡ä»¶ã€‚å½“ä½ ä½¿ç”¨è‡ªå·±çš„æ•°æ®é›†æ—¶ï¼Œéœ€è¦æŒ‡å®šä¸€äº›é¢å¤–çš„å‚æ•°ï¼š
- `train_file` å’Œ `validation_file` åˆ†åˆ«æŒ‡å®šæ‚¨çš„è®­ç»ƒå’ŒéªŒè¯æ–‡ä»¶çš„è·¯å¾„ã€‚
- `text_column` æ˜¯è¾“å…¥è¦è¿›è¡Œæ‘˜è¦çš„æ–‡æœ¬ã€‚
- `summary_column` æ˜¯ç›®æ ‡è¾“å‡ºçš„æ–‡æœ¬ã€‚

ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†çš„æ‘˜è¦è„šæœ¬çœ‹èµ·æ¥æ˜¯è¿™æ ·çš„ï¼š


```bash
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --train_file path_to_csv_or_jsonlines_file \
    --validation_file path_to_csv_or_jsonlines_file \
    --text_column text_column_name \
    --summary_column summary_column_name \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --predict_with_generate
```

## æµ‹è¯•è„šæœ¬

é€šå¸¸ï¼Œåœ¨æäº¤æ•´ä¸ªæ•°æ®é›†ä¹‹å‰ï¼Œæœ€å¥½å…ˆåœ¨è¾ƒå°‘çš„æ•°æ®é›†ç¤ºä¾‹ä¸Šè¿è¡Œè„šæœ¬ï¼Œä»¥ç¡®ä¿ä¸€åˆ‡æŒ‰é¢„æœŸå·¥ä½œ,å› ä¸ºå®Œæ•´æ•°æ®é›†çš„å¤„ç†å¯èƒ½éœ€è¦èŠ±è´¹å‡ ä¸ªå°æ—¶çš„æ—¶é—´ã€‚ä½¿ç”¨ä»¥ä¸‹å‚æ•°å°†æ•°æ®é›†æˆªæ–­ä¸ºæœ€å¤§æ ·æœ¬æ•°ï¼š

- `max_train_samples`
- `max_eval_samples`
- `max_predict_samples`


```bash
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path google-t5/t5-small \
    --max_train_samples 50 \
    --max_eval_samples 50 \
    --max_predict_samples 50 \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --predict_with_generate
```

å¹¶éæ‰€æœ‰ç¤ºä¾‹è„šæœ¬éƒ½æ”¯æŒ`max_predict_samples`å‚æ•°ã€‚å¦‚æœæ‚¨ä¸ç¡®å®šæ‚¨çš„è„šæœ¬æ˜¯å¦æ”¯æŒæ­¤å‚æ•°ï¼Œè¯·æ·»åŠ `-h`å‚æ•°è¿›è¡Œæ£€æŸ¥ï¼š

```bash
examples/pytorch/summarization/run_summarization.py -h
```

## ä»checkpointæ¢å¤è®­ç»ƒ

å¦ä¸€ä¸ªæœ‰ç”¨çš„é€‰é¡¹æ˜¯ä»ä¹‹å‰çš„checkpointæ¢å¤è®­ç»ƒã€‚è¿™å°†ç¡®ä¿åœ¨è®­ç»ƒä¸­æ–­æ—¶ï¼Œæ‚¨å¯ä»¥ä»ä¹‹å‰åœæ­¢çš„åœ°æ–¹ç»§ç»­è¿›è¡Œï¼Œè€Œæ— éœ€é‡æ–°å¼€å§‹ã€‚æœ‰ä¸¤ç§æ–¹æ³•å¯ä»¥ä»checkpointæ¢å¤è®­ç»ƒã€‚

```bash
python examples/pytorch/summarization/run_summarization.py
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --resume_from_checkpoint path_to_specific_checkpoint \
    --predict_with_generate
```

## åˆ†äº«æ¨¡å‹

æ‰€æœ‰è„šæœ¬éƒ½å¯ä»¥å°†æ‚¨çš„æœ€ç»ˆæ¨¡å‹ä¸Šä¼ åˆ°[Model Hub](https://huggingface.co/models)ã€‚åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨å·²ç™»å½•Hugging Faceï¼š

```bash
hf auth login
```

ç„¶åï¼Œåœ¨è„šæœ¬ä¸­æ·»åŠ `push_to_hub`å‚æ•°ã€‚è¿™ä¸ªå‚æ•°ä¼šåˆ›å»ºä¸€ä¸ªå¸¦æœ‰æ‚¨Hugging Faceç”¨æˆ·åå’Œ`output_dir`ä¸­æŒ‡å®šçš„æ–‡ä»¶å¤¹åç§°çš„ä»“åº“ã€‚

ä¸ºäº†ç»™æ‚¨çš„ä»“åº“æŒ‡å®šä¸€ä¸ªç‰¹å®šçš„åç§°ï¼Œä½¿ç”¨`push_to_hub_model_id`å‚æ•°æ¥æ·»åŠ å®ƒã€‚è¯¥ä»“åº“å°†è‡ªåŠ¨åˆ—å‡ºåœ¨æ‚¨çš„å‘½åç©ºé—´ä¸‹ã€‚

ä»¥ä¸‹ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä¸Šä¼ å…·æœ‰ç‰¹å®šä»“åº“åç§°çš„æ¨¡å‹ï¼š


```bash
python examples/pytorch/summarization/run_summarization.py
    --model_name_or_path google-t5/t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config "3.0.0" \
    --source_prefix "summarize: " \
    --push_to_hub \
    --push_to_hub_model_id finetuned-t5-cnn_dailymail \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --predict_with_generate
```

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\serialization.md
============================================================



# å¯¼å‡ºä¸º ONNX

åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½² ğŸ¤— Transformers æ¨¡å‹é€šå¸¸éœ€è¦æˆ–è€…èƒ½å¤Ÿå—ç›Šäºï¼Œå°†æ¨¡å‹å¯¼å‡ºä¸ºå¯åœ¨ä¸“é—¨çš„è¿è¡Œæ—¶å’Œç¡¬ä»¶ä¸ŠåŠ è½½å’Œæ‰§è¡Œçš„åºåˆ—åŒ–æ ¼å¼ã€‚

ğŸ¤— Optimum æ˜¯ Transformers çš„æ‰©å±•ï¼Œå¯ä»¥é€šè¿‡å…¶ `exporters` æ¨¡å—å°†æ¨¡å‹ä» PyTorch æˆ– TensorFlow å¯¼å‡ºä¸º ONNX åŠ TFLite ç­‰åºåˆ—åŒ–æ ¼å¼ã€‚ğŸ¤— Optimum è¿˜æä¾›äº†ä¸€å¥—æ€§èƒ½ä¼˜åŒ–å·¥å…·ï¼Œå¯ä»¥åœ¨ç›®æ ‡ç¡¬ä»¶ä¸Šä»¥æœ€é«˜æ•ˆç‡è®­ç»ƒå’Œè¿è¡Œæ¨¡å‹ã€‚

æœ¬æŒ‡å—æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨ ğŸ¤— Optimum å°† ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºä¸º ONNXã€‚æœ‰å…³å°†æ¨¡å‹å¯¼å‡ºä¸º TFLite çš„æŒ‡å—ï¼Œè¯·å‚è€ƒ [å¯¼å‡ºä¸º TFLite é¡µé¢](tflite)ã€‚

## å¯¼å‡ºä¸º ONNX

[ONNX (Open Neural Network eXchange å¼€æ”¾ç¥ç»ç½‘ç»œäº¤æ¢)](http://onnx.ai) æ˜¯ä¸€ä¸ªå¼€æ”¾çš„æ ‡å‡†ï¼Œå®ƒå®šä¹‰äº†ä¸€ç»„é€šç”¨çš„è¿ç®—ç¬¦å’Œä¸€ç§é€šç”¨çš„æ–‡ä»¶æ ¼å¼ï¼Œç”¨äºè¡¨ç¤ºåŒ…æ‹¬ PyTorch å’Œ TensorFlow åœ¨å†…çš„å„ç§æ¡†æ¶ä¸­çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚å½“ä¸€ä¸ªæ¨¡å‹è¢«å¯¼å‡ºä¸º ONNXæ—¶ï¼Œè¿™äº›è¿ç®—ç¬¦è¢«ç”¨äºæ„å»ºè®¡ç®—å›¾ï¼ˆé€šå¸¸è¢«ç§°ä¸º*ä¸­é—´è¡¨ç¤º*ï¼‰ï¼Œè¯¥å›¾è¡¨ç¤ºæ•°æ®åœ¨ç¥ç»ç½‘ç»œä¸­çš„æµåŠ¨ã€‚

é€šè¿‡å…¬å¼€å…·æœ‰æ ‡å‡†åŒ–è¿ç®—ç¬¦å’Œæ•°æ®ç±»å‹çš„å›¾ï¼ŒONNXä½¿å¾—æ¨¡å‹èƒ½å¤Ÿè½»æ¾åœ¨ä¸åŒæ·±åº¦å­¦ä¹ æ¡†æ¶é—´åˆ‡æ¢ã€‚ä¾‹å¦‚ï¼Œåœ¨ PyTorch ä¸­è®­ç»ƒçš„æ¨¡å‹å¯ä»¥è¢«å¯¼å‡ºä¸º ONNXï¼Œç„¶åå†å¯¼å…¥åˆ° TensorFlowï¼ˆåä¹‹äº¦ç„¶ï¼‰ã€‚

å¯¼å‡ºä¸º ONNX åï¼Œæ¨¡å‹å¯ä»¥ï¼š
- é€šè¿‡ [å›¾ä¼˜åŒ–ï¼ˆgraph optimizationï¼‰](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/optimization) å’Œ [é‡åŒ–ï¼ˆquantizationï¼‰](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/quantization) ç­‰æŠ€æœ¯è¿›è¡Œæ¨ç†ä¼˜åŒ–ã€‚ 
- é€šè¿‡ [`ORTModelForXXX` ç±»](https://huggingface.co/docs/optimum/onnxruntime/package_reference/modeling_ort) ä½¿ç”¨ ONNX Runtime è¿è¡Œï¼Œå®ƒåŒæ ·éµå¾ªä½ ç†Ÿæ‚‰çš„ Transformers ä¸­çš„ `AutoModel` APIã€‚
- ä½¿ç”¨ [ä¼˜åŒ–æ¨ç†æµæ°´çº¿ï¼ˆpipelineï¼‰](https://huggingface.co/docs/optimum/main/en/onnxruntime/usage_guides/pipelines) è¿è¡Œï¼Œå…¶ API ä¸ ğŸ¤— Transformers ä¸­çš„ [`pipeline`] å‡½æ•°ç›¸åŒã€‚

ğŸ¤— Optimum é€šè¿‡åˆ©ç”¨é…ç½®å¯¹è±¡æä¾›å¯¹ ONNX å¯¼å‡ºçš„æ”¯æŒã€‚å¤šç§æ¨¡å‹æ¶æ„å·²ç»æœ‰ç°æˆçš„é…ç½®å¯¹è±¡ï¼Œå¹¶ä¸”é…ç½®å¯¹è±¡ä¹Ÿè¢«è®¾è®¡å¾—æ˜“äºæ‰©å±•ä»¥é€‚ç”¨äºå…¶ä»–æ¶æ„ã€‚

ç°æœ‰çš„é…ç½®åˆ—è¡¨è¯·å‚è€ƒ [ğŸ¤— Optimum æ–‡æ¡£](https://huggingface.co/docs/optimum/exporters/onnx/overview)ã€‚

æœ‰ä¸¤ç§æ–¹å¼å¯ä»¥å°† ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºä¸º ONNXï¼Œè¿™é‡Œæˆ‘ä»¬å±•ç¤ºè¿™ä¸¤ç§æ–¹æ³•ï¼š

- ä½¿ç”¨ ğŸ¤— Optimum çš„ CLIï¼ˆå‘½ä»¤è¡Œï¼‰å¯¼å‡ºã€‚
- ä½¿ç”¨ ğŸ¤— Optimum çš„ `optimum.onnxruntime` æ¨¡å—å¯¼å‡ºã€‚

### ä½¿ç”¨ CLI å°† ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºä¸º ONNX

è¦å°† ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºä¸º ONNXï¼Œé¦–å…ˆéœ€è¦å®‰è£…é¢å¤–çš„ä¾èµ–é¡¹ï¼š

```bash
pip install optimum-onnx
```

è¯·å‚é˜… [ğŸ¤— Optimum æ–‡æ¡£](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model#exporting-a-model-to-onnx-using-the-cli) ä»¥æŸ¥çœ‹æ‰€æœ‰å¯ç”¨å‚æ•°ï¼Œæˆ–è€…åœ¨å‘½ä»¤è¡Œä¸­æŸ¥çœ‹å¸®åŠ©ï¼š

```bash
optimum-cli export onnx --help
```

è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œä»¥ä» ğŸ¤— Hub å¯¼å‡ºæ¨¡å‹çš„æ£€æŸ¥ç‚¹ï¼ˆcheckpointï¼‰ï¼Œä»¥ `distilbert/distilbert-base-uncased-distilled-squad` ä¸ºä¾‹ï¼š

```bash
optimum-cli export onnx --model distilbert/distilbert-base-uncased-distilled-squad distilbert_base_uncased_squad_onnx/
```

ä½ åº”è¯¥èƒ½åœ¨æ—¥å¿—ä¸­çœ‹åˆ°å¯¼å‡ºè¿›åº¦ä»¥åŠç”Ÿæˆçš„ `model.onnx` æ–‡ä»¶çš„ä¿å­˜ä½ç½®ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```bash
Validating ONNX model distilbert_base_uncased_squad_onnx/model.onnx...
	-[âœ“] ONNX model output names match reference model (start_logits, end_logits)
	- Validating ONNX Model output "start_logits":
		-[âœ“] (2, 16) matches (2, 16)
		-[âœ“] all values close (atol: 0.0001)
	- Validating ONNX Model output "end_logits":
		-[âœ“] (2, 16) matches (2, 16)
		-[âœ“] all values close (atol: 0.0001)
The ONNX export succeeded and the exported model was saved at: distilbert_base_uncased_squad_onnx
```

ä¸Šé¢çš„ç¤ºä¾‹è¯´æ˜äº†ä» ğŸ¤— Hub å¯¼å‡ºæ£€æŸ¥ç‚¹çš„è¿‡ç¨‹ã€‚å¯¼å‡ºæœ¬åœ°æ¨¡å‹æ—¶ï¼Œé¦–å…ˆéœ€è¦ç¡®ä¿å°†æ¨¡å‹çš„æƒé‡å’Œåˆ†è¯å™¨æ–‡ä»¶ä¿å­˜åœ¨åŒä¸€ç›®å½•ï¼ˆ`local_path`ï¼‰ä¸­ã€‚åœ¨ä½¿ç”¨ CLI æ—¶ï¼Œå°† `local_path` ä¼ é€’ç»™ `model` å‚æ•°ï¼Œè€Œä¸æ˜¯ ğŸ¤— Hub ä¸Šçš„æ£€æŸ¥ç‚¹åç§°ï¼Œå¹¶æä¾› `--task` å‚æ•°ã€‚ä½ å¯ä»¥åœ¨ [ğŸ¤— Optimum æ–‡æ¡£](https://huggingface.co/docs/optimum/exporters/task_manager)ä¸­æŸ¥çœ‹æ”¯æŒçš„ä»»åŠ¡åˆ—è¡¨ã€‚å¦‚æœæœªæä¾› `task` å‚æ•°ï¼Œå°†é»˜è®¤å¯¼å‡ºä¸å¸¦ç‰¹å®šä»»åŠ¡å¤´çš„æ¨¡å‹æ¶æ„ã€‚

```bash
optimum-cli export onnx --model local_path --task question-answering distilbert_base_uncased_squad_onnx/
```

ç”Ÿæˆçš„ `model.onnx` æ–‡ä»¶å¯ä»¥åœ¨æ”¯æŒ ONNX æ ‡å‡†çš„ [è®¸å¤šåŠ é€Ÿå¼•æ“ï¼ˆacceleratorsï¼‰](https://onnx.ai/supported-tools.html#deployModel) ä¹‹ä¸€ä¸Šè¿è¡Œã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ [ONNX Runtime](https://onnxruntime.ai/) åŠ è½½å’Œè¿è¡Œæ¨¡å‹ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
>>> from transformers import AutoTokenizer
>>> from optimum.onnxruntime import ORTModelForQuestionAnswering

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert_base_uncased_squad_onnx")
>>> model = ORTModelForQuestionAnswering.from_pretrained("distilbert_base_uncased_squad_onnx")
>>> inputs = tokenizer("What am I using?", "Using DistilBERT with ONNX Runtime!", return_tensors="pt")
>>> outputs = model(**inputs)
```

### ä½¿ç”¨ `optimum.onnxruntime` å°† ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºä¸º ONNX

é™¤äº† CLI ä¹‹å¤–ï¼Œä½ è¿˜å¯ä»¥ä½¿ç”¨ä»£ç å°† ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºä¸º ONNXï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
>>> from optimum.onnxruntime import ORTModelForSequenceClassification
>>> from transformers import AutoTokenizer

>>> model_checkpoint = "distilbert_base_uncased_squad"
>>> save_directory = "onnx/"

>>> # ä» transformers åŠ è½½æ¨¡å‹å¹¶å°†å…¶å¯¼å‡ºä¸º ONNX
>>> ort_model = ORTModelForSequenceClassification.from_pretrained(model_checkpoint, export=True)
>>> tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

>>> # ä¿å­˜ onnx æ¨¡å‹ä»¥åŠåˆ†è¯å™¨
>>> ort_model.save_pretrained(save_directory)
>>> tokenizer.save_pretrained(save_directory)
```

### å¯¼å‡ºå°šæœªæ”¯æŒçš„æ¶æ„çš„æ¨¡å‹

å¦‚æœä½ æƒ³è¦ä¸ºå½“å‰æ— æ³•å¯¼å‡ºçš„æ¨¡å‹æ·»åŠ æ”¯æŒï¼Œè¯·å…ˆæ£€æŸ¥ [`optimum.exporters.onnx`](https://huggingface.co/docs/optimum/exporters/onnx/overview) æ˜¯å¦æ”¯æŒè¯¥æ¨¡å‹ï¼Œå¦‚æœä¸æ”¯æŒï¼Œä½ å¯ä»¥ [ç›´æ¥ä¸º ğŸ¤— Optimum è´¡çŒ®ä»£ç ](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/contribute)ã€‚

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\task_summary.md
============================================================



# ğŸ¤— Transformers èƒ½åšä»€ä¹ˆ

ğŸ¤— Transformersæ˜¯ä¸€ä¸ªç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ã€è®¡ç®—æœºè§†è§‰å’ŒéŸ³é¢‘å’Œè¯­éŸ³å¤„ç†ä»»åŠ¡çš„é¢„è®­ç»ƒæ¨¡å‹åº“ã€‚è¯¥åº“ä¸ä»…åŒ…å«Transformeræ¨¡å‹ï¼Œè¿˜åŒ…æ‹¬ç”¨äºè®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„ç°ä»£å·ç§¯ç½‘ç»œç­‰éTransformeræ¨¡å‹ã€‚å¦‚æœæ‚¨çœ‹çœ‹ä»Šå¤©æœ€å—æ¬¢è¿çš„ä¸€äº›æ¶ˆè´¹äº§å“ï¼Œæ¯”å¦‚æ™ºèƒ½æ‰‹æœºã€åº”ç”¨ç¨‹åºå’Œç”µè§†ï¼Œå¾ˆå¯èƒ½èƒŒåéƒ½æœ‰æŸç§æ·±åº¦å­¦ä¹ æŠ€æœ¯çš„æ”¯æŒã€‚æƒ³è¦ä»æ‚¨æ™ºèƒ½æ‰‹æœºæ‹æ‘„çš„ç…§ç‰‡ä¸­åˆ é™¤èƒŒæ™¯å¯¹è±¡å—ï¼Ÿè¿™é‡Œæ˜¯ä¸€ä¸ªå…¨æ™¯åˆ†å‰²ä»»åŠ¡çš„ä¾‹å­ï¼ˆå¦‚æœæ‚¨è¿˜ä¸äº†è§£è¿™æ˜¯ä»€ä¹ˆæ„æ€ï¼Œæˆ‘ä»¬å°†åœ¨ä»¥ä¸‹éƒ¨åˆ†è¿›è¡Œæè¿°ï¼ï¼‰ã€‚

æœ¬é¡µé¢æä¾›äº†ä½¿ç”¨ğŸ¤— Transformersåº“ä»…ç”¨ä¸‰è¡Œä»£ç è§£å†³ä¸åŒçš„è¯­éŸ³å’ŒéŸ³é¢‘ã€è®¡ç®—æœºè§†è§‰å’ŒNLPä»»åŠ¡çš„æ¦‚è¿°ï¼


## éŸ³é¢‘
éŸ³é¢‘å’Œè¯­éŸ³å¤„ç†ä»»åŠ¡ä¸å…¶ä»–æ¨¡æ€ç•¥æœ‰ä¸åŒï¼Œä¸»è¦æ˜¯å› ä¸ºéŸ³é¢‘ä½œä¸ºè¾“å…¥æ˜¯ä¸€ä¸ªè¿ç»­çš„ä¿¡å·ã€‚ä¸æ–‡æœ¬ä¸åŒï¼ŒåŸå§‹éŸ³é¢‘æ³¢å½¢ä¸èƒ½åƒå¥å­å¯ä»¥è¢«åˆ’åˆ†ä¸ºå•è¯é‚£æ ·è¢«æ•´é½åœ°åˆ†å‰²æˆç¦»æ•£çš„å—ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œé€šå¸¸åœ¨å›ºå®šçš„æ—¶é—´é—´éš”å†…å¯¹åŸå§‹éŸ³é¢‘ä¿¡å·è¿›è¡Œé‡‡æ ·ã€‚å¦‚æœåœ¨æ¯ä¸ªæ—¶é—´é—´éš”å†…é‡‡æ ·æ›´å¤šæ ·æœ¬ï¼Œé‡‡æ ·ç‡å°±ä¼šæ›´é«˜ï¼ŒéŸ³é¢‘æ›´æ¥è¿‘åŸå§‹éŸ³é¢‘æºã€‚

ä»¥å‰çš„æ–¹æ³•æ˜¯é¢„å¤„ç†éŸ³é¢‘ä»¥ä»ä¸­æå–æœ‰ç”¨çš„ç‰¹å¾ã€‚ç°åœ¨æ›´å¸¸è§çš„åšæ³•æ˜¯ç›´æ¥å°†åŸå§‹éŸ³é¢‘æ³¢å½¢è¾“å…¥åˆ°ç‰¹å¾ç¼–ç å™¨ä¸­ï¼Œä»¥æå–éŸ³é¢‘è¡¨ç¤ºã€‚è¿™æ ·å¯ä»¥ç®€åŒ–é¢„å¤„ç†æ­¥éª¤ï¼Œå¹¶å…è®¸æ¨¡å‹å­¦ä¹ æœ€é‡è¦çš„ç‰¹å¾ã€‚

### éŸ³é¢‘åˆ†ç±»

éŸ³é¢‘åˆ†ç±»æ˜¯ä¸€é¡¹å°†éŸ³é¢‘æ•°æ®ä»é¢„å®šä¹‰çš„ç±»åˆ«é›†åˆä¸­è¿›è¡Œæ ‡è®°çš„ä»»åŠ¡ã€‚è¿™æ˜¯ä¸€ä¸ªå¹¿æ³›çš„ç±»åˆ«ï¼Œå…·æœ‰è®¸å¤šå…·ä½“çš„åº”ç”¨ï¼Œå…¶ä¸­ä¸€äº›åŒ…æ‹¬ï¼š

* å£°å­¦åœºæ™¯åˆ†ç±»ï¼šä½¿ç”¨åœºæ™¯æ ‡ç­¾ï¼ˆ"åŠå…¬å®¤"ã€"æµ·æ»©"ã€"ä½“è‚²åœº"ï¼‰å¯¹éŸ³é¢‘è¿›è¡Œæ ‡è®°ã€‚
* å£°å­¦äº‹ä»¶æ£€æµ‹ï¼šä½¿ç”¨å£°éŸ³äº‹ä»¶æ ‡ç­¾ï¼ˆ"æ±½è½¦å–‡å­å£°"ã€"é²¸é±¼å«å£°"ã€"ç»ç’ƒç ´ç¢å£°"ï¼‰å¯¹éŸ³é¢‘è¿›è¡Œæ ‡è®°ã€‚
* æ ‡è®°ï¼šå¯¹åŒ…å«å¤šç§å£°éŸ³çš„éŸ³é¢‘è¿›è¡Œæ ‡è®°ï¼ˆé¸Ÿé¸£ã€ä¼šè®®ä¸­çš„è¯´è¯äººè¯†åˆ«ï¼‰ã€‚
* éŸ³ä¹åˆ†ç±»ï¼šä½¿ç”¨æµæ´¾æ ‡ç­¾ï¼ˆ"é‡‘å±"ã€"å˜»å“ˆ"ã€"ä¹¡æ‘"ï¼‰å¯¹éŸ³ä¹è¿›è¡Œæ ‡è®°ã€‚

```py
>>> from transformers import pipeline

>>> classifier = pipeline(task="audio-classification", model="superb/hubert-base-superb-er")
>>> preds = classifier("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> preds
[{'score': 0.4532, 'label': 'hap'},
 {'score': 0.3622, 'label': 'sad'},
 {'score': 0.0943, 'label': 'neu'},
 {'score': 0.0903, 'label': 'ang'}]
```

### è‡ªåŠ¨è¯­éŸ³è¯†åˆ«

è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å°†è¯­éŸ³è½¬å½•ä¸ºæ–‡æœ¬ã€‚è¿™æ˜¯æœ€å¸¸è§çš„éŸ³é¢‘ä»»åŠ¡ä¹‹ä¸€ï¼Œéƒ¨åˆ†åŸå› æ˜¯å› ä¸ºè¯­éŸ³æ˜¯äººç±»äº¤æµçš„è‡ªç„¶å½¢å¼ã€‚å¦‚ä»Šï¼ŒASRç³»ç»ŸåµŒå…¥åœ¨æ™ºèƒ½æŠ€æœ¯äº§å“ä¸­ï¼Œå¦‚æ‰¬å£°å™¨ã€ç”µè¯å’Œæ±½è½¦ã€‚æˆ‘ä»¬å¯ä»¥è¦æ±‚è™šæ‹ŸåŠ©æ‰‹æ’­æ”¾éŸ³ä¹ã€è®¾ç½®æé†’å’Œå‘Šè¯‰æˆ‘ä»¬å¤©æ°”ã€‚

ä½†æ˜¯ï¼ŒTransformeræ¶æ„å¸®åŠ©è§£å†³çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯ä½èµ„æºè¯­è¨€ã€‚é€šè¿‡åœ¨å¤§é‡è¯­éŸ³æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œä»…åœ¨ä¸€ä¸ªä½èµ„æºè¯­è¨€çš„ä¸€å°æ—¶æ ‡è®°è¯­éŸ³æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä»ç„¶å¯ä»¥äº§ç”Ÿä¸ä»¥å‰åœ¨100å€æ›´å¤šæ ‡è®°æ•°æ®ä¸Šè®­ç»ƒçš„ASRç³»ç»Ÿç›¸æ¯”é«˜è´¨é‡çš„ç»“æœã€‚

```py
>>> from transformers import pipeline

>>> transcriber = pipeline(task="automatic-speech-recognition", model="openai/whisper-small")
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}
```

## è®¡ç®—æœºè§†è§‰

è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­æœ€æ—©æˆåŠŸä¹‹ä¸€æ˜¯ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆ[CNN](glossary#convolution)ï¼‰è¯†åˆ«é‚®æ”¿ç¼–ç æ•°å­—å›¾åƒã€‚å›¾åƒç”±åƒç´ ç»„æˆï¼Œæ¯ä¸ªåƒç´ éƒ½æœ‰ä¸€ä¸ªæ•°å€¼ã€‚è¿™ä½¿å¾—å°†å›¾åƒè¡¨ç¤ºä¸ºåƒç´ å€¼çŸ©é˜µå˜å¾—å®¹æ˜“ã€‚æ¯ä¸ªåƒç´ å€¼ç»„åˆæè¿°äº†å›¾åƒçš„é¢œè‰²ã€‚

è®¡ç®—æœºè§†è§‰ä»»åŠ¡å¯ä»¥é€šè¿‡ä»¥ä¸‹ä¸¤ç§é€šç”¨æ–¹å¼è§£å†³ï¼š

1. ä½¿ç”¨å·ç§¯æ¥å­¦ä¹ å›¾åƒçš„å±‚æ¬¡ç‰¹å¾ï¼Œä»ä½çº§ç‰¹å¾åˆ°é«˜çº§æŠ½è±¡ç‰¹å¾ã€‚
2. å°†å›¾åƒåˆ†æˆå—ï¼Œå¹¶ä½¿ç”¨Transformeré€æ­¥å­¦ä¹ æ¯ä¸ªå›¾åƒå—å¦‚ä½•ç›¸äº’å…³è”ä»¥å½¢æˆå›¾åƒã€‚ä¸CNNåå¥½çš„è‡ªåº•å‘ä¸Šæ–¹æ³•ä¸åŒï¼Œè¿™ç§æ–¹æ³•æœ‰ç‚¹åƒä»ä¸€ä¸ªæ¨¡ç³Šçš„å›¾åƒå¼€å§‹ï¼Œç„¶åé€æ¸å°†å…¶èšç„¦æ¸…æ™°ã€‚

### å›¾åƒåˆ†ç±»

å›¾åƒåˆ†ç±»å°†æ•´ä¸ªå›¾åƒä»é¢„å®šä¹‰çš„ç±»åˆ«é›†åˆä¸­è¿›è¡Œæ ‡è®°ã€‚åƒå¤§å¤šæ•°åˆ†ç±»ä»»åŠ¡ä¸€æ ·ï¼Œå›¾åƒåˆ†ç±»æœ‰è®¸å¤šå®é™…ç”¨ä¾‹ï¼Œå…¶ä¸­ä¸€äº›åŒ…æ‹¬ï¼š

* åŒ»ç–—ä¿å¥ï¼šæ ‡è®°åŒ»å­¦å›¾åƒä»¥æ£€æµ‹ç–¾ç—…æˆ–ç›‘æµ‹æ‚£è€…å¥åº·çŠ¶å†µ
* ç¯å¢ƒï¼šæ ‡è®°å«æ˜Ÿå›¾åƒä»¥ç›‘æµ‹æ£®æ—ç ä¼ã€æä¾›é‡å¤–ç®¡ç†ä¿¡æ¯æˆ–æ£€æµ‹é‡ç«
* å†œä¸šï¼šæ ‡è®°å†œä½œç‰©å›¾åƒä»¥ç›‘æµ‹æ¤ç‰©å¥åº·æˆ–ç”¨äºåœŸåœ°ä½¿ç”¨ç›‘æµ‹çš„å«æ˜Ÿå›¾åƒ
* ç”Ÿæ€å­¦ï¼šæ ‡è®°åŠ¨ç‰©æˆ–æ¤ç‰©ç‰©ç§çš„å›¾åƒä»¥ç›‘æµ‹é‡ç”ŸåŠ¨ç‰©ç§ç¾¤æˆ–è·Ÿè¸ªæ¿’å±ç‰©ç§

```py
>>> from transformers import pipeline

>>> classifier = pipeline(task="image-classification")
>>> preds = classifier(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> print(*preds, sep="\n")
{'score': 0.4335, 'label': 'lynx, catamount'}
{'score': 0.0348, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}
{'score': 0.0324, 'label': 'snow leopard, ounce, Panthera uncia'}
{'score': 0.0239, 'label': 'Egyptian cat'}
{'score': 0.0229, 'label': 'tiger cat'}
```

### ç›®æ ‡æ£€æµ‹

ä¸å›¾åƒåˆ†ç±»ä¸åŒï¼Œç›®æ ‡æ£€æµ‹åœ¨å›¾åƒä¸­è¯†åˆ«å¤šä¸ªå¯¹è±¡ä»¥åŠè¿™äº›å¯¹è±¡åœ¨å›¾åƒä¸­çš„ä½ç½®ï¼ˆç”±è¾¹ç•Œæ¡†å®šä¹‰ï¼‰ã€‚ç›®æ ‡æ£€æµ‹çš„ä¸€äº›ç¤ºä¾‹åº”ç”¨åŒ…æ‹¬ï¼š

* è‡ªåŠ¨é©¾é©¶è½¦è¾†ï¼šæ£€æµ‹æ—¥å¸¸äº¤é€šå¯¹è±¡ï¼Œå¦‚å…¶ä»–è½¦è¾†ã€è¡Œäººå’Œçº¢ç»¿ç¯
* é¥æ„Ÿï¼šç¾å®³ç›‘æµ‹ã€åŸå¸‚è§„åˆ’å’Œå¤©æ°”é¢„æŠ¥
* ç¼ºé™·æ£€æµ‹ï¼šæ£€æµ‹å»ºç­‘ç‰©ä¸­çš„è£‚ç¼æˆ–ç»“æ„æŸåï¼Œä»¥åŠåˆ¶é€ ä¸šäº§å“ç¼ºé™·


```py
>>> from transformers import pipeline

>>> detector = pipeline(task="object-detection")
>>> preds = detector(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"], "box": pred["box"]} for pred in preds]
>>> preds
[{'score': 0.9865,
  'label': 'cat',
  'box': {'xmin': 178, 'ymin': 154, 'xmax': 882, 'ymax': 598}}]
```

### å›¾åƒåˆ†å‰²

å›¾åƒåˆ†å‰²æ˜¯ä¸€é¡¹åƒç´ çº§ä»»åŠ¡ï¼Œå°†å›¾åƒä¸­çš„æ¯ä¸ªåƒç´ åˆ†é…ç»™ä¸€ä¸ªç±»åˆ«ã€‚å®ƒä¸ä½¿ç”¨è¾¹ç•Œæ¡†æ ‡è®°å’Œé¢„æµ‹å›¾åƒä¸­çš„å¯¹è±¡çš„ç›®æ ‡æ£€æµ‹ä¸åŒï¼Œå› ä¸ºåˆ†å‰²æ›´åŠ ç²¾ç»†ã€‚åˆ†å‰²å¯ä»¥åœ¨åƒç´ çº§åˆ«æ£€æµ‹å¯¹è±¡ã€‚æœ‰å‡ ç§ç±»å‹çš„å›¾åƒåˆ†å‰²ï¼š

* å®ä¾‹åˆ†å‰²ï¼šé™¤äº†æ ‡è®°å¯¹è±¡çš„ç±»åˆ«å¤–ï¼Œè¿˜æ ‡è®°æ¯ä¸ªå¯¹è±¡çš„ä¸åŒå®ä¾‹ï¼ˆâ€œdog-1â€ï¼Œâ€œdog-2â€ï¼‰
* å…¨æ™¯åˆ†å‰²ï¼šè¯­ä¹‰åˆ†å‰²å’Œå®ä¾‹åˆ†å‰²çš„ç»„åˆï¼› å®ƒä½¿ç”¨è¯­ä¹‰ç±»ä¸ºæ¯ä¸ªåƒç´ æ ‡è®°å¹¶æ ‡è®°æ¯ä¸ªå¯¹è±¡çš„ä¸åŒå®ä¾‹

åˆ†å‰²ä»»åŠ¡å¯¹äºè‡ªåŠ¨é©¾é©¶è½¦è¾†å¾ˆæœ‰å¸®åŠ©ï¼Œå¯ä»¥åˆ›å»ºå‘¨å›´ä¸–ç•Œçš„åƒç´ çº§åœ°å›¾ï¼Œä»¥ä¾¿å®ƒä»¬å¯ä»¥åœ¨è¡Œäººå’Œå…¶ä»–è½¦è¾†å‘¨å›´å®‰å…¨å¯¼èˆªã€‚å®ƒè¿˜é€‚ç”¨äºåŒ»å­¦æˆåƒï¼Œå…¶ä¸­ä»»åŠ¡çš„æ›´ç²¾ç»†ç²’åº¦å¯ä»¥å¸®åŠ©è¯†åˆ«å¼‚å¸¸ç»†èƒæˆ–å™¨å®˜ç‰¹å¾ã€‚å›¾åƒåˆ†å‰²ä¹Ÿå¯ä»¥ç”¨äºç”µå­å•†åŠ¡ï¼Œé€šè¿‡æ‚¨çš„ç›¸æœºåœ¨ç°å®ä¸–ç•Œä¸­è¦†ç›–ç‰©ä½“æ¥è™šæ‹Ÿè¯•ç©¿è¡£æœæˆ–åˆ›å»ºå¢å¼ºç°å®ä½“éªŒã€‚

```py
>>> from transformers import pipeline

>>> segmenter = pipeline(task="image-segmentation")
>>> preds = segmenter(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> print(*preds, sep="\n")
{'score': 0.9879, 'label': 'LABEL_184'}
{'score': 0.9973, 'label': 'snow'}
{'score': 0.9972, 'label': 'cat'}
```

### æ·±åº¦ä¼°è®¡

æ·±åº¦ä¼°è®¡é¢„æµ‹å›¾åƒä¸­æ¯ä¸ªåƒç´ åˆ°ç›¸æœºçš„è·ç¦»ã€‚è¿™ä¸ªè®¡ç®—æœºè§†è§‰ä»»åŠ¡å¯¹äºåœºæ™¯ç†è§£å’Œé‡å»ºå°¤ä¸ºé‡è¦ã€‚ä¾‹å¦‚ï¼Œåœ¨è‡ªåŠ¨é©¾é©¶æ±½è½¦ä¸­ï¼Œè½¦è¾†éœ€è¦äº†è§£è¡Œäººã€äº¤é€šæ ‡å¿—å’Œå…¶ä»–è½¦è¾†ç­‰ç‰©ä½“çš„è·ç¦»ï¼Œä»¥é¿å…éšœç¢ç‰©å’Œç¢°æ’ã€‚æ·±åº¦ä¿¡æ¯è¿˜æœ‰åŠ©äºä»2Då›¾åƒæ„å»º3Dè¡¨ç¤ºï¼Œå¹¶å¯ç”¨äºåˆ›å»ºç”Ÿç‰©ç»“æ„æˆ–å»ºç­‘ç‰©çš„é«˜è´¨é‡3Dè¡¨ç¤ºã€‚

æœ‰ä¸¤ç§æ–¹æ³•å¯ä»¥è¿›è¡Œæ·±åº¦ä¼°è®¡ï¼š

* stereoï¼ˆç«‹ä½“ï¼‰ï¼šé€šè¿‡æ¯”è¾ƒåŒä¸€å›¾åƒçš„ä¸¤ä¸ªç•¥å¾®ä¸åŒè§’åº¦çš„å›¾åƒæ¥ä¼°è®¡æ·±åº¦
* monocularï¼ˆå•ç›®ï¼‰ï¼šä»å•ä¸ªå›¾åƒä¸­ä¼°è®¡æ·±åº¦


```py
>>> from transformers import pipeline

>>> depth_estimator = pipeline(task="depth-estimation")
>>> preds = depth_estimator(
...     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
```

## è‡ªç„¶è¯­è¨€å¤„ç†

NLPä»»åŠ¡æ˜¯æœ€å¸¸è§çš„ç±»å‹ä¹‹ä¸€ï¼Œå› ä¸ºæ–‡æœ¬æ˜¯æˆ‘ä»¬è¿›è¡Œäº¤æµçš„è‡ªç„¶æ–¹å¼ã€‚ä¸ºäº†è®©æ–‡æœ¬å˜æˆæ¨¡å‹è¯†åˆ«çš„æ ¼å¼ï¼Œéœ€è¦å¯¹å…¶è¿›è¡Œåˆ†è¯ã€‚è¿™æ„å‘³ç€å°†ä¸€æ®µæ–‡æœ¬åˆ†æˆå•ç‹¬çš„å•è¯æˆ–å­è¯ï¼ˆ`tokens`ï¼‰ï¼Œç„¶åå°†è¿™äº›`tokens`è½¬æ¢ä¸ºæ•°å­—ã€‚å› æ­¤ï¼Œå¯ä»¥å°†ä¸€æ®µæ–‡æœ¬è¡¨ç¤ºä¸ºä¸€ç³»åˆ—æ•°å­—ï¼Œä¸€æ—¦æœ‰äº†ä¸€ç³»åˆ—çš„æ•°å­—ï¼Œå°±å¯ä»¥å°†å…¶è¾“å…¥åˆ°æ¨¡å‹ä¸­ä»¥è§£å†³å„ç§NLPä»»åŠ¡ï¼

### æ–‡æœ¬åˆ†ç±»

åƒä»»ä½•æ¨¡æ€çš„åˆ†ç±»ä»»åŠ¡ä¸€æ ·ï¼Œæ–‡æœ¬åˆ†ç±»å°†ä¸€æ®µæ–‡æœ¬ï¼ˆå¯ä»¥æ˜¯å¥å­çº§åˆ«ã€æ®µè½æˆ–æ–‡æ¡£ï¼‰ä»é¢„å®šä¹‰çš„ç±»åˆ«é›†åˆä¸­è¿›è¡Œæ ‡è®°ã€‚æ–‡æœ¬åˆ†ç±»æœ‰è®¸å¤šå®é™…åº”ç”¨ï¼Œå…¶ä¸­ä¸€äº›åŒ…æ‹¬ï¼š

* æƒ…æ„Ÿåˆ†æï¼šæ ¹æ®æŸäº›ææ€§ï¼ˆå¦‚`ç§¯æ`æˆ–`æ¶ˆæ`ï¼‰å¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°ï¼Œå¯ä»¥æ”¯æŒæ”¿æ²»ã€é‡‘èå’Œè¥é”€ç­‰é¢†åŸŸçš„å†³ç­–åˆ¶å®š
* å†…å®¹åˆ†ç±»ï¼šæ ¹æ®æŸäº›ä¸»é¢˜å¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°ï¼Œæœ‰åŠ©äºç»„ç»‡å’Œè¿‡æ»¤æ–°é—»å’Œç¤¾äº¤åª’ä½“æè¦ä¸­çš„ä¿¡æ¯ï¼ˆ`å¤©æ°”`ã€`ä½“è‚²`ã€`é‡‘è`ç­‰ï¼‰


```py
>>> from transformers import pipeline

>>> classifier = pipeline(task="sentiment-analysis")
>>> preds = classifier("Hugging Face is the best thing since sliced bread!")
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> preds
[{'score': 0.9991, 'label': 'POSITIVE'}]
```

### Tokenåˆ†ç±»

åœ¨ä»»ä½•NLPä»»åŠ¡ä¸­ï¼Œæ–‡æœ¬éƒ½ç»è¿‡é¢„å¤„ç†ï¼Œå°†æ–‡æœ¬åºåˆ—åˆ†æˆå•ä¸ªå•è¯æˆ–å­è¯ã€‚è¿™äº›è¢«ç§°ä¸º[tokens](/glossary#token)ã€‚Tokenåˆ†ç±»å°†æ¯ä¸ª`token`åˆ†é…ä¸€ä¸ªæ¥è‡ªé¢„å®šä¹‰ç±»åˆ«é›†çš„æ ‡ç­¾ã€‚

ä¸¤ç§å¸¸è§çš„Tokenåˆ†ç±»æ˜¯ï¼š

* å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ï¼šæ ¹æ®å®ä½“ç±»åˆ«ï¼ˆå¦‚ç»„ç»‡ã€äººå‘˜ã€ä½ç½®æˆ–æ—¥æœŸï¼‰å¯¹`token`è¿›è¡Œæ ‡è®°ã€‚NERåœ¨ç”Ÿç‰©åŒ»å­¦è®¾ç½®ä¸­ç‰¹åˆ«å—æ¬¢è¿ï¼Œå¯ä»¥æ ‡è®°åŸºå› ã€è›‹ç™½è´¨å’Œè¯ç‰©åç§°ã€‚
* è¯æ€§æ ‡æ³¨ï¼ˆPOSï¼‰ï¼šæ ¹æ®å…¶è¯æ€§ï¼ˆå¦‚åè¯ã€åŠ¨è¯æˆ–å½¢å®¹è¯ï¼‰å¯¹æ ‡è®°è¿›è¡Œæ ‡è®°ã€‚POSå¯¹äºå¸®åŠ©ç¿»è¯‘ç³»ç»Ÿäº†è§£ä¸¤ä¸ªç›¸åŒçš„å•è¯å¦‚ä½•åœ¨è¯­æ³•ä¸Šä¸åŒå¾ˆæœ‰ç”¨ï¼ˆä½œä¸ºåè¯çš„é“¶è¡Œä¸ä½œä¸ºåŠ¨è¯çš„é“¶è¡Œï¼‰ã€‚

```py
>>> from transformers import pipeline

>>> classifier = pipeline(task="ner")
>>> preds = classifier("Hugging Face is a French company based in New York City.")
>>> preds = [
...     {
...         "entity": pred["entity"],
...         "score": round(pred["score"], 4),
...         "index": pred["index"],
...         "word": pred["word"],
...         "start": pred["start"],
...         "end": pred["end"],
...     }
...     for pred in preds
... ]
>>> print(*preds, sep="\n")
{'entity': 'I-ORG', 'score': 0.9968, 'index': 1, 'word': 'Hu', 'start': 0, 'end': 2}
{'entity': 'I-ORG', 'score': 0.9293, 'index': 2, 'word': '##gging', 'start': 2, 'end': 7}
{'entity': 'I-ORG', 'score': 0.9763, 'index': 3, 'word': 'Face', 'start': 8, 'end': 12}
{'entity': 'I-MISC', 'score': 0.9983, 'index': 6, 'word': 'French', 'start': 18, 'end': 24}
{'entity': 'I-LOC', 'score': 0.999, 'index': 10, 'word': 'New', 'start': 42, 'end': 45}
{'entity': 'I-LOC', 'score': 0.9987, 'index': 11, 'word': 'York', 'start': 46, 'end': 50}
{'entity': 'I-LOC', 'score': 0.9992, 'index': 12, 'word': 'City', 'start': 51, 'end': 55}
```

### é—®ç­”

é—®ç­”æ˜¯å¦ä¸€ä¸ª`token-level`çš„ä»»åŠ¡ï¼Œè¿”å›ä¸€ä¸ªé—®é¢˜çš„ç­”æ¡ˆï¼Œæœ‰æ—¶å¸¦æœ‰ä¸Šä¸‹æ–‡ï¼ˆå¼€æ”¾é¢†åŸŸï¼‰ï¼Œæœ‰æ—¶ä¸å¸¦ä¸Šä¸‹æ–‡ï¼ˆå°é—­é¢†åŸŸï¼‰ã€‚æ¯å½“æˆ‘ä»¬å‘è™šæ‹ŸåŠ©æ‰‹æå‡ºé—®é¢˜æ—¶ï¼Œä¾‹å¦‚è¯¢é—®ä¸€å®¶é¤å…æ˜¯å¦è¥ä¸šï¼Œå°±ä¼šå‘ç”Ÿè¿™ç§æƒ…å†µã€‚å®ƒè¿˜å¯ä»¥æä¾›å®¢æˆ·æˆ–æŠ€æœ¯æ”¯æŒï¼Œå¹¶å¸®åŠ©æœç´¢å¼•æ“æ£€ç´¢æ‚¨è¦æ±‚çš„ç›¸å…³ä¿¡æ¯ã€‚

æœ‰ä¸¤ç§å¸¸è§çš„é—®ç­”ç±»å‹ï¼š

* æå–å¼ï¼šç»™å®šä¸€ä¸ªé—®é¢˜å’Œä¸€äº›ä¸Šä¸‹æ–‡ï¼Œç­”æ¡ˆæ˜¯ä»æ¨¡å‹å¿…é¡»æå–çš„ä¸Šä¸‹æ–‡ä¸­çš„ä¸€æ®µæ–‡æœ¬è·¨åº¦ã€‚
* æŠ½è±¡å¼ï¼šç»™å®šä¸€ä¸ªé—®é¢˜å’Œä¸€äº›ä¸Šä¸‹æ–‡ï¼Œç­”æ¡ˆä»ä¸Šä¸‹æ–‡ä¸­ç”Ÿæˆã€‚

### æ‘˜è¦

æ‘˜è¦ä»è¾ƒé•¿çš„æ–‡æœ¬ä¸­åˆ›å»ºä¸€ä¸ªè¾ƒçŸ­çš„ç‰ˆæœ¬ï¼ŒåŒæ—¶å°½å¯èƒ½ä¿ç•™åŸå§‹æ–‡æ¡£çš„å¤§éƒ¨åˆ†å«ä¹‰ã€‚æ‘˜è¦æ˜¯ä¸€ä¸ªåºåˆ—åˆ°åºåˆ—çš„ä»»åŠ¡ï¼›å®ƒè¾“å‡ºæ¯”è¾“å…¥æ›´çŸ­çš„æ–‡æœ¬åºåˆ—ã€‚æœ‰è®¸å¤šé•¿ç¯‡æ–‡æ¡£å¯ä»¥è¿›è¡Œæ‘˜è¦ï¼Œä»¥å¸®åŠ©è¯»è€…å¿«é€Ÿäº†è§£ä¸»è¦è¦ç‚¹ã€‚æ³•æ¡ˆã€æ³•å¾‹å’Œè´¢åŠ¡æ–‡ä»¶ã€ä¸“åˆ©å’Œç§‘å­¦è®ºæ–‡ç­‰æ–‡æ¡£å¯ä»¥æ‘˜è¦ï¼Œä»¥èŠ‚çœè¯»è€…çš„æ—¶é—´å¹¶ä½œä¸ºé˜…è¯»è¾…åŠ©å·¥å…·ã€‚

åƒé—®ç­”ä¸€æ ·ï¼Œæ‘˜è¦æœ‰ä¸¤ç§ç±»å‹ï¼š

* æå–å¼ï¼šä»åŸå§‹æ–‡æœ¬ä¸­è¯†åˆ«å’Œæå–æœ€é‡è¦çš„å¥å­
* æŠ½è±¡å¼ï¼šä»åŸå§‹æ–‡æœ¬ç”Ÿæˆç›®æ ‡æ‘˜è¦ï¼ˆå¯èƒ½åŒ…æ‹¬ä¸åœ¨è¾“å…¥æ–‡æ¡£ä¸­çš„æ–°å•è¯ï¼‰

### ç¿»è¯‘

ç¿»è¯‘å°†ä¸€ç§è¯­è¨€çš„æ–‡æœ¬åºåˆ—è½¬æ¢ä¸ºå¦ä¸€ç§è¯­è¨€ã€‚å®ƒå¯¹äºå¸®åŠ©æ¥è‡ªä¸åŒèƒŒæ™¯çš„äººä»¬ç›¸äº’äº¤æµã€å¸®åŠ©ç¿»è¯‘å†…å®¹ä»¥å¸å¼•æ›´å¹¿æ³›çš„å—ä¼—ï¼Œç”šè‡³æˆä¸ºå­¦ä¹ å·¥å…·ä»¥å¸®åŠ©äººä»¬å­¦ä¹ ä¸€é—¨æ–°è¯­è¨€éƒ½éå¸¸é‡è¦ã€‚é™¤äº†æ‘˜è¦ä¹‹å¤–ï¼Œç¿»è¯‘ä¹Ÿæ˜¯ä¸€ä¸ªåºåˆ—åˆ°åºåˆ—çš„ä»»åŠ¡ï¼Œæ„å‘³ç€æ¨¡å‹æ¥æ”¶è¾“å…¥åºåˆ—å¹¶è¿”å›ç›®æ ‡è¾“å‡ºåºåˆ—ã€‚

åœ¨æ—©æœŸï¼Œç¿»è¯‘æ¨¡å‹å¤§å¤šæ˜¯å•è¯­çš„ï¼Œä½†æœ€è¿‘ï¼Œè¶Šæ¥è¶Šå¤šçš„äººå¯¹å¯ä»¥åœ¨å¤šç§è¯­è¨€ä¹‹é—´è¿›è¡Œç¿»è¯‘çš„å¤šè¯­è¨€æ¨¡å‹æ„Ÿå…´è¶£ã€‚

```py
>>> from transformers import pipeline

>>> text = "translate English to French: Hugging Face is a community-based open-source platform for machine learning."
>>> translator = pipeline(task="translation", model="google-t5/t5-small")
>>> translator(text)
[{'translation_text': "Hugging Face est une tribune communautaire de l'apprentissage des machines."}]
```

### è¯­è¨€æ¨¡å‹

è¯­è¨€æ¨¡å‹æ˜¯ä¸€ç§é¢„æµ‹æ–‡æœ¬åºåˆ—ä¸­å•è¯çš„ä»»åŠ¡ã€‚å®ƒå·²æˆä¸ºä¸€ç§éå¸¸æµè¡Œçš„NLPä»»åŠ¡ï¼Œå› ä¸ºé¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹å¯ä»¥å¾®è°ƒç”¨äºè®¸å¤šå…¶ä»–ä¸‹æ¸¸ä»»åŠ¡ã€‚æœ€è¿‘ï¼Œäººä»¬å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¡¨ç°å‡ºäº†æå¤§çš„å…´è¶£ï¼Œè¿™äº›æ¨¡å‹å±•ç¤ºäº†`zero learning`æˆ–`few-shot learning`çš„èƒ½åŠ›ã€‚è¿™æ„å‘³ç€æ¨¡å‹å¯ä»¥è§£å†³å®ƒæœªè¢«æ˜ç¡®è®­ç»ƒè¿‡çš„ä»»åŠ¡ï¼è¯­è¨€æ¨¡å‹å¯ç”¨äºç”Ÿæˆæµç•…å’Œä»¤äººä¿¡æœçš„æ–‡æœ¬ï¼Œä½†éœ€è¦å°å¿ƒï¼Œå› ä¸ºæ–‡æœ¬å¯èƒ½å¹¶ä¸æ€»æ˜¯å‡†ç¡®çš„ã€‚

æœ‰ä¸¤ç§ç±»å‹çš„è¯è¯­æ¨¡å‹ï¼š

* causalï¼šæ¨¡å‹çš„ç›®æ ‡æ˜¯é¢„æµ‹åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ª`token`ï¼Œè€Œæœªæ¥çš„`tokens`è¢«é®ç›–ã€‚

    ```py
    >>> from transformers import pipeline

    >>> prompt = "Hugging Face is a community-based open-source platform for machine learning."
    >>> generator = pipeline(task="text-generation")
    >>> generator(prompt)  # doctest: +SKIP
    ```

* maskedï¼šæ¨¡å‹çš„ç›®æ ‡æ˜¯é¢„æµ‹åºåˆ—ä¸­è¢«é®è”½çš„`token`ï¼ŒåŒæ—¶å…·æœ‰å¯¹åºåˆ—ä¸­æ‰€æœ‰`tokens`çš„å®Œå…¨è®¿é—®æƒé™ã€‚

    ```py
    >>> text = "Hugging Face is a community-based open-source <mask> for machine learning."
    >>> fill_mask = pipeline(task="fill-mask")
    >>> preds = fill_mask(text, top_k=1)
    >>> preds = [
    ...     {
    ...         "score": round(pred["score"], 4),
    ...         "token": pred["token"],
    ...         "token_str": pred["token_str"],
    ...         "sequence": pred["sequence"],
    ...     }
    ...     for pred in preds
    ... ]
    >>> preds
    [{'score': 0.2236,
      'token': 1761,
      'token_str': ' platform',
      'sequence': 'Hugging Face is a community-based open-source platform for machine learning.'}]
    ```

## å¤šæ¨¡æ€

å¤šæ¨¡æ€ä»»åŠ¡è¦æ±‚æ¨¡å‹å¤„ç†å¤šç§æ•°æ®æ¨¡æ€ï¼ˆæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘ï¼‰ä»¥è§£å†³ç‰¹å®šé—®é¢˜ã€‚å›¾åƒæè¿°æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€ä»»åŠ¡çš„ä¾‹å­ï¼Œå…¶ä¸­æ¨¡å‹å°†å›¾åƒä½œä¸ºè¾“å…¥å¹¶è¾“å‡ºæè¿°å›¾åƒæˆ–å›¾åƒæŸäº›å±æ€§çš„æ–‡æœ¬åºåˆ—ã€‚

è™½ç„¶å¤šæ¨¡æ€æ¨¡å‹å¤„ç†ä¸åŒçš„æ•°æ®ç±»å‹æˆ–æ¨¡æ€ï¼Œä½†å†…éƒ¨é¢„å¤„ç†æ­¥éª¤å¸®åŠ©æ¨¡å‹å°†æ‰€æœ‰æ•°æ®ç±»å‹è½¬æ¢ä¸º`embeddings`ï¼ˆå‘é‡æˆ–æ•°å­—åˆ—è¡¨ï¼ŒåŒ…å«æœ‰å…³æ•°æ®çš„æœ‰æ„ä¹‰ä¿¡æ¯ï¼‰ã€‚å¯¹äºåƒå›¾åƒæè¿°è¿™æ ·çš„ä»»åŠ¡ï¼Œæ¨¡å‹å­¦ä¹ å›¾åƒåµŒå…¥å’Œæ–‡æœ¬åµŒå…¥ä¹‹é—´çš„å…³ç³»ã€‚

### æ–‡æ¡£é—®ç­”

æ–‡æ¡£é—®ç­”æ˜¯ä»æ–‡æ¡£ä¸­å›ç­”è‡ªç„¶è¯­è¨€é—®é¢˜çš„ä»»åŠ¡ã€‚ä¸`token-level`é—®ç­”ä»»åŠ¡ä¸åŒï¼Œæ–‡æ¡£é—®ç­”å°†åŒ…å«é—®é¢˜çš„æ–‡æ¡£çš„å›¾åƒä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›ç­”æ¡ˆã€‚æ–‡æ¡£é—®ç­”å¯ç”¨äºè§£æç»“æ„åŒ–æ–‡æ¡£å¹¶ä»ä¸­æå–å…³é”®ä¿¡æ¯ã€‚åœ¨ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œå¯ä»¥ä»æ”¶æ®ä¸­æå–æ€»é‡‘é¢å’Œæ‰¾é›¶é‡‘é¢ã€‚

```py
>>> from transformers import pipeline
>>> from PIL import Image
>>> import requests

>>> url = "https://huggingface.co/datasets/hf-internal-testing/example-documents/resolve/main/jpeg_images/2.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> doc_question_answerer = pipeline("document-question-answering", model="magorshunov/layoutlm-invoices")
>>> preds = doc_question_answerer(
...     question="What is the total amount?",
...     image=image,
... )
>>> preds
[{'score': 0.8531, 'answer': '17,000', 'start': 4, 'end': 4}]
```

å¸Œæœ›è¿™ä¸ªé¡µé¢ä¸ºæ‚¨æä¾›äº†ä¸€äº›æœ‰å…³æ¯ç§æ¨¡æ€ä¸­æ‰€æœ‰ç±»å‹ä»»åŠ¡çš„èƒŒæ™¯ä¿¡æ¯ä»¥åŠæ¯ä¸ªä»»åŠ¡çš„å®é™…é‡è¦æ€§ã€‚åœ¨[ä¸‹ä¸€èŠ‚](tasks_explained)ä¸­ï¼Œæ‚¨å°†äº†è§£Transformerså¦‚ä½•è§£å†³è¿™äº›ä»»åŠ¡ã€‚

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\tiktoken.md
============================================================

# Transformersä¸Tiktonkençš„äº’æ“ä½œæ€§

åœ¨ğŸ¤— transformersä¸­ï¼Œå½“ä½¿ç”¨`from_pretrained`æ–¹æ³•ä»HubåŠ è½½æ¨¡å‹æ—¶ï¼Œå¦‚æœæ¨¡å‹åŒ…å«tiktokenæ ¼å¼çš„`tokenizer.model`æ–‡ä»¶ï¼Œæ¡†æ¶å¯ä»¥æ— ç¼æ”¯æŒtiktokenæ¨¡å‹æ–‡ä»¶ï¼Œå¹¶è‡ªåŠ¨å°†å…¶è½¬æ¢ä¸ºæˆ‘ä»¬çš„[å¿«é€Ÿè¯ç¬¦åŒ–å™¨](https://huggingface.co/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)ã€‚

### å·²çŸ¥åŒ…å«`tiktoken.model`æ–‡ä»¶å‘å¸ƒçš„æ¨¡å‹ï¼š
    - gpt2
    - llama3

## ä½¿ç”¨ç¤ºä¾‹

ä¸ºäº†åœ¨transformersä¸­æ­£ç¡®åŠ è½½`tiktoken`æ–‡ä»¶ï¼Œè¯·ç¡®ä¿`tiktoken.model`æ–‡ä»¶æ˜¯tiktokenæ ¼å¼çš„ï¼Œå¹¶ä¸”ä¼šåœ¨åŠ è½½`from_pretrained`æ—¶è‡ªåŠ¨åŠ è½½ã€‚ä»¥ä¸‹å±•ç¤ºå¦‚ä½•ä»åŒä¸€ä¸ªæ–‡ä»¶ä¸­åŠ è½½è¯ç¬¦åŒ–å™¨(tokenizer)å’Œæ¨¡å‹ï¼š

```py
from transformers import AutoTokenizer

model_id = "meta-llama/Meta-Llama-3-8B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_id, subfolder="original") 
```
## åˆ›å»ºtiktokenè¯ç¬¦åŒ–å™¨(tokenizer)

`tokenizer.model`æ–‡ä»¶ä¸­ä¸åŒ…å«ä»»ä½•é¢å¤–çš„è¯ç¬¦(token)æˆ–æ¨¡å¼å­—ç¬¦ä¸²(pattern strings)çš„ä¿¡æ¯ã€‚å¦‚æœè¿™äº›ä¿¡æ¯å¾ˆé‡è¦ï¼Œéœ€è¦å°†è¯ç¬¦åŒ–å™¨(tokenizer)è½¬æ¢ä¸ºé€‚ç”¨äº[`PreTrainedTokenizerFast`]ç±»çš„`tokenizer.json`æ ¼å¼ã€‚

ä½¿ç”¨[tiktoken.get_encoding](https://github.com/openai/tiktoken/blob/63527649963def8c759b0f91f2eb69a40934e468/tiktoken/registry.py#L63)ç”Ÿæˆ`tokenizer.model`æ–‡ä»¶ï¼Œå†ä½¿ç”¨[`convert_tiktoken_to_fast`]å‡½æ•°å°†å…¶è½¬æ¢ä¸º`tokenizer.json`æ–‡ä»¶ã€‚

```py

from transformers.integrations.tiktoken import convert_tiktoken_to_fast
from tiktoken import get_encoding

# You can load your custom encoding or the one provided by OpenAI
encoding = get_encoding("gpt2")
convert_tiktoken_to_fast(encoding, "config/save/dir")
```

ç”Ÿæˆçš„`tokenizer.json`æ–‡ä»¶å°†è¢«ä¿å­˜åˆ°æŒ‡å®šçš„ç›®å½•ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡[`PreTrainedTokenizerFast`]ç±»æ¥åŠ è½½ã€‚

```py
tokenizer = PreTrainedTokenizerFast.from_pretrained("config/save/dir")
```

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\tokenizer_summary.md
============================================================



# åˆ†è¯å™¨çš„æ‘˜è¦
[[open-in-colab]]

åœ¨è¿™ä¸ªé¡µé¢ï¼Œæˆ‘ä»¬æ¥ä»”ç»†ç ”ç©¶åˆ†è¯çš„çŸ¥è¯†ã€‚
<Youtube id="VFp38yj8h3A"/>

æ­£å¦‚æˆ‘ä»¬åœ¨[the preprocessing tutorial](preprocessing)æ‰€çœ‹åˆ°çš„é‚£æ ·ï¼Œå¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯å°±æ˜¯å°†ä¸€æ®µæ–‡æœ¬åˆ†å‰²æˆå¾ˆå¤šå•è¯æˆ–è€…å­å•è¯ï¼Œ
è¿™äº›å•è¯æˆ–è€…å­å•è¯ç„¶åä¼šé€šè¿‡ä¸€ä¸ªæŸ¥è¯¢è¡¨æ ¼è¢«è½¬æ¢åˆ°idï¼Œå°†å•è¯æˆ–è€…å­å•è¯è½¬æ¢åˆ°idæ˜¯å¾ˆç›´æˆªäº†å½“çš„ï¼Œä¹Ÿå°±æ˜¯ä¸€ä¸ªç®€å•çš„æ˜ å°„ï¼Œ
æ‰€ä»¥è¿™ä¹ˆæ¥çœ‹ï¼Œæˆ‘ä»¬ä¸»è¦å…³æ³¨å°†ä¸€æ®µæ–‡æœ¬åˆ†å‰²æˆå¾ˆå¤šå•è¯æˆ–è€…å¾ˆå¤šå­å•è¯ï¼ˆåƒï¼šå¯¹ä¸€æ®µæ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼‰ï¼Œæ›´åŠ å‡†ç¡®çš„æ¥è¯´ï¼Œæˆ‘ä»¬å°†å…³æ³¨
åœ¨ğŸ¤— Transformerså†…ç”¨åˆ°çš„ä¸‰ç§ä¸»è¦ç±»å‹çš„åˆ†è¯å™¨ï¼š[Byte-Pair Encoding (BPE)](#byte-pair-encoding), [WordPiece](#wordpiece), 
and [SentencePiece](#sentencepiece)ï¼Œå¹¶ä¸”ç»™å‡ºäº†ç¤ºä¾‹ï¼Œå“ªä¸ªæ¨¡å‹ç”¨åˆ°äº†å“ªç§ç±»å‹çš„åˆ†è¯å™¨ã€‚

æ³¨æ„åˆ°åœ¨æ¯ä¸ªæ¨¡å‹çš„ä¸»é¡µï¼Œä½ å¯ä»¥æŸ¥çœ‹æ–‡æ¡£ä¸Šç›¸å…³çš„åˆ†è¯å™¨ï¼Œå°±å¯ä»¥çŸ¥é“é¢„è®­ç»ƒæ¨¡å‹ä½¿ç”¨äº†å“ªç§ç±»å‹çš„åˆ†è¯å™¨ã€‚
ä¸¾ä¸ªä¾‹å­ï¼Œå¦‚æœæˆ‘ä»¬æŸ¥çœ‹[`BertTokenizer`]ï¼Œæˆ‘ä»¬å°±èƒ½çœ‹åˆ°æ¨¡å‹ä½¿ç”¨äº†[WordPiece](#wordpiece)ã€‚

## ä»‹ç»
å°†ä¸€æ®µæ–‡æœ¬åˆ†è¯åˆ°å°å—æ˜¯ä¸€ä¸ªæ¯”å®ƒçœ‹èµ·æ¥æ›´åŠ å›°éš¾çš„ä»»åŠ¡ï¼Œå¹¶ä¸”æœ‰å¾ˆå¤šæ–¹å¼æ¥å®ç°åˆ†è¯ï¼Œä¸¾ä¸ªä¾‹å­ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªå¥å­
`"Don't you love ğŸ¤— Transformers? We sure do."`

<Youtube id="nhJxYji1aho"/>

å¯¹è¿™æ®µæ–‡æœ¬åˆ†è¯çš„ä¸€ä¸ªç®€å•æ–¹å¼ï¼Œå°±æ˜¯ä½¿ç”¨ç©ºæ ¼æ¥åˆ†è¯ï¼Œå¾—åˆ°çš„ç»“æœæ˜¯ï¼š

```
["Don't", "you", "love", "ğŸ¤—", "Transformers?", "We", "sure", "do."]
```

ä¸Šé¢çš„åˆ†è¯æ˜¯ä¸€ä¸ªæ˜æ™ºçš„å¼€å§‹ï¼Œä½†æ˜¯å¦‚æœæˆ‘ä»¬æŸ¥çœ‹token `"Transformers?"` å’Œ `"do."`ï¼Œæˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°æ ‡ç‚¹ç¬¦å·é™„åœ¨å•è¯`"Transformer"` 
å’Œ `"do"`çš„åé¢ï¼Œè¿™å¹¶ä¸æ˜¯æœ€ç†æƒ³çš„æƒ…å†µã€‚æˆ‘ä»¬åº”è¯¥å°†æ ‡ç‚¹ç¬¦å·è€ƒè™‘è¿›æ¥ï¼Œè¿™æ ·ä¸€ä¸ªæ¨¡å‹å°±æ²¡å¿…è¦å­¦ä¹ ä¸€ä¸ªå•è¯å’Œæ¯ä¸ªå¯èƒ½è·Ÿåœ¨åé¢çš„
æ ‡ç‚¹ç¬¦å·çš„ä¸åŒçš„ç»„åˆï¼Œè¿™ä¹ˆç»„åˆçš„è¯ï¼Œæ¨¡å‹éœ€è¦å­¦ä¹ çš„ç»„åˆçš„æ•°é‡ä¼šæ€¥å‰§ä¸Šå‡ã€‚å°†æ ‡ç‚¹ç¬¦å·ä¹Ÿè€ƒè™‘è¿›æ¥ï¼Œå¯¹èŒƒä¾‹æ–‡æœ¬è¿›è¡Œåˆ†è¯çš„ç»“æœå°±æ˜¯ï¼š

```
["Don", "'", "t", "you", "love", "ğŸ¤—", "Transformers", "?", "We", "sure", "do", "."]
```

åˆ†è¯çš„ç»“æœæ›´å¥½äº†ï¼Œç„¶è€Œï¼Œè¿™ä¹ˆåšä¹Ÿæ˜¯ä¸å¥½çš„ï¼Œåˆ†è¯æ€ä¹ˆå¤„ç†å•è¯`"Don't"`ï¼Œ`"Don't"`çš„å«ä¹‰æ˜¯`"do not"`ï¼Œæ‰€ä»¥è¿™ä¹ˆåˆ†è¯`["Do", "n't"]`
ä¼šæ›´å¥½ã€‚ç°åœ¨å¼€å§‹äº‹æƒ…å°±å¼€å§‹å˜å¾—å¤æ‚èµ·æ¥äº†ï¼Œéƒ¨åˆ†çš„åŸå› æ˜¯æ¯ä¸ªæ¨¡å‹éƒ½æœ‰å®ƒè‡ªå·±çš„åˆ†è¯ç±»å‹ã€‚ä¾èµ–äºæˆ‘ä»¬åº”ç”¨åœ¨æ–‡æœ¬åˆ†è¯ä¸Šçš„è§„åˆ™ï¼Œ
ç›¸åŒçš„æ–‡æœ¬ä¼šäº§ç”Ÿä¸åŒçš„åˆ†è¯è¾“å‡ºã€‚ç”¨åœ¨è®­ç»ƒæ•°æ®ä¸Šçš„åˆ†è¯è§„åˆ™ï¼Œè¢«ç”¨æ¥å¯¹è¾“å…¥åšåˆ†è¯æ“ä½œï¼Œä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹æ‰ä¼šæ­£ç¡®çš„æ‰§è¡Œã€‚

[spaCy](https://spacy.io/) and [Moses](http://www.statmt.org/moses/?n=Development.GetStarted) æ˜¯ä¸¤ä¸ªå—æ¬¢è¿çš„åŸºäºè§„åˆ™çš„
åˆ†è¯å™¨ã€‚å°†è¿™ä¸¤ä¸ªåˆ†è¯å™¨åº”ç”¨åœ¨ç¤ºä¾‹æ–‡æœ¬ä¸Šï¼Œ*spaCy* å’Œ *Moses*ä¼šè¾“å‡ºç±»ä¼¼ä¸‹é¢çš„ç»“æœï¼š

```
["Do", "n't", "you", "love", "ğŸ¤—", "Transformers", "?", "We", "sure", "do", "."]
```

å¯è§ä¸Šé¢çš„åˆ†è¯ä½¿ç”¨åˆ°äº†ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·çš„åˆ†è¯æ–¹å¼ï¼Œä»¥åŠåŸºäºè§„åˆ™çš„åˆ†è¯æ–¹å¼ã€‚ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·åˆ†è¯ä»¥åŠåŸºäºè§„åˆ™çš„åˆ†è¯éƒ½æ˜¯å•è¯åˆ†è¯çš„ä¾‹å­ã€‚
ä¸é‚£ä¹ˆä¸¥æ ¼çš„æ¥è¯´ï¼Œå•è¯åˆ†è¯çš„å®šä¹‰å°±æ˜¯å°†å¥å­åˆ†å‰²åˆ°å¾ˆå¤šå•è¯ã€‚ç„¶è€Œå°†æ–‡æœ¬åˆ†å‰²åˆ°æ›´å°çš„å—æ˜¯ç¬¦åˆç›´è§‰çš„ï¼Œå½“å¤„ç†å¤§å‹æ–‡æœ¬è¯­æ–™åº“æ—¶ï¼Œä¸Šé¢çš„
åˆ†è¯æ–¹æ³•ä¼šå¯¼è‡´å¾ˆå¤šé—®é¢˜ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·åˆ†è¯é€šå¸¸ä¼šäº§ç”Ÿä¸€ä¸ªéå¸¸å¤§çš„è¯å…¸ï¼ˆä½¿ç”¨åˆ°çš„æ‰€æœ‰ä¸é‡å¤çš„å•è¯å’Œtokensçš„é›†åˆï¼‰ã€‚
åƒï¼š[Transformer XL](model_doc/transformerxl)ä½¿ç”¨ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·åˆ†è¯ï¼Œç»“æœä¼šäº§ç”Ÿä¸€ä¸ªå¤§å°æ˜¯267,735çš„è¯å…¸ï¼

è¿™ä¹ˆå¤§çš„ä¸€ä¸ªè¯å…¸å®¹é‡ï¼Œè¿«ä½¿æ¨¡å‹æœ‰ç€ä¸€ä¸ªå·¨å¤§çš„embeddingçŸ©é˜µï¼Œä»¥åŠå·¨å¤§çš„è¾“å…¥å’Œè¾“å‡ºå±‚ï¼Œè¿™ä¼šå¢åŠ å†…å­˜ä½¿ç”¨é‡ï¼Œä¹Ÿä¼šæé«˜æ—¶é—´å¤æ‚åº¦ã€‚é€šå¸¸
æƒ…å†µä¸‹ï¼Œtransformersæ¨¡å‹å‡ ä¹æ²¡æœ‰è¯å…¸å®¹é‡å¤§äº50,000çš„ï¼Œç‰¹åˆ«æ˜¯åªåœ¨ä¸€ç§è¯­è¨€ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹ã€‚

æ‰€ä»¥å¦‚æœç®€å•çš„ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·åˆ†è¯è®©äººä¸æ»¡æ„ï¼Œä¸ºä»€ä¹ˆä¸ç®€å•çš„å¯¹å­—ç¬¦åˆ†è¯ï¼Ÿ

<Youtube id="ssLq_EK2jLE"/>

å°½ç®¡å­—ç¬¦åˆ†è¯æ˜¯éå¸¸ç®€å•çš„ï¼Œå¹¶ä¸”èƒ½æå¤§çš„å‡å°‘å†…å­˜ä½¿ç”¨ï¼Œé™ä½æ—¶é—´å¤æ‚åº¦ï¼Œä½†æ˜¯è¿™æ ·åšä¼šè®©æ¨¡å‹å¾ˆéš¾å­¦åˆ°æœ‰æ„ä¹‰çš„è¾“å…¥è¡¨è¾¾ã€‚åƒï¼š
æ¯”èµ·å­¦åˆ°å•è¯`"today"`çš„ä¸€ä¸ªæœ‰æ„ä¹‰çš„ä¸Šä¸‹æ–‡ç‹¬ç«‹çš„è¡¨è¾¾ï¼Œå­¦åˆ°å­—æ¯`"t"`çš„ä¸€ä¸ªæœ‰æ„ä¹‰çš„ä¸Šä¸‹æ–‡ç‹¬ç«‹çš„è¡¨è¾¾æ˜¯ç›¸å½“å›°éš¾çš„ã€‚å› æ­¤ï¼Œ
å­—ç¬¦åˆ†è¯ç»å¸¸ä¼šä¼´éšç€æ€§èƒ½çš„ä¸‹é™ã€‚æ‰€ä»¥ä¸ºäº†è·å¾—æœ€å¥½çš„ç»“æœï¼Œtransformersæ¨¡å‹åœ¨å•è¯çº§åˆ«åˆ†è¯å’Œå­—ç¬¦çº§åˆ«åˆ†è¯ä¹‹é—´ä½¿ç”¨äº†ä¸€ä¸ªæŠ˜ä¸­çš„æ–¹æ¡ˆ
è¢«ç§°ä½œ**å­è¯**åˆ†è¯ã€‚

## å­è¯åˆ†è¯

<Youtube id="zHvTiHr506c"/>

å­è¯åˆ†è¯ç®—æ³•ä¾èµ–è¿™æ ·çš„åŸåˆ™ï¼šé¢‘ç¹ä½¿ç”¨çš„å•è¯ä¸åº”è¯¥è¢«åˆ†å‰²æˆæ›´å°çš„å­è¯ï¼Œä½†æ˜¯å¾ˆå°‘ä½¿ç”¨çš„å•è¯åº”è¯¥è¢«åˆ†è§£åˆ°æœ‰æ„ä¹‰çš„å­è¯ã€‚ä¸¾ä¸ªä¾‹å­ï¼š
`"annoyingly"`èƒ½è¢«çœ‹ä½œä¸€ä¸ªå¾ˆå°‘ä½¿ç”¨çš„å•è¯ï¼Œèƒ½è¢«åˆ†è§£æˆ`"annoying"`å’Œ`"ly"`ã€‚`"annoying"`å’Œ`"ly"`ä½œä¸ºç‹¬ç«‹åœ°å­è¯ï¼Œå‡ºç°
çš„æ¬¡æ•°éƒ½å¾ˆé¢‘ç¹ï¼Œè€Œä¸”ä¸æ­¤åŒæ—¶å•è¯`"annoyingly"`çš„å«ä¹‰å¯ä»¥é€šè¿‡ç»„åˆ`"annoying"`å’Œ`"ly"`çš„å«ä¹‰æ¥è·å¾—ã€‚åœ¨ç²˜åˆå’Œèƒ¶æ°´è¯­è¨€ä¸Šï¼Œ
åƒTurkishè¯­è¨€ï¼Œè¿™ä¹ˆåšæ˜¯ç›¸å½“æœ‰ç”¨çš„ï¼Œåœ¨è¿™æ ·çš„è¯­è¨€é‡Œï¼Œé€šè¿‡çº¿æ€§ç»„åˆå­è¯ï¼Œå¤§å¤šæ•°æƒ…å†µä¸‹ä½ èƒ½å½¢æˆä»»æ„é•¿çš„å¤æ‚çš„å•è¯ã€‚

å­è¯åˆ†è¯å…è®¸æ¨¡å‹æœ‰ä¸€ä¸ªåˆç†çš„è¯å…¸å¤§å°ï¼Œè€Œä¸”èƒ½å­¦åˆ°æœ‰æ„ä¹‰çš„ä¸Šä¸‹æ–‡ç‹¬ç«‹åœ°è¡¨è¾¾ã€‚é™¤æ­¤ä»¥å¤–ï¼Œå­è¯åˆ†è¯å¯ä»¥è®©æ¨¡å‹å¤„ç†ä»¥å‰ä»æ¥æ²¡è§è¿‡çš„å•è¯ï¼Œ
æ–¹å¼æ˜¯é€šè¿‡åˆ†è§£è¿™äº›å•è¯åˆ°å·²çŸ¥çš„å­è¯ï¼Œä¸¾ä¸ªä¾‹å­ï¼š[`~transformers.BertTokenizer`]å¯¹å¥å­`"I have a new GPU!"`åˆ†è¯çš„ç»“æœå¦‚ä¸‹ï¼š

```py
>>> from transformers import BertTokenizer

>>> tokenizer = BertTokenizer.from_pretrained("google-bert/bert-base-uncased")
>>> tokenizer.tokenize("I have a new GPU!")
["i", "have", "a", "new", "gp", "##u", "!"]
```

å› ä¸ºæˆ‘ä»¬æ­£åœ¨è€ƒè™‘ä¸åŒºåˆ†å¤§å°å†™çš„æ¨¡å‹ï¼Œå¥å­é¦–å…ˆè¢«è½¬æ¢æˆå°å†™å­—æ¯å½¢å¼ã€‚æˆ‘ä»¬å¯ä»¥è§åˆ°å•è¯`["i", "have", "a", "new"]`åœ¨åˆ†è¯å™¨
çš„è¯å…¸å†…ï¼Œä½†æ˜¯è¿™ä¸ªå•è¯`"gpu"`ä¸åœ¨è¯å…¸å†…ã€‚æ‰€ä»¥ï¼Œåˆ†è¯å™¨å°†`"gpu"`åˆ†å‰²æˆå·²çŸ¥çš„å­è¯`["gp" and "##u"]`ã€‚`"##"`æ„å‘³ç€å‰©ä¸‹çš„
tokenåº”è¯¥é™„ç€åœ¨å‰é¢é‚£ä¸ªtokençš„åé¢ï¼Œä¸å¸¦ç©ºæ ¼çš„é™„ç€ï¼ˆåˆ†è¯çš„è§£ç æˆ–è€…åå‘ï¼‰ã€‚

å¦å¤–ä¸€ä¸ªä¾‹å­ï¼Œ[`~transformers.XLNetTokenizer`]å¯¹å‰é¢çš„æ–‡æœ¬ä¾‹å­åˆ†è¯ç»“æœå¦‚ä¸‹ï¼š

```py
>>> from transformers import XLNetTokenizer

>>> tokenizer = XLNetTokenizer.from_pretrained("xlnet/xlnet-base-cased")
>>> tokenizer.tokenize("Don't you love ğŸ¤— Transformers? We sure do.")
["â–Don", "'", "t", "â–you", "â–love", "â–", "ğŸ¤—", "â–", "Transform", "ers", "?", "â–We", "â–sure", "â–do", "."]
```

å½“æˆ‘ä»¬æŸ¥çœ‹[SentencePiece](#sentencepiece)æ—¶ä¼šå›è¿‡å¤´æ¥è§£é‡Šè¿™äº›`"â–"`ç¬¦å·çš„å«ä¹‰ã€‚æ­£å¦‚ä½ èƒ½è§åˆ°çš„ï¼Œå¾ˆå°‘ä½¿ç”¨çš„å•è¯
`"Transformers"`èƒ½è¢«åˆ†å‰²åˆ°æ›´åŠ é¢‘ç¹ä½¿ç”¨çš„å­è¯`"Transform"`å’Œ`"ers"`ã€‚

ç°åœ¨è®©æˆ‘ä»¬æ¥çœ‹çœ‹ä¸åŒçš„å­è¯åˆ†å‰²ç®—æ³•æ˜¯æ€ä¹ˆå·¥ä½œçš„ï¼Œæ³¨æ„åˆ°æ‰€æœ‰çš„è¿™äº›åˆ†è¯ç®—æ³•ä¾èµ–äºæŸäº›è®­ç»ƒçš„æ–¹å¼ï¼Œè¿™äº›è®­ç»ƒé€šå¸¸åœ¨è¯­æ–™åº“ä¸Šå®Œæˆï¼Œ
ç›¸åº”çš„æ¨¡å‹ä¹Ÿæ˜¯åœ¨è¿™ä¸ªè¯­æ–™åº“ä¸Šè®­ç»ƒçš„ã€‚

<a id='byte-pair-encoding'></a>

### Byte-Pair Encoding (BPE)

Byte-Pair Encoding (BPE)æ¥è‡ªäº[Neural Machine Translation of Rare Words with Subword Units (Sennrich et
al., 2015)](https://huggingface.co/papers/1508.07909)ã€‚BPEä¾èµ–äºä¸€ä¸ªé¢„åˆ†è¯å™¨ï¼Œè¿™ä¸ªé¢„åˆ†è¯å™¨ä¼šå°†è®­ç»ƒæ•°æ®åˆ†å‰²æˆå•è¯ã€‚é¢„åˆ†è¯å¯ä»¥æ˜¯ç®€å•çš„
ç©ºæ ¼åˆ†è¯ï¼Œåƒï¼šï¼š[GPT-2](model_doc/gpt2)ï¼Œ[RoBERTa](model_doc/roberta)ã€‚æ›´åŠ å…ˆè¿›çš„é¢„åˆ†è¯æ–¹å¼åŒ…æ‹¬äº†åŸºäºè§„åˆ™çš„åˆ†è¯ï¼Œåƒï¼š [XLM](model_doc/xlm)ï¼Œ[FlauBERT](model_doc/flaubert)ï¼ŒFlauBERTåœ¨å¤§å¤šæ•°è¯­è¨€ä½¿ç”¨äº†Mosesï¼Œæˆ–è€…[GPT](model_doc/gpt)ï¼ŒGPT
ä½¿ç”¨äº†Spacyå’Œftfyï¼Œç»Ÿè®¡äº†è®­ç»ƒè¯­æ–™åº“ä¸­æ¯ä¸ªå•è¯çš„é¢‘æ¬¡ã€‚

åœ¨é¢„åˆ†è¯ä»¥åï¼Œç”Ÿæˆäº†å•è¯çš„é›†åˆï¼Œä¹Ÿç¡®å®šäº†è®­ç»ƒæ•°æ®ä¸­æ¯ä¸ªå•è¯å‡ºç°çš„é¢‘æ¬¡ã€‚ä¸‹ä¸€æ­¥ï¼ŒBPEäº§ç”Ÿäº†ä¸€ä¸ªåŸºç¡€è¯å…¸ï¼ŒåŒ…å«äº†é›†åˆä¸­æ‰€æœ‰çš„ç¬¦å·ï¼Œ
BPEå­¦ä¹ èåˆçš„è§„åˆ™-ç»„åˆåŸºç¡€è¯å…¸ä¸­çš„ä¸¤ä¸ªç¬¦å·æ¥å½¢æˆä¸€ä¸ªæ–°çš„ç¬¦å·ã€‚BPEä¼šä¸€ç›´å­¦ä¹ ç›´åˆ°è¯å…¸çš„å¤§å°æ»¡è¶³äº†æœŸæœ›çš„è¯å…¸å¤§å°çš„è¦æ±‚ã€‚æ³¨æ„åˆ°
æœŸæœ›çš„è¯å…¸å¤§å°æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œåœ¨è®­ç»ƒè¿™ä¸ªåˆ†è¯å™¨ä»¥å‰å°±éœ€è¦äººä¸ºæŒ‡å®šã€‚

ä¸¾ä¸ªä¾‹å­ï¼Œè®©æˆ‘ä»¬å‡è®¾åœ¨é¢„åˆ†è¯ä»¥åï¼Œä¸‹é¢çš„å•è¯é›†åˆä»¥åŠä»–ä»¬çš„é¢‘æ¬¡éƒ½å·²ç»ç¡®å®šå¥½äº†ï¼š

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

æ‰€ä»¥ï¼ŒåŸºç¡€çš„è¯å…¸æ˜¯`["b", "g", "h", "n", "p", "s", "u"]`ã€‚å°†æ‰€æœ‰å•è¯åˆ†å‰²æˆåŸºç¡€è¯å…¸å†…çš„ç¬¦å·ï¼Œå°±å¯ä»¥è·å¾—ï¼š

```
("h" "u" "g", 10), ("p" "u" "g", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "u" "g" "s", 5)
```
BPEæ¥ç€ä¼šç»Ÿè®¡æ¯ä¸ªå¯èƒ½çš„ç¬¦å·å¯¹çš„é¢‘æ¬¡ï¼Œç„¶åæŒ‘å‡ºå‡ºç°æœ€é¢‘ç¹çš„çš„ç¬¦å·å¯¹ï¼Œåœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œ`"h"`è·Ÿäº†`"u"`å‡ºç°äº†10 + 5 = 15æ¬¡
ï¼ˆ10æ¬¡æ˜¯å‡ºç°äº†10æ¬¡`"hug"`ï¼Œ5æ¬¡æ˜¯å‡ºç°äº†5æ¬¡`"hugs"`ï¼‰ã€‚ç„¶è€Œï¼Œæœ€é¢‘ç¹çš„ç¬¦å·å¯¹æ˜¯`"u"`åé¢è·Ÿäº†ä¸ª`"g"`ï¼Œæ€»å…±å‡ºç°äº†10 + 5 + 5
= 20æ¬¡ã€‚å› æ­¤ï¼Œåˆ†è¯å™¨å­¦åˆ°çš„ç¬¬ä¸€ä¸ªèåˆè§„åˆ™æ˜¯ç»„åˆæ‰€æœ‰çš„`"u"`åé¢è·Ÿäº†ä¸ª`"g"`ç¬¦å·ã€‚ä¸‹ä¸€æ­¥ï¼Œ`"ug"`è¢«åŠ å…¥åˆ°äº†è¯å…¸å†…ã€‚å•è¯çš„é›†åˆ
å°±å˜æˆäº†ï¼š

```
("h" "ug", 10), ("p" "ug", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "ug" "s", 5)
```

BPEæ¥ç€ä¼šç»Ÿè®¡å‡ºä¸‹ä¸€ä¸ªæœ€æ™®éçš„å‡ºç°é¢‘æ¬¡æœ€å¤§çš„ç¬¦å·å¯¹ã€‚ä¹Ÿå°±æ˜¯`"u"`åé¢è·Ÿäº†ä¸ª`"n"`ï¼Œå‡ºç°äº†16æ¬¡ã€‚`"u"`ï¼Œ`"n"`è¢«èåˆæˆäº†`"un"`ã€‚
ä¹Ÿè¢«åŠ å…¥åˆ°äº†è¯å…¸ä¸­ï¼Œå†ä¸‹ä¸€ä¸ªå‡ºç°é¢‘æ¬¡æœ€å¤§çš„ç¬¦å·å¯¹æ˜¯`"h"`åé¢è·Ÿäº†ä¸ª`"ug"`ï¼Œå‡ºç°äº†15æ¬¡ã€‚åˆä¸€æ¬¡è¿™ä¸ªç¬¦å·å¯¹è¢«èåˆæˆäº†`"hug"`ï¼Œ
ä¹Ÿè¢«åŠ å…¥åˆ°äº†è¯å…¸ä¸­ã€‚

åœ¨å½“å‰è¿™æ­¥ï¼Œè¯å…¸æ˜¯`["b", "g", "h", "n", "p", "s", "u", "ug", "un", "hug"]`ï¼Œæˆ‘ä»¬çš„å•è¯é›†åˆåˆ™æ˜¯ï¼š

```
("hug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("hug" "s", 5)
```

å‡è®¾ï¼Œthe Byte-Pair Encodingåœ¨è¿™ä¸ªæ—¶å€™åœæ­¢è®­ç»ƒï¼Œå­¦åˆ°çš„èåˆè§„åˆ™å¹¶åº”ç”¨åˆ°å…¶ä»–æ–°çš„å•è¯ä¸Šï¼ˆåªè¦è¿™äº›æ–°å•è¯ä¸åŒ…æ‹¬ä¸åœ¨åŸºç¡€è¯å…¸å†…çš„ç¬¦å·
å°±è¡Œï¼‰ã€‚ä¸¾ä¸ªä¾‹å­ï¼Œå•è¯`"bug"`ä¼šè¢«åˆ†è¯åˆ°`["b", "ug"]`ï¼Œä½†æ˜¯`"mug"`ä¼šè¢«åˆ†è¯åˆ°`["<unk>", "ug"]`ï¼Œå› ä¸ºç¬¦å·`"m"`ä¸åœ¨åŸºç¡€è¯å…¸å†…ã€‚
é€šå¸¸æ¥çœ‹çš„è¯ï¼Œå•ä¸ªå­—æ¯åƒ`"m"`ä¸ä¼šè¢«`"<unk>"`ç¬¦å·æ›¿æ¢æ‰ï¼Œå› ä¸ºè®­ç»ƒæ•°æ®é€šå¸¸åŒ…æ‹¬äº†æ¯ä¸ªå­—æ¯ï¼Œæ¯ä¸ªå­—æ¯è‡³å°‘å‡ºç°äº†ä¸€æ¬¡ï¼Œä½†æ˜¯åœ¨ç‰¹æ®Šçš„ç¬¦å·
ä¸­ä¹Ÿå¯èƒ½å‘ç”Ÿåƒemojisã€‚

å°±åƒä¹‹å‰æåˆ°çš„é‚£æ ·ï¼Œè¯å…¸çš„å¤§å°ï¼Œä¸¾ä¸ªä¾‹å­ï¼ŒåŸºç¡€è¯å…¸çš„å¤§å° + èåˆçš„æ•°é‡ï¼Œæ˜¯ä¸€ä¸ªéœ€è¦é…ç½®çš„è¶…å‚æ•°ã€‚ä¸¾ä¸ªä¾‹å­ï¼š[GPT](model_doc/gpt)
çš„è¯å…¸å¤§å°æ˜¯40,478ï¼Œå› ä¸ºGPTæœ‰ç€478ä¸ªåŸºç¡€è¯å…¸å†…çš„å­—ç¬¦ï¼Œåœ¨40,000æ¬¡èåˆä»¥åé€‰æ‹©äº†åœæ­¢è®­ç»ƒã€‚

#### Byte-level BPE

ä¸€ä¸ªåŒ…å«äº†æ‰€æœ‰å¯èƒ½çš„åŸºç¡€å­—ç¬¦çš„åŸºç¡€å­—å…¸å¯èƒ½ä¼šéå¸¸å¤§ï¼Œå¦‚æœè€ƒè™‘å°†æ‰€æœ‰çš„unicodeå­—ç¬¦ä½œä¸ºåŸºç¡€å­—ç¬¦ã€‚ä¸ºäº†æ‹¥æœ‰ä¸€ä¸ªæ›´å¥½çš„åŸºç¡€è¯å…¸ï¼Œ[GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)ä½¿ç”¨äº†å­—èŠ‚
ä½œä¸ºåŸºç¡€è¯å…¸ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸èªæ˜çš„æŠ€å·§ï¼Œè¿«ä½¿åŸºç¡€è¯å…¸æ˜¯256å¤§å°ï¼Œè€Œä¸”ç¡®ä¿äº†æ‰€æœ‰åŸºç¡€å­—ç¬¦åŒ…å«åœ¨è¿™ä¸ªè¯å…¸å†…ã€‚ä½¿ç”¨äº†å…¶ä»–çš„è§„åˆ™
æ¥å¤„ç†æ ‡ç‚¹ç¬¦å·ï¼Œè¿™ä¸ªGPT2çš„åˆ†è¯å™¨èƒ½å¯¹æ¯ä¸ªæ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Œä¸éœ€è¦ä½¿ç”¨åˆ°<unk>ç¬¦å·ã€‚[GPT-2](model_doc/gpt)æœ‰ä¸€ä¸ªå¤§å°æ˜¯50,257
çš„è¯å…¸ï¼Œå¯¹åº”åˆ°256å­—èŠ‚çš„åŸºç¡€tokensï¼Œä¸€ä¸ªç‰¹æ®Šçš„æ–‡æœ¬ç»“æŸtokenï¼Œè¿™äº›ç¬¦å·ç»è¿‡äº†50,000æ¬¡èåˆå­¦ä¹ ã€‚

<a id='wordpiece'></a>

### WordPiece

WordPieceæ˜¯å­è¯åˆ†è¯ç®—æ³•ï¼Œè¢«ç”¨åœ¨[BERT](model_doc/bert)ï¼Œ[DistilBERT](model_doc/distilbert)ï¼Œå’Œ[Electra](model_doc/electra)ã€‚
è¿™ä¸ªç®—æ³•å‘å¸ƒåœ¨[Japanese and Korean
Voice Search (Schuster et al., 2012)](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf)
å’ŒBPEéå¸¸ç›¸ä¼¼ã€‚WordPieceé¦–å…ˆåˆå§‹åŒ–ä¸€ä¸ªè¯å…¸ï¼Œè¿™ä¸ªè¯å…¸åŒ…å«äº†å‡ºç°åœ¨è®­ç»ƒæ•°æ®ä¸­çš„æ¯ä¸ªå­—ç¬¦ï¼Œç„¶åé€’è¿›çš„å­¦ä¹ ä¸€ä¸ªç»™å®šæ•°é‡çš„èåˆè§„åˆ™ã€‚å’ŒBPEç›¸æ¯”è¾ƒï¼Œ
WordPieceä¸ä¼šé€‰æ‹©å‡ºç°é¢‘æ¬¡æœ€å¤§çš„ç¬¦å·å¯¹ï¼Œè€Œæ˜¯é€‰æ‹©äº†åŠ å…¥åˆ°å­—å…¸ä»¥åèƒ½æœ€å¤§åŒ–è®­ç»ƒæ•°æ®ä¼¼ç„¶å€¼çš„ç¬¦å·å¯¹ã€‚

æ‰€ä»¥è¿™åˆ°åº•æ„å‘³ç€ä»€ä¹ˆï¼Ÿå‚è€ƒå‰é¢çš„ä¾‹å­ï¼Œæœ€å¤§åŒ–è®­ç»ƒæ•°æ®çš„ä¼¼ç„¶å€¼ï¼Œç­‰ä»·äºæ‰¾åˆ°ä¸€ä¸ªç¬¦å·å¯¹ï¼Œå®ƒä»¬çš„æ¦‚ç‡é™¤ä»¥è¿™ä¸ªç¬¦å·å¯¹ä¸­ç¬¬ä¸€ä¸ªç¬¦å·çš„æ¦‚ç‡ï¼Œ
æ¥ç€é™¤ä»¥ç¬¬äºŒä¸ªç¬¦å·çš„æ¦‚ç‡ï¼Œåœ¨æ‰€æœ‰çš„ç¬¦å·å¯¹ä¸­å•†æœ€å¤§ã€‚åƒï¼šå¦‚æœ`"ug"`çš„æ¦‚ç‡é™¤ä»¥`"u"`é™¤ä»¥`"g"`çš„æ¦‚ç‡çš„å•†ï¼Œæ¯”å…¶ä»–ä»»ä½•ç¬¦å·å¯¹æ›´å¤§ï¼Œ
è¿™ä¸ªæ—¶å€™æ‰èƒ½èåˆ`"u"`å’Œ`"g"`ã€‚ç›´è§‰ä¸Šï¼ŒWordPieceï¼Œå’ŒBPEæœ‰ç‚¹ç‚¹ä¸åŒï¼ŒWordPieceæ˜¯è¯„ä¼°èåˆä¸¤ä¸ªç¬¦å·ä¼šå¤±å»çš„é‡ï¼Œæ¥ç¡®ä¿è¿™ä¹ˆåšæ˜¯å€¼å¾—çš„ã€‚

<a id='unigram'></a>

### Unigram

Unigramæ˜¯ä¸€ä¸ªå­è¯åˆ†è¯å™¨ç®—æ³•ï¼Œä»‹ç»è§[Subword Regularization: Improving Neural Network Translation
Models with Multiple Subword Candidates (Kudo, 2018)](https://huggingface.co/papers/1804.10959)ã€‚å’ŒBPEæˆ–è€…WordPieceç›¸æ¯”è¾ƒ
ï¼ŒUnigramä½¿ç”¨å¤§é‡çš„ç¬¦å·æ¥åˆå§‹åŒ–å®ƒçš„åŸºç¡€å­—å…¸ï¼Œç„¶åé€æ¸çš„ç²¾ç®€æ¯ä¸ªç¬¦å·æ¥è·å¾—ä¸€ä¸ªæ›´å°çš„è¯å…¸ã€‚ä¸¾ä¾‹æ¥çœ‹åŸºç¡€è¯å…¸èƒ½å¤Ÿå¯¹åº”æ‰€æœ‰çš„é¢„åˆ†è¯
çš„å•è¯ä»¥åŠæœ€å¸¸è§çš„å­å­—ç¬¦ä¸²ã€‚Unigramæ²¡æœ‰ç›´æ¥ç”¨åœ¨ä»»ä½•transformersçš„ä»»ä½•æ¨¡å‹ä¸­ï¼Œä½†æ˜¯å’Œ[SentencePiece](#sentencepiece)ä¸€èµ·è”åˆä½¿ç”¨ã€‚

åœ¨æ¯ä¸ªè®­ç»ƒçš„æ­¥éª¤ï¼ŒUnigramç®—æ³•åœ¨å½“å‰è¯å…¸çš„è®­ç»ƒæ•°æ®ä¸Šå®šä¹‰äº†ä¸€ä¸ªæŸå¤±å‡½æ•°ï¼ˆç»å¸¸å®šä¹‰ä¸ºlogä¼¼ç„¶å‡½æ•°çš„ï¼‰ï¼Œè¿˜å®šä¹‰äº†ä¸€ä¸ªunigramè¯­è¨€æ¨¡å‹ã€‚
ç„¶åï¼Œå¯¹è¯å…¸å†…çš„æ¯ä¸ªç¬¦å·ï¼Œç®—æ³•ä¼šè®¡ç®—å¦‚æœè¿™ä¸ªç¬¦å·ä»è¯å…¸å†…ç§»é™¤ï¼Œæ€»çš„æŸå¤±ä¼šå‡é«˜å¤šå°‘ã€‚Unigramç„¶åä¼šç§»é™¤ç™¾åˆ†ä¹‹pçš„ç¬¦å·ï¼Œè¿™äº›ç¬¦å·çš„loss
å‡é«˜æ˜¯æœ€ä½çš„ï¼ˆpé€šå¸¸æ˜¯10%æˆ–è€…20%ï¼‰ï¼Œåƒï¼šè¿™äº›åœ¨è®­ç»ƒæ•°æ®ä¸Šå¯¹æ€»çš„æŸå¤±å½±å“æœ€å°çš„ç¬¦å·ã€‚é‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œç›´åˆ°è¯å…¸å·²ç»è¾¾åˆ°äº†æœŸæœ›çš„å¤§å°ã€‚
ä¸ºäº†ä»»ä½•å•è¯éƒ½èƒ½è¢«åˆ†è¯ï¼ŒUnigramç®—æ³•æ€»æ˜¯ä¿ç•™åŸºç¡€çš„å­—ç¬¦ã€‚

å› ä¸ºUnigramä¸æ˜¯åŸºäºèåˆè§„åˆ™ï¼ˆå’ŒBPEä»¥åŠWordPieceç›¸æ¯”è¾ƒï¼‰ï¼Œåœ¨è®­ç»ƒä»¥åç®—æ³•æœ‰å‡ ç§æ–¹å¼æ¥åˆ†è¯ï¼Œå¦‚æœä¸€ä¸ªè®­ç»ƒå¥½çš„Unigramåˆ†è¯å™¨
çš„è¯å…¸æ˜¯è¿™ä¸ªï¼š

```
["b", "g", "h", "n", "p", "s", "u", "ug", "un", "hug"],
```
`"hugs"`å¯ä»¥è¢«åˆ†è¯æˆ`["hug", "s"]`, `["h", "ug", "s"]`æˆ–è€…`["h", "u", "g", "s"]`ã€‚æ‰€ä»¥é€‰æ‹©å“ªä¸€ä¸ªå‘¢ï¼ŸUnigramåœ¨ä¿å­˜
è¯å…¸çš„æ—¶å€™è¿˜ä¼šä¿å­˜è®­ç»ƒè¯­æ–™åº“å†…æ¯ä¸ªtokençš„æ¦‚ç‡ï¼Œæ‰€ä»¥åœ¨è®­ç»ƒä»¥åå¯ä»¥è®¡ç®—æ¯ä¸ªå¯èƒ½çš„åˆ†è¯ç»“æœçš„æ¦‚ç‡ã€‚å®é™…ä¸Šç®—æ³•ç®€å•çš„é€‰æ‹©æ¦‚ç‡
æœ€å¤§çš„é‚£ä¸ªåˆ†è¯ç»“æœï¼Œä½†æ˜¯ä¹Ÿä¼šæä¾›æ¦‚ç‡æ¥æ ¹æ®åˆ†è¯ç»“æœçš„æ¦‚ç‡æ¥é‡‡æ ·ä¸€ä¸ªå¯èƒ½çš„åˆ†è¯ç»“æœã€‚

åˆ†è¯å™¨åœ¨æŸå¤±å‡½æ•°ä¸Šè®­ç»ƒï¼Œè¿™äº›æŸå¤±å‡½æ•°å®šä¹‰äº†è¿™äº›æ¦‚ç‡ã€‚å‡è®¾è®­ç»ƒæ•°æ®åŒ…å«äº†è¿™äº›å•è¯ $x_{1}$, $\dots$, $x_{N}$ï¼Œä¸€ä¸ªå•è¯$x_{i}$
çš„æ‰€æœ‰å¯èƒ½çš„åˆ†è¯ç»“æœçš„é›†åˆå®šä¹‰ä¸º$S(x_{i})$ï¼Œç„¶åæ€»çš„æŸå¤±å°±å¯ä»¥å®šä¹‰ä¸ºï¼š

$$\mathcal{L} = -\sum_{i=1}^{N} \log \left ( \sum_{x \in S(x_{i})} p(x) \right )$$

<a id='sentencepiece'></a>

### SentencePiece
ç›®å‰ä¸ºæ­¢æè¿°çš„æ‰€æœ‰åˆ†è¯ç®—æ³•éƒ½æœ‰ç›¸åŒçš„é—®é¢˜ï¼šå®ƒä»¬éƒ½å‡è®¾è¾“å…¥çš„æ–‡æœ¬ä½¿ç”¨ç©ºæ ¼æ¥åˆ†å¼€å•è¯ã€‚ç„¶è€Œï¼Œä¸æ˜¯æ‰€æœ‰çš„è¯­è¨€éƒ½ä½¿ç”¨ç©ºæ ¼æ¥åˆ†å¼€å•è¯ã€‚
ä¸€ä¸ªå¯èƒ½çš„è§£å†³æ–¹æ¡ˆæ˜¯ä½¿ç”¨æŸç§è¯­è¨€ç‰¹å®šçš„é¢„åˆ†è¯å™¨ã€‚åƒï¼š[XLM](model_doc/xlm)ä½¿ç”¨äº†ä¸€ä¸ªç‰¹å®šçš„ä¸­æ–‡ã€æ—¥è¯­å’ŒThaiçš„é¢„åˆ†è¯å™¨ã€‚
ä¸ºäº†æ›´åŠ å¹¿æ³›çš„è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œ[SentencePiece: A simple and language independent subword tokenizer and
detokenizer for Neural Text Processing (Kudo et al., 2018)](https://huggingface.co/papers/1808.06226)
å°†è¾“å…¥æ–‡æœ¬çœ‹ä½œä¸€ä¸ªåŸå§‹çš„è¾“å…¥æµï¼Œå› æ­¤ä½¿ç”¨çš„ç¬¦åˆé›†åˆä¸­ä¹ŸåŒ…æ‹¬äº†ç©ºæ ¼ã€‚SentencePieceç„¶åä¼šä½¿ç”¨BPEæˆ–è€…unigramç®—æ³•æ¥äº§ç”Ÿåˆé€‚çš„
è¯å…¸ã€‚

ä¸¾ä¾‹æ¥è¯´ï¼Œ[`XLNetTokenizer`]ä½¿ç”¨äº†SentencePieceï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆä¸Šé¢çš„ä¾‹å­ä¸­`"â–"`ç¬¦å·åŒ…å«åœ¨è¯å…¸å†…ã€‚SentencePieceè§£ç æ˜¯éå¸¸å®¹æ˜“çš„ï¼Œå› ä¸ºæ‰€æœ‰çš„tokensèƒ½è¢«concatenateèµ·æ¥ï¼Œç„¶åå°†`"â–"`æ›¿æ¢æˆç©ºæ ¼ã€‚

åº“å†…æ‰€æœ‰ä½¿ç”¨äº†SentencePieceçš„transformersæ¨¡å‹ï¼Œä¼šå’Œunigramç»„åˆèµ·æ¥ä½¿ç”¨ï¼Œåƒï¼šä½¿ç”¨äº†SentencePieceçš„æ¨¡å‹æ˜¯[ALBERT](model_doc/albert), 
[XLNet](model_doc/xlnet)ï¼Œ[Marian](model_doc/marian)ï¼Œå’Œ[T5](model_doc/t5)ã€‚

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\training.md
============================================================



# å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹

[[open-in-colab]]

ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æœ‰è®¸å¤šæ˜¾è‘—çš„å¥½å¤„ã€‚å®ƒé™ä½äº†è®¡ç®—æˆæœ¬ï¼Œå‡å°‘äº†ç¢³æ’æ”¾ï¼ŒåŒæ—¶å…è®¸æ‚¨ä½¿ç”¨æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œè€Œæ— éœ€ä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ªã€‚ğŸ¤— Transformers æä¾›äº†æ¶‰åŠå„ç§ä»»åŠ¡çš„æˆåƒä¸Šä¸‡çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚å½“æ‚¨ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æ—¶ï¼Œæ‚¨éœ€è¦åœ¨ä¸ä»»åŠ¡ç›¸å…³çš„æ•°æ®é›†ä¸Šè®­ç»ƒè¯¥æ¨¡å‹ã€‚è¿™ç§æ“ä½œè¢«ç§°ä¸ºå¾®è°ƒï¼Œæ˜¯ä¸€ç§éå¸¸å¼ºå¤§çš„è®­ç»ƒæŠ€æœ¯ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæ‚¨å°†ä½¿ç”¨æ‚¨é€‰æ‹©çš„æ·±åº¦å­¦ä¹ æ¡†æ¶æ¥å¾®è°ƒä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼š

* ä½¿ç”¨ ğŸ¤— Transformers çš„ [`Trainer`] æ¥å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚
* åœ¨ TensorFlow ä¸­ä½¿ç”¨ Keras æ¥å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚
* åœ¨åŸç”Ÿ PyTorch ä¸­å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚

<a id='data-processing'></a>

## å‡†å¤‡æ•°æ®é›†

<Youtube id="_BZearw7f0w"/>

åœ¨æ‚¨è¿›è¡Œé¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒä¹‹å‰ï¼Œéœ€è¦ä¸‹è½½ä¸€ä¸ªæ•°æ®é›†å¹¶ä¸ºè®­ç»ƒåšå¥½å‡†å¤‡ã€‚ä¹‹å‰çš„æ•™ç¨‹å‘æ‚¨å±•ç¤ºäº†å¦‚ä½•å¤„ç†è®­ç»ƒæ•°æ®ï¼Œç°åœ¨æ‚¨æœ‰æœºä¼šå°†è¿™äº›æŠ€èƒ½ä»˜è¯¸å®è·µï¼

é¦–å…ˆï¼ŒåŠ è½½[Yelpè¯„è®º](https://huggingface.co/datasets/Yelp/yelp_review_full)æ•°æ®é›†ï¼š

```py
>>> from datasets import load_dataset

>>> dataset = load_dataset("yelp_review_full")
>>> dataset["train"][100]
{'label': 0,
 'text': 'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\nThe cashier took my friends\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\"serving off their orders\\" when they didn\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\nThe manager was rude when giving me my order. She didn\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\nI\'ve eaten at various McDonalds restaurants for over 30 years. I\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'}
```

æ­£å¦‚æ‚¨ç°åœ¨æ‰€çŸ¥ï¼Œæ‚¨éœ€è¦ä¸€ä¸ª`tokenizer`æ¥å¤„ç†æ–‡æœ¬ï¼ŒåŒ…æ‹¬å¡«å……å’Œæˆªæ–­æ“ä½œä»¥å¤„ç†å¯å˜çš„åºåˆ—é•¿åº¦ã€‚å¦‚æœè¦ä¸€æ¬¡æ€§å¤„ç†æ‚¨çš„æ•°æ®é›†ï¼Œå¯ä»¥ä½¿ç”¨ ğŸ¤— Datasets çš„ [`map`](https://huggingface.co/docs/datasets/process#map) æ–¹æ³•ï¼Œå°†é¢„å¤„ç†å‡½æ•°åº”ç”¨äºæ•´ä¸ªæ•°æ®é›†ï¼š

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-cased")


>>> def tokenize_function(examples):
...     return tokenizer(examples["text"], padding="max_length", truncation=True)


>>> tokenized_datasets = dataset.map(tokenize_function, batched=True)
```
å¦‚æœæ„¿æ„çš„è¯ï¼Œæ‚¨å¯ä»¥ä»å®Œæ•´æ•°æ®é›†æå–ä¸€ä¸ªè¾ƒå°å­é›†æ¥è¿›è¡Œå¾®è°ƒï¼Œä»¥å‡å°‘è®­ç»ƒæ‰€éœ€çš„æ—¶é—´ï¼š

```py
>>> small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
>>> small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))
```

<a id='trainer'></a>

## è®­ç»ƒ

æ­¤æ—¶ï¼Œæ‚¨åº”è¯¥æ ¹æ®æ‚¨è®­ç»ƒæ‰€ç”¨çš„æ¡†æ¶æ¥é€‰æ‹©å¯¹åº”çš„æ•™ç¨‹ç« èŠ‚ã€‚æ‚¨å¯ä»¥ä½¿ç”¨å³ä¾§çš„é“¾æ¥è·³è½¬åˆ°æ‚¨æƒ³è¦çš„ç« èŠ‚ - å¦‚æœæ‚¨æƒ³éšè—æŸä¸ªæ¡†æ¶å¯¹åº”çš„æ‰€æœ‰æ•™ç¨‹å†…å®¹ï¼Œåªéœ€ä½¿ç”¨å³ä¸Šè§’çš„æŒ‰é’®ï¼


<Youtube id="nvBXf7s7vTI"/>

## ä½¿ç”¨ PyTorch Trainer è¿›è¡Œè®­ç»ƒ

ğŸ¤— Transformers æä¾›äº†ä¸€ä¸ªä¸“ä¸ºè®­ç»ƒ ğŸ¤— Transformers æ¨¡å‹è€Œä¼˜åŒ–çš„ [`Trainer`] ç±»ï¼Œä½¿æ‚¨æ— éœ€æ‰‹åŠ¨ç¼–å†™è‡ªå·±çš„è®­ç»ƒå¾ªç¯æ­¥éª¤è€Œæ›´è½»æ¾åœ°å¼€å§‹è®­ç»ƒæ¨¡å‹ã€‚[`Trainer`] API æ”¯æŒå„ç§è®­ç»ƒé€‰é¡¹å’ŒåŠŸèƒ½ï¼Œå¦‚æ—¥å¿—è®°å½•ã€æ¢¯åº¦ç´¯ç§¯å’Œæ··åˆç²¾åº¦ã€‚

é¦–å…ˆåŠ è½½æ‚¨çš„æ¨¡å‹å¹¶æŒ‡å®šæœŸæœ›çš„æ ‡ç­¾æ•°é‡ã€‚æ ¹æ® Yelp Review [æ•°æ®é›†å¡ç‰‡](https://huggingface.co/datasets/Yelp/yelp_review_full#data-fields)ï¼Œæ‚¨çŸ¥é“æœ‰äº”ä¸ªæ ‡ç­¾ï¼š


```py
>>> from transformers import AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained("google-bert/bert-base-cased", num_labels=5)
```

<Tip>

æ‚¨å°†ä¼šçœ‹åˆ°ä¸€ä¸ªè­¦å‘Šï¼Œæåˆ°ä¸€äº›é¢„è®­ç»ƒæƒé‡æœªè¢«ä½¿ç”¨ï¼Œä»¥åŠä¸€äº›æƒé‡è¢«éšæœºåˆå§‹åŒ–ã€‚ä¸ç”¨æ‹…å¿ƒï¼Œè¿™æ˜¯å®Œå…¨æ­£å¸¸çš„ï¼BERT æ¨¡å‹çš„é¢„è®­ç»ƒ`head`è¢«ä¸¢å¼ƒï¼Œå¹¶æ›¿æ¢ä¸ºä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„åˆ†ç±»`head`ã€‚æ‚¨å°†åœ¨æ‚¨çš„åºåˆ—åˆ†ç±»ä»»åŠ¡ä¸Šå¾®è°ƒè¿™ä¸ªæ–°æ¨¡å‹`head`ï¼Œå°†é¢„è®­ç»ƒæ¨¡å‹çš„çŸ¥è¯†è½¬ç§»ç»™å®ƒã€‚

</Tip>

### è®­ç»ƒè¶…å‚æ•°

æ¥ä¸‹æ¥ï¼Œåˆ›å»ºä¸€ä¸ª [`TrainingArguments`] ç±»ï¼Œå…¶ä¸­åŒ…å«æ‚¨å¯ä»¥è°ƒæ•´çš„æ‰€æœ‰è¶…å‚æ•°ä»¥åŠç”¨äºæ¿€æ´»ä¸åŒè®­ç»ƒé€‰é¡¹çš„æ ‡å¿—ã€‚å¯¹äºæœ¬æ•™ç¨‹ï¼Œæ‚¨å¯ä»¥ä»é»˜è®¤çš„è®­ç»ƒ[è¶…å‚æ•°](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)å¼€å§‹ï¼Œä½†éšæ—¶å¯ä»¥å°è¯•ä¸åŒçš„è®¾ç½®ä»¥æ‰¾åˆ°æœ€ä½³è®¾ç½®ã€‚

æŒ‡å®šä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹çš„ä½ç½®ï¼š

```py
>>> from transformers import TrainingArguments

>>> training_args = TrainingArguments(output_dir="test_trainer")
```

### è¯„ä¼°

[`Trainer`] åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸ä¼šè‡ªåŠ¨è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚æ‚¨éœ€è¦å‘ [`Trainer`] ä¼ é€’ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—å’Œå±•ç¤ºæŒ‡æ ‡ã€‚[ğŸ¤— Evaluate](https://huggingface.co/docs/evaluate/index) åº“æä¾›äº†ä¸€ä¸ªç®€å•çš„ [`accuracy`](https://huggingface.co/spaces/evaluate-metric/accuracy) å‡½æ•°ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ [`evaluate.load`] å‡½æ•°åŠ è½½å®ƒï¼ˆæœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…æ­¤[å¿«é€Ÿå…¥é—¨](https://huggingface.co/docs/evaluate/a_quick_tour)ï¼‰ï¼š

```py
>>> import numpy as np
>>> import evaluate

>>> metric = evaluate.load("accuracy")
```
åœ¨ `metric` ä¸Šè°ƒç”¨ [`~evaluate.compute`] æ¥è®¡ç®—æ‚¨çš„é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚åœ¨å°†é¢„æµ‹ä¼ é€’ç»™ `compute` ä¹‹å‰ï¼Œæ‚¨éœ€è¦å°†é¢„æµ‹è½¬æ¢ä¸º`logits`ï¼ˆè¯·è®°ä½ï¼Œæ‰€æœ‰ ğŸ¤— Transformers æ¨¡å‹éƒ½è¿”å›å¯¹`logits`ï¼‰ï¼š

```py
>>> def compute_metrics(eval_pred):
...     logits, labels = eval_pred
...     predictions = np.argmax(logits, axis=-1)
...     return metric.compute(predictions=predictions, references=labels)
```

å¦‚æœæ‚¨å¸Œæœ›åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ç›‘è§†è¯„ä¼°æŒ‡æ ‡ï¼Œè¯·åœ¨æ‚¨çš„è®­ç»ƒå‚æ•°ä¸­æŒ‡å®š `eval_strategy` å‚æ•°ï¼Œä»¥åœ¨æ¯ä¸ª`epoch`ç»“æŸæ—¶å±•ç¤ºè¯„ä¼°æŒ‡æ ‡ï¼š

```py
>>> from transformers import TrainingArguments, Trainer

>>> training_args = TrainingArguments(output_dir="test_trainer", eval_strategy="epoch")
```

### è®­ç»ƒå™¨

åˆ›å»ºä¸€ä¸ªåŒ…å«æ‚¨çš„æ¨¡å‹ã€è®­ç»ƒå‚æ•°ã€è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†ä»¥åŠè¯„ä¼°å‡½æ•°çš„ [`Trainer`] å¯¹è±¡ï¼š


```py
>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=small_train_dataset,
...     eval_dataset=small_eval_dataset,
...     compute_metrics=compute_metrics,
... )
```
ç„¶åè°ƒç”¨[`~transformers.Trainer.train`]ä»¥å¾®è°ƒæ¨¡å‹ï¼š

```py
>>> trainer.train()
```

<a id='pytorch_native'></a>

## åœ¨åŸç”Ÿ PyTorch ä¸­è®­ç»ƒ

<Youtube id="Dh9CL8fyG80"/>

[`Trainer`] è´Ÿè´£è®­ç»ƒå¾ªç¯ï¼Œå…è®¸æ‚¨åœ¨ä¸€è¡Œä»£ç ä¸­å¾®è°ƒæ¨¡å‹ã€‚å¯¹äºå–œæ¬¢ç¼–å†™è‡ªå·±è®­ç»ƒå¾ªç¯çš„ç”¨æˆ·ï¼Œæ‚¨ä¹Ÿå¯ä»¥åœ¨åŸç”Ÿ PyTorch ä¸­å¾®è°ƒ ğŸ¤— Transformers æ¨¡å‹ã€‚

ç°åœ¨ï¼Œæ‚¨å¯èƒ½éœ€è¦é‡æ–°å¯åŠ¨æ‚¨çš„`notebook`ï¼Œæˆ–æ‰§è¡Œä»¥ä¸‹ä»£ç ä»¥é‡Šæ”¾ä¸€äº›å†…å­˜ï¼š

```py
del model
del trainer
torch.cuda.empty_cache()
```

æ¥ä¸‹æ¥ï¼Œæ‰‹åŠ¨å¤„ç† `tokenized_dataset` ä»¥å‡†å¤‡è¿›è¡Œè®­ç»ƒã€‚

1. ç§»é™¤ text åˆ—ï¼Œå› ä¸ºæ¨¡å‹ä¸æ¥å—åŸå§‹æ–‡æœ¬ä½œä¸ºè¾“å…¥ï¼š

    ```py
    >>> tokenized_datasets = tokenized_datasets.remove_columns(["text"])
    ```

2. å°† label åˆ—é‡å‘½åä¸º labelsï¼Œå› ä¸ºæ¨¡å‹æœŸæœ›å‚æ•°çš„åç§°ä¸º labelsï¼š

    ```py
    >>> tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
    ```

3. è®¾ç½®æ•°æ®é›†çš„æ ¼å¼ä»¥è¿”å› PyTorch å¼ é‡è€Œä¸æ˜¯`lists`ï¼š

    ```py
    >>> tokenized_datasets.set_format("torch")
    ```

æ¥ç€ï¼Œåˆ›å»ºä¸€ä¸ªå…ˆå‰å±•ç¤ºçš„æ•°æ®é›†çš„è¾ƒå°å­é›†ï¼Œä»¥åŠ é€Ÿå¾®è°ƒè¿‡ç¨‹

```py
>>> small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
>>> small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))
```

### DataLoader

æ‚¨çš„è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†åˆ›å»ºä¸€ä¸ª`DataLoader`ç±»ï¼Œä»¥ä¾¿å¯ä»¥è¿­ä»£å¤„ç†æ•°æ®æ‰¹æ¬¡

```py
>>> from torch.utils.data import DataLoader

>>> train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)
>>> eval_dataloader = DataLoader(small_eval_dataset, batch_size=8)
```

åŠ è½½æ‚¨çš„æ¨¡å‹ï¼Œå¹¶æŒ‡å®šæœŸæœ›çš„æ ‡ç­¾æ•°é‡ï¼š

```py
>>> from transformers import AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained("google-bert/bert-base-cased", num_labels=5)
```

### Optimizer and learning rate scheduler

åˆ›å»ºä¸€ä¸ª`optimizer`å’Œ`learning rate scheduler`ä»¥è¿›è¡Œæ¨¡å‹å¾®è°ƒã€‚è®©æˆ‘ä»¬ä½¿ç”¨ PyTorch ä¸­çš„ [AdamW](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) ä¼˜åŒ–å™¨ï¼š

```py
>>> from torch.optim import AdamW

>>> optimizer = AdamW(model.parameters(), lr=5e-5)
```

åˆ›å»ºæ¥è‡ª [`Trainer`] çš„é»˜è®¤`learning rate scheduler`ï¼š


```py
>>> from transformers import get_scheduler

>>> num_epochs = 3
>>> num_training_steps = num_epochs * len(train_dataloader)
>>> lr_scheduler = get_scheduler(
...     name="linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps
... )
```

æœ€åï¼ŒæŒ‡å®š `device` ä»¥ä½¿ç”¨ GPUï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰ã€‚å¦åˆ™ï¼Œä½¿ç”¨ CPU è¿›è¡Œè®­ç»ƒå¯èƒ½éœ€è¦å‡ ä¸ªå°æ—¶ï¼Œè€Œä¸æ˜¯å‡ åˆ†é’Ÿã€‚


```py
>>> import torch

>>> device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
>>> model.to(device)
```

<Tip>

å¦‚æœæ²¡æœ‰ GPUï¼Œå¯ä»¥é€šè¿‡notebookå¹³å°å¦‚ [Colaboratory](https://colab.research.google.com/) æˆ– [SageMaker StudioLab](https://studiolab.sagemaker.aws/) æ¥å…è´¹è·å¾—äº‘ç«¯GPUä½¿ç”¨ã€‚

</Tip>

ç°åœ¨æ‚¨å·²ç»å‡†å¤‡å¥½è®­ç»ƒäº†ï¼ğŸ¥³

### è®­ç»ƒå¾ªç¯

ä¸ºäº†è·Ÿè¸ªè®­ç»ƒè¿›åº¦ï¼Œä½¿ç”¨ [tqdm](https://tqdm.github.io/) åº“æ¥æ·»åŠ ä¸€ä¸ªè¿›åº¦æ¡ï¼Œæ˜¾ç¤ºè®­ç»ƒæ­¥æ•°çš„è¿›å±•ï¼š

```py
>>> from tqdm.auto import tqdm

>>> progress_bar = tqdm(range(num_training_steps))

>>> model.train()
>>> for epoch in range(num_epochs):
...     for batch in train_dataloader:
...         batch = {k: v.to(device) for k, v in batch.items()}
...         outputs = model(**batch)
...         loss = outputs.loss
...         loss.backward()

...         optimizer.step()
...         lr_scheduler.step()
...         optimizer.zero_grad()
...         progress_bar.update(1)
```

### è¯„ä¼°

å°±åƒæ‚¨åœ¨ [`Trainer`] ä¸­æ·»åŠ äº†ä¸€ä¸ªè¯„ä¼°å‡½æ•°ä¸€æ ·ï¼Œå½“æ‚¨ç¼–å†™è‡ªå·±çš„è®­ç»ƒå¾ªç¯æ—¶ï¼Œæ‚¨éœ€è¦åšåŒæ ·çš„äº‹æƒ…ã€‚ä½†ä¸åœ¨æ¯ä¸ª`epoch`ç»“æŸæ—¶è®¡ç®—å’Œå±•ç¤ºæŒ‡æ ‡ä¸åŒï¼Œè¿™ä¸€æ¬¡æ‚¨å°†ä½¿ç”¨ [`~evaluate.add_batch`] ç´¯ç§¯æ‰€æœ‰æ‰¹æ¬¡ï¼Œå¹¶åœ¨æœ€åè®¡ç®—æŒ‡æ ‡ã€‚

```py
>>> import evaluate

>>> metric = evaluate.load("accuracy")
>>> model.eval()
>>> for batch in eval_dataloader:
...     batch = {k: v.to(device) for k, v in batch.items()}
...     with torch.no_grad():
...         outputs = model(**batch)

...     logits = outputs.logits
...     predictions = torch.argmax(logits, dim=-1)
...     metric.add_batch(predictions=predictions, references=batch["labels"])

>>> metric.compute()
```

<a id='additional-resources'></a>

## é™„åŠ èµ„æº

æ›´å¤šå¾®è°ƒä¾‹å­å¯å‚è€ƒå¦‚ä¸‹é“¾æ¥ï¼š

- [ğŸ¤— Transformers ç¤ºä¾‹](https://github.com/huggingface/transformers/tree/main/examples) åŒ…å«ç”¨äºåœ¨ PyTorch å’Œ TensorFlow ä¸­è®­ç»ƒå¸¸è§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„è„šæœ¬ã€‚

- [ğŸ¤— Transformers ç¬”è®°](notebooks) åŒ…å«é’ˆå¯¹ç‰¹å®šä»»åŠ¡åœ¨ PyTorch å’Œ TensorFlow ä¸­å¾®è°ƒæ¨¡å‹çš„å„ç§`notebook`ã€‚

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\internal\audio_utils.md
============================================================



#  `FeatureExtractors`çš„å·¥å…·

æ­¤é¡µé¢åˆ—å‡ºäº†éŸ³é¢‘ [`FeatureExtractor`] å¯ä»¥ä½¿ç”¨çš„æ‰€æœ‰å®ç”¨å‡½æ•°ï¼Œä»¥ä¾¿ä½¿ç”¨å¸¸è§çš„ç®—æ³•ï¼ˆå¦‚ *Short Time Fourier Transform* æˆ– *log mel spectrogram*ï¼‰ä»åŸå§‹éŸ³é¢‘ä¸­è®¡ç®—ç‰¹æ®Šç‰¹å¾ã€‚

å…¶ä¸­å¤§å¤šæ•°ä»…åœ¨æ‚¨ç ”ç©¶åº“ä¸­éŸ³é¢‘processorsçš„ä»£ç æ—¶æœ‰ç”¨ã€‚


## éŸ³é¢‘è½¬æ¢

[[autodoc]] audio_utils.hertz_to_mel

[[autodoc]] audio_utils.mel_to_hertz

[[autodoc]] audio_utils.mel_filter_bank

[[autodoc]] audio_utils.optimal_fft_length

[[autodoc]] audio_utils.window_function

[[autodoc]] audio_utils.spectrogram

[[autodoc]] audio_utils.power_to_db

[[autodoc]] audio_utils.amplitude_to_db

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\internal\file_utils.md
============================================================



# é€šç”¨å·¥å…·

æ­¤é¡µé¢åˆ—å‡ºäº†åœ¨`utils.py`æ–‡ä»¶ä¸­æ‰¾åˆ°çš„æ‰€æœ‰Transformersé€šç”¨å®ç”¨å‡½æ•°ã€‚

å…¶ä¸­å¤§å¤šæ•°ä»…åœ¨æ‚¨ç ”ç©¶åº“ä¸­çš„é€šç”¨ä»£ç æ—¶æ‰æœ‰ç”¨ã€‚

## Enumså’Œnamedtuples(å‘½åå…ƒç»„)

[[autodoc]] utils.ExplicitEnum

[[autodoc]] utils.PaddingStrategy

[[autodoc]] utils.TensorType

## ç‰¹æ®Šçš„è£…é¥°å‡½æ•°

[[autodoc]] utils.add_start_docstrings

[[autodoc]] utils.add_start_docstrings_to_model_forward

[[autodoc]] utils.add_end_docstrings

[[autodoc]] utils.add_code_sample_docstrings

[[autodoc]] utils.replace_return_docstrings

## å…¶ä»–å®ç”¨ç¨‹åº

[[autodoc]] utils._LazyModule

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\internal\generation_utils.md
============================================================



# ç”¨äºç”Ÿæˆçš„å·¥å…·

æ­¤é¡µé¢åˆ—å‡ºäº†æ‰€æœ‰ç”± [`~generation.GenerationMixin.generate`]ã€‚

## ç”Ÿæˆè¾“å‡º

[`~generation.GenerationMixin.generate`] çš„è¾“å‡ºæ˜¯ [`~utils.ModelOutput`] çš„ä¸€ä¸ªå­ç±»çš„å®ä¾‹ã€‚è¿™ä¸ªè¾“å‡ºæ˜¯ä¸€ç§åŒ…å« [`~generation.GenerationMixin.generate`] è¿”å›çš„æ‰€æœ‰ä¿¡æ¯æ•°æ®ç»“æ„ï¼Œä½†ä¹Ÿå¯ä»¥ä½œä¸ºå…ƒç»„æˆ–å­—å…¸ä½¿ç”¨ã€‚
è¿™é‡Œæ˜¯ä¸€ä¸ªä¾‹å­ï¼š


```python
from transformers import GPT2Tokenizer, GPT2LMHeadModel

tokenizer = GPT2Tokenizer.from_pretrained("openai-community/gpt2")
model = GPT2LMHeadModel.from_pretrained("openai-community/gpt2")

inputs = tokenizer("Hello, my dog is cute and ", return_tensors="pt")
generation_output = model.generate(**inputs, return_dict_in_generate=True, output_scores=True)
```

`generation_output` çš„å¯¹è±¡æ˜¯ [`~generation.GenerateDecoderOnlyOutput`] çš„ä¸€ä¸ªå®ä¾‹ï¼Œä»è¯¥ç±»çš„æ–‡æ¡£ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œè¿™æ„å‘³ç€å®ƒå…·æœ‰ä»¥ä¸‹å±æ€§ï¼š

- `sequences`: ç”Ÿæˆçš„tokensåºåˆ—
- `scores`ï¼ˆå¯é€‰ï¼‰: æ¯ä¸ªç”Ÿæˆæ­¥éª¤çš„è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°
- `hidden_states`ï¼ˆå¯é€‰ï¼‰: æ¯ä¸ªç”Ÿæˆæ­¥éª¤æ¨¡å‹çš„hidden states
- `attentions`ï¼ˆå¯é€‰ï¼‰: æ¯ä¸ªç”Ÿæˆæ­¥éª¤æ¨¡å‹çš„æ³¨æ„åŠ›æƒé‡

åœ¨è¿™é‡Œï¼Œç”±äºæˆ‘ä»¬ä¼ é€’äº† `output_scores=True`ï¼Œæˆ‘ä»¬å…·æœ‰ `scores` å±æ€§ã€‚ä½†æˆ‘ä»¬æ²¡æœ‰ `hidden_states` å’Œ `attentions`ï¼Œå› ä¸ºæ²¡æœ‰ä¼ é€’ `output_hidden_states=True` æˆ– `output_attentions=True`ã€‚

æ‚¨å¯ä»¥åƒé€šå¸¸ä¸€æ ·è®¿é—®æ¯ä¸ªå±æ€§ï¼Œå¦‚æœè¯¥å±æ€§æœªè¢«æ¨¡å‹è¿”å›ï¼Œåˆ™å°†è·å¾— `None`ã€‚ä¾‹å¦‚ï¼Œåœ¨è¿™é‡Œ `generation_output.scores` æ˜¯è¯­è¨€å»ºæ¨¡å¤´çš„æ‰€æœ‰ç”Ÿæˆé¢„æµ‹åˆ†æ•°ï¼Œè€Œ `generation_output.attentions` ä¸º `None`ã€‚

å½“æˆ‘ä»¬å°† `generation_output` å¯¹è±¡ç”¨ä½œå…ƒç»„æ—¶ï¼Œå®ƒåªä¿ç•™é `None` å€¼çš„å±æ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨è¿™é‡Œå®ƒæœ‰ä¸¤ä¸ªå…ƒç´ ï¼Œ`loss` ç„¶åæ˜¯ `logits`ï¼Œæ‰€ä»¥


```python
generation_output[:2]
```

å°†è¿”å›å…ƒç»„`(generation_output.sequences, generation_output.scores)`ã€‚

å½“æˆ‘ä»¬å°†`generation_output`å¯¹è±¡ç”¨ä½œå­—å…¸æ—¶ï¼Œå®ƒåªä¿ç•™é`None`çš„å±æ€§ã€‚ä¾‹å¦‚ï¼Œå®ƒæœ‰ä¸¤ä¸ªé”®ï¼Œåˆ†åˆ«æ˜¯`sequences`å’Œ`scores`ã€‚

æˆ‘ä»¬åœ¨æ­¤è®°å½•æ‰€æœ‰è¾“å‡ºç±»å‹ã€‚


### PyTorch

[[autodoc]] generation.GenerateDecoderOnlyOutput

[[autodoc]] generation.GenerateEncoderDecoderOutput

[[autodoc]] generation.GenerateBeamDecoderOnlyOutput

[[autodoc]] generation.GenerateBeamEncoderDecoderOutput

## LogitsProcessor

[`LogitsProcessor`] å¯ä»¥ç”¨äºä¿®æ”¹è¯­è¨€æ¨¡å‹å¤´çš„é¢„æµ‹åˆ†æ•°ä»¥è¿›è¡Œç”Ÿæˆ


### PyTorch

[[autodoc]] AlternatingCodebooksLogitsProcessor
    - __call__

[[autodoc]] ClassifierFreeGuidanceLogitsProcessor
    - __call__

[[autodoc]] EncoderNoRepeatNGramLogitsProcessor
    - __call__

[[autodoc]] EncoderRepetitionPenaltyLogitsProcessor
    - __call__

[[autodoc]] EpsilonLogitsWarper
    - __call__

[[autodoc]] EtaLogitsWarper
    - __call__

[[autodoc]] ExponentialDecayLengthPenalty
    - __call__

[[autodoc]] ForcedBOSTokenLogitsProcessor
    - __call__

[[autodoc]] ForcedEOSTokenLogitsProcessor
    - __call__

[[autodoc]] InfNanRemoveLogitsProcessor
    - __call__

[[autodoc]] LogitNormalization
    - __call__

[[autodoc]] LogitsProcessor
    - __call__

[[autodoc]] LogitsProcessorList
    - __call__

[[autodoc]] MinLengthLogitsProcessor
    - __call__

[[autodoc]] MinNewTokensLengthLogitsProcessor
    - __call__

[[autodoc]] NoBadWordsLogitsProcessor
    - __call__

[[autodoc]] NoRepeatNGramLogitsProcessor
    - __call__

[[autodoc]] PrefixConstrainedLogitsProcessor
    - __call__

[[autodoc]] RepetitionPenaltyLogitsProcessor
    - __call__

[[autodoc]] SequenceBiasLogitsProcessor
    - __call__

[[autodoc]] SuppressTokensAtBeginLogitsProcessor
    - __call__

[[autodoc]] SuppressTokensLogitsProcessor
    - __call__

[[autodoc]] TemperatureLogitsWarper
    - __call__

[[autodoc]] TopKLogitsWarper
    - __call__

[[autodoc]] TopPLogitsWarper
    - __call__

[[autodoc]] TypicalLogitsWarper
    - __call__

[[autodoc]] UnbatchedClassifierFreeGuidanceLogitsProcessor
    - __call__

[[autodoc]] WhisperTimeStampLogitsProcessor
    - __call__

## StoppingCriteria

å¯ä»¥ä½¿ç”¨[`StoppingCriteria`]æ¥æ›´æ”¹åœæ­¢ç”Ÿæˆçš„æ—¶é—´ï¼ˆé™¤äº†EOS tokenä»¥å¤–çš„æ–¹æ³•ï¼‰ã€‚è¯·æ³¨æ„ï¼Œè¿™ä»…é€‚ç”¨äºæˆ‘ä»¬çš„PyTorchå®ç°ã€‚


[[autodoc]] StoppingCriteria
    - __call__

[[autodoc]] StoppingCriteriaList
    - __call__

[[autodoc]] MaxLengthCriteria
    - __call__

[[autodoc]] MaxTimeCriteria
    - __call__

## Streamers

[[autodoc]] TextStreamer

[[autodoc]] TextIteratorStreamer

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\internal\image_processing_utils.md
============================================================



# Image Processorsçš„å·¥å…·

æ­¤é¡µé¢åˆ—å‡ºäº†image processorsä½¿ç”¨çš„æ‰€æœ‰å®ç”¨å‡½æ•°åŠŸèƒ½ï¼Œä¸»è¦æ˜¯ç”¨äºå¤„ç†å›¾åƒçš„åŠŸèƒ½å˜æ¢ã€‚

å…¶ä¸­å¤§å¤šæ•°ä»…åœ¨æ‚¨ç ”ç©¶åº“ä¸­image processorsçš„ä»£ç æ—¶æœ‰ç”¨ã€‚


## å›¾åƒè½¬æ¢

[[autodoc]] image_transforms.center_crop

[[autodoc]] image_transforms.center_to_corners_format

[[autodoc]] image_transforms.corners_to_center_format

[[autodoc]] image_transforms.id_to_rgb

[[autodoc]] image_transforms.normalize

[[autodoc]] image_transforms.pad

[[autodoc]] image_transforms.rgb_to_id

[[autodoc]] image_transforms.rescale

[[autodoc]] image_transforms.resize

[[autodoc]] image_transforms.to_pil_image

## ImageProcessingMixin

[[autodoc]] image_processing_utils.ImageProcessingMixin

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\internal\modeling_utils.md
============================================================



# è‡ªå®šä¹‰å±‚å’Œå·¥å…·

æ­¤é¡µé¢åˆ—å‡ºäº†åº“ä½¿ç”¨çš„æ‰€æœ‰è‡ªå®šä¹‰å±‚ï¼Œä»¥åŠå®ƒä¸ºæ¨¡å‹æä¾›çš„å®ç”¨å‡½æ•°ã€‚

å…¶ä¸­å¤§å¤šæ•°åªæœ‰åœ¨æ‚¨ç ”ç©¶åº“ä¸­æ¨¡å‹çš„ä»£ç æ—¶æ‰æœ‰ç”¨ã€‚


## Pytorchè‡ªå®šä¹‰æ¨¡å—

[[autodoc]] pytorch_utils.Conv1D

## PyTorchå¸®åŠ©å‡½æ•°

[[autodoc]] pytorch_utils.apply_chunking_to_forward

[[autodoc]] pytorch_utils.prune_linear_layer

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\internal\pipelines_utils.md
============================================================



# pipelinesçš„å·¥å…·


æ­¤é¡µé¢åˆ—å‡ºäº†åº“ä¸ºpipelinesæä¾›çš„æ‰€æœ‰å®ç”¨ç¨‹åºåŠŸèƒ½ã€‚

å…¶ä¸­å¤§å¤šæ•°åªæœ‰åœ¨æ‚¨ç ”ç©¶åº“ä¸­æ¨¡å‹çš„ä»£ç æ—¶æ‰æœ‰ç”¨ã€‚


## å‚æ•°å¤„ç†

[[autodoc]] pipelines.ArgumentHandler

[[autodoc]] pipelines.ZeroShotClassificationArgumentHandler

[[autodoc]] pipelines.QuestionAnsweringArgumentHandler

## æ•°æ®æ ¼å¼

[[autodoc]] pipelines.PipelineDataFormat

[[autodoc]] pipelines.CsvPipelineDataFormat

[[autodoc]] pipelines.JsonPipelineDataFormat

[[autodoc]] pipelines.PipedPipelineDataFormat

## å®ç”¨å‡½æ•°

[[autodoc]] pipelines.PipelineException

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\internal\time_series_utils.md
============================================================



# æ—¶é—´åºåˆ—å·¥å…·


æ­¤é¡µé¢åˆ—å‡ºäº†å¯ç”¨äºæ—¶é—´åºåˆ—ç±»æ¨¡å‹çš„æ‰€æœ‰å®ç”¨å‡½æ•°å’Œç±»ã€‚

å…¶ä¸­å¤§å¤šæ•°ä»…åœ¨æ‚¨ç ”ç©¶æ—¶é—´åºåˆ—æ¨¡å‹çš„ä»£ç ï¼Œæˆ–å¸Œæœ›æ·»åŠ åˆ°åˆ†å¸ƒè¾“å‡ºç±»é›†åˆæ—¶æœ‰ç”¨ã€‚


## è¾“å‡ºåˆ†å¸ƒ

[[autodoc]] time_series_utils.NormalOutput

[[autodoc]] time_series_utils.StudentTOutput

[[autodoc]] time_series_utils.NegativeBinomialOutput

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\internal\tokenization_utils.md
============================================================



# Tokenizersçš„å·¥å…·

å¹¶ä¿ç•™æ ¼å¼ï¼šæ­¤é¡µé¢åˆ—å‡ºäº†tokenizersä½¿ç”¨çš„æ‰€æœ‰å®ç”¨å‡½æ•°ï¼Œä¸»è¦æ˜¯ç±»
[`~tokenization_utils_base.PreTrained TokenizerBase`] å®ç°äº†å¸¸ç”¨æ–¹æ³•ä¹‹é—´çš„
[`PreTrained Tokenizer`] å’Œ [`PreTrained TokenizerFast`] ä»¥åŠæ··åˆç±»
[`~tokenization_utils_base.SpecialTokens Mixin`]ã€‚

å…¶ä¸­å¤§å¤šæ•°åªæœ‰åœ¨æ‚¨ç ”ç©¶åº“ä¸­tokenizersçš„ä»£ç æ—¶æ‰æœ‰ç”¨ã€‚


## PreTrainedTokenizerBase

[[autodoc]] tokenization_utils_base.PreTrainedTokenizerBase
    - __call__
    - all


## Enumså’Œnamedtuples(å‘½åå…ƒç»„)

[[autodoc]] tokenization_utils_base.TruncationStrategy

[[autodoc]] tokenization_utils_base.CharSpan

[[autodoc]] tokenization_utils_base.TokenSpan

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\internal\trainer_utils.md
============================================================



# Trainerçš„å·¥å…·

æ­¤é¡µé¢åˆ—å‡ºäº† [`Trainer`] ä½¿ç”¨çš„æ‰€æœ‰å®ç”¨å‡½æ•°ã€‚

å…¶ä¸­å¤§å¤šæ•°ä»…åœ¨æ‚¨ç ”ç©¶åº“ä¸­Trainerçš„ä»£ç æ—¶æœ‰ç”¨ã€‚


## å·¥å…·

[[autodoc]] EvalPrediction

[[autodoc]] IntervalStrategy

[[autodoc]] enable_full_determinism

[[autodoc]] set_seed

[[autodoc]] torch_distributed_zero_first

## Callbackså†…éƒ¨æœºåˆ¶

[[autodoc]] trainer_callback.CallbackHandler

## Trainerå‚æ•°è§£æ

[[autodoc]] HfArgumentParser

## Debugå·¥å…·

[[autodoc]] debug_utils.DebugUnderflowOverflow

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\main_classes\callback.md
============================================================



# Callbacks


Callbackså¯ä»¥ç”¨æ¥è‡ªå®šä¹‰PyTorch [Trainer]ä¸­è®­ç»ƒå¾ªç¯è¡Œä¸ºçš„å¯¹è±¡ï¼ˆæ­¤åŠŸèƒ½å°šæœªåœ¨TensorFlowä¸­å®ç°ï¼‰ï¼Œè¯¥å¯¹è±¡å¯ä»¥æ£€æŸ¥è®­ç»ƒå¾ªç¯çŠ¶æ€ï¼ˆç”¨äºè¿›åº¦æŠ¥å‘Šã€åœ¨TensorBoardæˆ–å…¶ä»–MLå¹³å°ä¸Šè®°å½•æ—¥å¿—ç­‰ï¼‰ï¼Œå¹¶åšå‡ºå†³ç­–ï¼ˆä¾‹å¦‚æå‰åœæ­¢ï¼‰ã€‚

Callbacksæ˜¯â€œåªè¯»â€çš„ä»£ç ç‰‡æ®µï¼Œé™¤äº†å®ƒä»¬è¿”å›çš„[TrainerControl]å¯¹è±¡å¤–ï¼Œå®ƒä»¬ä¸èƒ½æ›´æ”¹è®­ç»ƒå¾ªç¯ä¸­çš„ä»»ä½•å†…å®¹ã€‚å¯¹äºéœ€è¦æ›´æ”¹è®­ç»ƒå¾ªç¯çš„è‡ªå®šä¹‰ï¼Œæ‚¨åº”è¯¥ç»§æ‰¿[Trainer]å¹¶é‡è½½æ‚¨éœ€è¦çš„æ–¹æ³•ï¼ˆæœ‰å…³ç¤ºä¾‹ï¼Œè¯·å‚è§[trainer](trainer)ï¼‰ã€‚

é»˜è®¤æƒ…å†µä¸‹ï¼Œ`TrainingArguments.report_to` è®¾ç½®ä¸º"all"ï¼Œç„¶å[Trainer]å°†ä½¿ç”¨ä»¥ä¸‹callbacksã€‚


- [`DefaultFlowCallback`]ï¼Œå®ƒå¤„ç†é»˜è®¤çš„æ—¥å¿—è®°å½•ã€ä¿å­˜å’Œè¯„ä¼°è¡Œä¸º
- [`PrinterCallback`] æˆ– [`ProgressCallback`]ï¼Œç”¨äºæ˜¾ç¤ºè¿›åº¦å’Œæ‰“å°æ—¥å¿—ï¼ˆå¦‚æœé€šè¿‡[`TrainingArguments`]åœç”¨tqdmï¼Œåˆ™ä½¿ç”¨ç¬¬ä¸€ä¸ªå‡½æ•°ï¼›å¦åˆ™ä½¿ç”¨ç¬¬äºŒä¸ªï¼‰ã€‚
- [`~integrations.TensorBoardCallback`]ï¼Œå¦‚æœTensorBoardå¯è®¿é—®ï¼ˆé€šè¿‡PyTorchç‰ˆæœ¬ >= 1.4 æˆ–è€… tensorboardXï¼‰ã€‚
- [`~integrations.WandbCallback`]ï¼Œå¦‚æœå®‰è£…äº†[wandb](https://www.wandb.com/)ã€‚
- [`~integrations.CometCallback`]ï¼Œå¦‚æœå®‰è£…äº†[comet_ml](https://www.comet.com/site/)ã€‚
- [`~integrations.MLflowCallback`]ï¼Œå¦‚æœå®‰è£…äº†[mlflow](https://www.mlflow.org/)ã€‚
- [`~integrations.AzureMLCallback`]ï¼Œå¦‚æœå®‰è£…äº†[azureml-sdk](https://pypi.org/project/azureml-sdk/)ã€‚
- [`~integrations.CodeCarbonCallback`]ï¼Œå¦‚æœå®‰è£…äº†[codecarbon](https://pypi.org/project/codecarbon/)ã€‚
- [`~integrations.ClearMLCallback`]ï¼Œå¦‚æœå®‰è£…äº†[clearml](https://github.com/allegroai/clearml)ã€‚
- [`~integrations.DagsHubCallback`]ï¼Œå¦‚æœå®‰è£…äº†[dagshub](https://dagshub.com/)ã€‚
- [`~integrations.FlyteCallback`]ï¼Œå¦‚æœå®‰è£…äº†[flyte](https://flyte.org/)ã€‚
- [`~integrations.DVCLiveCallback`]ï¼Œå¦‚æœå®‰è£…äº†[dvclive](https://dvc.org/doc/dvclive)ã€‚
- [`~integrations.SwanLabCallback`]ï¼Œå¦‚æœå®‰è£…äº†[swanlab](http://swanlab.cn/)ã€‚

å¦‚æœå®‰è£…äº†ä¸€ä¸ªè½¯ä»¶åŒ…ï¼Œä½†æ‚¨ä¸å¸Œæœ›ä½¿ç”¨ç›¸å…³çš„é›†æˆï¼Œæ‚¨å¯ä»¥å°† `TrainingArguments.report_to` æ›´æ”¹ä¸ºä»…åŒ…å«æ‚¨æƒ³è¦ä½¿ç”¨çš„é›†æˆçš„åˆ—è¡¨ï¼ˆä¾‹å¦‚ `["azure_ml", "wandb"]`ï¼‰ã€‚

å®ç°callbacksçš„ä¸»è¦ç±»æ˜¯[`TrainerCallback`]ã€‚å®ƒè·å–ç”¨äºå®ä¾‹åŒ–[`Trainer`]çš„[`TrainingArguments`]ï¼Œå¯ä»¥é€šè¿‡[`TrainerState`]è®¿é—®è¯¥Trainerçš„å†…éƒ¨çŠ¶æ€ï¼Œå¹¶å¯ä»¥é€šè¿‡[`TrainerControl`]å¯¹è®­ç»ƒå¾ªç¯æ‰§è¡Œä¸€äº›æ“ä½œã€‚


## å¯ç”¨çš„Callbacks

è¿™é‡Œæ˜¯åº“é‡Œå¯ç”¨[`TrainerCallback`]çš„åˆ—è¡¨ï¼š

[[autodoc]] integrations.CometCallback 
    - setup

[[autodoc]] DefaultFlowCallback

[[autodoc]] PrinterCallback

[[autodoc]] ProgressCallback

[[autodoc]] EarlyStoppingCallback

[[autodoc]] integrations.TensorBoardCallback

[[autodoc]] integrations.WandbCallback 
    - setup

[[autodoc]] integrations.MLflowCallback 
    - setup

[[autodoc]] integrations.AzureMLCallback

[[autodoc]] integrations.CodeCarbonCallback

[[autodoc]] integrations.ClearMLCallback

[[autodoc]] integrations.DagsHubCallback

[[autodoc]] integrations.FlyteCallback

[[autodoc]] integrations.DVCLiveCallback
    - setup

[[autodoc]] integrations.SwanLabCallback
    - setup

## TrainerCallback

[[autodoc]] TrainerCallback

ä»¥ä¸‹æ˜¯å¦‚ä½•ä½¿ç”¨PyTorchæ³¨å†Œè‡ªå®šä¹‰callbackçš„ç¤ºä¾‹ï¼š

[`Trainer`]:

```python
class MyCallback(TrainerCallback):
    "A callback that prints a message at the beginning of training"

    def on_train_begin(self, args, state, control, **kwargs):
        print("Starting training")


trainer = Trainer(
    model,
    args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    callbacks=[MyCallback],  # We can either pass the callback class this way or an instance of it (MyCallback())
)
```

æ³¨å†Œcallbackçš„å¦ä¸€ç§æ–¹å¼æ˜¯è°ƒç”¨ `trainer.add_callback()`ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š


```python
trainer = Trainer(...)
trainer.add_callback(MyCallback)
# Alternatively, we can pass an instance of the callback class
trainer.add_callback(MyCallback())
```

## TrainerState

[[autodoc]] TrainerState

## TrainerControl

[[autodoc]] TrainerControl

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\main_classes\configuration.md
============================================================



# Configuration

åŸºç±»[`PreTrainedConfig`]å®ç°äº†ä»æœ¬åœ°æ–‡ä»¶æˆ–ç›®å½•åŠ è½½/ä¿å­˜é…ç½®çš„å¸¸è§æ–¹æ³•ï¼Œæˆ–ä¸‹è½½åº“æä¾›çš„é¢„è®­ç»ƒæ¨¡å‹é…ç½®ï¼ˆä»HuggingFaceçš„AWS S3åº“ä¸­ä¸‹è½½ï¼‰ã€‚

æ¯ä¸ªæ´¾ç”Ÿçš„é…ç½®ç±»éƒ½å®ç°äº†ç‰¹å®šäºæ¨¡å‹çš„å±æ€§ã€‚æ‰€æœ‰é…ç½®ç±»ä¸­å…±åŒå­˜åœ¨çš„å±æ€§æœ‰ï¼š`hidden_size`ã€`num_attention_heads` å’Œ `num_hidden_layers`ã€‚æ–‡æœ¬æ¨¡å‹è¿›ä¸€æ­¥æ·»åŠ äº† `vocab_size`ã€‚


## PreTrainedConfig

[[autodoc]] PreTrainedConfig
    - push_to_hub
    - all

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\main_classes\data_collator.md
============================================================



# Data Collator

Data collatorsæ˜¯ä¸€ä¸ªå¯¹è±¡ï¼Œé€šè¿‡ä½¿ç”¨æ•°æ®é›†å…ƒç´ åˆ—è¡¨ä½œä¸ºè¾“å…¥æ¥å½¢æˆä¸€ä¸ªæ‰¹æ¬¡ã€‚è¿™äº›å…ƒç´ ä¸ `train_dataset` æˆ– `eval_dataset` çš„å…ƒç´ ç±»å‹ç›¸åŒã€‚

ä¸ºäº†èƒ½å¤Ÿæ„å»ºæ‰¹æ¬¡ï¼ŒData collatorså¯èƒ½ä¼šåº”ç”¨ä¸€äº›é¢„å¤„ç†ï¼ˆæ¯”å¦‚å¡«å……ï¼‰ã€‚å…¶ä¸­ä¸€äº›ï¼ˆæ¯”å¦‚[`DataCollatorForLanguageModeling`]ï¼‰è¿˜ä¼šåœ¨å½¢æˆçš„æ‰¹æ¬¡ä¸Šåº”ç”¨ä¸€äº›éšæœºæ•°æ®å¢å¼ºï¼ˆæ¯”å¦‚éšæœºæ©ç ï¼‰ã€‚

åœ¨[ç¤ºä¾‹è„šæœ¬](../examples)æˆ–[ç¤ºä¾‹notebooks](../notebooks)ä¸­å¯ä»¥æ‰¾åˆ°ä½¿ç”¨çš„ç¤ºä¾‹ã€‚


## Default data collator

[[autodoc]] data.data_collator.default_data_collator

## DefaultDataCollator

[[autodoc]] data.data_collator.DefaultDataCollator

## DataCollatorWithPadding

[[autodoc]] data.data_collator.DataCollatorWithPadding

## DataCollatorForTokenClassification

[[autodoc]] data.data_collator.DataCollatorForTokenClassification

## DataCollatorForSeq2Seq

[[autodoc]] data.data_collator.DataCollatorForSeq2Seq

## DataCollatorForLanguageModeling

[[autodoc]] data.data_collator.DataCollatorForLanguageModeling
    - numpy_mask_tokens
    - torch_mask_tokens

## DataCollatorForWholeWordMask

[[autodoc]] data.data_collator.DataCollatorForWholeWordMask
    - numpy_mask_tokens
    - torch_mask_tokens

## DataCollatorForPermutationLanguageModeling

[[autodoc]] data.data_collator.DataCollatorForPermutationLanguageModeling
    - numpy_mask_tokens
    - torch_mask_tokens

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\main_classes\deepspeed.md
============================================================



# DeepSpeedé›†æˆ

[DeepSpeed](https://github.com/deepspeedai/DeepSpeed)å®ç°äº†[ZeROè®ºæ–‡](https://huggingface.co/papers/1910.02054)ä¸­æè¿°çš„æ‰€æœ‰å†…å®¹ã€‚ç›®å‰ï¼Œå®ƒæä¾›å¯¹ä»¥ä¸‹åŠŸèƒ½çš„å…¨é¢æ”¯æŒï¼š

1. ä¼˜åŒ–å™¨çŠ¶æ€åˆ†åŒºï¼ˆZeRO stage 1ï¼‰
2. æ¢¯åº¦åˆ†åŒºï¼ˆZeRO stage 2ï¼‰
3. å‚æ•°åˆ†åŒºï¼ˆZeRO stage 3ï¼‰
4. è‡ªå®šä¹‰æ··åˆç²¾åº¦è®­ç»ƒå¤„ç†
5. ä¸€ç³»åˆ—åŸºäºCUDAæ‰©å±•çš„å¿«é€Ÿä¼˜åŒ–å™¨
6. ZeRO-Offload åˆ° CPU å’Œ NVMe

ZeRO-Offloadæœ‰å…¶è‡ªå·±çš„ä¸“é—¨è®ºæ–‡ï¼š[ZeRO-Offload: Democratizing Billion-Scale Model Training](https://huggingface.co/papers/2101.06840)ã€‚è€ŒNVMeæ”¯æŒåœ¨è®ºæ–‡[ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning](https://huggingface.co/papers/2104.07857)ä¸­è¿›è¡Œäº†æè¿°ã€‚

DeepSpeed ZeRO-2ä¸»è¦ç”¨äºè®­ç»ƒï¼Œå› ä¸ºå®ƒçš„ç‰¹æ€§å¯¹æ¨ç†æ²¡æœ‰ç”¨å¤„ã€‚

DeepSpeed ZeRO-3ä¹Ÿå¯ä»¥ç”¨äºæ¨ç†ï¼Œå› ä¸ºå®ƒå…è®¸å°†å•ä¸ªGPUæ— æ³•åŠ è½½çš„å¤§æ¨¡å‹åŠ è½½åˆ°å¤šä¸ªGPUä¸Šã€‚

ğŸ¤— Transformersé€šè¿‡ä»¥ä¸‹ä¸¤ç§æ–¹å¼é›†æˆäº†[DeepSpeed](https://github.com/deepspeedai/DeepSpeed)ï¼š

1. é€šè¿‡[`Trainer`]é›†æˆæ ¸å¿ƒçš„DeepSpeedåŠŸèƒ½ã€‚è¿™æ˜¯ä¸€ç§â€œä¸ºæ‚¨å®Œæˆä¸€åˆ‡â€å¼çš„é›†æˆ - æ‚¨åªéœ€æä¾›è‡ªå®šä¹‰é…ç½®æ–‡ä»¶æˆ–ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡æ¿é…ç½®æ–‡ä»¶ã€‚æœ¬æ–‡æ¡£çš„å¤§éƒ¨åˆ†å†…å®¹éƒ½é›†ä¸­åœ¨è¿™ä¸ªåŠŸèƒ½ä¸Šã€‚
2. å¦‚æœæ‚¨ä¸ä½¿ç”¨[`Trainer`]å¹¶å¸Œæœ›åœ¨è‡ªå·±çš„Trainerä¸­é›†æˆDeepSpeedï¼Œé‚£ä¹ˆåƒ`from_pretrained`å’Œ`from_config`è¿™æ ·çš„æ ¸å¿ƒåŠŸèƒ½å‡½æ•°å°†åŒ…æ‹¬ZeRO stage 3åŠä»¥ä¸Šçš„DeepSpeedçš„åŸºç¡€éƒ¨åˆ†ï¼Œå¦‚`zero.Init`ã€‚è¦åˆ©ç”¨æ­¤åŠŸèƒ½ï¼Œè¯·é˜…è¯»æœ‰å…³[éTrainer DeepSpeedé›†æˆ](#nontrainer-deepspeed-integration)çš„æ–‡æ¡£ã€‚

é›†æˆçš„å†…å®¹ï¼š

è®­ç»ƒï¼š

1. DeepSpeed ZeROè®­ç»ƒæ”¯æŒå®Œæ•´çš„ZeRO stages 1ã€2å’Œ3ï¼Œä»¥åŠZeRO-Infinityï¼ˆCPUå’ŒNVMe offloadï¼‰ã€‚

æ¨ç†ï¼š

1. DeepSpeed ZeROæ¨ç†æ”¯æŒZeRO stage 3å’ŒZeRO-Infinityã€‚å®ƒä½¿ç”¨ä¸è®­ç»ƒç›¸åŒçš„ZeROåè®®ï¼Œä½†ä¸ä½¿ç”¨ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œåªæœ‰stage 3ä¸æ¨ç†ç›¸å…³ã€‚æ›´å¤šè¯¦ç»†ä¿¡æ¯è¯·å‚é˜…ï¼š[zero-inference](#zero-inference)ã€‚

æ­¤å¤–è¿˜æœ‰DeepSpeedæ¨ç† - è¿™æ˜¯ä¸€ç§å®Œå…¨ä¸åŒçš„æŠ€æœ¯ï¼Œå®ƒä½¿ç”¨å¼ é‡å¹¶è¡Œè€Œä¸æ˜¯ZeROï¼ˆå³å°†æ¨å‡ºï¼‰ã€‚


<a id='deepspeed-trainer-integration'></a>


## Trainer DeepSpeed é›†æˆ


<a id='deepspeed-installation'></a>

### å®‰è£…

é€šè¿‡pypiå®‰è£…åº“ï¼š


```bash
pip install deepspeed
```

æˆ–é€šè¿‡ `transformers` çš„ `extras`å®‰è£…ï¼š

```bash
pip install transformers[deepspeed]
```

æˆ–åœ¨ [DeepSpeed çš„ GitHub é¡µé¢](https://github.com/deepspeedai/DeepSpeed#installation) å’Œ
[é«˜çº§å®‰è£…](https://www.deepspeed.ai/tutorials/advanced-install/) ä¸­æŸ¥æ‰¾æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚

å¦‚æœæ„å»ºè¿‡ç¨‹ä¸­ä»ç„¶é‡åˆ°é—®é¢˜ï¼Œè¯·é¦–å…ˆç¡®ä¿é˜…è¯» [CUDA æ‰©å±•å®‰è£…æ³¨æ„äº‹é¡¹](trainer#cuda-extension-installation-notes)ã€‚

å¦‚æœæ‚¨æ²¡æœ‰é¢„å…ˆæ„å»ºæ‰©å±•è€Œæ˜¯åœ¨è¿è¡Œæ—¶æ„å»ºå®ƒä»¬ï¼Œè€Œä¸”æ‚¨å°è¯•äº†ä»¥ä¸Šæ‰€æœ‰è§£å†³æ–¹æ¡ˆéƒ½æ— æ•ˆï¼Œä¸‹ä¸€æ­¥å¯ä»¥å°è¯•åœ¨å®‰è£…ä¹‹å‰é¢„å…ˆæ„å»ºæ‰©å±•ã€‚

è¿›è¡Œ DeepSpeed çš„æœ¬åœ°æ„å»ºï¼š


```bash
git clone https://github.com/deepspeedai/DeepSpeed/
cd DeepSpeed
rm -rf build
TORCH_CUDA_ARCH_LIST="8.6" DS_BUILD_CPU_ADAM=1 DS_BUILD_UTILS=1 pip install . \
--global-option="build_ext" --global-option="-j8" --no-cache -v \
--disable-pip-version-check 2>&1 | tee build.log
```

å¦‚æœæ‚¨æ‰“ç®—ä½¿ç”¨ NVMe offloadï¼Œæ‚¨è¿˜éœ€è¦åœ¨ä¸Šè¿°è¯´æ˜ä¸­æ·»åŠ  `DS_BUILD_AIO=1`ï¼ˆå¹¶ä¸”è¿˜éœ€è¦åœ¨ç³»ç»ŸèŒƒå›´å†…å®‰è£… *libaio-dev*ï¼‰ã€‚

ç¼–è¾‘ `TORCH_CUDA_ARCH_LIST` ä»¥æ’å…¥æ‚¨æ‰“ç®—ä½¿ç”¨çš„ GPU å¡çš„æ¶æ„ä»£ç ã€‚å‡è®¾æ‚¨çš„æ‰€æœ‰å¡éƒ½æ˜¯ç›¸åŒçš„ï¼Œæ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è·å–æ¶æ„ï¼š

```bash
CUDA_VISIBLE_DEVICES=0 python -c "import torch; print(torch.cuda.get_device_capability())"
```

å› æ­¤ï¼Œå¦‚æœæ‚¨å¾—åˆ° `8, 6`ï¼Œåˆ™ä½¿ç”¨ `TORCH_CUDA_ARCH_LIST="8.6"`ã€‚å¦‚æœæ‚¨æœ‰å¤šä¸ªä¸åŒçš„å¡ï¼Œæ‚¨å¯ä»¥åƒè¿™æ ·åˆ—å‡ºæ‰€æœ‰å¡ `TORCH_CUDA_ARCH_LIST="6.1;8.6"`ã€‚

å¦‚æœæ‚¨éœ€è¦åœ¨å¤šå°æœºå™¨ä¸Šä½¿ç”¨ç›¸åŒçš„è®¾ç½®ï¼Œè¯·åˆ›å»ºä¸€ä¸ªäºŒè¿›åˆ¶ wheelï¼š


```bash
git clone https://github.com/deepspeedai/DeepSpeed/
cd DeepSpeed
rm -rf build
TORCH_CUDA_ARCH_LIST="8.6" DS_BUILD_CPU_ADAM=1 DS_BUILD_UTILS=1 \
python setup.py build_ext -j8 bdist_wheel
```

å®ƒå°†ç”Ÿæˆç±»ä¼¼äº `dist/deepspeed-0.3.13+8cd046f-cp38-cp38-linux_x86_64.whl` çš„æ–‡ä»¶ï¼Œç°åœ¨æ‚¨å¯ä»¥åœ¨æœ¬åœ°æˆ–ä»»ä½•å…¶ä»–æœºå™¨ä¸Šå®‰è£…å®ƒï¼Œå¦‚ `pip install deepspeed-0.3.13+8cd046f-cp38-cp38-linux_x86_64.whl`ã€‚

å†æ¬¡æé†’ç¡®ä¿è°ƒæ•´ `TORCH_CUDA_ARCH_LIST` ä»¥åŒ¹é…ç›®æ ‡æ¶æ„ã€‚

æ‚¨å¯ä»¥åœ¨[è¿™é‡Œ](https://developer.nvidia.com/cuda-gpus)æ‰¾åˆ°å®Œæ•´çš„ NVIDIA GPU åˆ—è¡¨åŠå…¶å¯¹åº”çš„ **è®¡ç®—èƒ½åŠ›**ï¼ˆä¸æ­¤ä¸Šä¸‹æ–‡ä¸­çš„æ¶æ„ç›¸åŒï¼‰ã€‚

æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ£€æŸ¥ PyTorch æ„å»ºæ—¶ä½¿ç”¨çš„æ¶æ„ï¼š


```bash
python -c "import torch; print(torch.cuda.get_arch_list())"
```

ä»¥ä¸‹æ˜¯å¦‚ä½•æŸ¥æ‰¾å·²å®‰è£… GPU ä¸­çš„ä¸€å¼ å¡çš„æ¶æ„ã€‚ä¾‹å¦‚ï¼Œå¯¹äº GPU 0ï¼š

```bash
CUDA_VISIBLE_DEVICES=0 python -c "import torch; \
print(torch.cuda.get_device_properties(torch.device('cuda')))"
```

å¦‚æœè¾“å‡ºç»“æœå¦‚ä¸‹ï¼š

```bash
_CudaDeviceProperties(name='GeForce RTX 3090', major=8, minor=6, total_memory=24268MB, multi_processor_count=82)
```

ç„¶åæ‚¨å°±çŸ¥é“è¿™å¼ å¡çš„æ¶æ„æ˜¯ `8.6`ã€‚

æ‚¨ä¹Ÿå¯ä»¥å®Œå…¨çœç•¥ `TORCH_CUDA_ARCH_LIST`ï¼Œç„¶åæ„å»ºç¨‹åºå°†è‡ªåŠ¨æŸ¥è¯¢æ„å»ºæ‰€åœ¨çš„ GPU çš„æ¶æ„ã€‚è¿™å¯èƒ½ä¸ç›®æ ‡æœºå™¨ä¸Šçš„ GPU ä¸åŒ¹é…ï¼Œå› æ­¤æœ€å¥½æ˜ç¡®æŒ‡å®šæ‰€éœ€çš„æ¶æ„ã€‚

å¦‚æœå°è¯•äº†æ‰€æœ‰å»ºè®®çš„æ–¹æ³•ä»ç„¶é‡åˆ°æ„å»ºé—®é¢˜ï¼Œè¯·ç»§ç»­åœ¨ [Deepspeed](https://github.com/deepspeedai/DeepSpeed/issues)çš„ GitHub Issue ä¸Šæäº¤é—®é¢˜ã€‚


<a id='deepspeed-multi-gpu'></a>

### å¤šGPUå¯ç”¨

ä¸ºäº†å¯ç”¨DeepSpeed é›†æˆï¼Œè°ƒæ•´ [`Trainer`] çš„å‘½ä»¤è¡Œå‚æ•°ï¼Œæ·»åŠ ä¸€ä¸ªæ–°çš„å‚æ•° `--deepspeed ds_config.json`ï¼Œå…¶ä¸­ `ds_config.json` æ˜¯ DeepSpeed é…ç½®æ–‡ä»¶ï¼Œå¦‚æ–‡æ¡£ [è¿™é‡Œ](https://www.deepspeed.ai/docs/config-json/) æ‰€è¿°ã€‚æ–‡ä»¶å‘½åç”±æ‚¨å†³å®šã€‚
å»ºè®®ä½¿ç”¨ DeepSpeed çš„ `add_config_arguments` ç¨‹åºå°†å¿…è¦çš„å‘½ä»¤è¡Œå‚æ•°æ·»åŠ åˆ°æ‚¨çš„ä»£ç ä¸­ã€‚
æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜… [DeepSpeed çš„å‚æ•°è§£æ](https://deepspeed.readthedocs.io/en/latest/initialize.html#argument-parsing) æ–‡æ¡£ã€‚

åœ¨è¿™é‡Œï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ‚¨å–œæ¬¢çš„å¯åŠ¨å™¨ã€‚æ‚¨å¯ä»¥ç»§ç»­ä½¿ç”¨ PyTorch å¯åŠ¨å™¨ï¼š


```bash
torch.distributed.run --nproc_per_node=2 your_program.py <normal cl args> --deepspeed ds_config.json
```

æˆ–ä½¿ç”¨ç”± `deepspeed` æä¾›çš„å¯åŠ¨å™¨ï¼š


```bash
deepspeed --num_gpus=2 your_program.py <normal cl args> --deepspeed ds_config.json
```


æ­£å¦‚æ‚¨æ‰€è§ï¼Œè¿™ä¸¤ä¸ªå¯åŠ¨å™¨çš„å‚æ•°ä¸åŒï¼Œä½†å¯¹äºå¤§å¤šæ•°éœ€æ±‚ï¼Œä»»ä½•ä¸€ä¸ªéƒ½å¯ä»¥æ»¡è¶³å·¥ä½œéœ€æ±‚ã€‚æœ‰å…³å¦‚ä½•é…ç½®å„ä¸ªèŠ‚ç‚¹å’Œ GPU çš„å®Œæ•´è¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ [æ­¤å¤„](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node)ã€‚

å½“æ‚¨ä½¿ç”¨ `deepspeed` å¯åŠ¨å™¨å¹¶ä¸”å¸Œæœ›ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„ GPU æ—¶ï¼Œæ‚¨å¯ä»¥ç®€å•åœ°çœç•¥ `--num_gpus` æ ‡å¿—ã€‚

ä»¥ä¸‹æ˜¯åœ¨ DeepSpeed ä¸­å¯ç”¨ä½¿ç”¨æ‰€æœ‰å¯ç”¨ GPUæƒ…å†µä¸‹ï¼Œ è¿è¡Œ `run_translation.py` çš„ç¤ºä¾‹ï¼š


```bash
deepspeed examples/pytorch/translation/run_translation.py \
--deepspeed tests/deepspeed/ds_config_zero3.json \
--model_name_or_path google-t5/t5-small --per_device_train_batch_size 1 \
--output_dir output_dir --fp16 \
--do_train --max_train_samples 500 --num_train_epochs 1 \
--dataset_name wmt16 --dataset_config "ro-en" \
--source_lang en --target_lang ro
```

è¯·æ³¨æ„ï¼Œåœ¨ DeepSpeed æ–‡æ¡£ä¸­ï¼Œæ‚¨å¯èƒ½ä¼šçœ‹åˆ° `--deepspeed --deepspeed_config ds_config.json` - å³ä¸¤ä¸ªä¸ DeepSpeed ç›¸å…³çš„å‚æ•°ï¼Œä½†ä¸ºç®€å•èµ·è§ï¼Œå¹¶ä¸”å› ä¸ºå·²ç»æœ‰å¾ˆå¤šå‚æ•°è¦å¤„ç†ï¼Œæˆ‘ä»¬å°†ä¸¤è€…åˆå¹¶ä¸ºä¸€ä¸ªå•ä¸€å‚æ•°ã€‚

æœ‰å…³ä¸€äº›å®é™…ä½¿ç”¨ç¤ºä¾‹ï¼Œè¯·å‚é˜… [æ­¤å¸–](https://github.com/huggingface/transformers/issues/8771#issuecomment-759248400)ã€‚



<a id='deepspeed-one-gpu'></a>

### å•GPUå¯ç”¨

è¦ä½¿ç”¨ä¸€å¼  GPU å¯ç”¨ DeepSpeedï¼Œè°ƒæ•´ [`Trainer`] çš„å‘½ä»¤è¡Œå‚æ•°å¦‚ä¸‹ï¼š


```bash
deepspeed --num_gpus=1 examples/pytorch/translation/run_translation.py \
--deepspeed tests/deepspeed/ds_config_zero2.json \
--model_name_or_path google-t5/t5-small --per_device_train_batch_size 1 \
--output_dir output_dir --fp16 \
--do_train --max_train_samples 500 --num_train_epochs 1 \
--dataset_name wmt16 --dataset_config "ro-en" \
--source_lang en --target_lang ro
```

è¿™ä¸å¤š GPU çš„æƒ…å†µå‡ ä¹ç›¸åŒï¼Œä½†åœ¨è¿™é‡Œæˆ‘ä»¬é€šè¿‡ `--num_gpus=1` æ˜ç¡®å‘Šè¯‰ DeepSpeed ä»…ä½¿ç”¨ä¸€å¼  GPUã€‚é»˜è®¤æƒ…å†µä¸‹ï¼ŒDeepSpeed å¯ç”¨ç»™å®šèŠ‚ç‚¹ä¸Šå¯ä»¥çœ‹åˆ°çš„æ‰€æœ‰ GPUã€‚å¦‚æœæ‚¨ä¸€å¼€å§‹åªæœ‰ä¸€å¼  GPUï¼Œé‚£ä¹ˆæ‚¨ä¸éœ€è¦è¿™ä¸ªå‚æ•°ã€‚ä»¥ä¸‹ [æ–‡æ¡£](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node) è®¨è®ºäº†å¯åŠ¨å™¨çš„é€‰é¡¹ã€‚

ä¸ºä»€ä¹ˆè¦åœ¨ä»…ä½¿ç”¨ä¸€å¼  GPU çš„æƒ…å†µä¸‹ä½¿ç”¨ DeepSpeed å‘¢ï¼Ÿ

1. å®ƒå…·æœ‰ ZeRO-offload åŠŸèƒ½ï¼Œå¯ä»¥å°†ä¸€äº›è®¡ç®—å’Œå†…å­˜å§”æ‰˜ç»™ä¸»æœºçš„ CPU å’Œ å†…å­˜ï¼Œä»è€Œä¸ºæ¨¡å‹çš„éœ€æ±‚ä¿ç•™æ›´å¤š GPU èµ„æº - ä¾‹å¦‚æ›´å¤§çš„æ‰¹å¤„ç†å¤§å°ï¼Œæˆ–å¯ç”¨æ­£å¸¸æƒ…å†µä¸‹æ— æ³•å®¹çº³çš„éå¸¸å¤§æ¨¡å‹ã€‚
2. å®ƒæä¾›äº†æ™ºèƒ½çš„ GPU å†…å­˜ç®¡ç†ç³»ç»Ÿï¼Œæœ€å°åŒ–å†…å­˜ç¢ç‰‡ï¼Œè¿™å†æ¬¡å…è®¸æ‚¨å®¹çº³æ›´å¤§çš„æ¨¡å‹å’Œæ•°æ®æ‰¹æ¬¡ã€‚

è™½ç„¶æ¥ä¸‹æ¥æˆ‘ä»¬å°†è¯¦ç»†è®¨è®ºé…ç½®ï¼Œä½†åœ¨å•ä¸ª GPU ä¸Šé€šè¿‡ DeepSpeed å®ç°å·¨å¤§æ€§èƒ½æå‡çš„å…³é”®æ˜¯åœ¨é…ç½®æ–‡ä»¶ä¸­è‡³å°‘æœ‰ä»¥ä¸‹é…ç½®ï¼š


```json
{
  "zero_optimization": {
     "stage": 2,
     "offload_optimizer": {
         "device": "cpu",
         "pin_memory": true
     },
     "allgather_partitions": true,
     "allgather_bucket_size": 2e8,
     "reduce_scatter": true,
     "reduce_bucket_size": 2e8,
     "overlap_comm": true,
     "contiguous_gradients": true
  }
}
```

è¿™ä¼šå¯ç”¨`optimizer offload`å’Œä¸€äº›å…¶ä»–é‡è¦åŠŸèƒ½ã€‚æ‚¨å¯ä»¥å°è¯•ä¸åŒçš„bufferå¤§å°ï¼Œæœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ä¸‹é¢çš„è®¨è®ºã€‚

å…³äºè¿™ç§å¯ç”¨ç±»å‹çš„å®é™…ä½¿ç”¨ç¤ºä¾‹ï¼Œè¯·å‚é˜… [æ­¤å¸–](https://github.com/huggingface/transformers/issues/8771#issuecomment-759176685)ã€‚

æ‚¨è¿˜å¯ä»¥å°è¯•ä½¿ç”¨æœ¬æ–‡åé¢è¿›ä¸€æ­¥è§£é‡Šçš„æ”¯æŒ`CPU å’Œ NVMe offload`åŠŸèƒ½çš„ZeRO-3 ã€‚


<!--- TODO: Benchmark whether we can get better performance out of ZeRO-3 vs. ZeRO-2 on a single GPU, and then
recommend ZeRO-3 config as starting one. -->

æ³¨æ„ï¼š

- å¦‚æœæ‚¨éœ€è¦åœ¨ç‰¹å®šçš„ GPU ä¸Šè¿è¡Œï¼Œè€Œä¸æ˜¯ GPU 0ï¼Œåˆ™æ— æ³•ä½¿ç”¨ `CUDA_VISIBLE_DEVICES` æ¥é™åˆ¶å¯ç”¨ GPU çš„å¯è§èŒƒå›´ã€‚ç›¸åï¼Œæ‚¨å¿…é¡»ä½¿ç”¨ä»¥ä¸‹è¯­æ³•ï¼š

  ```bash
  deepspeed --include localhost:1 examples/pytorch/translation/run_translation.py ...
  ```

  åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å‘Šè¯‰ DeepSpeed ä½¿ç”¨ GPU 1ï¼ˆç¬¬äºŒä¸ª GPUï¼‰ã€‚



<a id='deepspeed-multi-node'></a>

### å¤šèŠ‚ç‚¹å¯ç”¨

è¿™ä¸€éƒ¨åˆ†çš„ä¿¡æ¯ä¸ä»…é€‚ç”¨äº DeepSpeed é›†æˆï¼Œä¹Ÿé€‚ç”¨äºä»»ä½•å¤šèŠ‚ç‚¹ç¨‹åºã€‚ä½† DeepSpeed æä¾›äº†ä¸€ä¸ªæ¯”å…¶ä»–å¯åŠ¨å™¨æ›´æ˜“äºä½¿ç”¨çš„ `deepspeed` å¯åŠ¨å™¨ï¼Œé™¤éæ‚¨åœ¨ SLURM ç¯å¢ƒä¸­ã€‚

åœ¨æœ¬èŠ‚ï¼Œè®©æˆ‘ä»¬å‡è®¾æ‚¨æœ‰ä¸¤ä¸ªèŠ‚ç‚¹ï¼Œæ¯ä¸ªèŠ‚ç‚¹æœ‰ 8 å¼  GPUã€‚æ‚¨å¯ä»¥é€šè¿‡ `ssh hostname1` è®¿é—®ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ï¼Œé€šè¿‡ `ssh hostname2` è®¿é—®ç¬¬äºŒä¸ªèŠ‚ç‚¹ï¼Œä¸¤è€…å¿…é¡»èƒ½å¤Ÿåœ¨æœ¬åœ°é€šè¿‡ ssh æ— å¯†ç æ–¹å¼ç›¸äº’è®¿é—®ã€‚å½“ç„¶ï¼Œæ‚¨éœ€è¦å°†è¿™äº›ä¸»æœºï¼ˆèŠ‚ç‚¹ï¼‰åç§°é‡å‘½åä¸ºæ‚¨å®é™…ä½¿ç”¨çš„ä¸»æœºåç§°ã€‚


#### torch.distributed.runå¯åŠ¨å™¨


ä¾‹å¦‚ï¼Œè¦ä½¿ç”¨ `torch.distributed.run`ï¼Œæ‚¨å¯ä»¥æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š

```bash
python -m torch.distributed.run --nproc_per_node=8 --nnode=2 --node_rank=0 --master_addr=hostname1 \
--master_port=9901 your_program.py <normal cl args> --deepspeed ds_config.json
```

æ‚¨å¿…é¡» ssh åˆ°æ¯ä¸ªèŠ‚ç‚¹ï¼Œå¹¶åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šè¿è¡Œç›¸åŒçš„å‘½ä»¤ï¼ä¸ç”¨æ‹…å¿ƒï¼Œå¯åŠ¨å™¨ä¼šç­‰å¾…ä¸¤ä¸ªèŠ‚ç‚¹åŒæ­¥å®Œæˆã€‚

æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜… [torchrun](https://pytorch.org/docs/stable/elastic/run.html)ã€‚é¡ºä¾¿è¯´ä¸€ä¸‹ï¼Œè¿™ä¹Ÿæ˜¯æ›¿ä»£äº†å‡ ä¸ª PyTorch ç‰ˆæœ¬å‰çš„ `torch.distributed.launch` çš„å¯åŠ¨å™¨ã€‚


#### deepspeedå¯åŠ¨å™¨

è¦æ”¹ç”¨ `deepspeed` å¯åŠ¨å™¨ï¼Œé¦–å…ˆéœ€è¦åˆ›å»ºä¸€ä¸ª `hostfile` æ–‡ä»¶ï¼š

```
hostname1 slots=8
hostname2 slots=8
```
ç„¶åï¼Œæ‚¨å¯ä»¥è¿™æ ·å¯åŠ¨ï¼š

```bash
deepspeed --num_gpus 8 --num_nodes 2 --hostfile hostfile --master_addr hostname1 --master_port=9901 \
your_program.py <normal cl args> --deepspeed ds_config.json
```

ä¸ `torch.distributed.run` å¯åŠ¨å™¨ä¸åŒï¼Œ`deepspeed` å°†è‡ªåŠ¨åœ¨ä¸¤ä¸ªèŠ‚ç‚¹ä¸Šå¯åŠ¨æ­¤å‘½ä»¤ï¼

æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[èµ„æºé…ç½®ï¼ˆå¤šèŠ‚ç‚¹ï¼‰](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node)ã€‚


#### åœ¨ SLURM ç¯å¢ƒä¸­å¯åŠ¨

åœ¨ SLURM ç¯å¢ƒä¸­ï¼Œå¯ä»¥é‡‡ç”¨ä»¥ä¸‹æ–¹æ³•ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ª SLURM è„šæœ¬ `launch.slurm`ï¼Œæ‚¨éœ€è¦æ ¹æ®æ‚¨çš„å…·ä½“ SLURM ç¯å¢ƒè¿›è¡Œè°ƒæ•´ã€‚

```bash
#SBATCH --job-name=test-nodes        # name
#SBATCH --nodes=2                    # nodes
#SBATCH --ntasks-per-node=1          # crucial - only 1 task per dist per node!
#SBATCH --cpus-per-task=10           # number of cores per tasks
#SBATCH --gres=gpu:8                 # number of gpus
#SBATCH --time 20:00:00              # maximum execution time (HH:MM:SS)
#SBATCH --output=%x-%j.out           # output file name

export GPUS_PER_NODE=8
export MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)
export MASTER_PORT=9901

srun --jobid $SLURM_JOBID bash -c 'python -m torch.distributed.run \
 --nproc_per_node $GPUS_PER_NODE --nnodes $SLURM_NNODES --node_rank $SLURM_PROCID \
 --master_addr $MASTER_ADDR --master_port $MASTER_PORT \
your_program.py <normal cl args> --deepspeed ds_config.json'
```

å‰©ä¸‹çš„å°±æ˜¯è¿è¡Œå®ƒï¼š

```bash
sbatch launch.slurm
```

`srun` å°†è´Ÿè´£åœ¨æ‰€æœ‰èŠ‚ç‚¹ä¸ŠåŒæ—¶å¯åŠ¨ç¨‹åºã€‚


#### ä½¿ç”¨éå…±äº«æ–‡ä»¶ç³»ç»Ÿ

é»˜è®¤æƒ…å†µä¸‹ï¼ŒDeepSpeed å‡å®šå¤šèŠ‚ç‚¹ç¯å¢ƒä½¿ç”¨å…±äº«å­˜å‚¨ã€‚å¦‚æœä¸æ˜¯è¿™ç§æƒ…å†µï¼Œæ¯ä¸ªèŠ‚ç‚¹åªèƒ½çœ‹åˆ°æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿï¼Œä½ éœ€è¦è°ƒæ•´é…ç½®æ–‡ä»¶ï¼ŒåŒ…å«ä¸€ä¸ª [`checkpoint` éƒ¨åˆ†](https://www.deepspeed.ai/docs/config-json/#checkpoint-options)å¹¶è®¾ç½®å¦‚ä¸‹é€‰é¡¹ï¼š

```json
{
  "checkpoint": {
    "use_node_local_storage": true
  }
}
```

æˆ–è€…ï¼Œä½ è¿˜å¯ä»¥ä½¿ç”¨ [`Trainer`] çš„ `--save_on_each_node` å‚æ•°ï¼Œä¸Šè¿°é…ç½®å°†è‡ªåŠ¨æ·»åŠ ã€‚


<a id='deepspeed-notebook'></a>

### åœ¨Notebookså¯ç”¨

åœ¨å°†`notebook cells`ä½œä¸ºè„šæœ¬è¿è¡Œçš„æƒ…å†µä¸‹ï¼Œé—®é¢˜åœ¨äºæ²¡æœ‰æ­£å¸¸çš„ `deepspeed` å¯åŠ¨å™¨å¯ä¾èµ–ï¼Œå› æ­¤åœ¨æŸäº›è®¾ç½®ä¸‹ï¼Œæˆ‘ä»¬å¿…é¡»ä»¿çœŸè¿è¡Œå®ƒã€‚

å¦‚æœæ‚¨åªä½¿ç”¨ä¸€ä¸ª GPUï¼Œä»¥ä¸‹æ˜¯å¦‚ä½•è°ƒæ•´notebookä¸­çš„è®­ç»ƒä»£ç ä»¥ä½¿ç”¨ DeepSpeedã€‚

```python
# DeepSpeed requires a distributed environment even when only one process is used.
# This emulates a launcher in the notebook
import os

os.environ["MASTER_ADDR"] = "localhost"
os.environ["MASTER_PORT"] = "9994"  # modify if RuntimeError: Address already in use
os.environ["RANK"] = "0"
os.environ["LOCAL_RANK"] = "0"
os.environ["WORLD_SIZE"] = "1"

# Now proceed as normal, plus pass the deepspeed config file
training_args = TrainingArguments(..., deepspeed="ds_config_zero3.json")
trainer = Trainer(...)
trainer.train()
```

æ³¨æ„ï¼š`...` ä»£è¡¨æ‚¨ä¼ é€’ç»™å‡½æ•°çš„æ­£å¸¸å‚æ•°ã€‚

å¦‚æœè¦ä½¿ç”¨å¤šäºä¸€ä¸ª GPUï¼Œæ‚¨å¿…é¡»åœ¨ DeepSpeed ä¸­ä½¿ç”¨å¤šè¿›ç¨‹ç¯å¢ƒã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæ‚¨å¿…é¡»ä½¿ç”¨ä¸“é—¨çš„å¯åŠ¨å™¨æ¥å®ç°è¿™ä¸€ç›®çš„ï¼Œè€Œä¸èƒ½é€šè¿‡ä»¿çœŸæœ¬èŠ‚å¼€å¤´å‘ˆç°çš„åˆ†å¸ƒå¼ç¯å¢ƒæ¥å®Œæˆã€‚

å¦‚æœæƒ³è¦åœ¨notebookä¸­åŠ¨æ€åˆ›å»ºé…ç½®æ–‡ä»¶å¹¶ä¿å­˜åœ¨å½“å‰ç›®å½•ï¼Œæ‚¨å¯ä»¥åœ¨ä¸€ä¸ªä¸“ç”¨çš„cellä¸­ä½¿ç”¨ï¼š

```python no-style
%%bash
cat <<'EOT' > ds_config_zero3.json
{
    "fp16": {
        "enabled": "auto",
        "loss_scale": 0,
        "loss_scale_window": 1000,
        "initial_scale_power": 16,
        "hysteresis": 2,
        "min_loss_scale": 1
    },

    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": "auto",
            "betas": "auto",
            "eps": "auto",
            "weight_decay": "auto"
        }
    },

    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": "auto",
            "warmup_max_lr": "auto",
            "warmup_num_steps": "auto"
        }
    },

    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": true
        },
        "offload_param": {
            "device": "cpu",
            "pin_memory": true
        },
        "overlap_comm": true,
        "contiguous_gradients": true,
        "sub_group_size": 1e9,
        "reduce_bucket_size": "auto",
        "stage3_prefetch_bucket_size": "auto",
        "stage3_param_persistence_threshold": "auto",
        "stage3_max_live_parameters": 1e9,
        "stage3_max_reuse_distance": 1e9,
        "stage3_gather_16bit_weights_on_model_save": true
    },

    "gradient_accumulation_steps": "auto",
    "gradient_clipping": "auto",
    "steps_per_print": 2000,
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "wall_clock_breakdown": false
}
EOT
```

å¦‚æœè®­ç»ƒè„šæœ¬åœ¨ä¸€ä¸ªæ™®é€šæ–‡ä»¶ä¸­è€Œä¸æ˜¯åœ¨notebook cellsä¸­ï¼Œæ‚¨å¯ä»¥é€šè¿‡ç¬”è®°æœ¬ä¸­çš„ shell æ­£å¸¸å¯åŠ¨ `deepspeed`ã€‚ä¾‹å¦‚ï¼Œè¦ä½¿ç”¨ `run_translation.py`ï¼Œæ‚¨å¯ä»¥è¿™æ ·å¯åŠ¨ï¼š

```python no-style
!git clone https://github.com/huggingface/transformers
!cd transformers; deepspeed examples/pytorch/translation/run_translation.py ...
```

æˆ–è€…ä½¿ç”¨ `%%bash` é­”æœ¯å‘½ä»¤ï¼Œæ‚¨å¯ä»¥ç¼–å†™å¤šè¡Œä»£ç ï¼Œç”¨äºè¿è¡Œ shell ç¨‹åºï¼š

```python no-style
%%bash

git clone https://github.com/huggingface/transformers
cd transformers
deepspeed examples/pytorch/translation/run_translation.py ...
```

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨ä¸éœ€è¦æœ¬èŠ‚å¼€å¤´å‘ˆç°çš„ä»»ä½•ä»£ç ã€‚

æ³¨æ„ï¼šè™½ç„¶ `%%bash` é­”æœ¯å‘½ä»¤å¾ˆæ–¹ä¾¿ï¼Œä½†ç›®å‰å®ƒä¼šç¼“å†²è¾“å‡ºï¼Œå› æ­¤åœ¨è¿›ç¨‹å®Œæˆä¹‹å‰æ‚¨çœ‹ä¸åˆ°æ—¥å¿—ã€‚


<a id='deepspeed-config'></a>

### é…ç½®

æœ‰å…³å¯ä»¥åœ¨ DeepSpeed é…ç½®æ–‡ä»¶ä¸­ä½¿ç”¨çš„å®Œæ•´é…ç½®é€‰é¡¹çš„è¯¦ç»†æŒ‡å—ï¼Œè¯·å‚é˜…[ä»¥ä¸‹æ–‡æ¡£](https://www.deepspeed.ai/docs/config-json/)ã€‚

æ‚¨å¯ä»¥åœ¨ [DeepSpeedExamples ä»“åº“](https://github.com/deepspeedai/DeepSpeedExamples)ä¸­æ‰¾åˆ°è§£å†³å„ç§å®é™…éœ€æ±‚çš„æ•°åä¸ª DeepSpeed é…ç½®ç¤ºä¾‹ã€‚

```bash
git clone https://github.com/deepspeedai/DeepSpeedExamples
cd DeepSpeedExamples
find . -name '*json'
```

å»¶ç»­ä¸Šé¢çš„ä»£ç ï¼Œå‡è®¾æ‚¨è¦é…ç½® Lamb ä¼˜åŒ–å™¨ã€‚é‚£ä¹ˆæ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼åœ¨ç¤ºä¾‹çš„ `.json` æ–‡ä»¶ä¸­è¿›è¡Œæœç´¢ï¼š

```bash
grep -i Lamb $(find . -name '*json')
```

è¿˜å¯ä»¥åœ¨[ä¸»ä»“](https://github.com/deepspeedai/DeepSpeed)ä¸­æ‰¾åˆ°æ›´å¤šç¤ºä¾‹ã€‚

åœ¨ä½¿ç”¨ DeepSpeed æ—¶ï¼Œæ‚¨æ€»æ˜¯éœ€è¦æä¾›ä¸€ä¸ª DeepSpeed é…ç½®æ–‡ä»¶ï¼Œä½†æ˜¯ä¸€äº›é…ç½®å‚æ•°å¿…é¡»é€šè¿‡å‘½ä»¤è¡Œè¿›è¡Œé…ç½®ã€‚æ‚¨å°†åœ¨æœ¬æŒ‡å—çš„å‰©ä½™ç« èŠ‚æ‰¾åˆ°è¿™äº›ç»†å¾®å·®åˆ«ã€‚

ä¸ºäº†äº†è§£ DeepSpeed é…ç½®æ–‡ä»¶ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªæ¿€æ´» ZeRO stage 2 åŠŸèƒ½çš„ç¤ºä¾‹ï¼ŒåŒ…æ‹¬ä¼˜åŒ–å™¨çŠ¶æ€çš„ CPU offloadï¼Œä½¿ç”¨ `AdamW` ä¼˜åŒ–å™¨å’Œ `WarmupLR`  è°ƒåº¦å™¨ï¼Œå¹¶ä¸”å¦‚æœä¼ é€’äº† `--fp16` å‚æ•°å°†å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼š

```json
{
    "fp16": {
        "enabled": "auto",
        "loss_scale": 0,
        "loss_scale_window": 1000,
        "initial_scale_power": 16,
        "hysteresis": 2,
        "min_loss_scale": 1
    },

    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": "auto",
            "betas": "auto",
            "eps": "auto",
            "weight_decay": "auto"
        }
    },

    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": "auto",
            "warmup_max_lr": "auto",
            "warmup_num_steps": "auto"
        }
    },

    "zero_optimization": {
        "stage": 2,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": true
        },
        "allgather_partitions": true,
        "allgather_bucket_size": 2e8,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 2e8,
        "contiguous_gradients": true
    },

    "gradient_accumulation_steps": "auto",
    "gradient_clipping": "auto",
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
}
```

å½“æ‚¨æ‰§è¡Œç¨‹åºæ—¶ï¼ŒDeepSpeed å°†æŠŠå®ƒä» [`Trainer`] æ”¶åˆ°çš„é…ç½®æ—¥å¿—è¾“å‡ºåˆ°consoleï¼Œå› æ­¤æ‚¨å¯ä»¥çœ‹åˆ°ä¼ é€’ç»™å®ƒçš„æœ€ç»ˆé…ç½®ã€‚



<a id='deepspeed-config-passing'></a>

### ä¼ é€’é…ç½®

æ­£å¦‚æœ¬æ–‡æ¡£è®¨è®ºçš„é‚£æ ·ï¼Œé€šå¸¸å°† DeepSpeed é…ç½®ä½œä¸ºæŒ‡å‘ JSON æ–‡ä»¶çš„è·¯å¾„ä¼ é€’ï¼Œä½†å¦‚æœæ‚¨æ²¡æœ‰ä½¿ç”¨å‘½ä»¤è¡Œç•Œé¢é…ç½®è®­ç»ƒï¼Œè€Œæ˜¯é€šè¿‡ [`TrainingArguments`] å®ä¾‹åŒ– [`Trainer`]ï¼Œé‚£ä¹ˆå¯¹äº `deepspeed` å‚æ•°ï¼Œä½ å¯ä»¥ä¼ é€’ä¸€ä¸ªåµŒå¥—çš„ `dict`ã€‚è¿™ä½¿æ‚¨èƒ½å¤Ÿå³æ—¶åˆ›å»ºé…ç½®ï¼Œè€Œæ— éœ€åœ¨å°†å…¶ä¼ é€’ç»™ [`TrainingArguments`] ä¹‹å‰å°†å…¶å†™å…¥æ–‡ä»¶ç³»ç»Ÿã€‚

æ€»ç»“èµ·æ¥ï¼Œæ‚¨å¯ä»¥è¿™æ ·åšï¼š

```python
TrainingArguments(..., deepspeed="/path/to/ds_config.json")
```

æˆ–è€…:

```python
ds_config_dict = dict(scheduler=scheduler_params, optimizer=optimizer_params)
TrainingArguments(..., deepspeed=ds_config_dict)
```

<a id='deepspeed-config-shared'></a>

### å…±äº«é…ç½®


<Tip warning={true}>

è¿™ä¸€éƒ¨åˆ†æ˜¯å¿…è¯»çš„ã€‚

</Tip>

ä¸€äº›é…ç½®å€¼å¯¹äº [`Trainer`] å’Œ DeepSpeed æ­£å¸¸è¿è¡Œéƒ½æ˜¯å¿…éœ€çš„ï¼Œå› æ­¤ï¼Œä¸ºäº†é˜²æ­¢å®šä¹‰å†²çªåŠå¯¼è‡´çš„éš¾ä»¥æ£€æµ‹çš„é”™è¯¯ï¼Œæˆ‘ä»¬é€‰æ‹©é€šè¿‡ [`Trainer`] å‘½ä»¤è¡Œå‚æ•°é…ç½®è¿™äº›å€¼ã€‚

æ­¤å¤–ï¼Œä¸€äº›é…ç½®å€¼æ˜¯åŸºäºæ¨¡å‹çš„é…ç½®è‡ªåŠ¨æ´¾ç”Ÿçš„ï¼Œå› æ­¤ï¼Œä¸å…¶è®°ä½æ‰‹åŠ¨è°ƒæ•´å¤šä¸ªå€¼ï¼Œæœ€å¥½è®© [`Trainer`] ä¸ºæ‚¨åšå¤§éƒ¨åˆ†é…ç½®ã€‚

å› æ­¤ï¼Œåœ¨æœ¬æŒ‡å—çš„å…¶ä½™éƒ¨åˆ†ï¼Œæ‚¨å°†æ‰¾åˆ°ä¸€ä¸ªç‰¹æ®Šçš„é…ç½®å€¼ï¼š`auto`ï¼Œå½“è®¾ç½®æ—¶å°†è‡ªåŠ¨å°†å‚æ•°æ›¿æ¢ä¸ºæ­£ç¡®æˆ–æœ€æœ‰æ•ˆçš„å€¼ã€‚è¯·éšæ„é€‰æ‹©å¿½ç•¥æ­¤å»ºè®®æˆ–æ˜¾å¼è®¾ç½®è¯¥å€¼ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¯·åŠ¡å¿…ç¡®ä¿ [`Trainer`] å‚æ•°å’Œ DeepSpeed é…ç½®ä¿æŒä¸€è‡´ã€‚ä¾‹å¦‚ï¼Œæ‚¨æ˜¯å¦ä½¿ç”¨ç›¸åŒçš„å­¦ä¹ ç‡ã€æ‰¹é‡å¤§å°æˆ–æ¢¯åº¦ç´¯ç§¯è®¾ç½®ï¼Ÿå¦‚æœè¿™äº›ä¸åŒ¹é…ï¼Œè®­ç»ƒå¯èƒ½ä»¥éå¸¸éš¾ä»¥æ£€æµ‹çš„æ–¹å¼å¤±è´¥ã€‚è¯·é‡è§†è¯¥è­¦å‘Šã€‚

è¿˜æœ‰ä¸€äº›å‚æ•°æ˜¯ä»…é€‚ç”¨äº DeepSpeed çš„ï¼Œå¹¶ä¸”è¿™äº›å‚æ•°å¿…é¡»æ‰‹åŠ¨è®¾ç½®ä»¥é€‚åº”æ‚¨çš„éœ€æ±‚ã€‚

åœ¨æ‚¨è‡ªå·±çš„ç¨‹åºä¸­ï¼Œå¦‚æœæ‚¨æƒ³è¦ä½œä¸ºä¸»åŠ¨ä¿®æ”¹ DeepSpeed é…ç½®å¹¶ä»¥æ­¤é…ç½® [`TrainingArguments`]ï¼Œæ‚¨è¿˜å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•ã€‚æ­¥éª¤å¦‚ä¸‹ï¼š

1. åˆ›å»ºæˆ–åŠ è½½è¦ç”¨ä½œä¸»é…ç½®çš„ DeepSpeed é…ç½®
2. æ ¹æ®è¿™äº›å‚æ•°å€¼åˆ›å»º [`TrainingArguments`] å¯¹è±¡

è¯·æ³¨æ„ï¼Œä¸€äº›å€¼ï¼Œæ¯”å¦‚ `scheduler.params.total_num_steps`ï¼Œæ˜¯åœ¨ [`Trainer`] çš„ `train` è¿‡ç¨‹ä¸­è®¡ç®—çš„ï¼Œä½†å½“ç„¶æ‚¨ä¹Ÿå¯ä»¥è‡ªå·±è®¡ç®—è¿™äº›å€¼ã€‚


<a id='deepspeed-zero'></a>

### ZeRO

[Zero Redundancy Optimizer (ZeRO)](https://www.deepspeed.ai/tutorials/zero/) æ˜¯ DeepSpeed çš„å·¥ä½œæ ¸å¿ƒã€‚å®ƒæ”¯æŒ3ä¸ªä¸åŒçº§åˆ«ï¼ˆstagesï¼‰çš„ä¼˜åŒ–ã€‚Stage 1 å¯¹äºæ‰©å±•æ€§æ¥è¯´ä¸æ˜¯å¾ˆæœ‰è¶£ï¼Œå› æ­¤æœ¬æ–‡æ¡£é‡ç‚¹å…³æ³¨Stage 2å’ŒStage 3ã€‚Stage 3é€šè¿‡æœ€æ–°çš„ ZeRO-Infinity è¿›ä¸€æ­¥æ”¹è¿›ã€‚ä½ å¯ä»¥åœ¨ DeepSpeed æ–‡æ¡£ä¸­æ‰¾åˆ°æ›´è¯¦ç»†çš„ä¿¡æ¯ã€‚

é…ç½®æ–‡ä»¶çš„ `zero_optimization` éƒ¨åˆ†æ˜¯æœ€é‡è¦çš„éƒ¨åˆ†ï¼ˆ[æ–‡æ¡£](https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training)ï¼‰ï¼Œå› ä¸ºåœ¨è¿™é‡Œæ‚¨å®šä¹‰äº†è¦å¯ç”¨å“ªäº› ZeRO stages ä»¥åŠå¦‚ä½•é…ç½®å®ƒä»¬ã€‚æ‚¨å¯ä»¥åœ¨ DeepSpeed æ–‡æ¡£ä¸­æ‰¾åˆ°æ¯ä¸ªå‚æ•°çš„è§£é‡Šã€‚

è¿™ä¸€éƒ¨åˆ†å¿…é¡»é€šè¿‡ DeepSpeed é…ç½®æ–‡ä»¶å•ç‹¬é…ç½® - [`Trainer`] ä¸æä¾›ç›¸åº”çš„å‘½ä»¤è¡Œå‚æ•°ã€‚

æ³¨æ„ï¼šç›®å‰ DeepSpeed ä¸éªŒè¯å‚æ•°åç§°ï¼Œå› æ­¤å¦‚æœæ‚¨æ‹¼é”™äº†ä»»ä½•å‚æ•°ï¼Œå®ƒå°†ä½¿ç”¨æ‹¼å†™é”™è¯¯çš„å‚æ•°çš„é»˜è®¤è®¾ç½®ã€‚æ‚¨å¯ä»¥è§‚å¯Ÿ DeepSpeed å¼•æ“å¯åŠ¨æ—¥å¿—æ¶ˆæ¯ï¼Œçœ‹çœ‹å®ƒå°†ä½¿ç”¨å“ªäº›å€¼ã€‚

<a id='deepspeed-zero2-config'></a>

#### ZeRO-2 é…ç½®

ä»¥ä¸‹æ˜¯ ZeRO stage 2 çš„é…ç½®ç¤ºä¾‹ï¼š

```json
{
    "zero_optimization": {
        "stage": 2,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": true
        },
        "allgather_partitions": true,
        "allgather_bucket_size": 5e8,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 5e8,
        "contiguous_gradients": true
    }
}
```

**æ€§èƒ½è°ƒä¼˜ï¼š**

- å¯ç”¨ `offload_optimizer` åº”è¯¥å‡å°‘ GPU å†…å­˜ä½¿ç”¨ï¼ˆéœ€è¦ `"stage": 2`ï¼‰ã€‚
- `"overlap_comm": true` é€šè¿‡å¢åŠ  GPU å†…å­˜ä½¿ç”¨æ¥é™ä½all-reduce çš„å»¶è¿Ÿã€‚ `overlap_comm` ä½¿ç”¨äº† `allgather_bucket_size` å’Œ `reduce_bucket_size` å€¼çš„4.5å€ã€‚å› æ­¤ï¼Œå¦‚æœå®ƒä»¬è®¾ç½®ä¸º `5e8`ï¼Œè¿™å°†éœ€è¦ä¸€ä¸ª9GBçš„å†…å­˜å ç”¨ï¼ˆ`5e8 x 2Bytes x 2 x 4.5`ï¼‰ã€‚å› æ­¤ï¼Œå¦‚æœæ‚¨çš„ GPU å†…å­˜ä¸º8GBæˆ–æ›´å°ï¼Œä¸ºäº†é¿å…å‡ºç°OOMé”™è¯¯ï¼Œæ‚¨éœ€è¦å°†è¿™äº›å‚æ•°å‡å°åˆ°çº¦ `2e8`ï¼Œè¿™å°†éœ€è¦3.6GBã€‚å¦‚æœæ‚¨çš„ GPU å®¹é‡æ›´å¤§ï¼Œå½“æ‚¨å¼€å§‹é‡åˆ°OOMæ—¶ï¼Œä½ å¯èƒ½ä¹Ÿéœ€è¦è¿™æ ·åšã€‚
- å½“å‡å°è¿™äº›buffersæ—¶ï¼Œæ‚¨ä»¥æ›´æ…¢çš„é€šä¿¡é€Ÿåº¦æ¥æ¢å–æ›´å¤šçš„ GPU å†…å­˜ã€‚bufferså¤§å°è¶Šå°ï¼Œé€šä¿¡é€Ÿåº¦è¶Šæ…¢ï¼ŒGPU å¯ç”¨äºå…¶ä»–ä»»åŠ¡çš„å†…å­˜å°±è¶Šå¤šã€‚å› æ­¤ï¼Œå¦‚æœæ›´å¤§çš„æ‰¹å¤„ç†å¤§å°å¾ˆé‡è¦ï¼Œé‚£ä¹ˆç¨å¾®å‡æ…¢è®­ç»ƒæ—¶é—´å¯èƒ½æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æƒè¡¡ã€‚

æ­¤å¤–ï¼Œ`deepspeed==0.4.4` æ·»åŠ äº†ä¸€ä¸ªæ–°é€‰é¡¹ `round_robin_gradients`ï¼Œæ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å¯ç”¨ï¼š

```json
{
    "zero_optimization": {
        "round_robin_gradients": true
    }
}
```
è¿™æ˜¯ä¸€ä¸ªç”¨äº CPU offloading çš„stage 2ä¼˜åŒ–ï¼Œé€šè¿‡ç»†ç²’åº¦æ¢¯åº¦åˆ†åŒºåœ¨ ranks ä¹‹é—´å¹¶è¡Œå¤åˆ¶åˆ° CPU å†…å­˜ï¼Œä»è€Œå®ç°äº†æ€§èƒ½çš„æå‡ã€‚æ€§èƒ½ä¼˜åŠ¿éšç€æ¢¯åº¦ç´¯ç§¯æ­¥éª¤ï¼ˆåœ¨ä¼˜åŒ–å™¨æ­¥éª¤ä¹‹é—´è¿›è¡Œæ›´å¤šå¤åˆ¶ï¼‰æˆ– GPU æ•°é‡ï¼ˆå¢åŠ å¹¶è¡Œæ€§ï¼‰å¢åŠ è€Œå¢åŠ ã€‚

<a id='deepspeed-zero3-config'></a>

#### ZeRO-3 é…ç½®

ä»¥ä¸‹æ˜¯ ZeRO stage 3çš„é…ç½®ç¤ºä¾‹ï¼š

```json
{
    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": true
        },
        "offload_param": {
            "device": "cpu",
            "pin_memory": true
        },
        "overlap_comm": true,
        "contiguous_gradients": true,
        "sub_group_size": 1e9,
        "reduce_bucket_size": "auto",
        "stage3_prefetch_bucket_size": "auto",
        "stage3_param_persistence_threshold": "auto",
        "stage3_max_live_parameters": 1e9,
        "stage3_max_reuse_distance": 1e9,
        "stage3_gather_16bit_weights_on_model_save": true
    }
}
```

å¦‚æœæ‚¨å› ä¸ºä½ çš„æ¨¡å‹æˆ–æ¿€æ´»å€¼è¶…è¿‡ GPU å†…å­˜è€Œé‡åˆ°OOMé—®é¢˜ï¼Œå¹¶ä¸”æ‚¨æœ‰æœªä½¿ç”¨çš„ CPU å†…å­˜ï¼Œå¯ä»¥é€šè‚¡ç¥¨ä½¿ç”¨ `"device": "cpu"` å°†ä¼˜åŒ–å™¨çŠ¶æ€å’Œå‚æ•°å¸è½½åˆ° CPU å†…å­˜ä¸­ï¼Œæ¥è§£å†³è¿™ä¸ªé™åˆ¶ã€‚å¦‚æœæ‚¨ä¸æƒ³å¸è½½åˆ° CPU å†…å­˜ï¼Œå¯ä»¥åœ¨ `device` æ¡ç›®ä¸­ä½¿ç”¨ `none` ä»£æ›¿ `cpu`ã€‚å°†ä¼˜åŒ–å™¨çŠ¶æ€å¸è½½åˆ° NVMe ä¸Šä¼šåœ¨åé¢è¿›ä¸€æ­¥è®¨è®ºã€‚

é€šè¿‡å°† `pin_memory` è®¾ç½®ä¸º `true` å¯ç”¨å›ºå®šå†…å­˜ã€‚æ­¤åŠŸèƒ½ä¼šä»¥å‡å°‘å¯ç”¨äºå…¶ä»–è¿›ç¨‹çš„å†…å­˜ä¸ºä»£ä»·æ¥æé«˜ååé‡ã€‚å›ºå®šå†…å­˜è¢«åˆ†é…ç»™ç‰¹å®šè¯·æ±‚å®ƒçš„è¿›ç¨‹ï¼Œé€šå¸¸æ¯”æ™®é€š CPU å†…å­˜è®¿é—®é€Ÿåº¦æ›´å¿«ã€‚

**æ€§èƒ½è°ƒä¼˜ï¼š**

- `stage3_max_live_parameters`: `1e9`
- `stage3_max_reuse_distance`: `1e9`

å¦‚æœé‡åˆ°OOMé—®é¢˜ï¼Œè¯·å‡å° `stage3_max_live_parameters` å’Œ `stage3_max_reuse_distance`ã€‚å®ƒä»¬å¯¹æ€§èƒ½çš„å½±å“åº”è¯¥å¾ˆå°ï¼Œé™¤éæ‚¨æ­£åœ¨è¿›è¡Œæ¿€æ´»å€¼checkpointingã€‚`1e9` å¤§çº¦ä¼šæ¶ˆè€— ~2GBã€‚å†…å­˜ç”± `stage3_max_live_parameters` å’Œ `stage3_max_reuse_distance` å…±äº«ï¼Œæ‰€ä»¥å®ƒä¸æ˜¯å åŠ çš„ï¼Œè€Œæ˜¯æ€»å…±2GBã€‚

`stage3_max_live_parameters` æ˜¯åœ¨ä»»ä½•ç»™å®šæ—¶é—´è¦åœ¨ GPU ä¸Šä¿ç•™å¤šå°‘ä¸ªå®Œæ•´å‚æ•°çš„ä¸Šé™ã€‚"reuse distance" æ˜¯æˆ‘ä»¬ç”¨æ¥ç¡®å®šå‚æ•°åœ¨å°†æ¥ä½•æ—¶ä¼šå†æ¬¡ä½¿ç”¨çš„åº¦é‡æ ‡å‡†ï¼Œæˆ‘ä»¬ä½¿ç”¨ `stage3_max_reuse_distance` æ¥å†³å®šæ˜¯ä¸¢å¼ƒå‚æ•°è¿˜æ˜¯ä¿ç•™å‚æ•°ã€‚å¦‚æœä¸€ä¸ªå‚æ•°åœ¨ä¸ä¹…çš„å°†æ¥ï¼ˆå°äº `stage3_max_reuse_distance`ï¼‰å°†è¢«å†æ¬¡ä½¿ç”¨ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°†å…¶ä¿ç•™ä»¥å‡å°‘é€šä¿¡å¼€é”€ã€‚è¿™åœ¨å¯ç”¨æ¿€æ´»å€¼checkpoingæ—¶éå¸¸æœ‰ç”¨ï¼Œå…¶ä¸­æˆ‘ä»¬ä»¥å•å±‚ç²’åº¦è¿›è¡Œå‰å‘é‡è®¡ç®—å’Œåå‘ä¼ æ’­ï¼Œå¹¶å¸Œæœ›åœ¨åå‘ä¼ æ’­æœŸé—´ä¿ç•™å‰å‘é‡è®¡ç®—ä¸­çš„å‚æ•°ã€‚

ä»¥ä¸‹é…ç½®å€¼å–å†³äºæ¨¡å‹çš„éšè—å¤§å°ï¼š

- `reduce_bucket_size`: `hidden_size*hidden_size`
- `stage3_prefetch_bucket_size`: `0.9 * hidden_size * hidden_size`
- `stage3_param_persistence_threshold`: `10 * hidden_size`

å› æ­¤ï¼Œå°†è¿™äº›å€¼è®¾ç½®ä¸º `auto`ï¼Œ[`Trainer`] å°†è‡ªåŠ¨åˆ†é…æ¨èçš„å‚æ•°å€¼ã€‚å½“ç„¶ï¼Œå¦‚æœæ‚¨æ„¿æ„ï¼Œä¹Ÿå¯ä»¥æ˜¾å¼è®¾ç½®è¿™äº›å€¼ã€‚

`stage3_gather_16bit_weights_on_model_save` åœ¨æ¨¡å‹ä¿å­˜æ—¶å¯ç”¨æ¨¡å‹çš„ fp16 æƒé‡æ•´åˆã€‚å¯¹äºå¤§æ¨¡å‹å’Œå¤šä¸ª GPUï¼Œæ— è®ºæ˜¯åœ¨å†…å­˜è¿˜æ˜¯é€Ÿåº¦æ–¹é¢ï¼Œè¿™éƒ½æ˜¯ä¸€é¡¹æ˜‚è´µçš„æ“ä½œã€‚ç›®å‰å¦‚æœè®¡åˆ’æ¢å¤è®­ç»ƒï¼Œè¿™æ˜¯å¿…éœ€çš„ã€‚è¯·æ³¨æ„æœªæ¥çš„æ›´æ–°å¯èƒ½ä¼šåˆ é™¤æ­¤é™åˆ¶å¹¶è®©ä½¿ç”¨æ›´åŠ çµæ´»ã€‚

å¦‚æœæ‚¨ä» ZeRO-2 é…ç½®è¿ç§»ï¼Œè¯·æ³¨æ„ `allgather_partitions`ã€`allgather_bucket_size` å’Œ `reduce_scatter` é…ç½®å‚æ•°åœ¨ ZeRO-3 ä¸­ä¸è¢«ä½¿ç”¨ã€‚å¦‚æœä¿ç•™è¿™äº›é…ç½®æ–‡ä»¶ï¼Œå®ƒä»¬å°†è¢«å¿½ç•¥ã€‚

- `sub_group_size`: `1e9`

`sub_group_size` æ§åˆ¶åœ¨ä¼˜åŒ–å™¨æ­¥éª¤æœŸé—´æ›´æ–°å‚æ•°çš„ç²’åº¦ã€‚å‚æ•°è¢«åˆ†ç»„åˆ°å¤§å°ä¸º `sub_group_size` çš„æ¡¶ä¸­ï¼Œæ¯ä¸ªæ¡¶é€ä¸ªæ›´æ–°ã€‚åœ¨ ZeRO-Infinity ä¸­ä¸ NVMe offloadä¸€èµ·ä½¿ç”¨æ—¶ï¼Œ`sub_group_size` æ§åˆ¶äº†åœ¨ä¼˜åŒ–å™¨æ­¥éª¤æœŸé—´åœ¨ NVMe å’Œ CPU å†…å­˜ä¹‹é—´ç§»åŠ¨æ¨¡å‹çŠ¶æ€çš„ç²’åº¦ã€‚è¿™å¯ä»¥é˜²æ­¢éå¸¸å¤§çš„æ¨¡å‹è€—å°½ CPU å†…å­˜ã€‚

å½“ä¸ä½¿ç”¨ NVMe offloadæ—¶ï¼Œå¯ä»¥å°† `sub_group_size` ä¿ç•™ä¸ºå…¶é»˜è®¤å€¼ *1e9*ã€‚åœ¨ä»¥ä¸‹æƒ…å†µä¸‹ï¼Œæ‚¨å¯èƒ½éœ€è¦æ›´æ”¹å…¶é»˜è®¤å€¼ï¼š

1. åœ¨ä¼˜åŒ–å™¨æ­¥éª¤ä¸­é‡åˆ°OOMï¼šå‡å° `sub_group_size` ä»¥å‡å°‘ä¸´æ—¶buffersçš„å†…å­˜åˆ©ç”¨
2. ä¼˜åŒ–å™¨æ­¥éª¤èŠ±è´¹å¾ˆé•¿æ—¶é—´ï¼šå¢åŠ  `sub_group_size` ä»¥æé«˜ç”±äºå¢åŠ çš„æ•°æ®buffersè€Œå¯¼è‡´çš„å¸¦å®½åˆ©ç”¨ç‡ã€‚


#### ZeRO-0 é…ç½®

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬å°† Stage 0 å’Œ 1 æ”¾åœ¨æœ€åï¼Œå› ä¸ºå®ƒä»¬å¾ˆå°‘ä½¿ç”¨ã€‚

Stage 0 ç¦ç”¨äº†æ‰€æœ‰ç±»å‹çš„åˆ†ç‰‡ï¼Œåªæ˜¯å°† DeepSpeed ä½œä¸º DDP ä½¿ç”¨ã€‚æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å¯ç”¨ï¼š

```json
{
    "zero_optimization": {
        "stage": 0
    }
}
```

è¿™å°†å®è´¨ä¸Šç¦ç”¨ ZeROï¼Œè€Œæ— éœ€æ›´æ”¹å…¶ä»–ä»»ä½•å†…å®¹ã€‚


#### ZeRO-1 é…ç½®


Stage 1 ç­‰åŒäº Stage 2 å‡å»æ¢¯åº¦åˆ†ç‰‡ã€‚æ‚¨å¯ä»¥å°è¯•ä½¿ç”¨ä»¥ä¸‹é…ç½®ï¼Œä»…å¯¹ä¼˜åŒ–å™¨çŠ¶æ€è¿›è¡Œåˆ†ç‰‡ï¼Œä»¥ç¨å¾®åŠ é€Ÿï¼š


```json
{
    "zero_optimization": {
        "stage": 1
    }
}
```



<a id='deepspeed-nvme'></a>

### NVMe æ”¯æŒ

ZeRO-Infinity é€šè¿‡ä½¿ç”¨ NVMe å†…å­˜æ‰©å±• GPU å’Œ CPU å†…å­˜ï¼Œä»è€Œå…è®¸è®­ç»ƒéå¸¸å¤§çš„æ¨¡å‹ã€‚ç”±äºæ™ºèƒ½åˆ†åŒºå’Œå¹³é“ºç®—æ³•ï¼Œåœ¨offloadæœŸé—´æ¯ä¸ª GPU éœ€è¦å‘é€å’Œæ¥æ”¶éå¸¸å°é‡çš„æ•°æ®ï¼Œå› æ­¤ NVMe è¢«è¯æ˜é€‚ç”¨äºè®­ç»ƒè¿‡ç¨‹ä¸­æä¾›æ›´å¤§çš„æ€»å†…å­˜æ± ã€‚ZeRO-Infinity éœ€è¦å¯ç”¨ ZeRO-3ã€‚

ä»¥ä¸‹é…ç½®ç¤ºä¾‹å¯ç”¨ NVMe æ¥offloadä¼˜åŒ–å™¨çŠ¶æ€å’Œå‚æ•°ï¼š

```json
{
    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": {
            "device": "nvme",
            "nvme_path": "/local_nvme",
            "pin_memory": true,
            "buffer_count": 4,
            "fast_init": false
        },
        "offload_param": {
            "device": "nvme",
            "nvme_path": "/local_nvme",
            "pin_memory": true,
            "buffer_count": 5,
            "buffer_size": 1e8,
            "max_in_cpu": 1e9
        },
        "aio": {
            "block_size": 262144,
            "queue_depth": 32,
            "thread_count": 1,
            "single_submit": false,
            "overlap_events": true
        },
        "overlap_comm": true,
        "contiguous_gradients": true,
        "sub_group_size": 1e9,
        "reduce_bucket_size": "auto",
        "stage3_prefetch_bucket_size": "auto",
        "stage3_param_persistence_threshold": "auto",
        "stage3_max_live_parameters": 1e9,
        "stage3_max_reuse_distance": 1e9,
        "stage3_gather_16bit_weights_on_model_save": true
    },
}
```

æ‚¨å¯ä»¥é€‰æ‹©å°†ä¼˜åŒ–å™¨çŠ¶æ€å’Œå‚æ•°éƒ½å¸è½½åˆ° NVMeï¼Œä¹Ÿå¯ä»¥åªé€‰æ‹©å…¶ä¸­ä¸€ä¸ªï¼Œæˆ–è€…éƒ½ä¸é€‰æ‹©ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æœ‰å¤§é‡çš„ CPU å†…å­˜å¯ç”¨ï¼Œåªå¸è½½åˆ° CPU å†…å­˜è®­ç»ƒé€Ÿåº¦ä¼šæ›´å¿«ï¼ˆæç¤ºï¼š"device": "cpu"ï¼‰ã€‚

è¿™æ˜¯æœ‰å…³å¸è½½ [ä¼˜åŒ–å™¨çŠ¶æ€](https://www.deepspeed.ai/docs/config-json/#optimizer-offloading) å’Œ [å‚æ•°](https://www.deepspeed.ai/docs/config-json/#parameter-offloading) çš„å®Œæ•´æ–‡æ¡£ã€‚

ç¡®ä¿æ‚¨çš„ `nvme_path` å®é™…ä¸Šæ˜¯ä¸€ä¸ª NVMeï¼Œå› ä¸ºå®ƒä¸æ™®é€šç¡¬ç›˜æˆ– SSD ä¸€èµ·å·¥ä½œï¼Œä½†é€Ÿåº¦ä¼šæ…¢å¾—å¤šã€‚å¿«é€Ÿå¯æ‰©å±•çš„è®­ç»ƒæ˜¯æ ¹æ®ç°ä»£ NVMe ä¼ è¾“é€Ÿåº¦è®¾è®¡çš„ï¼ˆæˆªè‡³æœ¬æ–‡æ’°å†™æ—¶ï¼Œå¯ä»¥è¾¾åˆ° ~3.5GB/s è¯»å–ï¼Œ~3GB/s å†™å…¥çš„å³°å€¼é€Ÿåº¦ï¼‰ã€‚

ä¸ºäº†æ‰¾å‡ºæœ€ä½³çš„ `aio` é…ç½®å—ï¼Œæ‚¨å¿…é¡»åœ¨ç›®æ ‡è®¾ç½®ä¸Šè¿è¡Œä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œå…·ä½“æ“ä½œè¯·å‚è§[è¯´æ˜](https://github.com/deepspeedai/DeepSpeed/issues/998)ã€‚



<a id='deepspeed-zero2-zero3-performance'></a>

#### ZeRO-2 å’Œ ZeRO-3 æ€§èƒ½å¯¹æ¯”

å¦‚æœå…¶ä»–ä¸€åˆ‡éƒ½é…ç½®ç›¸åŒï¼ŒZeRO-3 å¯èƒ½æ¯” ZeRO-2 æ…¢ï¼Œå› ä¸ºå‰è€…é™¤äº† ZeRO-2 çš„æ“ä½œå¤–ï¼Œè¿˜å¿…é¡»æ”¶é›†æ¨¡å‹æƒé‡ã€‚å¦‚æœ ZeRO-2 æ»¡è¶³æ‚¨çš„éœ€æ±‚ï¼Œè€Œä¸”æ‚¨ä¸éœ€è¦æ‰©å±•åˆ°å‡ ä¸ª GPU ä»¥ä¸Šï¼Œé‚£ä¹ˆæ‚¨å¯ä»¥é€‰æ‹©ç»§ç»­ä½¿ç”¨å®ƒã€‚é‡è¦çš„æ˜¯è¦ç†è§£ï¼ŒZeRO-3 ä»¥é€Ÿåº¦ä¸ºä»£ä»·å®ç°äº†æ›´é«˜çš„å¯æ‰©å±•æ€§ã€‚

å¯ä»¥è°ƒæ•´ ZeRO-3 é…ç½®ä½¿å…¶æ€§èƒ½æ¥è¿‘ ZeRO-2ï¼š

- å°† `stage3_param_persistence_threshold` è®¾ç½®ä¸ºä¸€ä¸ªéå¸¸å¤§çš„æ•°å­— - å¤§äºæœ€å¤§çš„å‚æ•°ï¼Œä¾‹å¦‚ `6 * hidden_size * hidden_size`ã€‚è¿™å°†ä¿ç•™å‚æ•°åœ¨ GPU ä¸Šã€‚
- å…³é—­ `offload_params`ï¼Œå› ä¸º ZeRO-2 æ²¡æœ‰è¿™ä¸ªé€‰é¡¹ã€‚

å³ä½¿ä¸æ›´æ”¹ `stage3_param_persistence_threshold`ï¼Œä»…å°† `offload_params` å…³é—­ï¼Œæ€§èƒ½å¯èƒ½ä¼šæ˜¾è‘—æé«˜ã€‚å½“ç„¶ï¼Œè¿™äº›æ›´æ”¹å°†å½±å“æ‚¨å¯ä»¥è®­ç»ƒçš„æ¨¡å‹çš„å¤§å°ã€‚å› æ­¤ï¼Œè¿™äº›æ›´æ”¹å¯æ ¹æ®éœ€æ±‚å¸®åŠ©æ‚¨åœ¨å¯æ‰©å±•æ€§å’Œé€Ÿåº¦ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚



<a id='deepspeed-zero2-example'></a>

#### ZeRO-2 ç¤ºä¾‹

è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„ ZeRO-2 è‡ªåŠ¨é…ç½®æ–‡ä»¶ `ds_config_zero2.json`ï¼š

```json
{
    "fp16": {
        "enabled": "auto",
        "loss_scale": 0,
        "loss_scale_window": 1000,
        "initial_scale_power": 16,
        "hysteresis": 2,
        "min_loss_scale": 1
    },

    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": "auto",
            "betas": "auto",
            "eps": "auto",
            "weight_decay": "auto"
        }
    },

    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": "auto",
            "warmup_max_lr": "auto",
            "warmup_num_steps": "auto"
        }
    },

    "zero_optimization": {
        "stage": 2,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": true
        },
        "allgather_partitions": true,
        "allgather_bucket_size": 2e8,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 2e8,
        "contiguous_gradients": true
    },

    "gradient_accumulation_steps": "auto",
    "gradient_clipping": "auto",
    "steps_per_print": 2000,
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "wall_clock_breakdown": false
}
```

è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„æ‰‹åŠ¨è®¾ç½®çš„å¯ç”¨æ‰€æœ‰åŠŸèƒ½çš„ ZeRO-2 é…ç½®æ–‡ä»¶ã€‚ä¸»è¦æ˜¯ä¸ºäº†è®©æ‚¨çœ‹åˆ°å…¸å‹çš„å‚æ•°å€¼æ˜¯ä»€ä¹ˆæ ·çš„ï¼Œä½†æˆ‘ä»¬å¼ºçƒˆå»ºè®®ä½¿ç”¨å…¶ä¸­åŒ…å«å¤šä¸ª `auto` è®¾ç½®çš„é…ç½®æ–‡ä»¶ã€‚

```json
{
    "fp16": {
        "enabled": true,
        "loss_scale": 0,
        "loss_scale_window": 1000,
        "initial_scale_power": 16,
        "hysteresis": 2,
        "min_loss_scale": 1
    },

    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 3e-5,
            "betas": [0.8, 0.999],
            "eps": 1e-8,
            "weight_decay": 3e-7
        }
    },

    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": 0,
            "warmup_max_lr": 3e-5,
            "warmup_num_steps": 500
        }
    },

    "zero_optimization": {
        "stage": 2,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": true
        },
        "allgather_partitions": true,
        "allgather_bucket_size": 2e8,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 2e8,
        "contiguous_gradients": true
    },

    "steps_per_print": 2000,
    "wall_clock_breakdown": false
}
```

<a id='deepspeed-zero3-example'></a>

#### ZeRO-3 ç¤ºä¾‹

è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„ ZeRO-3 è‡ªåŠ¨é…ç½®æ–‡ä»¶ `ds_config_zero3.json`ï¼š

```json
{
    "fp16": {
        "enabled": "auto",
        "loss_scale": 0,
        "loss_scale_window": 1000,
        "initial_scale_power": 16,
        "hysteresis": 2,
        "min_loss_scale": 1
    },

    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": "auto",
            "betas": "auto",
            "eps": "auto",
            "weight_decay": "auto"
        }
    },

    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": "auto",
            "warmup_max_lr": "auto",
            "warmup_num_steps": "auto"
        }
    },

    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": true
        },
        "offload_param": {
            "device": "cpu",
            "pin_memory": true
        },
        "overlap_comm": true,
        "contiguous_gradients": true,
        "sub_group_size": 1e9,
        "reduce_bucket_size": "auto",
        "stage3_prefetch_bucket_size": "auto",
        "stage3_param_persistence_threshold": "auto",
        "stage3_max_live_parameters": 1e9,
        "stage3_max_reuse_distance": 1e9,
        "stage3_gather_16bit_weights_on_model_save": true
    },

    "gradient_accumulation_steps": "auto",
    "gradient_clipping": "auto",
    "steps_per_print": 2000,
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "wall_clock_breakdown": false
}
```

è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„ æ‰‹åŠ¨è®¾ç½®çš„å¯ç”¨æ‰€æœ‰åŠŸèƒ½çš„ZeRO-3 é…ç½®æ–‡ä»¶ã€‚ä¸»è¦æ˜¯ä¸ºäº†è®©æ‚¨çœ‹åˆ°å…¸å‹çš„å‚æ•°å€¼æ˜¯ä»€ä¹ˆæ ·çš„ï¼Œä½†æˆ‘ä»¬å¼ºçƒˆå»ºè®®ä½¿ç”¨å…¶ä¸­åŒ…å«å¤šä¸ª `auto` è®¾ç½®çš„é…ç½®æ–‡ä»¶ã€‚

```json
{
    "fp16": {
        "enabled": true,
        "loss_scale": 0,
        "loss_scale_window": 1000,
        "initial_scale_power": 16,
        "hysteresis": 2,
        "min_loss_scale": 1
    },

    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 3e-5,
            "betas": [0.8, 0.999],
            "eps": 1e-8,
            "weight_decay": 3e-7
        }
    },

    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": 0,
            "warmup_max_lr": 3e-5,
            "warmup_num_steps": 500
        }
    },

    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": true
        },
        "offload_param": {
            "device": "cpu",
            "pin_memory": true
        },
        "overlap_comm": true,
        "contiguous_gradients": true,
        "sub_group_size": 1e9,
        "reduce_bucket_size": 1e6,
        "stage3_prefetch_bucket_size": 0.94e6,
        "stage3_param_persistence_threshold": 1e4,
        "stage3_max_live_parameters": 1e9,
        "stage3_max_reuse_distance": 1e9,
        "stage3_gather_16bit_weights_on_model_save": true
    },

    "steps_per_print": 2000,
    "wall_clock_breakdown": false
}
```

#### å¦‚ä½•é€‰æ‹©æœ€ä½³æ€§èƒ½çš„ZeRO Stageå’Œ offloads

äº†è§£äº†è¿™äº›ä¸åŒstagesåï¼Œç°åœ¨æ‚¨éœ€è¦å†³å®šä½¿ç”¨å“ªä¸ªstageã€‚æœ¬èŠ‚å°†å°è¯•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚

é€šå¸¸ï¼Œä»¥ä¸‹è§„åˆ™é€‚ç”¨ï¼š

- é€Ÿåº¦æ–¹é¢ï¼ˆå·¦è¾¹æ¯”å³è¾¹å¿«ï¼‰

  stage 0ï¼ˆDDPï¼‰ > stage 1 > stage 2 > stage 2 + offload  > stage 3 > stage3 + offload

- GPUå†…å­˜ä½¿ç”¨æ–¹é¢ï¼ˆå³è¾¹æ¯”å·¦è¾¹æ›´èŠ‚çœGPUå†…å­˜ï¼‰

  stage 0ï¼ˆDDPï¼‰ < stage 1 < stage 2 < stage 2 + offload < stage 3 < stage 3 + offload

æ‰€ä»¥ï¼Œå½“æ‚¨å¸Œæœ›åœ¨å°½é‡ä½¿ç”¨è¾ƒå°‘æ•°é‡çš„GPUçš„åŒæ—¶è·å¾—æœ€å¿«çš„æ‰§è¡Œé€Ÿåº¦æ—¶ï¼Œå¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œã€‚æˆ‘ä»¬ä»æœ€å¿«çš„æ–¹æ³•å¼€å§‹ï¼Œå¦‚æœé‡åˆ°GPUå†…å­˜æº¢å‡ºï¼Œç„¶ååˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªé€Ÿåº¦è¾ƒæ…¢ä½†ä½¿ç”¨çš„GPUå†…å­˜æ›´å°‘çš„æ–¹æ³•ã€‚ä»¥æ­¤ç±»æ¨ã€‚

é¦–å…ˆï¼Œå°†æ‰¹é‡å¤§å°è®¾ç½®ä¸º1ï¼ˆæ‚¨å§‹ç»ˆå¯ä»¥ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯æ¥è·å¾—ä»»ä½•æ‰€éœ€çš„æœ‰æ•ˆæ‰¹é‡å¤§å°ï¼‰ã€‚


1. å¯ç”¨ `--gradient_checkpointing 1`ï¼ˆHF Trainerï¼‰æˆ–ç›´æ¥ `model.gradient_checkpointing_enable()` - å¦‚æœå‘ç”ŸOOMï¼ˆOut of Memoryï¼‰ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ã€‚
2. é¦–å…ˆå°è¯• ZeRO stage 2ã€‚å¦‚æœå‘ç”ŸOOMï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ã€‚
3. å°è¯• ZeRO stage 2 + `offload_optimizer` - å¦‚æœå‘ç”ŸOOMï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ã€‚
4. åˆ‡æ¢åˆ° ZeRO stage 3 - å¦‚æœå‘ç”ŸOOMï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ã€‚
5. å¯ç”¨ `offload_param` åˆ° `cpu` - å¦‚æœå‘ç”ŸOOMï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ã€‚
6. å¯ç”¨ `offload_optimizer` åˆ° `cpu` - å¦‚æœå‘ç”ŸOOMï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ã€‚
7. å¦‚æœä»ç„¶æ— æ³•é€‚åº”æ‰¹é‡å¤§å°ä¸º1ï¼Œè¯·é¦–å…ˆæ£€æŸ¥å„ç§é»˜è®¤å€¼å¹¶å°½å¯èƒ½é™ä½å®ƒä»¬ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½¿ç”¨ `generate` å¹¶ä¸”ä¸ä½¿ç”¨å®½æœç´¢æŸï¼Œå°†å…¶ç¼©å°ï¼Œå› ä¸ºå®ƒä¼šå ç”¨å¤§é‡å†…å­˜ã€‚
8. ç»å¯¹è¦ä½¿ç”¨æ··åˆåŠç²¾åº¦è€Œéfp32 - åœ¨AmpereåŠæ›´é«˜çš„GPUä¸Šä½¿ç”¨bf16ï¼Œåœ¨æ—§çš„GPUä½“ç³»ç»“æ„ä¸Šä½¿ç”¨fp16ã€‚
9. å¦‚æœä»ç„¶å‘ç”ŸOOMï¼Œå¯ä»¥æ·»åŠ æ›´å¤šç¡¬ä»¶æˆ–å¯ç”¨ZeRO-Infinity - å³åˆ‡æ¢ `offload_param` å’Œ `offload_optimizer` åˆ° `nvme`ã€‚æ‚¨éœ€è¦ç¡®ä¿å®ƒæ˜¯éå¸¸å¿«çš„NVMeã€‚ä½œä¸ºè¶£é—»ï¼Œæˆ‘æ›¾ç»èƒ½å¤Ÿåœ¨ä¸€ä¸ªå°å‹GPUä¸Šä½¿ç”¨BLOOM-176Bè¿›è¡Œæ¨ç†ï¼Œä½¿ç”¨äº†ZeRO-Infinityï¼Œå°½ç®¡é€Ÿåº¦éå¸¸æ…¢ã€‚ä½†å®ƒå¥æ•ˆäº†ï¼

å½“ç„¶ï¼Œæ‚¨ä¹Ÿå¯ä»¥æŒ‰ç›¸åçš„é¡ºåºè¿›è¡Œè¿™äº›æ­¥éª¤ï¼Œä»æœ€èŠ‚çœGPUå†…å­˜çš„é…ç½®å¼€å§‹ï¼Œç„¶åé€æ­¥åå‘è¿›è¡Œï¼Œæˆ–è€…å°è¯•è¿›è¡ŒäºŒåˆ†æ³•ã€‚

ä¸€æ—¦æ‚¨çš„æ‰¹é‡å¤§å°ä¸º1ä¸ä¼šå¯¼è‡´OOMï¼Œå°±æµ‹é‡æ‚¨çš„æœ‰æ•ˆååé‡ã€‚

æ¥ä¸‹æ¥å°è¯•å°†æ‰¹é‡å¤§å°å¢åŠ åˆ°å°½å¯èƒ½å¤§ï¼Œå› ä¸ºæ‰¹é‡å¤§å°è¶Šå¤§ï¼ŒGPUçš„æ•ˆç‡è¶Šé«˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å®ƒä»¬ä¹˜æ³•è¿ç®—çš„çŸ©é˜µå¾ˆå¤§æ—¶ã€‚

ç°åœ¨æ€§èƒ½ä¼˜åŒ–æ¸¸æˆå¼€å§‹äº†ã€‚æ‚¨å¯ä»¥å…³é—­ä¸€äº›offloadç‰¹æ€§ï¼Œæˆ–è€…é™ä½ZeRO stageï¼Œå¹¶å¢åŠ /å‡å°‘æ‰¹é‡å¤§å°ï¼Œå†æ¬¡æµ‹é‡æœ‰æ•ˆååé‡ã€‚åå¤å°è¯•ï¼Œç›´åˆ°æ»¡æ„ä¸ºæ­¢ã€‚

ä¸è¦èŠ±è´¹å¤ªå¤šæ—¶é—´ï¼Œä½†å¦‚æœæ‚¨å³å°†å¼€å§‹ä¸€ä¸ªä¸ºæœŸ3ä¸ªæœˆçš„è®­ç»ƒ - è¯·èŠ±å‡ å¤©æ—¶é—´æ‰¾åˆ°ååé‡æ–¹é¢æœ€æœ‰æ•ˆçš„è®¾ç½®ã€‚è¿™æ ·æ‚¨çš„è®­ç»ƒæˆæœ¬å°†æœ€ä½ï¼Œè€Œä¸”æ‚¨ä¼šæ›´å¿«åœ°å®Œæˆè®­ç»ƒã€‚åœ¨å½“å‰å¿«èŠ‚å¥çš„æœºå™¨å­¦ä¹ ä¸–ç•Œä¸­ï¼Œå¦‚æœæ‚¨èŠ±è´¹ä¸€ä¸ªé¢å¤–çš„æœˆä»½æ¥è®­ç»ƒæŸæ ·ä¸œè¥¿ï¼Œä½ å¾ˆå¯èƒ½ä¼šé”™è¿‡ä¸€ä¸ªé»„é‡‘æœºä¼šã€‚å½“ç„¶ï¼Œè¿™åªæ˜¯æˆ‘åˆ†äº«çš„ä¸€ç§è§‚å¯Ÿï¼Œæˆ‘å¹¶ä¸æ˜¯åœ¨å‚¬ä¿ƒä½ ã€‚åœ¨å¼€å§‹è®­ç»ƒBLOOM-176Bä¹‹å‰ï¼Œæˆ‘èŠ±äº†2å¤©æ—¶é—´è¿›è¡Œè¿™ä¸ªè¿‡ç¨‹ï¼ŒæˆåŠŸå°†ååé‡ä»90 TFLOPsæé«˜åˆ°150 TFLOPsï¼è¿™ä¸€åŠªåŠ›ä¸ºæˆ‘ä»¬èŠ‚çœäº†ä¸€ä¸ªå¤šæœˆçš„è®­ç»ƒæ—¶é—´ã€‚

è¿™äº›æ³¨é‡Šä¸»è¦æ˜¯ä¸ºè®­ç»ƒæ¨¡å¼ç¼–å†™çš„ï¼Œä½†å®ƒä»¬åœ¨æ¨ç†ä¸­ä¹Ÿåº”è¯¥å¤§éƒ¨åˆ†é€‚ç”¨ã€‚ä¾‹å¦‚ï¼Œåœ¨æ¨ç†ä¸­ï¼ŒGradient Checkpointing æ˜¯æ— ç”¨çš„ï¼Œå› ä¸ºå®ƒåªåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœ‰ç”¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°ï¼Œå¦‚æœä½ æ­£åœ¨è¿›è¡Œå¤šGPUæ¨ç†å¹¶ä¸”ä¸ä½¿ç”¨ [DeepSpeed-Inference](https://www.deepspeed.ai/tutorials/inference-tutorial/)ï¼Œ[Accelerate](https://huggingface.co/blog/bloom-inference-pytorch-scripts) åº”è¯¥æä¾›æ›´ä¼˜è¶Šçš„æ€§èƒ½ã€‚

å…¶ä»–ä¸æ€§èƒ½ç›¸å…³çš„å¿«é€Ÿæ³¨é‡Šï¼š
- å¦‚æœæ‚¨ä»å¤´å¼€å§‹è®­ç»ƒæŸä¸ªæ¨¡å‹ï¼Œè¯·å°½é‡ç¡®ä¿å¼ é‡çš„å½¢çŠ¶å¯ä»¥è¢«16æ•´é™¤ï¼ˆä¾‹å¦‚éšè—å±‚å¤§å°ï¼‰ã€‚å¯¹äºæ‰¹é‡å¤§å°ï¼Œè‡³å°‘å°è¯•å¯è¢«2æ•´é™¤ã€‚å¦‚æœæ‚¨æƒ³ä»GPUä¸­æŒ¤å–æ›´é«˜æ€§èƒ½ï¼Œè¿˜æœ‰ä¸€äº›ç¡¬ä»¶ç‰¹å®šçš„[waveå’Œtileé‡åŒ–](https://developer.nvidia.com/blog/optimizing-gpu-performance-tensor-cores/)çš„å¯æ•´é™¤æ€§ã€‚



### Activation Checkpointing æˆ– Gradient Checkpointing

Activation Checkpointingå’ŒGradient Checkpointingæ˜¯æŒ‡ç›¸åŒæ–¹æ³•çš„ä¸¤ä¸ªä¸åŒæœ¯è¯­ã€‚è¿™ç¡®å®è®©äººæ„Ÿåˆ°å›°æƒ‘ï¼Œä½†äº‹å®å°±æ˜¯è¿™æ ·ã€‚

Gradient Checkpointingå…è®¸é€šè¿‡ç‰ºç‰²é€Ÿåº¦æ¥æ¢å–GPUå†…å­˜ï¼Œè¿™è¦ä¹ˆä½¿æ‚¨èƒ½å¤Ÿå…‹æœGPUå†…å­˜æº¢å‡ºï¼Œè¦ä¹ˆå¢åŠ æ‰¹é‡å¤§å°æ¥è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚

HF Transformers æ¨¡å‹å¯¹DeepSpeedçš„Activation Checkpointingä¸€æ— æ‰€çŸ¥ï¼Œå› æ­¤å¦‚æœå°è¯•åœ¨DeepSpeedé…ç½®æ–‡ä»¶ä¸­å¯ç”¨è¯¥åŠŸèƒ½ï¼Œä»€ä¹ˆéƒ½ä¸ä¼šå‘ç”Ÿã€‚

å› æ­¤ï¼Œæ‚¨æœ‰ä¸¤ç§æ–¹æ³•å¯ä»¥åˆ©ç”¨è¿™ä¸ªéå¸¸æœ‰ç›Šçš„åŠŸèƒ½ï¼š

1. å¦‚æœæ‚¨æƒ³ä½¿ç”¨ HF Transformers æ¨¡å‹ï¼Œä½ å¯ä»¥ä½¿ç”¨ `model.gradient_checkpointing_enable()` æˆ–åœ¨ HF Trainer ä¸­ä½¿ç”¨ `--gradient_checkpointing`ï¼Œå®ƒä¼šè‡ªåŠ¨ä¸ºæ‚¨å¯ç”¨è¿™ä¸ªåŠŸèƒ½ã€‚åœ¨è¿™é‡Œä½¿ç”¨äº† `torch.utils.checkpoint`ã€‚
2. å¦‚æœæ‚¨ç¼–å†™è‡ªå·±çš„æ¨¡å‹å¹¶å¸Œæœ›ä½¿ç”¨DeepSpeedçš„Activation Checkpointingï¼Œå¯ä»¥ä½¿ç”¨[è§„å®šçš„API](https://deepspeed.readthedocs.io/en/latest/activation-checkpointing.html)ã€‚æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ HF Transformers çš„æ¨¡å‹ä»£ç ï¼Œå°† `torch.utils.checkpoint` æ›¿æ¢ä¸º DeepSpeed çš„APIã€‚åè€…æ›´çµæ´»ï¼Œå› ä¸ºå®ƒå…è®¸æ‚¨å°†å‰å‘æ¿€æ´»å€¼å¸è½½åˆ°CPUå†…å­˜ï¼Œè€Œä¸æ˜¯é‡æ–°è®¡ç®—å®ƒä»¬ã€‚


### Optimizer å’Œ Scheduler

åªè¦ä½ ä¸å¯ç”¨ `offload_optimizer`ï¼Œæ‚¨å¯ä»¥æ··åˆä½¿ç”¨DeepSpeedå’ŒHuggingFaceçš„è°ƒåº¦å™¨å’Œä¼˜åŒ–å™¨ï¼Œä½†æœ‰ä¸€ä¸ªä¾‹å¤–ï¼Œå³ä¸è¦ä½¿ç”¨HuggingFaceè°ƒåº¦å™¨å’ŒDeepSpeedä¼˜åŒ–å™¨çš„ç»„åˆï¼š


| Combos       | HF Scheduler | DS Scheduler |
|:-------------|:-------------|:-------------|
| HF Optimizer | Yes          | Yes          |
| DS Optimizer | No           | Yes          |

åœ¨å¯ç”¨ `offload_optimizer` çš„æƒ…å†µä¸‹ï¼Œå¯ä»¥ä½¿ç”¨éDeepSpeedä¼˜åŒ–å™¨ï¼Œåªè¦è¯¥ä¼˜åŒ–å™¨å…·æœ‰CPUå’ŒGPUçš„å®ç°ï¼ˆé™¤äº†LAMBï¼‰ã€‚

<a id='deepspeed-optimizer'></a>

#### Optimizer

DeepSpeedçš„ä¸»è¦ä¼˜åŒ–å™¨åŒ…æ‹¬Adamã€AdamWã€OneBitAdamå’ŒLambã€‚è¿™äº›ä¼˜åŒ–å™¨å·²ç»ä¸ZeROè¿›è¡Œäº†å½»åº•çš„æµ‹è¯•ï¼Œå› æ­¤å»ºè®®ä½¿ç”¨å®ƒä»¬ã€‚ç„¶è€Œï¼Œä¹Ÿå¯ä»¥å¯¼å…¥`torch`ä¸­çš„å…¶ä»–ä¼˜åŒ–å™¨ã€‚å®Œæ•´çš„æ–‡æ¡£åœ¨[è¿™é‡Œ](https://www.deepspeed.ai/docs/config-json/#optimizer-parameters)ã€‚

å¦‚æœåœ¨é…ç½®æ–‡ä»¶ä¸­ä¸é…ç½®`optimizer`æ¡ç›®ï¼Œ[`Trainer`] å°†è‡ªåŠ¨å°†å…¶è®¾ç½®ä¸º `AdamW`ï¼Œå¹¶ä½¿ç”¨æä¾›çš„å€¼æˆ–ä»¥ä¸‹å‘½ä»¤è¡Œå‚æ•°çš„é»˜è®¤å€¼ï¼š`--learning_rate`ã€`--adam_beta1`ã€`--adam_beta2`ã€`--adam_epsilon` å’Œ `--weight_decay`ã€‚

ä»¥ä¸‹æ˜¯`AdamW` çš„è‡ªåŠ¨é…ç½®ç¤ºä¾‹ï¼š

```json
{
   "optimizer": {
       "type": "AdamW",
       "params": {
         "lr": "auto",
         "betas": "auto",
         "eps": "auto",
         "weight_decay": "auto"
       }
   }
}
```

è¯·æ³¨æ„ï¼Œå‘½ä»¤è¡Œå‚æ•°å°†è®¾ç½®é…ç½®æ–‡ä»¶ä¸­çš„å€¼ã€‚è¿™æ˜¯ä¸ºäº†æœ‰ä¸€ä¸ªæ˜ç¡®çš„å€¼æ¥æºï¼Œå¹¶é¿å…åœ¨ä¸åŒåœ°æ–¹è®¾ç½®å­¦ä¹ ç‡ç­‰å€¼æ—¶éš¾ä»¥æ‰¾åˆ°çš„é”™è¯¯ã€‚å‘½ä»¤è¡Œå‚æ•°é…ç½®é«˜äºå…¶ä»–ã€‚è¢«è¦†ç›–çš„å€¼åŒ…æ‹¬ï¼š

- `lr` çš„å€¼ä¸º `--learning_rate`
- `betas` çš„å€¼ä¸º `--adam_beta1 --adam_beta2`
- `eps` çš„å€¼ä¸º `--adam_epsilon`
- `weight_decay` çš„å€¼ä¸º `--weight_decay`

å› æ­¤ï¼Œè¯·è®°ä½åœ¨å‘½ä»¤è¡Œä¸Šè°ƒæ•´å…±äº«çš„è¶…å‚æ•°ã€‚

æ‚¨ä¹Ÿå¯ä»¥æ˜¾å¼åœ°è®¾ç½®è¿™äº›å€¼ï¼š

```json
{
   "optimizer": {
       "type": "AdamW",
       "params": {
         "lr": 0.001,
         "betas": [0.8, 0.999],
         "eps": 1e-8,
         "weight_decay": 3e-7
       }
   }
}
```

ä½†åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨éœ€è¦è‡ªå·±åŒæ­¥[`Trainer`]å‘½ä»¤è¡Œå‚æ•°å’ŒDeepSpeedé…ç½®ã€‚

å¦‚æœæ‚¨æƒ³ä½¿ç”¨ä¸Šé¢æœªåˆ—å‡ºçš„å…¶ä»–ä¼˜åŒ–å™¨ï¼Œæ‚¨å°†ä¸å¾—ä¸å°†å…¶æ·»åŠ åˆ°é¡¶å±‚é…ç½®ä¸­ã€‚

```json
{
   "zero_allow_untested_optimizer": true
}
```

ç±»ä¼¼äº `AdamW`ï¼Œæ‚¨å¯ä»¥é…ç½®å…¶ä»–å®˜æ–¹æ”¯æŒçš„ä¼˜åŒ–å™¨ã€‚åªæ˜¯è®°ä½è¿™äº›å¯èƒ½æœ‰ä¸åŒçš„é…ç½®å€¼ã€‚ä¾‹å¦‚ï¼Œå¯¹äºAdamï¼Œæ‚¨å¯èƒ½éœ€è¦å°† `weight_decay` è®¾ç½®åœ¨ `0.01` å·¦å³ã€‚

æ­¤å¤–ï¼Œå½“ä¸DeepSpeedçš„CPU Adamä¼˜åŒ–å™¨ä¸€èµ·ä½¿ç”¨æ—¶ï¼Œoffloadçš„æ•ˆæœæœ€å¥½ã€‚å¦‚æœæ‚¨æƒ³åœ¨offloadæ—¶ä½¿ç”¨ä¸åŒçš„ä¼˜åŒ–å™¨ï¼Œè‡ª `deepspeed==0.8.3` èµ·ï¼Œæ‚¨è¿˜éœ€è¦æ·»åŠ ï¼š


```json
{
   "zero_force_ds_cpu_optimizer": false
}
```
åˆ°é¡¶å±‚é…ç½®ä¸­ã€‚



<a id='deepspeed-scheduler'></a>

#### Scheduler

DeepSpeedæ”¯æŒ`LRRangeTest`ã€`OneCycle`ã€`WarmupLR`å’Œ`WarmupDecayLR`å­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚å®Œæ•´æ–‡æ¡£åœ¨[è¿™é‡Œ](https://www.deepspeed.ai/docs/config-json/#scheduler-parameters)ã€‚

ä»¥ä¸‹æ˜¯ğŸ¤— Transformers å’Œ DeepSpeed ä¹‹é—´çš„è°ƒåº¦å™¨é‡å éƒ¨åˆ†ï¼š

- é€šè¿‡ `--lr_scheduler_type constant_with_warmup` å®ç° `WarmupLR`
- é€šè¿‡ `--lr_scheduler_type linear` å®ç° `WarmupDecayLR`ã€‚è¿™ä¹Ÿæ˜¯ `--lr_scheduler_type` çš„é»˜è®¤å€¼ï¼Œå› æ­¤ï¼Œå¦‚æœä¸é…ç½®è°ƒåº¦å™¨ï¼Œè¿™å°†æ˜¯é»˜è®¤é…ç½®çš„è°ƒåº¦å™¨ã€‚

å¦‚æœåœ¨é…ç½®æ–‡ä»¶ä¸­ä¸é…ç½® `scheduler` æ¡ç›®ï¼Œ[`Trainer`] å°†ä½¿ç”¨ `--lr_scheduler_type`ã€`--learning_rate` å’Œ `--warmup_steps` çš„å€¼æ¥é…ç½®å…¶ğŸ¤— Transformers ç‰ˆæœ¬ã€‚

ä»¥ä¸‹æ˜¯ `WarmupLR` çš„è‡ªåŠ¨é…ç½®ç¤ºä¾‹ï¼š

```json
{
   "scheduler": {
         "type": "WarmupLR",
         "params": {
             "warmup_min_lr": "auto",
             "warmup_max_lr": "auto",
             "warmup_num_steps": "auto"
         }
     }
}
```

ç”±äºä½¿ç”¨äº† *"auto"*ï¼Œ[`Trainer`] çš„å‚æ•°å°†åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®æ­£ç¡®çš„å€¼ã€‚è¿™æ˜¯ä¸ºäº†æœ‰ä¸€ä¸ªæ˜ç¡®çš„å€¼æ¥æºï¼Œå¹¶é¿å…åœ¨ä¸åŒåœ°æ–¹è®¾ç½®å­¦ä¹ ç‡ç­‰å€¼æ—¶éš¾ä»¥æ‰¾åˆ°çš„é”™è¯¯ã€‚å‘½ä»¤è¡Œé…ç½®é«˜äºå…¶ä»–ã€‚è¢«è®¾ç½®çš„å€¼åŒ…æ‹¬ï¼š

- `warmup_min_lr` çš„å€¼ä¸º `0`ã€‚
- `warmup_max_lr` çš„å€¼ä¸º `--learning_rate`ã€‚
- `warmup_num_steps` çš„å€¼ä¸º `--warmup_steps`ï¼ˆå¦‚æœæä¾›ï¼‰ã€‚
- `total_num_steps` çš„å€¼ä¸º `--max_steps` æˆ–è€…å¦‚æœæ²¡æœ‰æä¾›ï¼Œå°†åœ¨è¿è¡Œæ—¶æ ¹æ®ç¯å¢ƒã€æ•°æ®é›†çš„å¤§å°å’Œå…¶ä»–å‘½ä»¤è¡Œå‚æ•°ï¼ˆå¯¹äº `WarmupDecayLR` æ¥è¯´éœ€è¦ï¼‰è‡ªåŠ¨æ¨å¯¼ã€‚

å½“ç„¶ï¼Œæ‚¨å¯ä»¥æ¥ç®¡ä»»ä½•æˆ–æ‰€æœ‰çš„é…ç½®å€¼ï¼Œå¹¶è‡ªè¡Œè®¾ç½®è¿™äº›å€¼ï¼š

```json
{
   "scheduler": {
         "type": "WarmupLR",
         "params": {
             "warmup_min_lr": 0,
             "warmup_max_lr": 0.001,
             "warmup_num_steps": 1000
         }
     }
}
```

ä½†åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨éœ€è¦è‡ªå·±åŒæ­¥[`Trainer`]å‘½ä»¤è¡Œå‚æ•°å’ŒDeepSpeedé…ç½®ã€‚

ä¾‹å¦‚ï¼Œå¯¹äº `WarmupDecayLR`ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ¡ç›®ï¼š

```json
{
   "scheduler": {
         "type": "WarmupDecayLR",
         "params": {
             "last_batch_iteration": -1,
             "total_num_steps": "auto",
             "warmup_min_lr": "auto",
             "warmup_max_lr": "auto",
             "warmup_num_steps": "auto"
         }
     }
}
```

ç„¶åï¼Œ`total_num_steps`ã€`warmup_max_lr`ã€`warmup_num_steps` å’Œ `total_num_steps` å°†åœ¨åŠ è½½æ—¶è®¾ç½®ã€‚


<a id='deepspeed-fp32'></a>

### fp32ç²¾åº¦

DeepSpeedæ”¯æŒå®Œæ•´çš„fp32å’Œfp16æ··åˆç²¾åº¦ã€‚

ç”±äºfp16æ··åˆç²¾åº¦å…·æœ‰æ›´å°çš„å†…å­˜éœ€æ±‚å’Œæ›´å¿«çš„é€Ÿåº¦ï¼Œå”¯ä¸€ä¸ä½¿ç”¨å®ƒçš„æ—¶å€™æ˜¯å½“æ‚¨ä½¿ç”¨çš„æ¨¡å‹åœ¨è¿™ç§è®­ç»ƒæ¨¡å¼ä¸‹è¡¨ç°ä¸ä½³æ—¶ã€‚é€šå¸¸ï¼Œå½“æ¨¡å‹æ²¡æœ‰åœ¨fp16æ··åˆç²¾åº¦ä¸‹è¿›è¡Œé¢„è®­ç»ƒæ—¶ï¼ˆä¾‹å¦‚ï¼Œbf16é¢„è®­ç»ƒæ¨¡å‹ç»å¸¸å‡ºç°è¿™ç§æƒ…å†µï¼‰ï¼Œä¼šå‡ºç°è¿™ç§æƒ…å†µã€‚è¿™æ ·çš„æ¨¡å‹å¯èƒ½ä¼šå‘ç”Ÿæº¢å‡ºæˆ–ä¸‹æº¢ï¼Œå¯¼è‡´ `NaN` æŸå¤±ã€‚å¦‚æœæ˜¯è¿™ç§æƒ…å†µï¼Œé‚£ä¹ˆæ‚¨å°†å¸Œæœ›ä½¿ç”¨å®Œæ•´çš„fp32æ¨¡å¼ï¼Œé€šè¿‡æ˜¾å¼ç¦ç”¨é»˜è®¤å¯ç”¨çš„fp16æ··åˆç²¾åº¦æ¨¡å¼ï¼š

```json
{
    "fp16": {
        "enabled": false,
    }
}
```

å¦‚æœæ‚¨ä½¿ç”¨åŸºäºAmpereæ¶æ„çš„GPUï¼ŒPyTorchç‰ˆæœ¬1.7åŠæ›´é«˜ç‰ˆæœ¬å°†è‡ªåŠ¨åˆ‡æ¢åˆ°ä½¿ç”¨æ›´é«˜æ•ˆçš„tf32æ ¼å¼è¿›è¡Œä¸€äº›æ“ä½œï¼Œä½†ç»“æœä»å°†ä»¥fp32æ ¼å¼å‘ˆç°ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯å’ŒåŸºå‡†æµ‹è¯•ï¼Œè¯·å‚è§[TensorFloat-32(TF32) on Ampere devices](https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)ã€‚å¦‚æœå‡ºäºæŸç§åŸå› æ‚¨ä¸å¸Œæœ›ä½¿ç”¨å®ƒï¼Œè¯¥æ–‡æ¡£åŒ…æ‹¬æœ‰å…³å¦‚ä½•ç¦ç”¨æ­¤è‡ªåŠ¨è½¬æ¢çš„è¯´æ˜ã€‚

åœ¨ğŸ¤— Trainerä¸­ï¼Œä½ å¯ä»¥ä½¿ç”¨ `--tf32` æ¥å¯ç”¨å®ƒï¼Œæˆ–ä½¿ç”¨ `--tf32 0` æˆ– `--no_tf32` æ¥ç¦ç”¨å®ƒã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œä½¿ç”¨PyTorchçš„é»˜è®¤è®¾ç½®ã€‚



<a id='deepspeed-amp'></a>

### è‡ªåŠ¨æ··åˆç²¾åº¦

### fp16

è¦é…ç½®PyTorch AMP-like çš„ fp16ï¼ˆfloat16ï¼‰ æ¨¡å¼ï¼Œè¯·è®¾ç½®ï¼š

```json
{
    "fp16": {
        "enabled": "auto",
        "loss_scale": 0,
        "loss_scale_window": 1000,
        "initial_scale_power": 16,
        "hysteresis": 2,
        "min_loss_scale": 1
    }
}
```

å¹¶ä¸”ï¼Œ[`Trainer`]å°†æ ¹æ®`args.fp16_backend`çš„å€¼è‡ªåŠ¨å¯ç”¨æˆ–ç¦ç”¨å®ƒã€‚å…¶ä½™çš„é…ç½®å€¼ç”±æ‚¨å†³å®šã€‚

å½“ä¼ é€’`--fp16 --fp16_backend amp`æˆ–`--fp16_full_eval`å‘½ä»¤è¡Œå‚æ•°æ—¶ï¼Œæ­¤æ¨¡å¼å°†è¢«å¯ç”¨ã€‚

æ‚¨ä¹Ÿå¯ä»¥æ˜¾å¼åœ°å¯ç”¨/ç¦ç”¨æ­¤æ¨¡å¼ï¼š

```json
{
    "fp16": {
        "enabled": true,
        "loss_scale": 0,
        "loss_scale_window": 1000,
        "initial_scale_power": 16,
        "hysteresis": 2,
        "min_loss_scale": 1
    }
}
```

ä½†æ˜¯ä¹‹åæ‚¨éœ€è¦è‡ªå·±åŒæ­¥[`Trainer`]å‘½ä»¤è¡Œå‚æ•°å’ŒDeepSpeedé…ç½®ã€‚

ä»¥ä¸‹æ˜¯[ç›¸å…³æ–‡æ¡£](https://www.deepspeed.ai/docs/config-json/#fp16-training-options)


### bf16

å¦‚æœéœ€è¦ä½¿ç”¨bfloat16è€Œä¸æ˜¯fp16ï¼Œé‚£ä¹ˆå¯ä»¥ä½¿ç”¨ä»¥ä¸‹é…ç½®éƒ¨åˆ†ï¼š

```json
{
    "bf16": {
        "enabled": "auto"
    }
}
```

bf16å…·æœ‰ä¸fp32ç›¸åŒçš„åŠ¨æ€èŒƒå›´ï¼Œå› æ­¤ä¸éœ€è¦æŸå¤±ç¼©æ”¾ã€‚

å½“ä¼ é€’`--bf16`æˆ–`--bf16_full_eval`å‘½ä»¤è¡Œå‚æ•°æ—¶ï¼Œå¯ç”¨æ­¤æ¨¡å¼ã€‚

æ‚¨è¿˜å¯ä»¥æ˜¾å¼åœ°å¯ç”¨/ç¦ç”¨æ­¤æ¨¡å¼ï¼š

```json
{
    "bf16": {
        "enabled": true
    }
}
```

<Tip>

åœ¨`deepspeed==0.6.0`ç‰ˆæœ¬ä¸­ï¼Œbf16æ”¯æŒæ˜¯æ–°çš„å®éªŒæ€§åŠŸèƒ½ã€‚

å¦‚æœæ‚¨å¯ç”¨äº†bf16æ¥è¿›è¡Œ[æ¢¯åº¦ç´¯ç§¯](#gradient-accumulation)ï¼Œæ‚¨éœ€è¦æ„è¯†åˆ°å®ƒä¼šä»¥bf16ç´¯ç§¯æ¢¯åº¦ï¼Œè¿™å¯èƒ½ä¸æ˜¯æ‚¨æƒ³è¦çš„ï¼Œå› ä¸ºè¿™ç§æ ¼å¼çš„ä½ç²¾åº¦å¯èƒ½ä¼šå¯¼è‡´lossy accumulationã€‚

ä¿®å¤è¿™ä¸ªé—®é¢˜çš„å·¥ä½œæ­£åœ¨åŠªåŠ›è¿›è¡Œï¼ŒåŒæ—¶æä¾›äº†ä½¿ç”¨æ›´é«˜ç²¾åº¦çš„`dtype`ï¼ˆfp16æˆ–fp32ï¼‰çš„é€‰é¡¹ã€‚

</Tip>


### NCCLé›†åˆ

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæœ‰ä¸¤ç§æ•°æ®ç±»å‹ï¼š`dtype`å’Œç”¨äºé€šä¿¡æ”¶é›†æ“ä½œçš„`dtype`ï¼Œå¦‚å„ç§å½’çº¦å’Œæ”¶é›†/åˆ†æ•£æ“ä½œã€‚

æ‰€æœ‰çš„gather/scatteræ“ä½œéƒ½æ˜¯åœ¨æ•°æ®ç›¸åŒçš„`dtype`ä¸­æ‰§è¡Œçš„ï¼Œæ‰€ä»¥å¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨bf16çš„è®­ç»ƒæ¨¡å¼ï¼Œé‚£ä¹ˆå®ƒå°†åœ¨bf16ä¸­è¿›è¡Œgatheræ“ä½œ - gatheræ“ä½œæ˜¯éæŸå¤±æ€§çš„ã€‚

å„ç§reduceæ“ä½œå¯èƒ½ä¼šæ˜¯éå¸¸æŸå¤±æ€§çš„ï¼Œä¾‹å¦‚å½“æ¢¯åº¦åœ¨å¤šä¸ªgpuä¸Šå¹³å‡æ—¶ï¼Œå¦‚æœé€šä¿¡æ˜¯åœ¨fp16æˆ–bf16ä¸­è¿›è¡Œçš„ï¼Œé‚£ä¹ˆç»“æœå¯èƒ½æ˜¯æœ‰æŸå¤±æ€§çš„ - å› ä¸ºå½“åœ¨ä¸€ä¸ªä½ç²¾åº¦ä¸­æ·»åŠ å¤šä¸ªæ•°å­—æ—¶ï¼Œç»“æœå¯èƒ½ä¸æ˜¯ç²¾ç¡®çš„ã€‚æ›´ç³Ÿç³•çš„æ˜¯ï¼Œbf16æ¯”fp16å…·æœ‰æ›´ä½çš„ç²¾åº¦ã€‚é€šå¸¸ï¼Œå½“å¹³å‡æ¢¯åº¦æ—¶ï¼ŒæŸå¤±æœ€å°ï¼Œè¿™äº›æ¢¯åº¦é€šå¸¸éå¸¸å°ã€‚å› æ­¤ï¼Œå¯¹äºåŠç²¾åº¦è®­ç»ƒï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œfp16è¢«ç”¨ä½œreductionæ“ä½œçš„é»˜è®¤å€¼ã€‚ä½†æ˜¯ï¼Œæ‚¨å¯ä»¥å®Œå…¨æ§åˆ¶è¿™ä¸ªåŠŸèƒ½ï¼Œå¦‚æœä½ é€‰æ‹©çš„è¯ï¼Œæ‚¨å¯ä»¥æ·»åŠ ä¸€ä¸ªå°çš„å¼€é”€ï¼Œå¹¶ç¡®ä¿reductionså°†ä½¿ç”¨fp32ä½œä¸ºç´¯ç§¯æ•°æ®ç±»å‹ï¼Œåªæœ‰å½“ç»“æœå‡†å¤‡å¥½æ—¶ï¼Œå®ƒæ‰ä¼šé™çº§åˆ°æ‚¨åœ¨è®­ç»ƒä¸­ä½¿ç”¨çš„åŠç²¾åº¦`dtype`ã€‚

è¦è¦†ç›–é»˜è®¤è®¾ç½®ï¼Œæ‚¨åªéœ€æ·»åŠ ä¸€ä¸ªæ–°çš„é…ç½®æ¡ç›®ï¼š

```json
{
    "communication_data_type": "fp32"
}
```

æ ¹æ®è¿™ä¸ªä¿¡æ¯ï¼Œæœ‰æ•ˆçš„å€¼åŒ…æ‹¬"fp16"ã€"bfp16"å’Œ"fp32"ã€‚

æ³¨æ„ï¼šåœ¨stage zero 3ä¸­ï¼Œbf16é€šä¿¡æ•°æ®ç±»å‹å­˜åœ¨ä¸€ä¸ªbugï¼Œè¯¥é—®é¢˜å·²åœ¨`deepspeed==0.8.1`ç‰ˆæœ¬ä¸­å¾—åˆ°ä¿®å¤ã€‚

<a id='deepspeed-bs'></a>

### Batch Size

é…ç½®batch sizeå¯ä»¥ä½¿ç”¨å¦‚ä¸‹å‚æ•°:

```json
{
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto"
}
```

å¹¶ä¸”ï¼Œ[`Trainer`]å°†è‡ªåŠ¨å°†`train_micro_batch_size_per_gpu`è®¾ç½®ä¸º`args.per_device_train_batch_size`çš„å€¼ï¼Œå¹¶å°†`train_batch_size`è®¾ç½®ä¸º`args.world_size * args.per_device_train_batch_size * args.gradient_accumulation_steps`ã€‚

æ‚¨ä¹Ÿå¯ä»¥æ˜¾å¼è®¾ç½®è¿™äº›å€¼ï¼š

```json
{
    "train_batch_size": 12,
    "train_micro_batch_size_per_gpu": 4
}
```

ä½†æ˜¯ï¼Œæ‚¨éœ€è¦è‡ªå·±åŒæ­¥[`Trainer`]å‘½ä»¤è¡Œå‚æ•°å’ŒDeepSpeedé…ç½®ã€‚


<a id='deepspeed-grad-acc'></a>

### Gradient Accumulation

é…ç½®gradient accumulationè®¾ç½®å¦‚ä¸‹:

```json
{
    "gradient_accumulation_steps": "auto"
}
```

å¹¶ä¸”ï¼Œ[`Trainer`]å°†è‡ªåŠ¨å°†å…¶è®¾ç½®ä¸º`args.gradient_accumulation_steps`çš„å€¼ã€‚

æ‚¨ä¹Ÿå¯ä»¥æ˜¾å¼è®¾ç½®è¿™ä¸ªå€¼ï¼š

```json
{
    "gradient_accumulation_steps": 3
}
```

ä½†æ˜¯ï¼Œæ‚¨éœ€è¦è‡ªå·±åŒæ­¥[`Trainer`]å‘½ä»¤è¡Œå‚æ•°å’ŒDeepSpeedé…ç½®ã€‚


<a id='deepspeed-grad-clip'></a>

### Gradient Clipping

é…ç½®gradient clippingå¦‚ä¸‹:

```json
{
    "gradient_clipping": "auto"
}
```

å¹¶ä¸”ï¼Œ[`Trainer`]å°†è‡ªåŠ¨å°†å…¶è®¾ç½®ä¸º`args.max_grad_norm`çš„å€¼ã€‚

æ‚¨ä¹Ÿå¯ä»¥æ˜¾å¼è®¾ç½®è¿™ä¸ªå€¼ï¼š

```json
{
    "gradient_clipping": 1.0
}
```

ä½†æ˜¯ï¼Œæ‚¨éœ€è¦è‡ªå·±åŒæ­¥[`Trainer`]å‘½ä»¤è¡Œå‚æ•°å’ŒDeepSpeedé…ç½®ã€‚



<a id='deepspeed-weight-extraction'></a>

### è·å–æ¨¡å‹æƒé‡

åªè¦æ‚¨ç»§ç»­ä½¿ç”¨DeepSpeedè¿›è¡Œè®­ç»ƒå’Œæ¢å¤ï¼Œæ‚¨å°±ä¸éœ€è¦æ‹…å¿ƒä»»ä½•äº‹æƒ…ã€‚DeepSpeedåœ¨å…¶è‡ªå®šä¹‰æ£€æŸ¥ç‚¹ä¼˜åŒ–å™¨æ–‡ä»¶ä¸­å­˜å‚¨fp32ä¸»æƒé‡ï¼Œè¿™äº›æ–‡ä»¶æ˜¯`global_step*/*optim_states.pt`ï¼ˆè¿™æ˜¯globæ¨¡å¼ï¼‰ï¼Œå¹¶ä¿å­˜åœ¨æ­£å¸¸çš„checkpointä¸‹ã€‚

**FP16æƒé‡ï¼š**

å½“æ¨¡å‹ä¿å­˜åœ¨ZeRO-2ä¸‹æ—¶ï¼Œæ‚¨æœ€ç»ˆä¼šå¾—åˆ°ä¸€ä¸ªåŒ…å«æ¨¡å‹æƒé‡çš„æ™®é€š`pytorch_model.bin`æ–‡ä»¶ï¼Œä½†å®ƒä»¬åªæ˜¯æƒé‡çš„fp16ç‰ˆæœ¬ã€‚

åœ¨ZeRO-3ä¸‹ï¼Œäº‹æƒ…è¦å¤æ‚å¾—å¤šï¼Œå› ä¸ºæ¨¡å‹æƒé‡åˆ†å¸ƒåœ¨å¤šä¸ªGPUä¸Šï¼Œå› æ­¤éœ€è¦`"stage3_gather_16bit_weights_on_model_save": true`æ‰èƒ½è®©`Trainer`ä¿å­˜fp16ç‰ˆæœ¬çš„æƒé‡ã€‚å¦‚æœè¿™ä¸ªè®¾ç½®æ˜¯`False`ï¼Œ`pytorch_model.bin`å°†ä¸ä¼šè¢«åˆ›å»ºã€‚è¿™æ˜¯å› ä¸ºé»˜è®¤æƒ…å†µä¸‹ï¼ŒDeepSpeedçš„`state_dict`åŒ…å«ä¸€ä¸ªå ä½ç¬¦è€Œä¸æ˜¯å®é™…çš„æƒé‡ã€‚å¦‚æœæˆ‘ä»¬ä¿å­˜è¿™ä¸ª`state_dict`ï¼Œå°±æ— æ³•å†åŠ è½½å®ƒäº†ã€‚


```json
{
    "zero_optimization": {
        "stage3_gather_16bit_weights_on_model_save": true
    }
}
```

**FP32æƒé‡ï¼š**

è™½ç„¶fp16æƒé‡é€‚åˆæ¢å¤è®­ç»ƒï¼Œä½†å¦‚æœæ‚¨å®Œæˆäº†æ¨¡å‹çš„å¾®è°ƒå¹¶å¸Œæœ›å°†å…¶ä¸Šä¼ åˆ°[models hub](https://huggingface.co/models)æˆ–ä¼ é€’ç»™å…¶ä»–äººï¼Œæ‚¨å¾ˆå¯èƒ½æƒ³è¦è·å–fp32æƒé‡ã€‚è¿™æœ€å¥½ä¸è¦åœ¨è®­ç»ƒæœŸé—´å®Œæˆï¼Œå› ä¸ºè¿™éœ€è¦å¤§é‡å†…å­˜ï¼Œå› æ­¤æœ€å¥½åœ¨è®­ç»ƒå®Œæˆåç¦»çº¿è¿›è¡Œã€‚ä½†æ˜¯ï¼Œå¦‚æœéœ€è¦å¹¶ä¸”æœ‰å……è¶³çš„ç©ºé—²CPUå†…å­˜ï¼Œå¯ä»¥åœ¨ç›¸åŒçš„è®­ç»ƒè„šæœ¬ä¸­å®Œæˆã€‚ä»¥ä¸‹éƒ¨åˆ†å°†è®¨è®ºè¿™ä¸¤ç§æ–¹æ³•ã€‚

**å®æ—¶FP32æƒé‡æ¢å¤ï¼š**

å¦‚æœæ‚¨çš„æ¨¡å‹å¾ˆå¤§ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒç»“æŸæ—¶å‡ ä¹æ²¡æœ‰å‰©ä½™çš„ç©ºé—²CPUå†…å­˜ï¼Œè¿™ç§æ–¹æ³•å¯èƒ½ä¸èµ·ä½œç”¨ã€‚

å¦‚æœæ‚¨è‡³å°‘ä¿å­˜äº†ä¸€ä¸ªæ£€æŸ¥ç‚¹ï¼Œå¹¶ä¸”æƒ³è¦ä½¿ç”¨æœ€æ–°çš„ä¸€ä¸ªï¼Œå¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š

```python
from transformers.trainer_utils import get_last_checkpoint
from deepspeed.utils.zero_to_fp32 import load_state_dict_from_zero_checkpoint

checkpoint_dir = get_last_checkpoint(trainer.args.output_dir)
fp32_model = load_state_dict_from_zero_checkpoint(trainer.model, checkpoint_dir)
```

å¦‚æœæ‚¨åœ¨ä½¿ç”¨`--load_best_model_at_end`ç±»ï¼š*~transformers.TrainingArguments*å‚æ•°ï¼ˆç”¨äºè·Ÿè¸ªæœ€ä½³
æ£€æŸ¥ç‚¹ï¼‰ï¼Œé‚£ä¹ˆä½ å¯ä»¥é¦–å…ˆæ˜¾å¼åœ°ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼Œç„¶åå†æ‰§è¡Œç›¸åŒçš„æ“ä½œï¼š

```python
from deepspeed.utils.zero_to_fp32 import load_state_dict_from_zero_checkpoint

checkpoint_dir = os.path.join(trainer.args.output_dir, "checkpoint-final")
trainer.deepspeed.save_checkpoint(checkpoint_dir)
fp32_model = load_state_dict_from_zero_checkpoint(trainer.model, checkpoint_dir)
```

<Tip>

æ³¨æ„ï¼Œä¸€æ—¦è¿è¡Œäº†`load_state_dict_from_zero_checkpoint`ï¼Œè¯¥æ¨¡å‹å°†ä¸å†å¯ä»¥åœ¨ç›¸åŒçš„åº”ç”¨ç¨‹åºçš„DeepSpeedä¸Šä¸‹æ–‡ä¸­ä½¿ç”¨ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæ‚¨éœ€è¦é‡æ–°åˆå§‹åŒ–deepspeedå¼•æ“ï¼Œå› ä¸º`model.load_state_dict(state_dict)`ä¼šä»å…¶ä¸­ç§»é™¤æ‰€æœ‰çš„DeepSpeedç›¸å…³ç‚¹ã€‚æ‰€ä»¥æ‚¨åªèƒ½è®­ç»ƒç»“æŸæ—¶è¿™æ ·åšã€‚

</Tip>

å½“ç„¶ï¼Œæ‚¨ä¸å¿…ä½¿ç”¨ç±»ï¼š*~transformers.Trainer*ï¼Œæ‚¨å¯ä»¥æ ¹æ®ä½ çš„éœ€æ±‚è°ƒæ•´ä¸Šé¢çš„ç¤ºä¾‹ã€‚

å¦‚æœæ‚¨å‡ºäºæŸç§åŸå› æƒ³è¦æ›´å¤šçš„ä¼˜åŒ–ï¼Œæ‚¨ä¹Ÿå¯ä»¥æå–æƒé‡çš„fp32 `state_dict`å¹¶æŒ‰ç…§ä»¥ä¸‹ç¤ºä¾‹è¿›è¡Œæ“ä½œï¼š

```python
from deepspeed.utils.zero_to_fp32 import get_fp32_state_dict_from_zero_checkpoint

state_dict = get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir)  # already on cpu
model = model.cpu()
model.load_state_dict(state_dict)
```

**ç¦»çº¿FP32æƒé‡æ¢å¤ï¼š**

DeepSpeedä¼šåˆ›å»ºä¸€ä¸ªç‰¹æ®Šçš„è½¬æ¢è„šæœ¬`zero_to_fp32.py`ï¼Œå¹¶å°†å…¶æ”¾ç½®åœ¨checkpointæ–‡ä»¶å¤¹çš„é¡¶å±‚ã€‚ä½¿ç”¨æ­¤è„šæœ¬ï¼Œæ‚¨å¯ä»¥åœ¨ä»»ä½•æ—¶å€™æå–æƒé‡ã€‚è¯¥è„šæœ¬æ˜¯ç‹¬ç«‹çš„ï¼Œæ‚¨ä¸å†éœ€è¦é…ç½®æ–‡ä»¶æˆ–`Trainer`æ¥æ‰§è¡Œæå–æ“ä½œã€‚

å‡è®¾æ‚¨çš„checkpointæ–‡ä»¶å¤¹å¦‚ä¸‹æ‰€ç¤ºï¼š

```bash
$ ls -l output_dir/checkpoint-1/
-rw-rw-r-- 1 stas stas 1.4K Mar 27 20:42 config.json
drwxrwxr-x 2 stas stas 4.0K Mar 25 19:52 global_step1/
-rw-rw-r-- 1 stas stas   12 Mar 27 13:16 latest
-rw-rw-r-- 1 stas stas 827K Mar 27 20:42 optimizer.pt
-rw-rw-r-- 1 stas stas 231M Mar 27 20:42 pytorch_model.bin
-rw-rw-r-- 1 stas stas  623 Mar 27 20:42 scheduler.pt
-rw-rw-r-- 1 stas stas 1.8K Mar 27 20:42 special_tokens_map.json
-rw-rw-r-- 1 stas stas 774K Mar 27 20:42 spiece.model
-rw-rw-r-- 1 stas stas 1.9K Mar 27 20:42 tokenizer_config.json
-rw-rw-r-- 1 stas stas  339 Mar 27 20:42 trainer_state.json
-rw-rw-r-- 1 stas stas 2.3K Mar 27 20:42 training_args.bin
-rwxrw-r-- 1 stas stas 5.5K Mar 27 13:16 zero_to_fp32.py*
```

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œåªæœ‰ä¸€ä¸ªDeepSpeedæ£€æŸ¥ç‚¹å­æ–‡ä»¶å¤¹*global_step1*ã€‚å› æ­¤ï¼Œè¦é‡æ„fp32æƒé‡ï¼Œåªéœ€è¿è¡Œï¼š

```bash
python zero_to_fp32.py . pytorch_model.bin
```

è¿™å°±æ˜¯å®ƒã€‚`pytorch_model.bin`ç°åœ¨å°†åŒ…å«ä»å¤šä¸ªGPUsåˆå¹¶çš„å®Œæ•´çš„fp32æ¨¡å‹æƒé‡ã€‚

è¯¥è„šæœ¬å°†è‡ªåŠ¨èƒ½å¤Ÿå¤„ç†ZeRO-2æˆ–ZeRO-3 checkpointã€‚

`python zero_to_fp32.py -h`å°†ä¸ºæ‚¨æä¾›ä½¿ç”¨ç»†èŠ‚ã€‚

è¯¥è„šæœ¬å°†é€šè¿‡æ–‡ä»¶`latest`çš„å†…å®¹è‡ªåŠ¨å‘ç°deepspeedå­æ–‡ä»¶å¤¹ï¼Œåœ¨å½“å‰ç¤ºä¾‹ä¸­ï¼Œå®ƒå°†åŒ…å«`global_step1`ã€‚

æ³¨æ„ï¼šç›®å‰è¯¥è„šæœ¬éœ€è¦2å€äºæœ€ç»ˆfp32æ¨¡å‹æƒé‡çš„é€šç”¨å†…å­˜ã€‚


### ZeRO-3 å’Œ Infinity Nuances

ZeRO-3ä¸ZeRO-2æœ‰å¾ˆå¤§çš„ä¸åŒï¼Œä¸»è¦æ˜¯å› ä¸ºå®ƒçš„å‚æ•°åˆ†ç‰‡åŠŸèƒ½ã€‚

ZeRO-Infinityè¿›ä¸€æ­¥æ‰©å±•äº†ZeRO-3ï¼Œä»¥æ”¯æŒNVMeå†…å­˜å’Œå…¶ä»–é€Ÿåº¦å’Œå¯æ‰©å±•æ€§æ”¹è¿›ã€‚

å°½ç®¡æ‰€æœ‰åŠªåŠ›éƒ½æ˜¯ä¸ºäº†åœ¨ä¸éœ€è¦å¯¹æ¨¡å‹è¿›è¡Œä»»ä½•ç‰¹æ®Šæ›´æ”¹çš„æƒ…å†µä¸‹å°±èƒ½æ­£å¸¸è¿è¡Œï¼Œä½†åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ‚¨å¯èƒ½éœ€è¦ä»¥ä¸‹ä¿¡æ¯ã€‚


#### æ„å»ºå¤§æ¨¡å‹

DeepSpeed/ZeRO-3å¯ä»¥å¤„ç†å‚æ•°é‡è¾¾åˆ°æ•°ä¸‡äº¿çš„æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹å¯èƒ½æ— æ³•é€‚åº”ç°æœ‰çš„å†…å­˜ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¦‚æœæ‚¨è¿˜æ˜¯å¸Œæœ›åˆå§‹åŒ–æ›´å¿«åœ°å‘ç”Ÿï¼Œå¯ä»¥ä½¿ç”¨*deepspeed.zero.Init()*ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆä¹Ÿæ˜¯ä¸€ä¸ªå‡½æ•°è£…é¥°å™¨ï¼‰æ¥åˆå§‹åŒ–æ¨¡å‹ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
from transformers import T5ForConditionalGeneration, T5Config
import deepspeed

with deepspeed.zero.Init():
    config = T5Config.from_pretrained("google-t5/t5-small")
    model = T5ForConditionalGeneration(config)
```

å¦‚æ‚¨æ‰€è§ï¼Œè¿™ä¼šä¸ºæ‚¨éšæœºåˆå§‹åŒ–ä¸€ä¸ªæ¨¡å‹ã€‚

å¦‚æœæ‚¨æƒ³ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œ`model_class.from_pretrained`å°†åœ¨`is_deepspeed_zero3_enabled()`è¿”å›`True`çš„æƒ…å†µä¸‹æ¿€æ´»æ­¤åŠŸèƒ½ï¼Œç›®å‰è¿™æ˜¯é€šè¿‡ä¼ é€’çš„DeepSpeedé…ç½®æ–‡ä»¶ä¸­çš„ZeRO-3é…ç½®éƒ¨åˆ†è®¾ç½®çš„ã€‚å› æ­¤ï¼Œåœ¨è°ƒç”¨`from_pretrained`ä¹‹å‰ï¼Œæ‚¨å¿…é¡»åˆ›å»º**TrainingArguments**å¯¹è±¡ã€‚ä»¥ä¸‹æ˜¯å¯èƒ½çš„é¡ºåºç¤ºä¾‹ï¼š

```python
from transformers import AutoModel, Trainer, TrainingArguments

training_args = TrainingArguments(..., deepspeed=ds_config)
model = AutoModel.from_pretrained("google-t5/t5-small")
trainer = Trainer(model=model, args=training_args, ...)
```

å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯å®˜æ–¹ç¤ºä¾‹è„šæœ¬ï¼Œå¹¶ä¸”å‘½ä»¤è¡Œå‚æ•°ä¸­åŒ…å«`--deepspeed ds_config.json`ä¸”å¯ç”¨äº†ZeRO-3é…ç½®ï¼Œé‚£ä¹ˆä¸€åˆ‡éƒ½å·²ç»ä¸ºæ‚¨å‡†å¤‡å¥½äº†ï¼Œå› ä¸ºè¿™æ˜¯ç¤ºä¾‹è„šæœ¬çš„ç¼–å†™æ–¹å¼ã€‚

æ³¨æ„ï¼šå¦‚æœæ¨¡å‹çš„fp16æƒé‡æ— æ³•é€‚åº”å•ä¸ªGPUçš„å†…å­˜ï¼Œåˆ™å¿…é¡»ä½¿ç”¨æ­¤åŠŸèƒ½ã€‚

æœ‰å…³æ­¤æ–¹æ³•å’Œå…¶ä»–ç›¸å…³åŠŸèƒ½çš„å®Œæ•´è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[æ„å»ºå¤§æ¨¡å‹](https://deepspeed.readthedocs.io/en/latest/zero3.html#constructing-massive-models)ã€‚

æ­¤å¤–ï¼Œåœ¨åŠ è½½fp16é¢„è®­ç»ƒæ¨¡å‹æ—¶ï¼Œæ‚¨å¸Œæœ›`from_pretrained`ä½¿ç”¨`dtype=torch.float16`ã€‚è¯¦æƒ…è¯·å‚è§[from_pretrained-torch-dtype](#from_pretrained-torch-dtype)ã€‚


#### å‚æ•°æ”¶é›†

åœ¨å¤šä¸ªGPUä¸Šä½¿ç”¨ZeRO-3æ—¶ï¼Œæ²¡æœ‰ä¸€ä¸ªGPUæ‹¥æœ‰æ‰€æœ‰å‚æ•°ï¼Œé™¤éå®ƒæ˜¯å½“å‰æ‰§è¡Œå±‚çš„å‚æ•°ã€‚å› æ­¤ï¼Œå¦‚æœæ‚¨éœ€è¦ä¸€æ¬¡è®¿é—®æ‰€æœ‰å±‚çš„æ‰€æœ‰å‚æ•°ï¼Œæœ‰ä¸€ä¸ªç‰¹å®šçš„æ–¹æ³•å¯ä»¥å®ç°ã€‚
æ‚¨å¯èƒ½ä¸éœ€è¦å®ƒï¼Œä½†å¦‚æœæ‚¨éœ€è¦ï¼Œè¯·å‚è€ƒ[å‚æ•°æ”¶é›†](https://deepspeed.readthedocs.io/en/latest/zero3.html#manual-parameter-coordination)ã€‚

ç„¶è€Œï¼Œæˆ‘ä»¬åœ¨å¤šä¸ªåœ°æ–¹ç¡®å®ä½¿ç”¨äº†å®ƒï¼Œå…¶ä¸­ä¸€ä¸ªä¾‹å­æ˜¯åœ¨`from_pretrained`ä¸­åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡ã€‚æˆ‘ä»¬ä¸€æ¬¡åŠ è½½ä¸€å±‚ï¼Œç„¶åç«‹å³å°†å…¶åˆ†åŒºåˆ°æ‰€æœ‰å‚ä¸çš„GPUä¸Šï¼Œå› ä¸ºå¯¹äºéå¸¸å¤§çš„æ¨¡å‹ï¼Œæ— æ³•åœ¨ä¸€ä¸ªGPUä¸Šä¸€æ¬¡æ€§åŠ è½½å¹¶å°†å…¶åˆ†å¸ƒåˆ°å¤šä¸ªGPUä¸Šï¼Œå› ä¸ºå†…å­˜é™åˆ¶ã€‚

æ­¤å¤–ï¼Œåœ¨ZeRO-3ä¸‹ï¼Œå¦‚æœæ‚¨ç¼–å†™è‡ªå·±çš„ä»£ç å¹¶é‡åˆ°çœ‹èµ·æ¥åƒè¿™æ ·çš„æ¨¡å‹å‚æ•°æƒé‡ï¼š

```python
tensor([1.0], device="cuda:0", dtype=torch.float16, requires_grad=True)
```

å¼ºè°ƒ`tensor([1.])`ï¼Œæˆ–è€…å¦‚æœæ‚¨é‡åˆ°ä¸€ä¸ªé”™è¯¯ï¼Œå®ƒè¯´å‚æ•°çš„å¤§å°æ˜¯`1`ï¼Œè€Œä¸æ˜¯æŸä¸ªæ›´å¤§çš„å¤šç»´å½¢çŠ¶ï¼Œè¿™æ„å‘³ç€å‚æ•°è¢«åˆ’åˆ†äº†ï¼Œä½ çœ‹åˆ°çš„æ˜¯ä¸€ä¸ªZeRO-3å ä½ç¬¦ã€‚



<a id='deepspeed-zero-inference'></a>


### ZeRO æ¨ç†

"ZeRO æ¨æ–­" ä½¿ç”¨ä¸ "ZeRO-3 è®­ç»ƒ" ç›¸åŒçš„é…ç½®ã€‚æ‚¨åªéœ€è¦å»æ‰ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨éƒ¨åˆ†ã€‚å®é™…ä¸Šï¼Œå¦‚æœæ‚¨å¸Œæœ›ä¸è®­ç»ƒå…±äº«ç›¸åŒçš„é…ç½®æ–‡ä»¶ï¼Œæ‚¨å¯ä»¥å°†å®ƒä»¬ä¿ç•™åœ¨é…ç½®æ–‡ä»¶ä¸­ï¼Œå®ƒä»¬åªä¼šè¢«å¿½ç•¥ã€‚

æ‚¨åªéœ€è¦ä¼ é€’é€šå¸¸çš„[`TrainingArguments`]å‚æ•°ã€‚ä¾‹å¦‚ï¼š

```bash
deepspeed --num_gpus=2 your_program.py <normal cl args> --do_eval --deepspeed ds_config.json
```

å”¯ä¸€çš„é‡è¦äº‹æƒ…æ˜¯æ‚¨éœ€è¦ä½¿ç”¨ZeRO-3é…ç½®ï¼Œå› ä¸ºZeRO-2å¯¹äºæ¨ç†æ²¡æœ‰ä»»ä½•ä¼˜åŠ¿ï¼Œå› ä¸ºåªæœ‰ZeRO-3æ‰å¯¹å‚æ•°è¿›è¡Œåˆ†ç‰‡ï¼Œè€ŒZeRO-1åˆ™å¯¹æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€è¿›è¡Œåˆ†ç‰‡ã€‚

ä»¥ä¸‹æ˜¯åœ¨DeepSpeedä¸‹è¿è¡Œ`run_translation.py`å¯ç”¨æ‰€æœ‰å¯ç”¨GPUçš„ç¤ºä¾‹ï¼š

```bash
deepspeed examples/pytorch/translation/run_translation.py \
--deepspeed tests/deepspeed/ds_config_zero3.json \
--model_name_or_path google-t5/t5-small --output_dir output_dir \
--do_eval --max_eval_samples 50 --warmup_steps 50  \
--max_source_length 128 --val_max_target_length 128 \
--per_device_eval_batch_size 4 \
--predict_with_generate --dataset_config "ro-en" --fp16 \
--source_lang en --target_lang ro --dataset_name wmt16 \
--source_prefix "translate English to Romanian: "
```

ç”±äºåœ¨æ¨ç†é˜¶æ®µï¼Œä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦ä¸éœ€è¦é¢å¤–çš„å¤§é‡å†…å­˜ï¼Œæ‚¨åº”è¯¥èƒ½å¤Ÿå°†æ›´å¤§çš„æ‰¹æ¬¡å’Œ/æˆ–åºåˆ—é•¿åº¦æ”¾åˆ°ç›¸åŒçš„ç¡¬ä»¶ä¸Šã€‚

æ­¤å¤–ï¼ŒDeepSpeedç›®å‰æ­£åœ¨å¼€å‘ä¸€ä¸ªåä¸ºDeepspeed-Inferenceçš„ç›¸å…³äº§å“ï¼Œå®ƒä¸ZeROæŠ€æœ¯æ— å…³ï¼Œè€Œæ˜¯ä½¿ç”¨å¼ é‡å¹¶è¡Œæ¥æ‰©å±•æ— æ³•é€‚åº”å•ä¸ªGPUçš„æ¨¡å‹ã€‚è¿™æ˜¯ä¸€ä¸ªæ­£åœ¨è¿›è¡Œçš„å·¥ä½œï¼Œä¸€æ—¦è¯¥äº§å“å®Œæˆï¼Œæˆ‘ä»¬å°†æä¾›é›†æˆã€‚


### å†…å­˜è¦æ±‚

ç”±äº DeepSpeed ZeRO å¯ä»¥å°†å†…å­˜å¸è½½åˆ° CPUï¼ˆå’Œ NVMeï¼‰ï¼Œè¯¥æ¡†æ¶æä¾›äº†ä¸€äº›å·¥å…·ï¼Œå…è®¸æ ¹æ®ä½¿ç”¨çš„ GPU æ•°é‡å‘ŠçŸ¥å°†éœ€è¦å¤šå°‘ CPU å’Œ GPU å†…å­˜ã€‚

è®©æˆ‘ä»¬ä¼°è®¡åœ¨å•ä¸ªGPUä¸Šå¾®è°ƒ"bigscience/T0_3B"æ‰€éœ€çš„å†…å­˜ï¼š

```bash
$ python -c 'from transformers import AutoModel; \
from deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_live; \
model = AutoModel.from_pretrained("bigscience/T0_3B"); \
estimate_zero3_model_states_mem_needs_all_live(model, num_gpus_per_node=1, num_nodes=1)'
[...]
Estimated memory needed for params, optim states and gradients for a:
HW: Setup with 1 node, 1 GPU per node.
SW: Model with 2783M total params, 65M largest layer params.
  per CPU  |  per GPU |   Options
   70.00GB |   0.25GB | offload_param=cpu , offload_optimizer=cpu , zero_init=1
   70.00GB |   0.25GB | offload_param=cpu , offload_optimizer=cpu , zero_init=0
   62.23GB |   5.43GB | offload_param=none, offload_optimizer=cpu , zero_init=1
   62.23GB |   5.43GB | offload_param=none, offload_optimizer=cpu , zero_init=0
    0.37GB |  46.91GB | offload_param=none, offload_optimizer=none, zero_init=1
   15.56GB |  46.91GB | offload_param=none, offload_optimizer=none, zero_init=0
```

å› æ­¤ï¼Œæ‚¨å¯ä»¥å°†æ¨¡å‹æ‹Ÿåˆåœ¨å•ä¸ª80GBçš„GPUä¸Šï¼Œä¸è¿›è¡ŒCPU offloadï¼Œæˆ–è€…ä½¿ç”¨å¾®å°çš„8GB GPUï¼Œä½†éœ€è¦çº¦60GBçš„CPUå†…å­˜ã€‚ï¼ˆè¯·æ³¨æ„ï¼Œè¿™ä»…æ˜¯å‚æ•°ã€ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦æ‰€éœ€çš„å†…å­˜ - æ‚¨è¿˜éœ€è¦ä¸ºCUDAå†…æ ¸ã€æ¿€æ´»å€¼å’Œä¸´æ—¶å˜é‡åˆ†é…æ›´å¤šçš„å†…å­˜ã€‚ï¼‰

ç„¶åï¼Œè¿™æ˜¯æˆæœ¬ä¸é€Ÿåº¦çš„æƒè¡¡ã€‚è´­ä¹°/ç§Ÿç”¨è¾ƒå°çš„ GPUï¼ˆæˆ–è¾ƒå°‘çš„ GPUï¼Œå› ä¸ºæ‚¨å¯ä»¥ä½¿ç”¨å¤šä¸ª GPU è¿›è¡Œ Deepspeed ZeROï¼‰ã€‚ä½†è¿™æ ·ä¼šæ›´æ…¢ï¼Œå› æ­¤å³ä½¿æ‚¨ä¸å…³å¿ƒå®ŒæˆæŸé¡¹ä»»åŠ¡çš„é€Ÿåº¦ï¼Œå‡é€Ÿä¹Ÿç›´æ¥å½±å“ GPU ä½¿ç”¨çš„æŒç»­æ—¶é—´ï¼Œä»è€Œå¯¼è‡´æ›´å¤§çš„æˆæœ¬ã€‚å› æ­¤ï¼Œè¯·è¿›è¡Œå®éªŒå¹¶æ¯”è¾ƒå“ªç§æ–¹æ³•æ•ˆæœæœ€å¥½ã€‚

å¦‚æœæ‚¨æœ‰è¶³å¤Ÿçš„GPUå†…å­˜ï¼Œè¯·ç¡®ä¿ç¦ç”¨CPU/NVMeå¸è½½ï¼Œå› ä¸ºè¿™ä¼šä½¿æ‰€æœ‰æ“ä½œæ›´å¿«ã€‚

ä¾‹å¦‚ï¼Œè®©æˆ‘ä»¬é‡å¤ç›¸åŒçš„æ“ä½œï¼Œä½¿ç”¨2ä¸ªGPUï¼š

```bash
$ python -c 'from transformers import AutoModel; \
from deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_live; \
model = AutoModel.from_pretrained("bigscience/T0_3B"); \
estimate_zero3_model_states_mem_needs_all_live(model, num_gpus_per_node=2, num_nodes=1)'
[...]
Estimated memory needed for params, optim states and gradients for a:
HW: Setup with 1 node, 2 GPUs per node.
SW: Model with 2783M total params, 65M largest layer params.
  per CPU  |  per GPU |   Options
   70.00GB |   0.25GB | offload_param=cpu , offload_optimizer=cpu , zero_init=1
   70.00GB |   0.25GB | offload_param=cpu , offload_optimizer=cpu , zero_init=0
   62.23GB |   2.84GB | offload_param=none, offload_optimizer=cpu , zero_init=1
   62.23GB |   2.84GB | offload_param=none, offload_optimizer=cpu , zero_init=0
    0.74GB |  23.58GB | offload_param=none, offload_optimizer=none, zero_init=1
   31.11GB |  23.58GB | offload_param=none, offload_optimizer=none, zero_init=0

```

æ‰€ä»¥ï¼Œæ‚¨éœ€è¦2ä¸ª32GBæˆ–æ›´é«˜çš„GPUï¼Œä¸”ä¸è¿›è¡ŒCPUå¸è½½ã€‚

å¦‚éœ€äº†è§£æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[å†…å­˜ä¼°ç®—å™¨](https://deepspeed.readthedocs.io/en/latest/memory.html)ã€‚



### å½’æ¡£Issues

è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æäº¤é—®é¢˜ï¼Œä»¥ä¾¿æˆ‘ä»¬èƒ½å¤Ÿè¿…é€Ÿæ‰¾åˆ°é—®é¢˜å¹¶å¸®åŠ©æ‚¨è§£é™¤å·¥ä½œé˜»å¡ã€‚

åœ¨æ‚¨çš„æŠ¥å‘Šä¸­ï¼Œè¯·å§‹ç»ˆåŒ…æ‹¬ä»¥ä¸‹å†…å®¹ï¼š

1. å®Œæ•´çš„Deepspeedé…ç½®æ–‡ä»¶
2. å¦‚æœä½¿ç”¨äº†[`Trainer`]ï¼Œåˆ™åŒ…æ‹¬å‘½ä»¤è¡Œå‚æ•°ï¼›å¦‚æœè‡ªå·±ç¼–å†™äº†Trainerè®¾ç½®ï¼Œåˆ™åŒ…æ‹¬[`TrainingArguments`]å‚æ•°ã€‚è¯·ä¸è¦å¯¼å‡º[`TrainingArguments`]ï¼Œå› ä¸ºå®ƒæœ‰å‡ åä¸ªä¸é—®é¢˜æ— å…³çš„æ¡ç›®ã€‚
3. è¾“å‡ºï¼š

    ```bash
    python -c 'import torch; print(f"torch: {torch.__version__}")'
    python -c 'import transformers; print(f"transformers: {transformers.__version__}")'
    python -c 'import deepspeed; print(f"deepspeed: {deepspeed.__version__}")'
    ```

4. å¦‚æœå¯èƒ½ï¼Œè¯·åŒ…å«ä¸€ä¸ªGoogle Colab notebooké“¾æ¥ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒæ¥é‡ç°é—®é¢˜ã€‚æ‚¨å¯ä»¥ä½¿ç”¨è¿™ä¸ª[notebook](https://github.com/stas00/porting/blob/master/transformers/deepspeed/DeepSpeed_on_colab_CLI.ipynb)ä½œä¸ºèµ·ç‚¹ã€‚
5. é™¤éä¸å¯èƒ½ï¼Œå¦åˆ™è¯·å§‹ç»ˆä½¿ç”¨æ ‡å‡†æ•°æ®é›†ï¼Œè€Œä¸æ˜¯è‡ªå®šä¹‰æ•°æ®é›†ã€‚
6. å¦‚æœå¯èƒ½ï¼Œå°è¯•ä½¿ç”¨ç°æœ‰[ç¤ºä¾‹](https://github.com/huggingface/transformers/tree/main/examples/pytorch)ä¹‹ä¸€æ¥é‡ç°é—®é¢˜ã€‚

éœ€è¦è€ƒè™‘çš„å› ç´ ï¼š

- Deepspeedé€šå¸¸ä¸æ˜¯é—®é¢˜çš„åŸå› ã€‚

  ä¸€äº›å·²æäº¤çš„é—®é¢˜è¢«è¯æ˜ä¸Deepspeedæ— å…³ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œä¸€æ—¦å°†Deepspeedä»è®¾ç½®ä¸­ç§»é™¤ï¼Œé—®é¢˜ä»ç„¶å­˜åœ¨ã€‚

  å› æ­¤ï¼Œå¦‚æœé—®é¢˜æ˜æ˜¾ä¸DeepSpeedç›¸å…³ï¼Œä¾‹å¦‚æ‚¨å¯ä»¥çœ‹åˆ°æœ‰ä¸€ä¸ªå¼‚å¸¸å¹¶ä¸”å¯ä»¥çœ‹åˆ°DeepSpeedæ¨¡å—æ¶‰åŠå…¶ä¸­ï¼Œè¯·å…ˆé‡æ–°æµ‹è¯•æ²¡æœ‰DeepSpeedçš„è®¾ç½®ã€‚åªæœ‰å½“é—®é¢˜ä»ç„¶å­˜åœ¨æ—¶ï¼Œæ‰å‘Deepspeedæä¾›æ‰€æœ‰å¿…éœ€çš„ç»†èŠ‚ã€‚

- å¦‚æœæ‚¨æ˜ç¡®é—®é¢˜æ˜¯åœ¨Deepspeedæ ¸å¿ƒä¸­è€Œä¸æ˜¯é›†æˆéƒ¨åˆ†ï¼Œè¯·ç›´æ¥å‘[Deepspeed](https://github.com/deepspeedai/DeepSpeed/)æäº¤é—®é¢˜ã€‚å¦‚æœæ‚¨ä¸ç¡®å®šï¼Œè¯·ä¸è¦æ‹…å¿ƒï¼Œæ— è®ºä½¿ç”¨å“ªä¸ªissueè·Ÿè¸ªé—®é¢˜éƒ½å¯ä»¥ï¼Œä¸€æ—¦æ‚¨å‘å¸ƒé—®é¢˜ï¼Œæˆ‘ä»¬ä¼šå¼„æ¸…æ¥šå¹¶å°†å…¶é‡å®šå‘åˆ°å¦ä¸€ä¸ªissueè·Ÿè¸ªï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰ã€‚



### Troubleshooting

#### å¯åŠ¨æ—¶`deepspeed`è¿›ç¨‹è¢«ç»ˆæ­¢ï¼Œæ²¡æœ‰å›æº¯

å¦‚æœå¯åŠ¨æ—¶`deepspeed`è¿›ç¨‹è¢«ç»ˆæ­¢ï¼Œæ²¡æœ‰å›æº¯ï¼Œè¿™é€šå¸¸æ„å‘³ç€ç¨‹åºå°è¯•åˆ†é…çš„CPUå†…å­˜è¶…è¿‡äº†ç³»ç»Ÿçš„é™åˆ¶æˆ–è¿›ç¨‹è¢«å…è®¸åˆ†é…çš„å†…å­˜ï¼Œæ“ä½œç³»ç»Ÿå†…æ ¸æ€æ­»äº†è¯¥è¿›ç¨‹ã€‚è¿™æ˜¯å› ä¸ºæ‚¨çš„é…ç½®æ–‡ä»¶å¾ˆå¯èƒ½å°†`offload_optimizer`æˆ–`offload_param`æˆ–ä¸¤è€…éƒ½é…ç½®ä¸ºå¸è½½åˆ°`cpu`ã€‚å¦‚æœæ‚¨æœ‰NVMeï¼Œå¯ä»¥å°è¯•åœ¨ZeRO-3ä¸‹å¸è½½åˆ°NVMeã€‚è¿™é‡Œæ˜¯å¦‚ä½•[ä¼°è®¡ç‰¹å®šæ¨¡å‹æ‰€éœ€çš„å†…å­˜](https://deepspeed.readthedocs.io/en/latest/memory.html)ã€‚

#### è®­ç»ƒå’Œ/æˆ–è¯„ä¼°/é¢„æµ‹lossä¸º`NaN`

è¿™ç§æƒ…å†µé€šå¸¸å‘ç”Ÿåœ¨ä½¿ç”¨bf16æ··åˆç²¾åº¦æ¨¡å¼é¢„è®­ç»ƒçš„æ¨¡å‹è¯•å›¾åœ¨fp16ï¼ˆå¸¦æˆ–ä¸å¸¦æ··åˆç²¾åº¦ï¼‰ä¸‹ä½¿ç”¨æ—¶ã€‚å¤§å¤šæ•°åœ¨TPUä¸Šè®­ç»ƒçš„æ¨¡å‹ä»¥åŠç”±è°·æ­Œå‘å¸ƒçš„æ¨¡å‹éƒ½å±äºè¿™ä¸ªç±»åˆ«ï¼ˆä¾‹å¦‚ï¼Œå‡ ä¹æ‰€æœ‰åŸºäºt5çš„æ¨¡å‹ï¼‰ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè§£å†³æ–¹æ¡ˆæ˜¯è¦ä¹ˆä½¿ç”¨fp32ï¼Œè¦ä¹ˆåœ¨æ”¯æŒçš„æƒ…å†µä¸‹ä½¿ç”¨bf16ï¼ˆå¦‚TPUã€Ampere GPUæˆ–æ›´æ–°çš„ç‰ˆæœ¬ï¼‰ã€‚

å¦ä¸€ä¸ªé—®é¢˜å¯èƒ½ä¸ä½¿ç”¨fp16æœ‰å…³ã€‚å½“æ‚¨é…ç½®æ­¤éƒ¨åˆ†æ—¶ï¼š

```json
{
    "fp16": {
        "enabled": "auto",
        "loss_scale": 0,
        "loss_scale_window": 1000,
        "initial_scale_power": 16,
        "hysteresis": 2,
        "min_loss_scale": 1
    }
}
```

å¹¶ä¸”æ‚¨åœ¨æ—¥å¿—ä¸­çœ‹åˆ°DeepspeedæŠ¥å‘Š`OVERFLOW`å¦‚ä¸‹

```
0%|                                                                                                                             | 0/189 [00:00<?, ?it/s]
 [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 262144
  1%|â–Œ                                                                                                                    | 1/189 [00:00<01:26,  2.17it/s]
 [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072.0
  1%|â–ˆâ–
 [...]
 [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1, reducing to 1
 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                                   | 27/189 [00:14<01:13,  2.21it/s]
 [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1, reducing to 1
 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                  | 28/189 [00:14<01:13,  2.18it/s]
 [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1, reducing to 1
 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                                  | 29/189 [00:15<01:13,  2.18it/s]
 [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1, reducing to 1
[...]
```

è¿™æ„å‘³ç€DeepspeedæŸå¤±ç¼©æ”¾å™¨æ— æ³•æ‰¾åˆ°ä¸€ä¸ªå…‹æœæŸå¤±æº¢å‡ºçš„ç¼©æ”¾ç³»æ•°ã€‚

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé€šå¸¸éœ€è¦æé«˜`initial_scale_power`çš„å€¼ã€‚å°†å…¶è®¾ç½®ä¸º`"initial_scale_power": 32`é€šå¸¸ä¼šè§£å†³é—®é¢˜ã€‚



### æ³¨æ„äº‹é¡¹

- å°½ç®¡ DeepSpeed æœ‰ä¸€ä¸ªå¯å®‰è£…çš„ PyPI åŒ…ï¼Œä½†å¼ºçƒˆå»ºè®®ä»æºä»£ç å®‰è£…å®ƒï¼Œä»¥æœ€å¥½åœ°åŒ¹é…æ‚¨çš„ç¡¬ä»¶ï¼Œå¦‚æœæ‚¨éœ€è¦å¯ç”¨æŸäº›åŠŸèƒ½ï¼Œå¦‚ 1-bit Adamï¼Œè¿™äº›åŠŸèƒ½åœ¨ pypi å‘è¡Œç‰ˆä¸­ä¸å¯ç”¨ã€‚
- æ‚¨ä¸å¿…ä½¿ç”¨ğŸ¤—  Transformersçš„ [`Trainer`] æ¥ä½¿ç”¨ DeepSpeed   - æ‚¨å¯ä»¥ä½¿ç”¨ä»»ä½•æ¨¡å‹ä¸è‡ªå·±çš„è®­ç»ƒå™¨ï¼Œæ‚¨è¿˜éœ€è¦æ ¹æ® [DeepSpeed é›†æˆè¯´æ˜](https://www.deepspeed.ai/getting-started/#writing-deepspeed-models) è°ƒæ•´åè€…ã€‚



## Non-Trainer Deepspeedé›†æˆ

å½“`Trainer`æ²¡æœ‰è¢«ä½¿ç”¨æ—¶ï¼Œ`~integrations.HfDeepSpeedConfig`è¢«ç”¨æ¥å°†Deepspeedé›†æˆåˆ°huggingfaceçš„Transformersæ ¸å¿ƒåŠŸèƒ½ä¸­ã€‚å®ƒå”¯ä¸€åšçš„äº‹æƒ…å°±æ˜¯åœ¨`from_pretrained`è°ƒç”¨æœŸé—´å¤„ç†Deepspeed ZeRO-3å‚æ•°æ”¶é›†å’Œå°†æ¨¡å‹è‡ªåŠ¨åˆ†å‰²åˆ°å¤šä¸ªGPUä¸Šã€‚é™¤æ­¤ä¹‹å¤–ï¼Œæ‚¨éœ€è¦è‡ªå·±å®Œæˆå…¶ä»–æ‰€æœ‰å·¥ä½œã€‚

å½“ä½¿ç”¨`Trainer`æ—¶ï¼Œæ‰€æœ‰äº‹æƒ…éƒ½è‡ªåŠ¨å¾—åˆ°äº†å¤„ç†ã€‚

å½“ä¸ä½¿ç”¨`Trainer`æ—¶ï¼Œä¸ºäº†é«˜æ•ˆåœ°éƒ¨ç½²Deepspeed ZeRO-3ï¼Œæ‚¨å¿…é¡»åœ¨å®ä¾‹åŒ–æ¨¡å‹ä¹‹å‰å®ä¾‹åŒ–`~integrations.HfDeepSpeedConfig`å¯¹è±¡å¹¶ä¿æŒè¯¥å¯¹è±¡æ´»è·ƒã€‚

å¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨Deepspeed ZeRO-1æˆ–ZeRO-2ï¼Œæ‚¨æ ¹æœ¬ä¸éœ€è¦ä½¿ç”¨`HfDeepSpeedConfig`ã€‚

ä»¥é¢„è®­ç»ƒæ¨¡å‹ä¸ºä¾‹:

```python
from transformers.integrations import HfDeepSpeedConfig
from transformers import AutoModel
import deepspeed

ds_config = {...}  # deepspeed config object or path to the file
# must run before instantiating the model to detect zero 3
dschf = HfDeepSpeedConfig(ds_config)  # keep this object alive
model = AutoModel.from_pretrained("openai-community/gpt2")
engine = deepspeed.initialize(model=model, config_params=ds_config, ...)
```

æˆ–è€…ä»¥éé¢„è®­ç»ƒæ¨¡å‹ä¸ºä¾‹ï¼š

```python
from transformers.integrations import HfDeepSpeedConfig
from transformers import AutoModel, AutoConfig
import deepspeed

ds_config = {...}  # deepspeed config object or path to the file
# must run before instantiating the model to detect zero 3
dschf = HfDeepSpeedConfig(ds_config)  # keep this object alive
config = AutoConfig.from_pretrained("openai-community/gpt2")
model = AutoModel.from_config(config)
engine = deepspeed.initialize(model=model, config_params=ds_config, ...)
```

è¯·æ³¨æ„ï¼Œå¦‚æœæ‚¨æ²¡æœ‰ä½¿ç”¨[`Trainer`]é›†æˆï¼Œæ‚¨å®Œå…¨éœ€è¦è‡ªå·±åŠ¨æ‰‹ã€‚åŸºæœ¬ä¸Šéµå¾ª[Deepspeed](https://www.deepspeed.ai/)ç½‘ç«™ä¸Šçš„æ–‡æ¡£ã€‚åŒæ—¶ï¼Œæ‚¨å¿…é¡»æ˜¾å¼é…ç½®é…ç½®æ–‡ä»¶ - ä¸èƒ½ä½¿ç”¨`"auto"`å€¼ï¼Œè€Œå¿…é¡»æ”¾å…¥å®é™…å€¼ã€‚


## HfDeepSpeedConfig

[[autodoc]] integrations.HfDeepSpeedConfig
    - all

### è‡ªå®šä¹‰DeepSpeed ZeROæ¨ç†

ä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼Œæ¼”ç¤ºäº†åœ¨æ— æ³•å°†æ¨¡å‹æ”¾å…¥å•ä¸ª GPU æ—¶å¦‚æœä¸ä½¿ç”¨[Trainer]è¿›è¡Œ DeepSpeed ZeRO æ¨ç† ã€‚è¯¥è§£å†³æ–¹æ¡ˆåŒ…æ‹¬ä½¿ç”¨é¢å¤–çš„ GPU æˆ–/å’Œå°† GPU å†…å­˜å¸è½½åˆ° CPU å†…å­˜ã€‚

è¿™é‡Œè¦ç†è§£çš„é‡è¦ç»†å¾®å·®åˆ«æ˜¯ï¼ŒZeROçš„è®¾è®¡æ–¹å¼å¯ä»¥è®©æ‚¨åœ¨ä¸åŒçš„GPUä¸Šå¹¶è¡Œå¤„ç†ä¸åŒçš„è¾“å…¥ã€‚

è¿™ä¸ªä¾‹å­æœ‰å¾ˆå¤šæ³¨é‡Šï¼Œå¹¶ä¸”æ˜¯è‡ªæ–‡æ¡£åŒ–çš„ã€‚

è¯·ç¡®ä¿ï¼š

1. å¦‚æœæ‚¨æœ‰è¶³å¤Ÿçš„GPUå†…å­˜ï¼ˆå› ä¸ºè¿™ä¼šå‡æ…¢é€Ÿåº¦ï¼‰ï¼Œç¦ç”¨CPU offloadã€‚
2. å¦‚æœæ‚¨æ‹¥æœ‰Ampereæ¶æ„æˆ–æ›´æ–°çš„GPUï¼Œå¯ç”¨bf16ä»¥åŠ å¿«é€Ÿåº¦ã€‚å¦‚æœæ‚¨æ²¡æœ‰è¿™ç§ç¡¬ä»¶ï¼Œåªè¦ä¸ä½¿ç”¨ä»»ä½•åœ¨bf16æ··åˆç²¾åº¦ä¸‹é¢„è®­ç»ƒçš„æ¨¡å‹ï¼ˆå¦‚å¤§å¤šæ•°t5æ¨¡å‹ï¼‰ï¼Œå°±å¯ä»¥å¯ç”¨fp16ã€‚å¦åˆ™è¿™äº›æ¨¡å‹é€šå¸¸åœ¨fp16ä¸­æº¢å‡ºï¼Œæ‚¨ä¼šçœ‹åˆ°è¾“å‡ºæ— æ•ˆç»“æœã€‚

```python
#!/usr/bin/env python

# This script demonstrates how to use Deepspeed ZeRO in an inference mode when one can't fit a model
# into a single GPU
#
# 1. Use 1 GPU with CPU offload
# 2. Or use multiple GPUs instead
#
# First you need to install deepspeed: pip install deepspeed
#
# Here we use a 3B "bigscience/T0_3B" model which needs about 15GB GPU RAM - so 1 largish or 2
# small GPUs can handle it. or 1 small GPU and a lot of CPU memory.
#
# To use a larger model like "bigscience/T0" which needs about 50GB, unless you have an 80GB GPU -
# you will need 2-4 gpus. And then you can adapt the script to handle more gpus if you want to
# process multiple inputs at once.
#
# The provided deepspeed config also activates CPU memory offloading, so chances are that if you
# have a lot of available CPU memory and you don't mind a slowdown you should be able to load a
# model that doesn't normally fit into a single GPU. If you have enough GPU memory the program will
# run faster if you don't want offload to CPU - so disable that section then.
#
# To deploy on 1 gpu:
#
# deepspeed --num_gpus 1 t0.py
# or:
# python -m torch.distributed.run --nproc_per_node=1 t0.py
#
# To deploy on 2 gpus:
#
# deepspeed --num_gpus 2 t0.py
# or:
# python -m torch.distributed.run --nproc_per_node=2 t0.py


from transformers import AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM
from transformers.integrations import HfDeepSpeedConfig
import deepspeed
import os
import torch

os.environ["TOKENIZERS_PARALLELISM"] = "false"  # To avoid warnings about parallelism in tokenizers

# distributed setup
local_rank = int(os.getenv("LOCAL_RANK", "0"))
world_size = int(os.getenv("WORLD_SIZE", "1"))
torch.cuda.set_device(local_rank)
deepspeed.init_distributed()

model_name = "bigscience/T0_3B"

config = AutoConfig.from_pretrained(model_name)
model_hidden_size = config.d_model

# batch size has to be divisible by world_size, but can be bigger than world_size
train_batch_size = 1 * world_size

# ds_config notes
#
# - enable bf16 if you use Ampere or higher GPU - this will run in mixed precision and will be
# faster.
#
# - for older GPUs you can enable fp16, but it'll only work for non-bf16 pretrained models - e.g.
# all official t5 models are bf16-pretrained
#
# - set offload_param.device to "none" or completely remove the `offload_param` section if you don't
# - want CPU offload
#
# - if using `offload_param` you can manually finetune stage3_param_persistence_threshold to control
# - which params should remain on gpus - the larger the value the smaller the offload size
#
# For in-depth info on Deepspeed config see
# https://huggingface.co/docs/transformers/main/main_classes/deepspeed

# keeping the same format as json for consistency, except it uses lower case for true/false
# fmt: off
ds_config = {
    "fp16": {
        "enabled": False
    },
    "bf16": {
        "enabled": False
    },
    "zero_optimization": {
        "stage": 3,
        "offload_param": {
            "device": "cpu",
            "pin_memory": True
        },
        "overlap_comm": True,
        "contiguous_gradients": True,
        "reduce_bucket_size": model_hidden_size * model_hidden_size,
        "stage3_prefetch_bucket_size": 0.9 * model_hidden_size * model_hidden_size,
        "stage3_param_persistence_threshold": 10 * model_hidden_size
    },
    "steps_per_print": 2000,
    "train_batch_size": train_batch_size,
    "train_micro_batch_size_per_gpu": 1,
    "wall_clock_breakdown": False
}
# fmt: on

# next line instructs transformers to partition the model directly over multiple gpus using
# deepspeed.zero.Init when model's `from_pretrained` method is called.
#
# **it has to be run before loading the model AutoModelForSeq2SeqLM.from_pretrained(model_name)**
#
# otherwise the model will first be loaded normally and only partitioned at forward time which is
# less efficient and when there is little CPU RAM may fail
dschf = HfDeepSpeedConfig(ds_config)  # keep this object alive

# now a model can be loaded.
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# initialise Deepspeed ZeRO and store only the engine object
ds_engine = deepspeed.initialize(model=model, config_params=ds_config)[0]
ds_engine.module.eval()  # inference

# Deepspeed ZeRO can process unrelated inputs on each GPU. So for 2 gpus you process 2 inputs at once.
# If you use more GPUs adjust for more.
# And of course if you have just one input to process you then need to pass the same string to both gpus
# If you use only one GPU, then you will have only rank 0.
rank = torch.distributed.get_rank()
if rank == 0:
    text_in = "Is this review positive or negative? Review: this is the best cast iron skillet you will ever buy"
elif rank == 1:
    text_in = "Is this review positive or negative? Review: this is the worst restaurant ever"

tokenizer = AutoTokenizer.from_pretrained(model_name)
inputs = tokenizer.encode(text_in, return_tensors="pt").to(device=local_rank)
with torch.no_grad():
    outputs = ds_engine.module.generate(inputs, synced_gpus=True)
text_out = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(f"rank{rank}:\n   in={text_in}\n  out={text_out}")
```

è®©æˆ‘ä»¬ä¿å­˜å®ƒä¸º `t0.py`å¹¶è¿è¡Œï¼š
```bash
$ deepspeed --num_gpus 2 t0.py
rank0:
   in=Is this review positive or negative? Review: this is the best cast iron skillet you will ever buy
  out=Positive
rank1:
   in=Is this review positive or negative? Review: this is the worst restaurant ever
  out=negative
```

è¿™æ˜¯ä¸€ä¸ªéå¸¸åŸºæœ¬çš„ä¾‹å­ï¼Œæ‚¨éœ€è¦æ ¹æ®è‡ªå·±çš„éœ€æ±‚è¿›è¡Œä¿®æ”¹ã€‚

### `generate` çš„å·®å¼‚

åœ¨ä½¿ç”¨ZeRO stage 3çš„å¤šGPUæ—¶ï¼Œéœ€è¦é€šè¿‡è°ƒç”¨`generate(..., synced_gpus=True)`æ¥åŒæ­¥GPUã€‚å¦‚æœä¸€ä¸ªGPUåœ¨å…¶å®ƒGPUä¹‹å‰å®Œæˆç”Ÿæˆï¼Œæ•´ä¸ªç³»ç»Ÿå°†æŒ‚èµ·ï¼Œå› ä¸ºå…¶ä»–GPUæ— æ³•ä»åœæ­¢ç”Ÿæˆçš„GPUæ¥æ”¶æƒé‡åˆ†ç‰‡ã€‚

ä»`transformers>=4.28`å¼€å§‹ï¼Œå¦‚æœæ²¡æœ‰æ˜ç¡®æŒ‡å®š`synced_gpus`ï¼Œæ£€æµ‹åˆ°è¿™äº›æ¡ä»¶åå®ƒå°†è‡ªåŠ¨è®¾ç½®ä¸º`True`ã€‚ä½†å¦‚æœæ‚¨éœ€è¦è¦†ç›–`synced_gpus`çš„å€¼ï¼Œä»ç„¶å¯ä»¥è¿™æ ·åšã€‚



## æµ‹è¯• DeepSpeed é›†æˆ

å¦‚æœæ‚¨æäº¤äº†ä¸€ä¸ªæ¶‰åŠDeepSpeedé›†æˆçš„PRï¼Œè¯·æ³¨æ„æˆ‘ä»¬çš„CircleCI PR CIè®¾ç½®æ²¡æœ‰GPUï¼Œå› æ­¤æˆ‘ä»¬åªåœ¨å¦ä¸€ä¸ªCIå¤œé—´è¿è¡Œéœ€è¦GPUçš„æµ‹è¯•ã€‚å› æ­¤ï¼Œå¦‚æœæ‚¨åœ¨PRä¸­è·å¾—ç»¿è‰²çš„CIæŠ¥å‘Šï¼Œå¹¶ä¸æ„å‘³ç€DeepSpeedæµ‹è¯•é€šè¿‡ã€‚

è¦è¿è¡ŒDeepSpeedæµ‹è¯•ï¼Œè¯·è‡³å°‘è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```bash
RUN_SLOW=1 pytest tests/deepspeed/test_deepspeed.py
```

å¦‚æœä½ æ›´æ”¹äº†ä»»ä½•æ¨¡å‹æˆ–PyTorchç¤ºä¾‹ä»£ç ï¼Œè¯·åŒæ—¶è¿è¡Œå¤šæ¨¡å‹æµ‹è¯•ã€‚ä»¥ä¸‹å°†è¿è¡Œæ‰€æœ‰DeepSpeedæµ‹è¯•ï¼š

```bash
RUN_SLOW=1 pytest tests/deepspeed
```

## ä¸»è¦çš„DeepSpeedèµ„æº

- [é¡¹ç›®GitHub](https://github.com/deepspeedai/DeepSpeed)
- [ä½¿ç”¨æ–‡æ¡£](https://www.deepspeed.ai/getting-started/)
- [APIæ–‡æ¡£](https://deepspeed.readthedocs.io/en/latest/index.html)
- [åšå®¢æ–‡ç« ](https://www.microsoft.com/en-us/research/search/?q=deepspeed)

è®ºæ–‡:

- [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://huggingface.co/papers/1910.02054)
- [ZeRO-Offload: Democratizing Billion-Scale Model Training](https://huggingface.co/papers/2101.06840)
- [ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning](https://huggingface.co/papers/2104.07857)

æœ€åï¼Œè¯·è®°ä½ï¼ŒHuggingFace [`Trainer`]ä»…é›†æˆäº†DeepSpeedï¼Œå› æ­¤å¦‚æœæ‚¨åœ¨ä½¿ç”¨DeepSpeedæ—¶é‡åˆ°ä»»ä½•é—®é¢˜æˆ–ç–‘é—®ï¼Œè¯·åœ¨[DeepSpeed GitHub](https://github.com/deepspeedai/DeepSpeed/issues)ä¸Šæäº¤ä¸€ä¸ªissueã€‚

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\main_classes\feature_extractor.md
============================================================



# Feature Extractor

Feature Extractorè´Ÿè´£ä¸ºéŸ³é¢‘æˆ–è§†è§‰æ¨¡å‹å‡†å¤‡è¾“å…¥ç‰¹å¾ã€‚è¿™åŒ…æ‹¬ä»åºåˆ—ä¸­æå–ç‰¹å¾ï¼Œä¾‹å¦‚ï¼Œå¯¹éŸ³é¢‘æ–‡ä»¶è¿›è¡Œé¢„å¤„ç†ä»¥ç”ŸæˆLog-Melé¢‘è°±ç‰¹å¾ï¼Œä»¥åŠä»å›¾åƒä¸­æå–ç‰¹å¾ï¼Œä¾‹å¦‚ï¼Œè£å‰ªå›¾åƒæ–‡ä»¶ï¼ŒåŒæ—¶è¿˜åŒ…æ‹¬å¡«å……ã€å½’ä¸€åŒ–å’Œè½¬æ¢ä¸ºNumPyã€PyTorchå’ŒTensorFlowå¼ é‡ã€‚


## FeatureExtractionMixin

[[autodoc]] feature_extraction_utils.FeatureExtractionMixin
    - from_pretrained
    - save_pretrained

## SequenceFeatureExtractor

[[autodoc]] SequenceFeatureExtractor
    - pad

## BatchFeature

[[autodoc]] BatchFeature

## ImageFeatureExtractionMixin

[[autodoc]] image_utils.ImageFeatureExtractionMixin

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\main_classes\image_processor.md
============================================================



# Image Processor

Image processorè´Ÿè´£ä¸ºè§†è§‰æ¨¡å‹å‡†å¤‡è¾“å…¥ç‰¹å¾å¹¶åæœŸå¤„ç†å¤„ç†å®ƒä»¬çš„è¾“å‡ºã€‚è¿™åŒ…æ‹¬è¯¸å¦‚è°ƒæ•´å¤§å°ã€å½’ä¸€åŒ–å’Œè½¬æ¢ä¸ºPyTorchå’ŒNumPyå¼ é‡ç­‰è½¬æ¢ã€‚å®ƒè¿˜å¯èƒ½åŒ…æ‹¬ç‰¹å®šäºæ¨¡å‹çš„åæœŸå¤„ç†ï¼Œä¾‹å¦‚å°†logitsè½¬æ¢ä¸ºåˆ†å‰²æ©ç ã€‚


## ImageProcessingMixin

[[autodoc]] image_processing_utils.ImageProcessingMixin
    - from_pretrained
    - save_pretrained

## BatchFeature

[[autodoc]] BatchFeature

## BaseImageProcessor

[[autodoc]] image_processing_utils.BaseImageProcessor

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\main_classes\logging.md
============================================================



# Logging

ğŸ¤— Transformersæ‹¥æœ‰ä¸€ä¸ªé›†ä¸­å¼çš„æ—¥å¿—ç³»ç»Ÿï¼Œå› æ­¤æ‚¨å¯ä»¥è½»æ¾è®¾ç½®åº“è¾“å‡ºçš„æ—¥å¿—è¯¦ç»†ç¨‹åº¦ã€‚

å½“å‰åº“çš„é»˜è®¤æ—¥å¿—è¯¦ç»†ç¨‹åº¦ä¸º`WARNING`ã€‚

è¦æ›´æ”¹æ—¥å¿—è¯¦ç»†ç¨‹åº¦ï¼Œåªéœ€ä½¿ç”¨å…¶ä¸­ä¸€ä¸ªç›´æ¥çš„setterã€‚ä¾‹å¦‚ï¼Œä»¥ä¸‹æ˜¯å¦‚ä½•å°†æ—¥å¿—è¯¦ç»†ç¨‹åº¦æ›´æ”¹ä¸ºINFOçº§åˆ«çš„æ–¹æ³•ï¼š

```python
import transformers

transformers.logging.set_verbosity_info()
```

æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ç¯å¢ƒå˜é‡`TRANSFORMERS_VERBOSITY`æ¥è¦†ç›–é»˜è®¤çš„æ—¥å¿—è¯¦ç»†ç¨‹åº¦ã€‚æ‚¨å¯ä»¥å°†å…¶è®¾ç½®ä¸ºä»¥ä¸‹çº§åˆ«ä¹‹ä¸€ï¼š`debug`ã€`info`ã€`warning`ã€`error`ã€`critical`ã€‚ä¾‹å¦‚ï¼š

```bash
TRANSFORMERS_VERBOSITY=error ./myprogram.py
```

æ­¤å¤–ï¼Œé€šè¿‡å°†ç¯å¢ƒå˜é‡`TRANSFORMERS_NO_ADVISORY_WARNINGS`è®¾ç½®ä¸º`true`ï¼ˆå¦‚*1*ï¼‰ï¼Œå¯ä»¥ç¦ç”¨ä¸€äº›`warnings`ã€‚è¿™å°†ç¦ç”¨[`logger.warning_advice`]è®°å½•çš„ä»»ä½•è­¦å‘Šã€‚ä¾‹å¦‚ï¼š

```bash
TRANSFORMERS_NO_ADVISORY_WARNINGS=1 ./myprogram.py
```

ä»¥ä¸‹æ˜¯å¦‚ä½•åœ¨æ‚¨è‡ªå·±çš„æ¨¡å—æˆ–è„šæœ¬ä¸­ä½¿ç”¨ä¸åº“ç›¸åŒçš„loggerçš„ç¤ºä¾‹ï¼š

```python
from transformers.utils import logging

logging.set_verbosity_info()
logger = logging.get_logger("transformers")
logger.info("INFO")
logger.warning("WARN")
```


æ­¤æ—¥å¿—æ¨¡å—çš„æ‰€æœ‰æ–¹æ³•éƒ½åœ¨ä¸‹é¢è¿›è¡Œäº†è®°å½•ï¼Œä¸»è¦çš„æ–¹æ³•åŒ…æ‹¬ [`logging.get_verbosity`] ç”¨äºè·å–loggerå½“å‰è¾“å‡ºæ—¥å¿—è¯¦ç»†ç¨‹åº¦çš„çº§åˆ«å’Œ [`logging.set_verbosity`] ç”¨äºå°†è¯¦ç»†ç¨‹åº¦è®¾ç½®ä¸ºæ‚¨é€‰æ‹©çš„çº§åˆ«ã€‚æŒ‰ç…§é¡ºåºï¼ˆä»æœ€ä¸è¯¦ç»†åˆ°æœ€è¯¦ç»†ï¼‰ï¼Œè¿™äº›çº§åˆ«ï¼ˆåŠå…¶ç›¸åº”çš„æ•´æ•°å€¼ï¼‰ä¸ºï¼š

- `transformers.logging.CRITICAL` æˆ– `transformers.logging.FATAL`ï¼ˆæ•´æ•°å€¼ï¼Œ50ï¼‰ï¼šä»…æŠ¥å‘Šæœ€å…³é”®çš„errorsã€‚
- `transformers.logging.ERROR`ï¼ˆæ•´æ•°å€¼ï¼Œ40ï¼‰ï¼šä»…æŠ¥å‘Šerrorsã€‚
- `transformers.logging.WARNING` æˆ– `transformers.logging.WARN`ï¼ˆæ•´æ•°å€¼ï¼Œ30ï¼‰ï¼šä»…æŠ¥å‘Šerrorå’Œwarningsã€‚è¿™æ˜¯åº“ä½¿ç”¨çš„é»˜è®¤çº§åˆ«ã€‚
- `transformers.logging.INFO`ï¼ˆæ•´æ•°å€¼ï¼Œ20ï¼‰ï¼šæŠ¥å‘Šerrorã€warningså’ŒåŸºæœ¬ä¿¡æ¯ã€‚
- `transformers.logging.DEBUG`ï¼ˆæ•´æ•°å€¼ï¼Œ10ï¼‰ï¼šæŠ¥å‘Šæ‰€æœ‰ä¿¡æ¯ã€‚

é»˜è®¤æƒ…å†µä¸‹ï¼Œå°†åœ¨æ¨¡å‹ä¸‹è½½æœŸé—´æ˜¾ç¤º`tqdm`è¿›åº¦æ¡ã€‚[`logging.disable_progress_bar`] å’Œ [`logging.enable_progress_bar`] å¯ç”¨äºç¦æ­¢æˆ–å¯ç”¨æ­¤è¡Œä¸ºã€‚

## `logging` vs `warnings`

Pythonæœ‰ä¸¤ä¸ªç»å¸¸ä¸€èµ·ä½¿ç”¨çš„æ—¥å¿—ç³»ç»Ÿï¼šå¦‚ä¸Šæ‰€è¿°çš„`logging`ï¼Œå’Œå¯¹ç‰¹å®šbucketsä¸­çš„è­¦å‘Šè¿›è¡Œè¿›ä¸€æ­¥åˆ†ç±»çš„`warnings`ï¼Œä¾‹å¦‚ï¼Œ`FutureWarning`ç”¨äºè¾“å‡ºå·²ç»è¢«å¼ƒç”¨çš„åŠŸèƒ½æˆ–è·¯å¾„ï¼Œ`DeprecationWarning`ç”¨äºæŒ‡ç¤ºå³å°†è¢«å¼ƒç”¨çš„å†…å®¹ã€‚

æˆ‘ä»¬åœ¨`transformers`åº“ä¸­åŒæ—¶ä½¿ç”¨è¿™ä¸¤ä¸ªç³»ç»Ÿã€‚æˆ‘ä»¬åˆ©ç”¨å¹¶è°ƒæ•´äº†`logging`çš„`captureWarning`æ–¹æ³•ï¼Œä»¥ä¾¿é€šè¿‡ä¸Šé¢çš„è¯¦ç»†ç¨‹åº¦settersæ¥ç®¡ç†è¿™äº›è­¦å‘Šæ¶ˆæ¯ã€‚

å¯¹äºåº“çš„å¼€å‘äººå‘˜ï¼Œè¿™æ„å‘³ç€ä»€ä¹ˆå‘¢ï¼Ÿæˆ‘ä»¬åº”è¯¥éµå¾ªä»¥ä¸‹å¯å‘æ³•åˆ™ï¼š
- åº“çš„å¼€å‘äººå‘˜å’Œä¾èµ–äº`transformers`çš„åº“åº”ä¼˜å…ˆä½¿ç”¨`warnings`
- `logging`åº”è¯¥ç”¨äºåœ¨æ—¥å¸¸é¡¹ç›®ä¸­ç»å¸¸ä½¿ç”¨å®ƒçš„ç”¨æˆ·

ä»¥ä¸‹æ˜¯`captureWarnings`æ–¹æ³•çš„å‚è€ƒã€‚

[[autodoc]] logging.captureWarnings

## Base setters

[[autodoc]] logging.set_verbosity_error

[[autodoc]] logging.set_verbosity_warning

[[autodoc]] logging.set_verbosity_info

[[autodoc]] logging.set_verbosity_debug

## Other functions

[[autodoc]] logging.get_verbosity

[[autodoc]] logging.set_verbosity

[[autodoc]] logging.get_logger

[[autodoc]] logging.enable_default_handler

[[autodoc]] logging.disable_default_handler

[[autodoc]] logging.enable_explicit_format

[[autodoc]] logging.reset_format

[[autodoc]] logging.enable_progress_bar

[[autodoc]] logging.disable_progress_bar

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\main_classes\model.md
============================================================

# æ¨¡å‹

åŸºç±» [`PreTrainedModel`] å®ç°äº†ä»æœ¬åœ°æ–‡ä»¶æˆ–ç›®å½•åŠ è½½/ä¿å­˜æ¨¡å‹çš„å¸¸ç”¨æ–¹æ³•ï¼Œæˆ–è€…ä»åº“ä¸Šæä¾›çš„é¢„è®­ç»ƒæ¨¡å‹é…ç½®ï¼ˆä» HuggingFace çš„ AWS S3 å­˜å‚¨åº“ä¸‹è½½ï¼‰åŠ è½½æ¨¡å‹ã€‚

[`PreTrainedModel`] å’Œ [`TFPreTrainedModel`] è¿˜å®ç°äº†ä¸€äº›æ‰€æœ‰æ¨¡å‹å…±æœ‰çš„æ–¹æ³•ï¼š

- åœ¨å‘é‡è¯åµŒå…¥å¢åŠ æ–°è¯æ±‡æ—¶è°ƒæ•´è¾“å…¥æ ‡è®°ï¼ˆtokenï¼‰çš„å¤§å°
- å¯¹æ¨¡å‹çš„æ³¨æ„åŠ›å¤´è¿›è¡Œä¿®å‰ªã€‚

å…¶ä»–çš„é€šç”¨æ–¹æ³•åœ¨ [`~modeling_utils.ModuleUtilsMixin`]ï¼ˆç”¨äº PyTorch æ¨¡å‹ï¼‰ä¸­å®šä¹‰ï¼›æ–‡æœ¬ç”Ÿæˆæ–¹é¢çš„æ–¹æ³•åˆ™å®šä¹‰åœ¨ [`~generation.GenerationMixin`]ï¼ˆç”¨äº PyTorch æ¨¡å‹ï¼‰ä¸­ã€‚

## PreTrainedModel

[[autodoc]] PreTrainedModel
    - push_to_hub
    - all

<a id='from_pretrained-torch-dtype'></a>

### å¤§æ¨¡å‹åŠ è½½

åœ¨ Transformers 4.20.0 ä¸­ï¼Œ[`~PreTrainedModel.from_pretrained`] æ–¹æ³•å·²é‡æ–°è®¾è®¡ï¼Œä»¥é€‚åº”ä½¿ç”¨ [Accelerate](https://huggingface.co/docs/accelerate/big_modeling) åŠ è½½å¤§å‹æ¨¡å‹çš„åœºæ™¯ã€‚è¿™éœ€è¦æ‚¨ä½¿ç”¨çš„ Accelerate å’Œ PyTorch ç‰ˆæœ¬æ»¡è¶³ï¼š Accelerate >= 0.9.0ï¼Œ PyTorch >= 1.9.0ã€‚é™¤äº†åˆ›å»ºå®Œæ•´æ¨¡å‹ï¼Œç„¶ååœ¨å…¶ä¸­åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼ˆè¿™ä¼šå ç”¨ä¸¤å€äºæ¨¡å‹å¤§å°çš„å†…å­˜ç©ºé—´ï¼Œä¸€ä¸ªç”¨äºéšæœºåˆå§‹åŒ–æ¨¡å‹ï¼Œä¸€ä¸ªç”¨äºé¢„è®­ç»ƒæƒé‡ï¼‰ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ç§é€‰é¡¹ï¼Œå°†æ¨¡å‹åˆ›å»ºä¸ºç©ºå£³ï¼Œç„¶ååªæœ‰åœ¨åŠ è½½é¢„è®­ç»ƒæƒé‡æ—¶æ‰å®ä¾‹åŒ–å…¶å‚æ•°ã€‚

æ­¤å¤–ï¼Œå¦‚æœå†…å­˜ä¸è¶³ä»¥æ”¾ä¸‹åŠ è½½æ•´ä¸ªæ¨¡å‹ï¼ˆç›®å‰ä»…é€‚ç”¨äºæ¨ç†ï¼‰ï¼Œæ‚¨å¯ä»¥ç›´æ¥å°†æ¨¡å‹æ”¾ç½®åœ¨ä¸åŒçš„è®¾å¤‡ä¸Šã€‚ä½¿ç”¨ `device_map="auto"`ï¼ŒAccelerate å°†ç¡®å®šå°†æ¯ä¸€å±‚æ”¾ç½®åœ¨å“ªä¸ªè®¾å¤‡ä¸Šï¼Œä»¥æœ€å¤§åŒ–ä½¿ç”¨æœ€å¿«çš„è®¾å¤‡ï¼ˆGPUï¼‰ï¼Œå¹¶å°†å…¶ä½™éƒ¨åˆ†å¸è½½åˆ° CPUï¼Œç”šè‡³ç¡¬ç›˜ä¸Šï¼ˆå¦‚æœæ‚¨æ²¡æœ‰è¶³å¤Ÿçš„ GPU å†…å­˜ æˆ– CPU å†…å­˜ï¼‰ã€‚å³ä½¿æ¨¡å‹åˆ†å¸ƒåœ¨å‡ ä¸ªè®¾å¤‡ä¸Šï¼Œå®ƒä¹Ÿå°†åƒæ‚¨é€šå¸¸æœŸæœ›çš„é‚£æ ·è¿è¡Œã€‚

```python
from transformers import AutoModelForSeq2SeqLM

t0pp = AutoModelForSeq2SeqLM.from_pretrained("bigscience/T0pp", device_map="auto")
```

æ‚¨å¯ä»¥é€šè¿‡ `hf_device_map` å±æ€§æ¥æŸ¥çœ‹æ¨¡å‹æ˜¯å¦‚ä½•åœ¨è®¾å¤‡ä¸Šåˆ†å‰²çš„ï¼š

```python
t0pp.hf_device_map
{'shared': 0,
 'decoder.embed_tokens': 0,
 'encoder': 0,
 'decoder.block.0': 0,
 'decoder.block.1': 1,
 'decoder.block.2': 1,
 'decoder.block.3': 1,
 'decoder.block.4': 1,
 'decoder.block.5': 1,
 'decoder.block.6': 1,
 'decoder.block.7': 1,
 'decoder.block.8': 1,
 'decoder.block.9': 1,
 'decoder.block.10': 1,
 'decoder.block.11': 1,
 'decoder.block.12': 1,
 'decoder.block.13': 1,
 'decoder.block.14': 1,
 'decoder.block.15': 1,
 'decoder.block.16': 1,
 'decoder.block.17': 1,
 'decoder.block.18': 1,
 'decoder.block.19': 1,
 'decoder.block.20': 1,
 'decoder.block.21': 1,
 'decoder.block.22': 'cpu',
 'decoder.block.23': 'cpu',
 'decoder.final_layer_norm': 'cpu',
 'decoder.dropout': 'cpu',
 'lm_head': 'cpu'}
```

æ‚¨è¿˜å¯ä»¥æŒ‰ç…§ç›¸åŒçš„æ ¼å¼ï¼ˆä¸€ä¸ªå±‚åç§°åˆ°è®¾å¤‡çš„æ˜ å°„å…³ç³»çš„å­—å…¸ï¼‰ç¼–å†™è‡ªå·±çš„è®¾å¤‡æ˜ å°„è§„åˆ™ã€‚å®ƒåº”è¯¥å°†æ¨¡å‹çš„æ‰€æœ‰å‚æ•°æ˜ å°„åˆ°ç»™å®šçš„è®¾å¤‡ä¸Šï¼Œå¦‚æœè¯¥å±‚çš„æ‰€æœ‰å­æ¨¡å—éƒ½åœ¨åŒä¸€è®¾å¤‡ä¸Šï¼Œæ‚¨ä¸å¿…è¯¦ç»†è¯´æ˜å…¶ä¸­æ‰€æœ‰å­æ¨¡å—çš„ä½ç½®ã€‚ä¾‹å¦‚ï¼Œä»¥ä¸‹è®¾å¤‡æ˜ å°„å¯¹äº T0pp å°†æ­£å¸¸å·¥ä½œï¼ˆåªè¦æ‚¨æœ‰ GPU å†…å­˜ï¼‰ï¼š

```python
device_map = {"shared": 0, "encoder": 0, "decoder": 1, "lm_head": 1}
```

å¦ä¸€ç§å‡å°‘æ¨¡å‹å†…å­˜å½±å“çš„æ–¹æ³•æ˜¯ä»¥è¾ƒä½ç²¾åº¦çš„ dtypeï¼ˆä¾‹å¦‚ `torch.float16`ï¼‰å®ä¾‹åŒ–å®ƒï¼Œæˆ–è€…ä½¿ç”¨ä¸‹é¢ä»‹ç»çš„ç›´æ¥é‡åŒ–æŠ€æœ¯ã€‚

### æ¨¡å‹å®ä¾‹åŒ– dtype

åœ¨ PyTorch ä¸‹ï¼Œæ¨¡å‹é€šå¸¸ä»¥ `torch.float32` æ ¼å¼å®ä¾‹åŒ–ã€‚å¦‚æœå°è¯•åŠ è½½æƒé‡ä¸º fp16 çš„æ¨¡å‹ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´é—®é¢˜ï¼Œå› ä¸ºå®ƒå°†éœ€è¦ä¸¤å€çš„å†…å­˜ã€‚ä¸ºäº†å…‹æœæ­¤é™åˆ¶ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ `dtype` å‚æ•°æ˜¾å¼ä¼ é€’æ‰€éœ€çš„ `dtype`ï¼š

```python
model = T5ForConditionalGeneration.from_pretrained("t5", dtype=torch.float16)
```
æˆ–è€…ï¼Œå¦‚æœæ‚¨å¸Œæœ›æ¨¡å‹å§‹ç»ˆä»¥æœ€ä¼˜çš„å†…å­˜æ¨¡å¼åŠ è½½ï¼Œåˆ™å¯ä»¥ä½¿ç”¨ç‰¹æ®Šå€¼ `"auto"`ï¼Œç„¶å `dtype` å°†è‡ªåŠ¨ä»æ¨¡å‹çš„æƒé‡ä¸­æ¨å¯¼å‡ºï¼š
```python
model = T5ForConditionalGeneration.from_pretrained("t5", dtype="auto")
```

ä¹Ÿå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å‘ŠçŸ¥ä»å¤´å¼€å§‹å®ä¾‹åŒ–çš„æ¨¡å‹è¦ä½¿ç”¨å“ªç§ `dtype`ï¼š

```python
config = T5Config.from_pretrained("t5")
model = AutoModel.from_config(config)
```

ç”±äº PyTorch çš„è®¾è®¡ï¼Œæ­¤åŠŸèƒ½ä»…é€‚ç”¨äºæµ®ç‚¹ç±»å‹ã€‚


## ModuleUtilsMixin

[[autodoc]] modeling_utils.ModuleUtilsMixin

## æ¨é€åˆ° Hub
[[autodoc]] utils.PushToHubMixin

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\main_classes\optimizer_schedules.md
============================================================



# Optimization

`.optimization` æ¨¡å—æä¾›äº†ï¼š

- ä¸€ä¸ªå¸¦æœ‰å›ºå®šæƒé‡è¡°å‡çš„ä¼˜åŒ–å™¨ï¼Œå¯ç”¨äºå¾®è°ƒæ¨¡å‹
- ç»§æ‰¿è‡ª `_LRSchedule` å¤šä¸ªè°ƒåº¦å™¨ï¼š
- ä¸€ä¸ªæ¢¯åº¦ç´¯ç§¯ç±»ï¼Œç”¨äºç´¯ç§¯å¤šä¸ªæ‰¹æ¬¡çš„æ¢¯åº¦

## AdaFactor (PyTorch)

[[autodoc]] Adafactor

## Schedules

### Learning Rate Schedules (Pytorch)

[[autodoc]] SchedulerType

[[autodoc]] get_scheduler

[[autodoc]] get_constant_schedule

[[autodoc]] get_constant_schedule_with_warmup

<img alt="" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_constant_schedule.png"/>

[[autodoc]] get_cosine_schedule_with_warmup

<img alt="" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_cosine_schedule.png"/>

[[autodoc]] get_cosine_with_hard_restarts_schedule_with_warmup

<img alt="" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_cosine_hard_restarts_schedule.png"/>

[[autodoc]] get_linear_schedule_with_warmup

<img alt="" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_linear_schedule.png"/>

[[autodoc]] get_polynomial_decay_schedule_with_warmup

[[autodoc]] get_inverse_sqrt_schedule

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\main_classes\output.md
============================================================



# æ¨¡å‹è¾“å‡º

æ‰€æœ‰æ¨¡å‹çš„è¾“å‡ºéƒ½æ˜¯ [`~utils.ModelOutput`] çš„å­ç±»çš„å®ä¾‹ã€‚è¿™äº›æ˜¯åŒ…å«æ¨¡å‹è¿”å›çš„æ‰€æœ‰ä¿¡æ¯çš„æ•°æ®ç»“æ„ï¼Œä½†ä¹Ÿå¯ä»¥ç”¨ä½œå…ƒç»„æˆ–å­—å…¸ã€‚

è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªä¾‹å­ï¼š

```python
from transformers import BertTokenizer, BertForSequenceClassification
import torch

tokenizer = BertTokenizer.from_pretrained("google-bert/bert-base-uncased")
model = BertForSequenceClassification.from_pretrained("google-bert/bert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1
outputs = model(**inputs, labels=labels)
```

`outputs` å¯¹è±¡æ˜¯ [`~modeling_outputs.SequenceClassifierOutput`]ï¼Œå¦‚ä¸‹é¢è¯¥ç±»çš„æ–‡æ¡£ä¸­æ‰€ç¤ºï¼Œå®ƒè¡¨ç¤ºå®ƒæœ‰ä¸€ä¸ªå¯é€‰çš„ `loss`ï¼Œä¸€ä¸ª `logits`ï¼Œä¸€ä¸ªå¯é€‰çš„ `hidden_states` å’Œä¸€ä¸ªå¯é€‰çš„ `attentions` å±æ€§ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æœ‰ `loss`ï¼Œå› ä¸ºæˆ‘ä»¬ä¼ é€’äº† `labels`ï¼Œä½†æˆ‘ä»¬æ²¡æœ‰ `hidden_states` å’Œ `attentions`ï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰ä¼ é€’ `output_hidden_states=True` æˆ– `output_attentions=True`ã€‚

<Tip>

å½“ä¼ é€’ `output_hidden_states=True` æ—¶ï¼Œæ‚¨å¯èƒ½å¸Œæœ› `outputs.hidden_states[-1]` ä¸ `outputs.last_hidden_states` å®Œå…¨åŒ¹é…ã€‚ç„¶è€Œï¼Œè¿™å¹¶ä¸æ€»æ˜¯æˆç«‹ã€‚ä¸€äº›æ¨¡å‹åœ¨è¿”å›æœ€åçš„ hidden stateæ—¶å¯¹å…¶åº”ç”¨å½’ä¸€åŒ–æˆ–å…¶ä»–åç»­å¤„ç†ã€‚

</Tip>


æ‚¨å¯ä»¥åƒå¾€å¸¸ä¸€æ ·è®¿é—®æ¯ä¸ªå±æ€§ï¼Œå¦‚æœæ¨¡å‹æœªè¿”å›è¯¥å±æ€§ï¼Œæ‚¨å°†å¾—åˆ° `None`ã€‚åœ¨è¿™é‡Œï¼Œä¾‹å¦‚ï¼Œ`outputs.loss` æ˜¯æ¨¡å‹è®¡ç®—çš„æŸå¤±ï¼Œè€Œ `outputs.attentions` æ˜¯ `None`ã€‚

å½“å°†æˆ‘ä»¬çš„ `outputs` å¯¹è±¡è§†ä¸ºå…ƒç»„æ—¶ï¼Œå®ƒä»…è€ƒè™‘é‚£äº›æ²¡æœ‰ `None` å€¼çš„å±æ€§ã€‚ä¾‹å¦‚è¿™é‡Œå®ƒæœ‰ä¸¤ä¸ªå…ƒç´ ï¼Œ`loss` å’Œ `logits`ï¼Œæ‰€ä»¥

```python
outputs[:2]
```

å°†è¿”å›å…ƒç»„ `(outputs.loss, outputs.logits)`ã€‚

å°†æˆ‘ä»¬çš„ `outputs` å¯¹è±¡è§†ä¸ºå­—å…¸æ—¶ï¼Œå®ƒä»…è€ƒè™‘é‚£äº›æ²¡æœ‰ `None` å€¼çš„å±æ€§ã€‚ä¾‹å¦‚åœ¨è¿™é‡Œå®ƒæœ‰ä¸¤ä¸ªé”®ï¼Œåˆ†åˆ«æ˜¯ `loss` å’Œ `logits`ã€‚

æˆ‘ä»¬åœ¨è¿™é‡Œè®°å½•äº†è¢«å¤šä¸ªç±»å‹æ¨¡å‹ä½¿ç”¨çš„é€šç”¨æ¨¡å‹è¾“å‡ºã€‚ç‰¹å®šè¾“å‡ºç±»å‹åœ¨å…¶ç›¸åº”çš„æ¨¡å‹é¡µé¢ä¸Šæœ‰æ–‡æ¡£ã€‚

## ModelOutput

[[autodoc]] utils.ModelOutput
    - to_tuple

## BaseModelOutput

[[autodoc]] modeling_outputs.BaseModelOutput

## BaseModelOutputWithPooling

[[autodoc]] modeling_outputs.BaseModelOutputWithPooling

## BaseModelOutputWithCrossAttentions

[[autodoc]] modeling_outputs.BaseModelOutputWithCrossAttentions

## BaseModelOutputWithPoolingAndCrossAttentions

[[autodoc]] modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions

## BaseModelOutputWithPast

[[autodoc]] modeling_outputs.BaseModelOutputWithPast

## BaseModelOutputWithPastAndCrossAttentions

[[autodoc]] modeling_outputs.BaseModelOutputWithPastAndCrossAttentions

## Seq2SeqModelOutput

[[autodoc]] modeling_outputs.Seq2SeqModelOutput

## CausalLMOutput

[[autodoc]] modeling_outputs.CausalLMOutput

## CausalLMOutputWithCrossAttentions

[[autodoc]] modeling_outputs.CausalLMOutputWithCrossAttentions

## CausalLMOutputWithPast

[[autodoc]] modeling_outputs.CausalLMOutputWithPast

## MaskedLMOutput

[[autodoc]] modeling_outputs.MaskedLMOutput

## Seq2SeqLMOutput

[[autodoc]] modeling_outputs.Seq2SeqLMOutput

## NextSentencePredictorOutput

[[autodoc]] modeling_outputs.NextSentencePredictorOutput

## SequenceClassifierOutput

[[autodoc]] modeling_outputs.SequenceClassifierOutput

## Seq2SeqSequenceClassifierOutput

[[autodoc]] modeling_outputs.Seq2SeqSequenceClassifierOutput

## MultipleChoiceModelOutput

[[autodoc]] modeling_outputs.MultipleChoiceModelOutput

## TokenClassifierOutput

[[autodoc]] modeling_outputs.TokenClassifierOutput

## QuestionAnsweringModelOutput

[[autodoc]] modeling_outputs.QuestionAnsweringModelOutput

## Seq2SeqQuestionAnsweringModelOutput

[[autodoc]] modeling_outputs.Seq2SeqQuestionAnsweringModelOutput

## Seq2SeqSpectrogramOutput

[[autodoc]] modeling_outputs.Seq2SeqSpectrogramOutput

## SemanticSegmenterOutput

[[autodoc]] modeling_outputs.SemanticSegmenterOutput

## ImageClassifierOutput

[[autodoc]] modeling_outputs.ImageClassifierOutput

## ImageClassifierOutputWithNoAttention

[[autodoc]] modeling_outputs.ImageClassifierOutputWithNoAttention

## DepthEstimatorOutput

[[autodoc]] modeling_outputs.DepthEstimatorOutput

## Wav2Vec2BaseModelOutput

[[autodoc]] modeling_outputs.Wav2Vec2BaseModelOutput

## XVectorOutput

[[autodoc]] modeling_outputs.XVectorOutput

## Seq2SeqTSModelOutput

[[autodoc]] modeling_outputs.Seq2SeqTSModelOutput

## Seq2SeqTSPredictionOutput

[[autodoc]] modeling_outputs.Seq2SeqTSPredictionOutput

## SampleTSPredictionOutput

[[autodoc]] modeling_outputs.SampleTSPredictionOutput

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\main_classes\pipelines.md
============================================================



# Pipelines

pipelinesæ˜¯ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨ç†çš„ä¸€ç§ç®€å•æ–¹æ³•ã€‚è¿™äº›pipelinesæ˜¯æŠ½è±¡äº†åº“ä¸­å¤§éƒ¨åˆ†å¤æ‚ä»£ç çš„å¯¹è±¡ï¼Œæä¾›äº†ä¸€ä¸ªä¸“ç”¨äºå¤šä¸ªä»»åŠ¡çš„ç®€å•APIï¼ŒåŒ…æ‹¬ä¸“åè¯†åˆ«ã€æ©ç è¯­è¨€å»ºæ¨¡ã€æƒ…æ„Ÿåˆ†æã€ç‰¹å¾æå–å’Œé—®ç­”ç­‰ã€‚è¯·å‚é˜…[ä»»åŠ¡æ‘˜è¦](../task_summary)ä»¥è·å–ä½¿ç”¨ç¤ºä¾‹ã€‚

æœ‰ä¸¤ç§pipelinesæŠ½è±¡ç±»éœ€è¦æ³¨æ„ï¼š

- [`pipeline`]ï¼Œå®ƒæ˜¯å°è£…æ‰€æœ‰å…¶ä»–pipelinesçš„æœ€å¼ºå¤§çš„å¯¹è±¡ã€‚
- é’ˆå¯¹ç‰¹å®šä»»åŠ¡pipelinesï¼Œé€‚ç”¨äº[éŸ³é¢‘](#audio)ã€[è®¡ç®—æœºè§†è§‰](#computer-vision)ã€[è‡ªç„¶è¯­è¨€å¤„ç†](#natural-language-processing)å’Œ[å¤šæ¨¡æ€](#multimodal)ä»»åŠ¡ã€‚

## pipelineæŠ½è±¡ç±»

*pipeline*æŠ½è±¡ç±»æ˜¯å¯¹æ‰€æœ‰å…¶ä»–å¯ç”¨pipelineçš„å°è£…ã€‚å®ƒå¯ä»¥åƒä»»ä½•å…¶ä»–pipelineä¸€æ ·å®ä¾‹åŒ–ï¼Œä½†è¿›ä¸€æ­¥æä¾›é¢å¤–çš„ä¾¿åˆ©æ€§ã€‚

ç®€å•è°ƒç”¨ä¸€ä¸ªé¡¹ç›®ï¼š


```python
>>> pipe = pipeline("text-classification")
>>> pipe("This restaurant is awesome")
[{'label': 'POSITIVE', 'score': 0.9998743534088135}]
```

å¦‚æœæ‚¨æƒ³ä½¿ç”¨ [hub](https://huggingface.co) ä¸Šçš„ç‰¹å®šæ¨¡å‹ï¼Œå¯ä»¥å¿½ç•¥ä»»åŠ¡ï¼Œå¦‚æœhubä¸Šçš„æ¨¡å‹å·²ç»å®šä¹‰äº†è¯¥ä»»åŠ¡ï¼š

```python
>>> pipe = pipeline(model="FacebookAI/roberta-large-mnli")
>>> pipe("This restaurant is awesome")
[{'label': 'NEUTRAL', 'score': 0.7313136458396912}]
```

è¦åœ¨å¤šä¸ªé¡¹ç›®ä¸Šè°ƒç”¨pipelineï¼Œå¯ä»¥ä½¿ç”¨*åˆ—è¡¨*è°ƒç”¨å®ƒã€‚

```python
>>> pipe = pipeline("text-classification")
>>> pipe(["This restaurant is awesome", "This restaurant is awful"])
[{'label': 'POSITIVE', 'score': 0.9998743534088135},
 {'label': 'NEGATIVE', 'score': 0.9996669292449951}]
```

ä¸ºäº†éå†æ•´ä¸ªæ•°æ®é›†ï¼Œå»ºè®®ç›´æ¥ä½¿ç”¨ `dataset`ã€‚è¿™æ„å‘³ç€æ‚¨ä¸éœ€è¦ä¸€æ¬¡æ€§åˆ†é…æ•´ä¸ªæ•°æ®é›†ï¼Œä¹Ÿä¸éœ€è¦è‡ªå·±è¿›è¡Œæ‰¹å¤„ç†ã€‚è¿™åº”è¯¥ä¸GPUä¸Šçš„è‡ªå®šä¹‰å¾ªç¯ä¸€æ ·å¿«ã€‚å¦‚æœä¸æ˜¯ï¼Œè¯·éšæ—¶æå‡ºissueã€‚

```python
import datasets
from transformers import pipeline
from transformers.pipelines.pt_utils import KeyDataset
from tqdm.auto import tqdm

pipe = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h", device=0)
dataset = datasets.load_dataset("superb", name="asr", split="test")

# KeyDataset (only *pt*) will simply return the item in the dict returned by the dataset item
# as we're not interested in the *target* part of the dataset. For sentence pair use KeyPairDataset
for out in tqdm(pipe(KeyDataset(dataset, "file"))):
    print(out)
    # {"text": "NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD NIGHT HUSBAND"}
    # {"text": ....}
    # ....
```

ä¸ºäº†æ–¹ä¾¿ä½¿ç”¨ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ç”Ÿæˆå™¨ï¼š


```python
from transformers import pipeline

pipe = pipeline("text-classification")


def data():
    while True:
        # This could come from a dataset, a database, a queue or HTTP request
        # in a server
        # Caveat: because this is iterative, you cannot use `num_workers > 1` variable
        # to use multiple threads to preprocess data. You can still have 1 thread that
        # does the preprocessing while the main runs the big inference
        yield "This is a test"


for out in pipe(data()):
    print(out)
    # {"text": "NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD NIGHT HUSBAND"}
    # {"text": ....}
    # ....
```

[[autodoc]] pipeline

## Pipeline batching

æ‰€æœ‰pipelineéƒ½å¯ä»¥ä½¿ç”¨æ‰¹å¤„ç†ã€‚è¿™å°†åœ¨pipelineä½¿ç”¨å…¶æµå¤„ç†åŠŸèƒ½æ—¶èµ·ä½œç”¨ï¼ˆå³ä¼ é€’åˆ—è¡¨æˆ– `Dataset` æˆ– `generator` æ—¶ï¼‰ã€‚

```python
from transformers import pipeline
from transformers.pipelines.pt_utils import KeyDataset
import datasets

dataset = datasets.load_dataset("imdb", name="plain_text", split="unsupervised")
pipe = pipeline("text-classification", device=0)
for out in pipe(KeyDataset(dataset, "text"), batch_size=8, truncation="only_first"):
    print(out)
    # [{'label': 'POSITIVE', 'score': 0.9998743534088135}]
    # Exactly the same output as before, but the content are passed
    # as batches to the model
```

<Tip warning={true}>

ç„¶è€Œï¼Œè¿™å¹¶ä¸è‡ªåŠ¨æ„å‘³ç€æ€§èƒ½æå‡ã€‚å®ƒå¯èƒ½æ˜¯ä¸€ä¸ª10å€çš„åŠ é€Ÿæˆ–5å€çš„å‡é€Ÿï¼Œå…·ä½“å–å†³äºç¡¬ä»¶ã€æ•°æ®å’Œå®é™…ä½¿ç”¨çš„æ¨¡å‹ã€‚

ä¸»è¦æ˜¯åŠ é€Ÿçš„ç¤ºä¾‹ï¼š

</Tip>

```python
from transformers import pipeline
from torch.utils.data import Dataset
from tqdm.auto import tqdm

pipe = pipeline("text-classification", device=0)


class MyDataset(Dataset):
    def __len__(self):
        return 5000

    def __getitem__(self, i):
        return "This is a test"


dataset = MyDataset()

for batch_size in [1, 8, 64, 256]:
    print("-" * 30)
    print(f"Streaming batch_size={batch_size}")
    for out in tqdm(pipe(dataset, batch_size=batch_size), total=len(dataset)):
        pass
```

```
# On GTX 970
------------------------------
Streaming no batching
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:26<00:00, 187.52it/s]
------------------------------
Streaming batch_size=8
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:04<00:00, 1205.95it/s]
------------------------------
Streaming batch_size=64
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:02<00:00, 2478.24it/s]
------------------------------
Streaming batch_size=256
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:01<00:00, 2554.43it/s]
(diminishing returns, saturated the GPU)
```

ä¸»è¦æ˜¯å‡é€Ÿçš„ç¤ºä¾‹ï¼š

```python
class MyDataset(Dataset):
    def __len__(self):
        return 5000

    def __getitem__(self, i):
        if i % 64 == 0:
            n = 100
        else:
            n = 1
        return "This is a test" * n
```

ä¸å…¶ä»–å¥å­ç›¸æ¯”ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸é•¿çš„å¥å­ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ**æ•´ä¸ª**æ‰¹æ¬¡å°†éœ€è¦400ä¸ªtokensçš„é•¿åº¦ï¼Œå› æ­¤æ•´ä¸ªæ‰¹æ¬¡å°†æ˜¯ [64, 400] è€Œä¸æ˜¯ [64, 4]ï¼Œä»è€Œå¯¼è‡´è¾ƒå¤§çš„å‡é€Ÿã€‚æ›´ç³Ÿç³•çš„æ˜¯ï¼Œåœ¨æ›´å¤§çš„æ‰¹æ¬¡ä¸Šï¼Œç¨‹åºä¼šå´©æºƒã€‚

```
------------------------------
Streaming no batching
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:05<00:00, 183.69it/s]
------------------------------
Streaming batch_size=8
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:03<00:00, 265.74it/s]
------------------------------
Streaming batch_size=64
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:26<00:00, 37.80it/s]
------------------------------
Streaming batch_size=256
  0%|                                                                                 | 0/1000 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/nicolas/src/transformers/test.py", line 42, in <module>
    for out in tqdm(pipe(dataset, batch_size=256), total=len(dataset)):
....
    q = q / math.sqrt(dim_per_head)  # (bs, n_heads, q_length, dim_per_head)
RuntimeError: CUDA out of memory. Tried to allocate 376.00 MiB (GPU 0; 3.95 GiB total capacity; 1.72 GiB already allocated; 354.88 MiB free; 2.46 GiB reserved in total by PyTorch)
```

å¯¹äºè¿™ä¸ªé—®é¢˜ï¼Œæ²¡æœ‰å¥½çš„ï¼ˆé€šç”¨ï¼‰è§£å†³æ–¹æ¡ˆï¼Œæ•ˆæœå¯èƒ½å› æ‚¨çš„ç”¨ä¾‹è€Œå¼‚ã€‚ç»éªŒæ³•åˆ™å¦‚ä¸‹ï¼š

å¯¹äºç”¨æˆ·ï¼Œä¸€ä¸ªç»éªŒæ³•åˆ™æ˜¯ï¼š

- **ä½¿ç”¨ç¡¬ä»¶æµ‹é‡è´Ÿè½½æ€§èƒ½ã€‚æµ‹é‡ã€æµ‹é‡ã€å†æµ‹é‡ã€‚çœŸå®çš„æ•°å­—æ˜¯å”¯ä¸€çš„æ–¹æ³•ã€‚**
- å¦‚æœå—åˆ°å»¶è¿Ÿçš„é™åˆ¶ï¼ˆè¿›è¡Œæ¨ç†çš„å®æ—¶äº§å“ï¼‰ï¼Œä¸è¦è¿›è¡Œæ‰¹å¤„ç†ã€‚
- å¦‚æœä½¿ç”¨CPUï¼Œä¸è¦è¿›è¡Œæ‰¹å¤„ç†ã€‚
- å¦‚æœæ‚¨åœ¨GPUä¸Šå¤„ç†çš„æ˜¯ååé‡ï¼ˆæ‚¨å¸Œæœ›åœ¨å¤§é‡é™æ€æ•°æ®ä¸Šè¿è¡Œæ¨¡å‹ï¼‰ï¼Œåˆ™ï¼š
  - å¦‚æœå¯¹åºåˆ—é•¿åº¦çš„å¤§å°æ²¡æœ‰æ¦‚å¿µï¼ˆ"è‡ªç„¶"æ•°æ®ï¼‰ï¼Œé»˜è®¤æƒ…å†µä¸‹ä¸è¦è¿›è¡Œæ‰¹å¤„ç†ï¼Œè¿›è¡Œæµ‹è¯•å¹¶å°è¯•é€æ¸æ·»åŠ ï¼Œæ·»åŠ OOMæ£€æŸ¥ä»¥åœ¨å¤±è´¥æ—¶æ¢å¤ï¼ˆå¦‚æœæ‚¨ä¸èƒ½æ§åˆ¶åºåˆ—é•¿åº¦ï¼Œå®ƒå°†åœ¨æŸäº›æ—¶å€™å¤±è´¥ï¼‰ã€‚
  - å¦‚æœæ‚¨çš„åºåˆ—é•¿åº¦éå¸¸è§„å¾‹ï¼Œé‚£ä¹ˆæ‰¹å¤„ç†æ›´æœ‰å¯èƒ½éå¸¸æœ‰è¶£ï¼Œè¿›è¡Œæµ‹è¯•å¹¶æ¨åŠ¨å®ƒï¼Œç›´åˆ°å‡ºç°OOMã€‚
  - GPUè¶Šå¤§ï¼Œæ‰¹å¤„ç†è¶Šæœ‰å¯èƒ½å˜å¾—æ›´æœ‰è¶£
- ä¸€æ—¦å¯ç”¨æ‰¹å¤„ç†ï¼Œç¡®ä¿èƒ½å¤Ÿå¾ˆå¥½åœ°å¤„ç†OOMã€‚

## Pipeline chunk batching

`zero-shot-classification` å’Œ `question-answering` åœ¨æŸç§æ„ä¹‰ä¸Šç¨å¾®ç‰¹æ®Šï¼Œå› ä¸ºå•ä¸ªè¾“å…¥å¯èƒ½ä¼šå¯¼è‡´æ¨¡å‹çš„å¤šæ¬¡å‰å‘ä¼ é€’ã€‚åœ¨æ­£å¸¸æƒ…å†µä¸‹ï¼Œè¿™å°†å¯¼è‡´ `batch_size` å‚æ•°çš„é—®é¢˜ã€‚

ä¸ºäº†è§„é¿è¿™ä¸ªé—®é¢˜ï¼Œè¿™ä¸¤ä¸ªpipelineéƒ½æœ‰ç‚¹ç‰¹æ®Šï¼Œå®ƒä»¬æ˜¯ `ChunkPipeline` è€Œä¸æ˜¯å¸¸è§„çš„ `Pipeline`ã€‚ç®€è€Œè¨€ä¹‹ï¼š


```python
preprocessed = pipe.preprocess(inputs)
model_outputs = pipe.forward(preprocessed)
outputs = pipe.postprocess(model_outputs)
```

ç°åœ¨å˜æˆï¼š


```python
all_model_outputs = []
for preprocessed in pipe.preprocess(inputs):
    model_outputs = pipe.forward(preprocessed)
    all_model_outputs.append(model_outputs)
outputs = pipe.postprocess(all_model_outputs)
```

è¿™å¯¹æ‚¨çš„ä»£ç åº”è¯¥æ˜¯éå¸¸ç›´è§‚çš„ï¼Œå› ä¸ºpipelineçš„ä½¿ç”¨æ–¹å¼æ˜¯ç›¸åŒçš„ã€‚

è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„è§†å›¾ï¼Œå› ä¸ºPipelineå¯ä»¥è‡ªåŠ¨å¤„ç†æ‰¹æ¬¡ï¼è¿™æ„å‘³ç€æ‚¨ä¸å¿…æ‹…å¿ƒæ‚¨çš„è¾“å…¥å®é™…ä¸Šä¼šè§¦å‘å¤šå°‘æ¬¡å‰å‘ä¼ é€’ï¼Œæ‚¨å¯ä»¥ç‹¬ç«‹äºè¾“å…¥ä¼˜åŒ– `batch_size`ã€‚å‰é¢éƒ¨åˆ†çš„æ³¨æ„äº‹é¡¹ä»ç„¶é€‚ç”¨ã€‚

## Pipelineè‡ªå®šä¹‰

å¦‚æœæ‚¨æƒ³è¦é‡è½½ç‰¹å®šçš„pipelineã€‚

è¯·éšæ—¶ä¸ºæ‚¨æ‰‹å¤´çš„ä»»åŠ¡åˆ›å»ºä¸€ä¸ªissueï¼ŒPipelineçš„ç›®æ ‡æ˜¯æ˜“äºä½¿ç”¨å¹¶æ”¯æŒå¤§å¤šæ•°æƒ…å†µï¼Œå› æ­¤ `transformers` å¯èƒ½æ”¯æŒæ‚¨çš„ç”¨ä¾‹ã€‚

å¦‚æœæ‚¨æƒ³ç®€å•åœ°å°è¯•ä¸€ä¸‹ï¼Œå¯ä»¥ï¼š

- ç»§æ‰¿æ‚¨é€‰æ‹©çš„pipeline

```python
class MyPipeline(TextClassificationPipeline):
    def postprocess():
        # Your code goes here
        scores = scores * 100
        # And here


my_pipeline = MyPipeline(model=model, tokenizer=tokenizer, ...)
# or if you use *pipeline* function, then:
my_pipeline = pipeline(model="xxxx", pipeline_class=MyPipeline)
```

è¿™æ ·å°±å¯ä»¥è®©æ‚¨ç¼–å†™æ‰€æœ‰æƒ³è¦çš„è‡ªå®šä¹‰ä»£ç ã€‚


## å®ç°ä¸€ä¸ªpipeline

[å®ç°ä¸€ä¸ªæ–°çš„pipeline](../add_new_pipeline)

## éŸ³é¢‘

å¯ç”¨äºéŸ³é¢‘ä»»åŠ¡çš„pipelineåŒ…æ‹¬ä»¥ä¸‹å‡ ç§ã€‚

### AudioClassificationPipeline

[[autodoc]] AudioClassificationPipeline
    - __call__
    - all

### AutomaticSpeechRecognitionPipeline

[[autodoc]] AutomaticSpeechRecognitionPipeline
    - __call__
    - all

### TextToAudioPipeline

[[autodoc]] TextToAudioPipeline
    - __call__
    - all


### ZeroShotAudioClassificationPipeline

[[autodoc]] ZeroShotAudioClassificationPipeline
    - __call__
    - all

## è®¡ç®—æœºè§†è§‰

å¯ç”¨äºè®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„pipelineåŒ…æ‹¬ä»¥ä¸‹å‡ ç§ã€‚

### DepthEstimationPipeline
[[autodoc]] DepthEstimationPipeline
    - __call__
    - all

### ImageClassificationPipeline

[[autodoc]] ImageClassificationPipeline
    - __call__
    - all

### ImageSegmentationPipeline

[[autodoc]] ImageSegmentationPipeline
    - __call__
    - all

### ImageToImagePipeline

[[autodoc]] ImageToImagePipeline
    - __call__
    - all

### ObjectDetectionPipeline

[[autodoc]] ObjectDetectionPipeline
    - __call__
    - all

### VideoClassificationPipeline

[[autodoc]] VideoClassificationPipeline
    - __call__
    - all

### ZeroShotImageClassificationPipeline

[[autodoc]] ZeroShotImageClassificationPipeline
    - __call__
    - all

### ZeroShotObjectDetectionPipeline

[[autodoc]] ZeroShotObjectDetectionPipeline
    - __call__
    - all

## è‡ªç„¶è¯­è¨€å¤„ç†

å¯ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„pipelineåŒ…æ‹¬ä»¥ä¸‹å‡ ç§ã€‚

### FillMaskPipeline

[[autodoc]] FillMaskPipeline
    - __call__
    - all

### NerPipeline

[[autodoc]] NerPipeline

See [`TokenClassificationPipeline`] for all details.

### QuestionAnsweringPipeline

[[autodoc]] QuestionAnsweringPipeline
    - __call__
    - all

### TableQuestionAnsweringPipeline

[[autodoc]] TableQuestionAnsweringPipeline
    - __call__

### TextClassificationPipeline

[[autodoc]] TextClassificationPipeline
    - __call__
    - all

### TextGenerationPipeline

[[autodoc]] TextGenerationPipeline
    - __call__
    - all

### TokenClassificationPipeline

[[autodoc]] TokenClassificationPipeline
    - __call__
    - all

### ZeroShotClassificationPipeline

[[autodoc]] ZeroShotClassificationPipeline
    - __call__
    - all

## å¤šæ¨¡æ€

å¯ç”¨äºå¤šæ¨¡æ€ä»»åŠ¡çš„pipelineåŒ…æ‹¬ä»¥ä¸‹å‡ ç§ã€‚

### DocumentQuestionAnsweringPipeline

[[autodoc]] DocumentQuestionAnsweringPipeline
    - __call__
    - all

### FeatureExtractionPipeline

[[autodoc]] FeatureExtractionPipeline
    - __call__
    - all

### ImageFeatureExtractionPipeline

[[autodoc]] ImageFeatureExtractionPipeline
    - __call__
    - all

### ImageTextToTextPipeline

[[autodoc]] ImageTextToTextPipeline
    - __call__
    - all

### MaskGenerationPipeline

[[autodoc]] MaskGenerationPipeline
    - __call__
    - all

### VisualQuestionAnsweringPipeline

[[autodoc]] VisualQuestionAnsweringPipeline
    - __call__
    - all

## Parent class: `Pipeline`

[[autodoc]] Pipeline

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\main_classes\processors.md
============================================================



# Processors

åœ¨ Transformers åº“ä¸­ï¼Œprocessorså¯ä»¥æœ‰ä¸¤ç§ä¸åŒçš„å«ä¹‰ï¼š
- ä¸ºå¤šæ¨¡æ€æ¨¡å‹ï¼Œä¾‹å¦‚[Wav2Vec2](../model_doc/wav2vec2)ï¼ˆè¯­éŸ³å’Œæ–‡æœ¬ï¼‰æˆ–[CLIP](../model_doc/clip)ï¼ˆæ–‡æœ¬å’Œè§†è§‰ï¼‰é¢„å¤„ç†è¾“å…¥çš„å¯¹è±¡
- åœ¨åº“çš„æ—§ç‰ˆæœ¬ä¸­ç”¨äºé¢„å¤„ç†GLUEæˆ–SQUADæ•°æ®çš„å·²å¼ƒç”¨å¯¹è±¡ã€‚

## å¤šæ¨¡æ€processors

ä»»ä½•å¤šæ¨¡æ€æ¨¡å‹éƒ½éœ€è¦ä¸€ä¸ªå¯¹è±¡æ¥ç¼–ç æˆ–è§£ç å°†å¤šä¸ªæ¨¡æ€ï¼ˆåŒ…æ‹¬æ–‡æœ¬ã€è§†è§‰å’ŒéŸ³é¢‘ï¼‰ç»„åˆåœ¨ä¸€èµ·çš„æ•°æ®ã€‚è¿™ç”±ç§°ä¸ºprocessorsçš„å¯¹è±¡å¤„ç†ï¼Œè¿™äº›processorså°†ä¸¤ä¸ªæˆ–å¤šä¸ªå¤„ç†å¯¹è±¡ç»„åˆåœ¨ä¸€èµ·ï¼Œä¾‹å¦‚tokenizersï¼ˆç”¨äºæ–‡æœ¬æ¨¡æ€ï¼‰ï¼Œimage processorsï¼ˆç”¨äºè§†è§‰ï¼‰å’Œfeature extractorsï¼ˆç”¨äºéŸ³é¢‘ï¼‰ã€‚

è¿™äº›processorsç»§æ‰¿è‡ªä»¥ä¸‹å®ç°ä¿å­˜å’ŒåŠ è½½åŠŸèƒ½çš„åŸºç±»ï¼š


[[autodoc]] ProcessorMixin

## å·²å¼ƒç”¨çš„processors

æ‰€æœ‰processoréƒ½éµå¾ªä¸ [`~data.processors.utils.DataProcessor`] ç›¸åŒçš„æ¶æ„ã€‚processorè¿”å›ä¸€ä¸ª [`~data.processors.utils.InputExample`] åˆ—è¡¨ã€‚è¿™äº› [`~data.processors.utils.InputExample`] å¯ä»¥è½¬æ¢ä¸º [`~data.processors.utils.InputFeatures`] ä»¥ä¾›è¾“é€åˆ°æ¨¡å‹ã€‚

[[autodoc]] data.processors.utils.DataProcessor

[[autodoc]] data.processors.utils.InputExample

[[autodoc]] data.processors.utils.InputFeatures

## GLUE

[General Language Understanding Evaluation (GLUE)](https://gluebenchmark.com/) æ˜¯ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°æ¨¡å‹åœ¨å„ç§ç°æœ‰çš„è‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚å®ƒä¸è®ºæ–‡ [GLUE: A multi-task benchmark and analysis platform for natural language understanding](https://openreview.net/pdf?id=rJ4km2R5t7) ä¸€åŒå‘å¸ƒã€‚

è¯¥åº“ä¸ºä»¥ä¸‹ä»»åŠ¡æä¾›äº†æ€»å…±10ä¸ªprocessorï¼šMRPCã€MNLIã€MNLIï¼ˆmismatchedï¼‰ã€CoLAã€SST2ã€STSBã€QQPã€QNLIã€RTE å’Œ WNLIã€‚

è¿™äº›processoræ˜¯ï¼š

- [`~data.processors.utils.MrpcProcessor`]
- [`~data.processors.utils.MnliProcessor`]
- [`~data.processors.utils.MnliMismatchedProcessor`]
- [`~data.processors.utils.Sst2Processor`]
- [`~data.processors.utils.StsbProcessor`]
- [`~data.processors.utils.QqpProcessor`]
- [`~data.processors.utils.QnliProcessor`]
- [`~data.processors.utils.RteProcessor`]
- [`~data.processors.utils.WnliProcessor`]

æ­¤å¤–ï¼Œè¿˜å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•ä»æ•°æ®æ–‡ä»¶åŠ è½½å€¼å¹¶å°†å…¶è½¬æ¢ä¸º [`~data.processors.utils.InputExample`] åˆ—è¡¨ã€‚

[[autodoc]] data.processors.glue.glue_convert_examples_to_features


## XNLI

[è·¨è¯­è¨€NLIè¯­æ–™åº“ï¼ˆXNLIï¼‰](https://www.nyu.edu/projects/bowman/xnli/) æ˜¯ä¸€ä¸ªè¯„ä¼°è·¨è¯­è¨€æ–‡æœ¬è¡¨ç¤ºè´¨é‡çš„åŸºå‡†æµ‹è¯•ã€‚XNLIæ˜¯ä¸€ä¸ªåŸºäº[*MultiNLI*](http://www.nyu.edu/projects/bowman/multinli/)çš„ä¼—åŒ…æ•°æ®é›†ï¼šâ€æ–‡æœ¬å¯¹â€œè¢«æ ‡è®°ä¸ºåŒ…å«15ç§ä¸åŒè¯­è¨€ï¼ˆåŒ…æ‹¬è‹±è¯­ç­‰é«˜èµ„æºè¯­è¨€å’Œæ–¯ç“¦å¸Œé‡Œè¯­ç­‰ä½èµ„æºè¯­è¨€ï¼‰çš„æ–‡æœ¬è•´æ¶µæ³¨é‡Šã€‚

å®ƒä¸è®ºæ–‡ [XNLI: Evaluating Cross-lingual Sentence Representations](https://huggingface.co/papers/1809.05053) ä¸€åŒå‘å¸ƒã€‚

è¯¥åº“æä¾›äº†åŠ è½½XNLIæ•°æ®çš„processorï¼š

- [`~data.processors.utils.XnliProcessor`]

è¯·æ³¨æ„ï¼Œç”±äºæµ‹è¯•é›†ä¸Šæœ‰â€œgoldâ€æ ‡ç­¾ï¼Œå› æ­¤è¯„ä¼°æ˜¯åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œçš„ã€‚

ä½¿ç”¨è¿™äº›processorçš„ç¤ºä¾‹åœ¨ [run_xnli.py](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification/run_xnli.py) è„šæœ¬ä¸­æä¾›ã€‚


## SQuAD

[æ–¯å¦ç¦é—®ç­”æ•°æ®é›†ï¼ˆSQuADï¼‰](https://rajpurkar.github.io/SQuAD-explorer//) æ˜¯ä¸€ä¸ªè¯„ä¼°æ¨¡å‹åœ¨é—®ç­”ä¸Šæ€§èƒ½çš„åŸºå‡†æµ‹è¯•ã€‚æœ‰ä¸¤ä¸ªç‰ˆæœ¬ï¼Œv1.1 å’Œ v2.0ã€‚ç¬¬ä¸€ä¸ªç‰ˆæœ¬ï¼ˆv1.1ï¼‰ä¸è®ºæ–‡ [SQuAD: 100,000+ Questions for Machine Comprehension of Text](https://huggingface.co/papers/1606.05250) ä¸€åŒå‘å¸ƒã€‚ç¬¬äºŒä¸ªç‰ˆæœ¬ï¼ˆv2.0ï¼‰ä¸è®ºæ–‡ [Know What You Don't Know: Unanswerable Questions for SQuAD](https://huggingface.co/papers/1806.03822) ä¸€åŒå‘å¸ƒã€‚

è¯¥åº“ä¸ºä¸¤ä¸ªç‰ˆæœ¬å„è‡ªæä¾›äº†ä¸€ä¸ªprocessorï¼š

### Processors

è¿™ä¸¤ä¸ªprocessoræ˜¯ï¼š

- [`~data.processors.utils.SquadV1Processor`]
- [`~data.processors.utils.SquadV2Processor`]

å®ƒä»¬éƒ½ç»§æ‰¿è‡ªæŠ½è±¡ç±» [`~data.processors.utils.SquadProcessor`]ã€‚

[[autodoc]] data.processors.squad.SquadProcessor
    - all

æ­¤å¤–ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•å°† SQuAD ç¤ºä¾‹è½¬æ¢ä¸ºå¯ç”¨ä½œæ¨¡å‹è¾“å…¥çš„ [`~data.processors.utils.SquadFeatures`]ã€‚

[[autodoc]] data.processors.squad.squad_convert_examples_to_features


è¿™äº›processorä»¥åŠå‰é¢æåˆ°çš„æ–¹æ³•å¯ä»¥ä¸åŒ…å«æ•°æ®çš„æ–‡ä»¶ä»¥åŠtensorflow_datasetsåŒ…ä¸€èµ·ä½¿ç”¨ã€‚ä¸‹é¢ç»™å‡ºäº†ç¤ºä¾‹ã€‚


### Exampleä½¿ç”¨

ä»¥ä¸‹æ˜¯ä½¿ç”¨processorä»¥åŠä½¿ç”¨æ•°æ®æ–‡ä»¶çš„è½¬æ¢æ–¹æ³•çš„ç¤ºä¾‹ï¼š

```python
# Loading a V2 processor
processor = SquadV2Processor()
examples = processor.get_dev_examples(squad_v2_data_dir)

# Loading a V1 processor
processor = SquadV1Processor()
examples = processor.get_dev_examples(squad_v1_data_dir)

features = squad_convert_examples_to_features(
    examples=examples,
    tokenizer=tokenizer,
    max_seq_length=max_seq_length,
    doc_stride=args.doc_stride,
    max_query_length=max_query_length,
    is_training=not evaluate,
)
```

ä½¿ç”¨ *tensorflow_datasets* å°±åƒä½¿ç”¨æ•°æ®æ–‡ä»¶ä¸€æ ·ç®€å•ï¼š

```python
# tensorflow_datasets only handle Squad V1.
tfds_examples = tfds.load("squad")
examples = SquadV1Processor().get_examples_from_dataset(tfds_examples, evaluate=evaluate)

features = squad_convert_examples_to_features(
    examples=examples,
    tokenizer=tokenizer,
    max_seq_length=max_seq_length,
    doc_stride=args.doc_stride,
    max_query_length=max_query_length,
    is_training=not evaluate,
)
```

å¦ä¸€ä¸ªä½¿ç”¨è¿™äº›processorçš„ç¤ºä¾‹åœ¨ [run_squad.py](https://github.com/huggingface/transformers/tree/main/examples/legacy/question-answering/run_squad.py) è„šæœ¬ä¸­æä¾›ã€‚

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\main_classes\quantization.md
============================================================



# é‡åŒ– ğŸ¤— Transformers æ¨¡å‹

## AWQé›†æˆ

AWQæ–¹æ³•å·²ç»åœ¨[*AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration*è®ºæ–‡](https://huggingface.co/papers/2306.00978)ä¸­å¼•å…¥ã€‚é€šè¿‡AWQï¼Œæ‚¨å¯ä»¥ä»¥4ä½ç²¾åº¦è¿è¡Œæ¨¡å‹ï¼ŒåŒæ—¶ä¿ç•™å…¶åŸå§‹æ€§èƒ½ï¼ˆå³æ²¡æœ‰æ€§èƒ½é™çº§ï¼‰ï¼Œå¹¶å…·æœ‰æ¯”ä¸‹é¢ä»‹ç»çš„å…¶ä»–é‡åŒ–æ–¹æ³•æ›´å‡ºè‰²çš„ååé‡ - è¾¾åˆ°ä¸çº¯`float16`æ¨ç†ç›¸ä¼¼çš„ååé‡ã€‚

æˆ‘ä»¬ç°åœ¨æ”¯æŒä½¿ç”¨ä»»ä½•AWQæ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œè¿™æ„å‘³ç€ä»»ä½•äººéƒ½å¯ä»¥åŠ è½½å’Œä½¿ç”¨åœ¨Hubä¸Šæ¨é€æˆ–æœ¬åœ°ä¿å­˜çš„AWQæƒé‡ã€‚è¯·æ³¨æ„ï¼Œä½¿ç”¨AWQéœ€è¦è®¿é—®NVIDIA GPUã€‚ç›®å‰ä¸æ”¯æŒCPUæ¨ç†ã€‚


### é‡åŒ–ä¸€ä¸ªæ¨¡å‹

æˆ‘ä»¬å»ºè®®ç”¨æˆ·æŸ¥çœ‹ç”Ÿæ€ç³»ç»Ÿä¸­ä¸åŒçš„ç°æœ‰å·¥å…·ï¼Œä»¥ä½¿ç”¨AWQç®—æ³•å¯¹å…¶æ¨¡å‹è¿›è¡Œé‡åŒ–ï¼Œä¾‹å¦‚ï¼š

- [`llm-awq`](https://github.com/mit-han-lab/llm-awq)ï¼Œæ¥è‡ªMIT Han Lab
- [`autoawq`](https://github.com/casper-hansen/AutoAWQ)ï¼Œæ¥è‡ª[`casper-hansen`](https://github.com/casper-hansen)
- Intel neural compressorï¼Œæ¥è‡ªIntel - é€šè¿‡[`optimum-intel`](https://huggingface.co/docs/optimum/main/en/intel/optimization_inc)ä½¿ç”¨

ç”Ÿæ€ç³»ç»Ÿä¸­å¯èƒ½å­˜åœ¨è®¸å¤šå…¶ä»–å·¥å…·ï¼Œè¯·éšæ—¶æå‡ºPRå°†å®ƒä»¬æ·»åŠ åˆ°åˆ—è¡¨ä¸­ã€‚
ç›®å‰ä¸ğŸ¤— Transformersçš„é›†æˆä»…é€‚ç”¨äºä½¿ç”¨`autoawq`å’Œ`llm-awq`é‡åŒ–åçš„æ¨¡å‹ã€‚å¤§å¤šæ•°ä½¿ç”¨`auto-awq`é‡åŒ–çš„æ¨¡å‹å¯ä»¥åœ¨ğŸ¤— Hubçš„[`TheBloke`](https://huggingface.co/TheBloke)å‘½åç©ºé—´ä¸‹æ‰¾åˆ°ï¼Œè¦ä½¿ç”¨`llm-awq`å¯¹æ¨¡å‹è¿›è¡Œé‡åŒ–ï¼Œè¯·å‚é˜…[`llm-awq`](https://github.com/mit-han-lab/llm-awq/)çš„ç¤ºä¾‹æ–‡ä»¶å¤¹ä¸­çš„[`convert_to_hf.py`](https://github.com/mit-han-lab/llm-awq/blob/main/examples/convert_to_hf.py)è„šæœ¬ã€‚


### åŠ è½½ä¸€ä¸ªé‡åŒ–çš„æ¨¡å‹

æ‚¨å¯ä»¥ä½¿ç”¨`from_pretrained`æ–¹æ³•ä»HubåŠ è½½ä¸€ä¸ªé‡åŒ–æ¨¡å‹ã€‚é€šè¿‡æ£€æŸ¥æ¨¡å‹é…ç½®æ–‡ä»¶ï¼ˆ`configuration.json`ï¼‰ä¸­æ˜¯å¦å­˜åœ¨`quantization_config`å±æ€§ï¼Œæ¥è¿›è¡Œç¡®è®¤æ¨é€çš„æƒé‡æ˜¯é‡åŒ–çš„ã€‚æ‚¨å¯ä»¥é€šè¿‡æ£€æŸ¥å­—æ®µ`quantization_config.quant_method`æ¥ç¡®è®¤æ¨¡å‹æ˜¯å¦ä»¥AWQæ ¼å¼è¿›è¡Œé‡åŒ–ï¼Œè¯¥å­—æ®µåº”è¯¥è®¾ç½®ä¸º`"awq"`ã€‚è¯·æ³¨æ„ï¼Œä¸ºäº†æ€§èƒ½åŸå› ï¼Œé»˜è®¤æƒ…å†µä¸‹åŠ è½½æ¨¡å‹å°†è®¾ç½®å…¶ä»–æƒé‡ä¸º`float16`ã€‚å¦‚æœæ‚¨æƒ³æ›´æ”¹è¿™ç§è®¾ç½®ï¼Œå¯ä»¥é€šè¿‡å°†`dtype`å‚æ•°è®¾ç½®ä¸º`torch.float32`æˆ–`torch.bfloat16`ã€‚åœ¨ä¸‹é¢çš„éƒ¨åˆ†ä¸­ï¼Œæ‚¨å¯ä»¥æ‰¾åˆ°ä¸€äº›ç¤ºä¾‹ç‰‡æ®µå’Œnotebookã€‚


## ç¤ºä¾‹ä½¿ç”¨

é¦–å…ˆï¼Œæ‚¨éœ€è¦å®‰è£…[`autoawq`](https://github.com/casper-hansen/AutoAWQ)åº“

```bash
pip install autoawq
```

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "TheBloke/zephyr-7B-alpha-AWQ"
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="cuda:0")
```

å¦‚æœæ‚¨é¦–å…ˆå°†æ¨¡å‹åŠ è½½åˆ°CPUä¸Šï¼Œè¯·ç¡®ä¿åœ¨ä½¿ç”¨ä¹‹å‰å°†å…¶ç§»åŠ¨åˆ°GPUè®¾å¤‡ä¸Šã€‚

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "TheBloke/zephyr-7B-alpha-AWQ"
model = AutoModelForCausalLM.from_pretrained(model_id).to("cuda:0")
```

### ç»“åˆ AWQ å’Œ Flash Attention

æ‚¨å¯ä»¥å°†AWQé‡åŒ–ä¸Flash Attentionç»“åˆèµ·æ¥ï¼Œå¾—åˆ°ä¸€ä¸ªæ—¢è¢«é‡åŒ–åˆæ›´å¿«é€Ÿçš„æ¨¡å‹ã€‚åªéœ€ä½¿ç”¨`from_pretrained`åŠ è½½æ¨¡å‹ï¼Œå¹¶ä¼ é€’`attn_implementation="flash_attention_2"`å‚æ•°ã€‚

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("TheBloke/zephyr-7B-alpha-AWQ", attn_implementation="flash_attention_2", device_map="cuda:0")
```

### åŸºå‡†æµ‹è¯•

æˆ‘ä»¬ä½¿ç”¨[`optimum-benchmark`](https://github.com/huggingface/optimum-benchmark)åº“è¿›è¡Œäº†ä¸€äº›é€Ÿåº¦ã€ååé‡å’Œå»¶è¿ŸåŸºå‡†æµ‹è¯•ã€‚

è¯·æ³¨æ„ï¼Œåœ¨ç¼–å†™æœ¬æ–‡æ¡£éƒ¨åˆ†æ—¶ï¼Œå¯ç”¨çš„é‡åŒ–æ–¹æ³•åŒ…æ‹¬ï¼š`awq`ã€`gptq`å’Œ`bitsandbytes`ã€‚

åŸºå‡†æµ‹è¯•åœ¨ä¸€å°NVIDIA-A100å®ä¾‹ä¸Šè¿è¡Œï¼Œä½¿ç”¨[`TheBloke/Mistral-7B-v0.1-AWQ`](https://huggingface.co/TheBloke/Mistral-7B-v0.1-AWQ)ä½œä¸ºAWQæ¨¡å‹ï¼Œ[`TheBloke/Mistral-7B-v0.1-GPTQ`](https://huggingface.co/TheBloke/Mistral-7B-v0.1-GPTQ)ä½œä¸ºGPTQæ¨¡å‹ã€‚æˆ‘ä»¬è¿˜å°†å…¶ä¸`bitsandbytes`é‡åŒ–æ¨¡å‹å’Œ`float16`æ¨¡å‹è¿›è¡Œäº†å¯¹æ¯”ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›ç»“æœç¤ºä¾‹ï¼š


<div style="text-align: center">
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/quantization/forward_memory_plot.png">
</div>

<div style="text-align: center">
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/quantization/generate_memory_plot.png">
</div>

<div style="text-align: center">
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/quantization/generate_throughput_plot.png">
</div>

<div style="text-align: center">
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/quantization/forward_latency_plot.png">
</div>

ä½ å¯ä»¥åœ¨[æ­¤é“¾æ¥](https://github.com/huggingface/optimum-benchmark/tree/main/examples/running-mistrals)ä¸­æ‰¾åˆ°å®Œæ•´çš„ç»“æœä»¥åŠåŒ…ç‰ˆæœ¬ã€‚

ä»ç»“æœæ¥çœ‹ï¼ŒAWQé‡åŒ–æ–¹æ³•æ˜¯æ¨ç†ã€æ–‡æœ¬ç”Ÿæˆä¸­æœ€å¿«çš„é‡åŒ–æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨æ–‡æœ¬ç”Ÿæˆçš„å³°å€¼å†…å­˜æ–¹é¢å±äºæœ€ä½ã€‚ç„¶è€Œï¼Œå¯¹äºæ¯æ‰¹æ•°æ®ï¼ŒAWQä¼¼ä¹æœ‰æœ€å¤§çš„å‰å‘å»¶è¿Ÿã€‚


### Google colab æ¼”ç¤º

æŸ¥çœ‹å¦‚ä½•åœ¨[Google Colabæ¼”ç¤º](https://colab.research.google.com/drive/1HzZH89yAXJaZgwJDhQj9LqSBux932BvY)ä¸­ä½¿ç”¨æ­¤é›†æˆï¼


### AwqConfig

[[autodoc]] AwqConfig

## GPT-QModel é›†æˆ

ğŸ¤— Transformerså·²ç»æ•´åˆäº†`optimum` APIï¼Œç”¨äºå¯¹è¯­è¨€æ¨¡å‹æ‰§è¡ŒGPTQé‡åŒ–ã€‚æ‚¨å¯ä»¥ä»¥8ã€4ã€3ç”šè‡³2ä½åŠ è½½å’Œé‡åŒ–æ‚¨çš„æ¨¡å‹ï¼Œè€Œæ€§èƒ½æ— æ˜æ˜¾ä¸‹é™ï¼Œå¹¶ä¸”æ¨ç†é€Ÿåº¦æ›´å¿«ï¼è¿™å—åˆ°å¤§å¤šæ•°GPUç¡¬ä»¶çš„æ”¯æŒã€‚

è¦äº†è§£æ›´å¤šå…³äºé‡åŒ–æ¨¡å‹çš„ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ï¼š
- [GPTQ](https://huggingface.co/papers/2210.17323)è®ºæ–‡
- `optimum`å…³äºGPTQé‡åŒ–çš„[æŒ‡å—](https://huggingface.co/docs/optimum/llm_quantization/usage_guides/quantization)
- ç”¨ä½œåç«¯çš„`GPT-QModel` (https://github.com/ModelCloud/GPTQModel)åº“


### è¦æ±‚

ä¸ºäº†è¿è¡Œä¸‹é¢çš„ä»£ç ï¼Œæ‚¨éœ€è¦å®‰è£…ï¼š

- å®‰è£…æœ€æ–°ç‰ˆæœ¬çš„ `GPT-QModel` åº“
`pip install gptqmodel --no-build-isolation`

- ä»æºä»£ç å®‰è£…æœ€æ–°ç‰ˆæœ¬çš„`optimum`
`pip install git+https://github.com/huggingface/optimum.git`

- ä»æºä»£ç å®‰è£…æœ€æ–°ç‰ˆæœ¬çš„`transformers`
`pip install git+https://github.com/huggingface/transformers.git`

- å®‰è£…æœ€æ–°ç‰ˆæœ¬çš„`accelerate`åº“ï¼š 
`pip install --upgrade accelerate`

è¯·æ³¨æ„ï¼Œç›®å‰GPTQé›†æˆä»…æ”¯æŒæ–‡æœ¬æ¨¡å‹ï¼Œå¯¹äºè§†è§‰ã€è¯­éŸ³æˆ–å¤šæ¨¡æ€æ¨¡å‹å¯èƒ½ä¼šé‡åˆ°é¢„æœŸä»¥å¤–ç»“æœã€‚

### åŠ è½½å’Œé‡åŒ–æ¨¡å‹

GPTQæ˜¯ä¸€ç§åœ¨ä½¿ç”¨é‡åŒ–æ¨¡å‹ä¹‹å‰éœ€è¦è¿›è¡Œæƒé‡æ ¡å‡†çš„é‡åŒ–æ–¹æ³•ã€‚å¦‚æœæ‚¨æƒ³ä»å¤´å¼€å§‹å¯¹transformersæ¨¡å‹è¿›è¡Œé‡åŒ–ï¼Œç”Ÿæˆé‡åŒ–æ¨¡å‹å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼ˆåœ¨Google Colabä¸Šå¯¹`facebook/opt-350m`æ¨¡å‹é‡åŒ–çº¦ä¸º5åˆ†é’Ÿï¼‰ã€‚

å› æ­¤ï¼Œæœ‰ä¸¤ç§ä¸åŒçš„æƒ…å†µä¸‹æ‚¨å¯èƒ½æƒ³ä½¿ç”¨GPTQé‡åŒ–æ¨¡å‹ã€‚ç¬¬ä¸€ç§æƒ…å†µæ˜¯åŠ è½½å·²ç»ç”±å…¶ä»–ç”¨æˆ·åœ¨Hubä¸Šé‡åŒ–çš„æ¨¡å‹ï¼Œç¬¬äºŒç§æƒ…å†µæ˜¯ä»å¤´å¼€å§‹å¯¹æ‚¨çš„æ¨¡å‹è¿›è¡Œé‡åŒ–å¹¶ä¿å­˜æˆ–æ¨é€åˆ°Hubï¼Œä»¥ä¾¿å…¶ä»–ç”¨æˆ·ä¹Ÿå¯ä»¥ä½¿ç”¨å®ƒã€‚


#### GPTQ é…ç½®

ä¸ºäº†åŠ è½½å’Œé‡åŒ–ä¸€ä¸ªæ¨¡å‹ï¼Œæ‚¨éœ€è¦åˆ›å»ºä¸€ä¸ª[`GPTQConfig`]ã€‚æ‚¨éœ€è¦ä¼ é€’`bits`çš„æ•°é‡ï¼Œä¸€ä¸ªç”¨äºæ ¡å‡†é‡åŒ–çš„`dataset`ï¼Œä»¥åŠæ¨¡å‹çš„`tokenizer`ä»¥å‡†å¤‡æ•°æ®é›†ã€‚

```python 
model_id = "facebook/opt-125m"
tokenizer = AutoTokenizer.from_pretrained(model_id)
gptq_config = GPTQConfig(bits=4, dataset = "c4", tokenizer=tokenizer)
```

è¯·æ³¨æ„ï¼Œæ‚¨å¯ä»¥å°†è‡ªå·±çš„æ•°æ®é›†ä»¥å­—ç¬¦ä¸²åˆ—è¡¨å½¢å¼ä¼ é€’åˆ°æ¨¡å‹ã€‚ç„¶è€Œï¼Œå¼ºçƒˆå»ºè®®æ‚¨ä½¿ç”¨GPTQè®ºæ–‡ä¸­æä¾›çš„æ•°æ®é›†ã€‚


```python
dataset = ["gptqmodel is an easy-to-use model quantization library with user-friendly apis, based on the GPTQ algorithm."]
quantization = GPTQConfig(bits=4, dataset = dataset, tokenizer=tokenizer)
```

#### é‡åŒ–

æ‚¨å¯ä»¥é€šè¿‡ä½¿ç”¨`from_pretrained`å¹¶è®¾ç½®`quantization_config`æ¥å¯¹æ¨¡å‹è¿›è¡Œé‡åŒ–ã€‚

```python
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=gptq_config)

```

è¯·æ³¨æ„ï¼Œæ‚¨éœ€è¦ä¸€ä¸ªGPUæ¥é‡åŒ–æ¨¡å‹ã€‚æˆ‘ä»¬å°†æ¨¡å‹æ”¾åœ¨cpuä¸­ï¼Œå¹¶å°†æ¨¡å—æ¥å›ç§»åŠ¨åˆ°gpuä¸­ï¼Œä»¥ä¾¿å¯¹å…¶è¿›è¡Œé‡åŒ–ã€‚

å¦‚æœæ‚¨æƒ³åœ¨ä½¿ç”¨ CPU å¸è½½çš„åŒæ—¶æœ€å¤§åŒ– GPU ä½¿ç”¨ç‡ï¼Œæ‚¨å¯ä»¥è®¾ç½® `device_map = "auto"`ã€‚


```python
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", quantization_config=gptq_config)
```

è¯·æ³¨æ„ï¼Œä¸æ”¯æŒç£ç›˜å¸è½½ã€‚æ­¤å¤–ï¼Œå¦‚æœç”±äºæ•°æ®é›†è€Œå†…å­˜ä¸è¶³ï¼Œæ‚¨å¯èƒ½éœ€è¦åœ¨`from_pretrained`ä¸­è®¾ç½®`max_memory`ã€‚æŸ¥çœ‹è¿™ä¸ª[æŒ‡å—](https://huggingface.co/docs/accelerate/usage_guides/big_modeling#designing-a-device-map)ä»¥äº†è§£æœ‰å…³`device_map`å’Œ`max_memory`çš„æ›´å¤šä¿¡æ¯ã€‚


<Tip warning={true}>
ç›®å‰ï¼ŒGPTQé‡åŒ–ä»…é€‚ç”¨äºæ–‡æœ¬æ¨¡å‹ã€‚æ­¤å¤–ï¼Œé‡åŒ–è¿‡ç¨‹å¯èƒ½ä¼šèŠ±è´¹å¾ˆå¤šæ—¶é—´ï¼Œå…·ä½“å–å†³äºç¡¬ä»¶æ€§èƒ½ï¼ˆ175Bæ¨¡å‹åœ¨NVIDIA A100ä¸Šéœ€è¦4å°æ—¶ï¼‰ã€‚è¯·åœ¨Hubä¸Šæ£€æŸ¥æ˜¯å¦æœ‰æ¨¡å‹çš„GPTQé‡åŒ–ç‰ˆæœ¬ã€‚å¦‚æœæ²¡æœ‰ï¼Œæ‚¨å¯ä»¥åœ¨GitHubä¸Šæäº¤éœ€æ±‚ã€‚ 
</Tip>

### æ¨é€é‡åŒ–æ¨¡å‹åˆ° ğŸ¤— Hub

æ‚¨å¯ä»¥ä½¿ç”¨`push_to_hub`å°†é‡åŒ–æ¨¡å‹åƒä»»ä½•æ¨¡å‹ä¸€æ ·æ¨é€åˆ°Hubã€‚é‡åŒ–é…ç½®å°†ä¸æ¨¡å‹ä¸€èµ·ä¿å­˜å’Œæ¨é€ã€‚

```python
quantized_model.push_to_hub("opt-125m-gptq")
tokenizer.push_to_hub("opt-125m-gptq")
```

å¦‚æœæ‚¨æƒ³åœ¨æœ¬åœ°è®¡ç®—æœºä¸Šä¿å­˜é‡åŒ–æ¨¡å‹ï¼Œæ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨`save_pretrained`æ¥å®Œæˆï¼š

```python
quantized_model.save_pretrained("opt-125m-gptq")
tokenizer.save_pretrained("opt-125m-gptq")
```

è¯·æ³¨æ„ï¼Œå¦‚æœæ‚¨é‡åŒ–æ¨¡å‹æ—¶æƒ³ä½¿ç”¨`device_map`ï¼Œè¯·ç¡®ä¿åœ¨ä¿å­˜ä¹‹å‰å°†æ•´ä¸ªæ¨¡å‹ç§»åŠ¨åˆ°æ‚¨çš„GPUæˆ–CPUä¹‹ä¸€ã€‚

```python
quantized_model.to("cpu")
quantized_model.save_pretrained("opt-125m-gptq")
```

### ä» ğŸ¤— Hub åŠ è½½ä¸€ä¸ªé‡åŒ–æ¨¡å‹

æ‚¨å¯ä»¥ä½¿ç”¨`from_pretrained`ä»HubåŠ è½½é‡åŒ–æ¨¡å‹ã€‚
è¯·ç¡®ä¿æ¨é€æƒé‡æ˜¯é‡åŒ–çš„ï¼Œæ£€æŸ¥æ¨¡å‹é…ç½®å¯¹è±¡ä¸­æ˜¯å¦å­˜åœ¨`quantization_config`å±æ€§ã€‚


```python
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained("{your_username}/opt-125m-gptq")
```

å¦‚æœæ‚¨æƒ³æ›´å¿«åœ°åŠ è½½æ¨¡å‹ï¼Œå¹¶ä¸”ä¸éœ€è¦åˆ†é…æ¯”å®é™…éœ€è¦å†…å­˜æ›´å¤šçš„å†…å­˜ï¼Œé‡åŒ–æ¨¡å‹ä¹Ÿä½¿ç”¨`device_map`å‚æ•°ã€‚ç¡®ä¿æ‚¨å·²å®‰è£…`accelerate`åº“ã€‚

```python
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained("{your_username}/opt-125m-gptq", device_map="auto")
```

### Exllamaå†…æ ¸åŠ å¿«æ¨ç†é€Ÿåº¦

ä¿ç•™æ ¼å¼ï¼šå¯¹äº 4 ä½æ¨¡å‹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ exllama å†…æ ¸æ¥æé«˜æ¨ç†é€Ÿåº¦ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå®ƒå¤„äºå¯ç”¨çŠ¶æ€ã€‚æ‚¨å¯ä»¥é€šè¿‡åœ¨ [`GPTQConfig`] ä¸­ä¼ é€’ `use_exllama` æ¥æ›´æ”¹æ­¤é…ç½®ã€‚è¿™å°†è¦†ç›–å­˜å‚¨åœ¨é…ç½®ä¸­çš„é‡åŒ–é…ç½®ã€‚è¯·æ³¨æ„ï¼Œæ‚¨åªèƒ½è¦†ç›–ä¸å†…æ ¸ç›¸å…³çš„å±æ€§ã€‚æ­¤å¤–ï¼Œå¦‚æœæ‚¨æƒ³ä½¿ç”¨ exllama å†…æ ¸ï¼Œæ•´ä¸ªæ¨¡å‹éœ€è¦å…¨éƒ¨éƒ¨ç½²åœ¨ gpus ä¸Šã€‚æ­¤å¤–ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ ç‰ˆæœ¬ > 0.4.2 çš„ Auto-GPTQ å¹¶ä¼ é€’ `device_map` = "cpu" æ¥æ‰§è¡Œ CPU æ¨ç†ã€‚å¯¹äº CPU æ¨ç†ï¼Œæ‚¨å¿…é¡»åœ¨ `GPTQConfig` ä¸­ä¼ é€’ `use_exllama = False`ã€‚

```py
import torch
gptq_config = GPTQConfig(bits=4)
model = AutoModelForCausalLM.from_pretrained("{your_username}/opt-125m-gptq", device_map="auto", quantization_config=gptq_config)
```

éšç€ exllamav2 å†…æ ¸çš„å‘å¸ƒï¼Œä¸ exllama å†…æ ¸ç›¸æ¯”ï¼Œæ‚¨å¯ä»¥è·å¾—æ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚æ‚¨åªéœ€åœ¨ [`GPTQConfig`] ä¸­ä¼ é€’ `exllama_config={"version": 2}`ï¼š

```py
import torch
gptq_config = GPTQConfig(bits=4, exllama_config={"version":2})
model = AutoModelForCausalLM.from_pretrained("{your_username}/opt-125m-gptq", device_map="auto", quantization_config = gptq_config)
```

è¯·æ³¨æ„ï¼Œç›®å‰ä»…æ”¯æŒ 4 ä½æ¨¡å‹ã€‚æ­¤å¤–ï¼Œå¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨ peft å¯¹é‡åŒ–æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå»ºè®®ç¦ç”¨ exllama å†…æ ¸ã€‚ 

æ‚¨å¯ä»¥åœ¨æ­¤æ‰¾åˆ°è¿™äº›å†…æ ¸çš„åŸºå‡†æµ‹è¯• [è¿™é‡Œ](https://github.com/huggingface/optimum/tree/main/tests/benchmark#gptq-benchmark)


#### å¾®è°ƒä¸€ä¸ªé‡åŒ–æ¨¡å‹

åœ¨Hugging Faceç”Ÿæ€ç³»ç»Ÿçš„å®˜æ–¹æ”¯æŒä¸‹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨GPTQè¿›è¡Œé‡åŒ–åçš„æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚ 
è¯·æŸ¥çœ‹`peft`åº“äº†è§£æ›´å¤šè¯¦æƒ…ã€‚

### ç¤ºä¾‹æ¼”ç¤º

è¯·æŸ¥çœ‹ Google Colab [notebook](https://colab.research.google.com/drive/1_TIrmuKOFhuRRiTWN94ilkUFu6ZX4ceb?usp=sharing)ï¼Œäº†è§£å¦‚ä½•ä½¿ç”¨GPTQé‡åŒ–æ‚¨çš„æ¨¡å‹ä»¥åŠå¦‚ä½•ä½¿ç”¨peftå¾®è°ƒé‡åŒ–æ¨¡å‹ã€‚

### GPTQConfig

[[autodoc]] GPTQConfig


## `bitsandbytes` é›†æˆ

ğŸ¤— Transformers ä¸ `bitsandbytes` ä¸Šæœ€å¸¸ç”¨çš„æ¨¡å—ç´§å¯†é›†æˆã€‚æ‚¨å¯ä»¥ä½¿ç”¨å‡ è¡Œä»£ç ä»¥ 8 ä½ç²¾åº¦åŠ è½½æ‚¨çš„æ¨¡å‹ã€‚
è‡ªbitsandbytesçš„0.37.0ç‰ˆæœ¬å‘å¸ƒä»¥æ¥ï¼Œå¤§å¤šæ•°GPUç¡¬ä»¶éƒ½æ”¯æŒè¿™ä¸€ç‚¹ã€‚

åœ¨[LLM.int8()](https://huggingface.co/papers/2208.07339)è®ºæ–‡ä¸­äº†è§£æ›´å¤šå…³äºé‡åŒ–æ–¹æ³•çš„ä¿¡æ¯ï¼Œæˆ–è€…åœ¨[åšå®¢æ–‡ç« ](https://huggingface.co/blog/hf-bitsandbytes-integration)ä¸­äº†è§£å…³äºåˆä½œçš„æ›´å¤šä¿¡æ¯ã€‚

è‡ªå…¶â€œ0.39.0â€ç‰ˆæœ¬å‘å¸ƒä»¥æ¥ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨FP4æ•°æ®ç±»å‹ï¼Œé€šè¿‡4ä½é‡åŒ–åŠ è½½ä»»ä½•æ”¯æŒâ€œdevice_mapâ€çš„æ¨¡å‹ã€‚

å¦‚æœæ‚¨æƒ³é‡åŒ–è‡ªå·±çš„ pytorch æ¨¡å‹ï¼Œè¯·æŸ¥çœ‹ ğŸ¤— Accelerate çš„[æ–‡æ¡£](https://huggingface.co/docs/accelerate/main/en/usage_guides/quantization)ã€‚

ä»¥ä¸‹æ˜¯æ‚¨å¯ä»¥ä½¿ç”¨â€œbitsandbytesâ€é›†æˆå®Œæˆçš„äº‹æƒ…

### é€šç”¨ç”¨æ³•

åªè¦æ‚¨çš„æ¨¡å‹æ”¯æŒä½¿ç”¨ ğŸ¤— Accelerate è¿›è¡ŒåŠ è½½å¹¶åŒ…å« `torch.nn.Linear` å±‚ï¼Œæ‚¨å¯ä»¥åœ¨è°ƒç”¨ [`~PreTrainedModel.from_pretrained`] æ–¹æ³•æ—¶ä½¿ç”¨ `load_in_8bit` æˆ– `load_in_4bit` å‚æ•°æ¥é‡åŒ–æ¨¡å‹ã€‚è¿™ä¹Ÿåº”è¯¥é€‚ç”¨äºä»»ä½•æ¨¡æ€ã€‚

```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

model_8bit = AutoModelForCausalLM.from_pretrained("facebook/opt-350m", quantization_config=BitsAndBytesConfig(load_in_8bit=True))
model_4bit = AutoModelForCausalLM.from_pretrained("facebook/opt-350m", quantization_config=BitsAndBytesConfig(load_in_4bit=True))
```

é»˜è®¤æƒ…å†µä¸‹ï¼Œæ‰€æœ‰å…¶ä»–æ¨¡å—ï¼ˆä¾‹å¦‚ `torch.nn.LayerNorm`ï¼‰å°†è¢«è½¬æ¢ä¸º `torch.float16` ç±»å‹ã€‚ä½†å¦‚æœæ‚¨æƒ³æ›´æ”¹å®ƒä»¬çš„ `dtype`ï¼Œå¯ä»¥é‡è½½ `dtype` å‚æ•°ï¼š

```python
>>> import torch
>>> from transformers import AutoModelForCausalLM, BitsAndBytesConfig

>>> model_8bit = AutoModelForCausalLM.from_pretrained("facebook/opt-350m", quantization_config=BitsAndBytesConfig(load_in_8bit=True), dtype=torch.float32)
>>> model_8bit.model.decoder.layers[-1].final_layer_norm.weight.dtype
torch.float32
```


### FP4 é‡åŒ– 

#### è¦æ±‚

ç¡®ä¿åœ¨è¿è¡Œä»¥ä¸‹ä»£ç æ®µä¹‹å‰å·²å®Œæˆä»¥ä¸‹è¦æ±‚ï¼š

- æœ€æ–°ç‰ˆæœ¬ `bitsandbytes` åº“
`pip install bitsandbytes>=0.39.0`

- å®‰è£…æœ€æ–°ç‰ˆæœ¬ `accelerate`
`pip install --upgrade accelerate`

- å®‰è£…æœ€æ–°ç‰ˆæœ¬ `transformers`
`pip install --upgrade transformers`

#### æç¤ºå’Œæœ€ä½³å®è·µ


- **é«˜çº§ç”¨æ³•ï¼š** è¯·å‚è€ƒ [æ­¤ Google Colab notebook](https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf) ä»¥è·å– 4 ä½é‡åŒ–é«˜çº§ç”¨æ³•å’Œæ‰€æœ‰å¯é€‰é€‰é¡¹ã€‚

- **ä½¿ç”¨ `batch_size=1` å®ç°æ›´å¿«çš„æ¨ç†ï¼š** è‡ª `bitsandbytes` çš„ `0.40.0` ç‰ˆæœ¬ä»¥æ¥ï¼Œè®¾ç½® `batch_size=1`ï¼Œæ‚¨å¯ä»¥ä»å¿«é€Ÿæ¨ç†ä¸­å—ç›Šã€‚è¯·æŸ¥çœ‹ [è¿™äº›å‘å¸ƒè¯´æ˜](https://github.com/TimDettmers/bitsandbytes/releases/tag/0.40.0) ï¼Œå¹¶ç¡®ä¿ä½¿ç”¨å¤§äº `0.40.0` çš„ç‰ˆæœ¬ä»¥ç›´æ¥åˆ©ç”¨æ­¤åŠŸèƒ½ã€‚

- **è®­ç»ƒï¼š** æ ¹æ® [QLoRA è®ºæ–‡](https://huggingface.co/papers/2305.14314)ï¼Œå¯¹äº4ä½åŸºæ¨¡å‹è®­ç»ƒï¼ˆä½¿ç”¨ LoRA é€‚é…å™¨ï¼‰ï¼Œåº”ä½¿ç”¨ `bnb_4bit_quant_type='nf4'`ã€‚

- **æ¨ç†ï¼š** å¯¹äºæ¨ç†ï¼Œ`bnb_4bit_quant_type` å¯¹æ€§èƒ½å½±å“ä¸å¤§ã€‚ä½†æ˜¯ä¸ºäº†ä¸æ¨¡å‹çš„æƒé‡ä¿æŒä¸€è‡´ï¼Œè¯·ç¡®ä¿ä½¿ç”¨ç›¸åŒçš„ `bnb_4bit_compute_dtype` å’Œ `dtype` å‚æ•°ã€‚

#### åŠ è½½ 4 ä½é‡åŒ–çš„å¤§æ¨¡å‹

åœ¨è°ƒç”¨ `.from_pretrained` æ–¹æ³•æ—¶ä½¿ç”¨ `load_in_4bit=True`ï¼Œå¯ä»¥å°†æ‚¨çš„å†…å­˜ä½¿ç”¨é‡å‡å°‘åˆ°å¤§çº¦åŸæ¥çš„ 1/4ã€‚

```python
# pip install transformers accelerate bitsandbytes
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "bigscience/bloom-1b7"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", quantization_config=BitsAndBytesConfig(load_in_4bit=True))
```

<Tip warning={true}>

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä¸€æ—¦æ¨¡å‹ä»¥ 4 ä½é‡åŒ–æ–¹å¼åŠ è½½ï¼Œå°±æ— æ³•å°†é‡åŒ–åçš„æƒé‡æ¨é€åˆ° Hub ä¸Šã€‚æ­¤å¤–ï¼Œæ‚¨ä¸èƒ½è®­ç»ƒ 4 ä½é‡åŒ–æƒé‡ï¼Œå› ä¸ºç›®å‰å°šä¸æ”¯æŒæ­¤åŠŸèƒ½ã€‚ä½†æ˜¯ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ 4 ä½é‡åŒ–æ¨¡å‹æ¥è®­ç»ƒé¢å¤–å‚æ•°ï¼Œè¿™å°†åœ¨ä¸‹ä¸€éƒ¨åˆ†ä¸­ä»‹ç»ã€‚

</Tip>

### åŠ è½½ 8 ä½é‡åŒ–çš„å¤§æ¨¡å‹

æ‚¨å¯ä»¥é€šè¿‡åœ¨è°ƒç”¨ `.from_pretrained` æ–¹æ³•æ—¶ä½¿ç”¨ `load_in_8bit=True` å‚æ•°ï¼Œå°†å†…å­˜éœ€æ±‚å¤§è‡´å‡åŠæ¥åŠ è½½æ¨¡å‹


```python
# pip install transformers accelerate bitsandbytes
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_id = "bigscience/bloom-1b7"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=BitsAndBytesConfig(load_in_8bit=True))
```

ç„¶åï¼Œåƒé€šå¸¸ä½¿ç”¨ `PreTrainedModel` ä¸€æ ·ä½¿ç”¨æ‚¨çš„æ¨¡å‹ã€‚

æ‚¨å¯ä»¥ä½¿ç”¨ `get_memory_footprint` æ–¹æ³•æ£€æŸ¥æ¨¡å‹çš„å†…å­˜å ç”¨ã€‚


```python
print(model.get_memory_footprint())
```

é€šè¿‡è¿™ç§é›†æˆï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨è¾ƒå°çš„è®¾å¤‡ä¸ŠåŠ è½½å¤§æ¨¡å‹å¹¶è¿è¡Œå®ƒä»¬è€Œæ²¡æœ‰ä»»ä½•é—®é¢˜ã€‚

<Tip warning={true}>

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä¸€æ—¦æ¨¡å‹ä»¥ 8 ä½é‡åŒ–æ–¹å¼åŠ è½½ï¼Œé™¤äº†ä½¿ç”¨æœ€æ–°çš„ `transformers` å’Œ `bitsandbytes` ä¹‹å¤–ï¼Œç›®å‰å°šæ— æ³•å°†é‡åŒ–åçš„æƒé‡æ¨é€åˆ° Hub ä¸Šã€‚æ­¤å¤–ï¼Œæ‚¨ä¸èƒ½è®­ç»ƒ 8 ä½é‡åŒ–æƒé‡ï¼Œå› ä¸ºç›®å‰å°šä¸æ”¯æŒæ­¤åŠŸèƒ½ã€‚ä½†æ˜¯ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ 8 ä½é‡åŒ–æ¨¡å‹æ¥è®­ç»ƒé¢å¤–å‚æ•°ï¼Œè¿™å°†åœ¨ä¸‹ä¸€éƒ¨åˆ†ä¸­ä»‹ç»ã€‚

æ³¨æ„ï¼Œ`device_map` æ˜¯å¯é€‰çš„ï¼Œä½†è®¾ç½® `device_map = 'auto'` æ›´é€‚åˆç”¨äºæ¨ç†ï¼Œå› ä¸ºå®ƒå°†æ›´æœ‰æ•ˆåœ°è°ƒåº¦å¯ç”¨èµ„æºä¸Šçš„æ¨¡å‹ã€‚


</Tip>

#### é«˜çº§ç”¨ä¾‹

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†ä»‹ç»ä½¿ç”¨ FP4 é‡åŒ–çš„ä¸€äº›é«˜çº§ç”¨ä¾‹ã€‚

##### æ›´æ”¹è®¡ç®—æ•°æ®ç±»å‹

è®¡ç®—æ•°æ®ç±»å‹ç”¨äºæ”¹å˜åœ¨è¿›è¡Œè®¡ç®—æ—¶ä½¿ç”¨çš„æ•°æ®ç±»å‹ã€‚ä¾‹å¦‚ï¼Œhidden stateså¯ä»¥æ˜¯ `float32`ï¼Œä½†ä¸ºäº†åŠ é€Ÿï¼Œè®¡ç®—æ—¶å¯ä»¥è¢«è®¾ç½®ä¸º `bf16`ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè®¡ç®—æ•°æ®ç±»å‹è¢«è®¾ç½®ä¸º `float32`ã€‚


```python
import torch
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)
```

#### ä½¿ç”¨ NF4ï¼ˆæ™®é€šæµ®ç‚¹æ•° 4ï¼‰æ•°æ®ç±»å‹

æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ NF4 æ•°æ®ç±»å‹ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹ä½¿ç”¨æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–çš„æƒé‡è€Œé€‚åº”çš„æ–°å‹ 4 ä½æ•°æ®ç±»å‹ã€‚è¦è¿è¡Œï¼š

```python
from transformers import BitsAndBytesConfig

nf4_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
)

model_nf4 = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=nf4_config)
```

#### ä½¿ç”¨åµŒå¥—é‡åŒ–è¿›è¡Œæ›´é«˜æ•ˆçš„å†…å­˜æ¨ç†

æˆ‘ä»¬è¿˜å»ºè®®ç”¨æˆ·ä½¿ç”¨åµŒå¥—é‡åŒ–æŠ€æœ¯ã€‚ä»æˆ‘ä»¬çš„ç»éªŒè§‚å¯Ÿæ¥çœ‹ï¼Œè¿™ç§æ–¹æ³•åœ¨ä¸å¢åŠ é¢å¤–æ€§èƒ½çš„æƒ…å†µä¸‹èŠ‚çœæ›´å¤šå†…å­˜ã€‚è¿™ä½¿å¾— llama-13b æ¨¡å‹èƒ½å¤Ÿåœ¨å…·æœ‰ 1024 ä¸ªåºåˆ—é•¿åº¦ã€1 ä¸ªæ‰¹æ¬¡å¤§å°å’Œ 4 ä¸ªæ¢¯åº¦ç´¯ç§¯æ­¥éª¤çš„ NVIDIA-T4 16GB ä¸Šè¿›è¡Œ fine-tuningã€‚

```python
from transformers import BitsAndBytesConfig

double_quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
)

model_double_quant = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=double_quant_config)
```

### å°†é‡åŒ–æ¨¡å‹æ¨é€åˆ°ğŸ¤— Hub

æ‚¨å¯ä»¥ä½¿ç”¨ `push_to_hub` æ–¹æ³•å°†é‡åŒ–æ¨¡å‹æ¨é€åˆ° Hub ä¸Šã€‚è¿™å°†é¦–å…ˆæ¨é€é‡åŒ–é…ç½®æ–‡ä»¶ï¼Œç„¶åæ¨é€é‡åŒ–æ¨¡å‹æƒé‡ã€‚
è¯·ç¡®ä¿ä½¿ç”¨ `bitsandbytes>0.37.2`ï¼ˆåœ¨æ’°å†™æœ¬æ–‡æ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ `bitsandbytes==0.38.0.post1`ï¼‰æ‰èƒ½ä½¿ç”¨æ­¤åŠŸèƒ½ã€‚


```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model = AutoModelForCausalLM.from_pretrained("bigscience/bloom-560m", quantization_config=BitsAndBytesConfig(load_in_8bit=True))
tokenizer = AutoTokenizer.from_pretrained("bigscience/bloom-560m")

model.push_to_hub("bloom-560m-8bit")
```

<Tip warning={true}>

å¯¹å¤§æ¨¡å‹ï¼Œå¼ºçƒˆé¼“åŠ±å°† 8 ä½é‡åŒ–æ¨¡å‹æ¨é€åˆ° Hub ä¸Šï¼Œä»¥ä¾¿è®©ç¤¾åŒºèƒ½å¤Ÿä»å†…å­˜å ç”¨å‡å°‘å’ŒåŠ è½½ä¸­å—ç›Šï¼Œä¾‹å¦‚åœ¨ Google Colab ä¸ŠåŠ è½½å¤§æ¨¡å‹ã€‚

</Tip>

### ä»ğŸ¤— HubåŠ è½½é‡åŒ–æ¨¡å‹

æ‚¨å¯ä»¥ä½¿ç”¨ `from_pretrained` æ–¹æ³•ä» Hub åŠ è½½é‡åŒ–æ¨¡å‹ã€‚è¯·ç¡®ä¿æ¨é€çš„æƒé‡æ˜¯é‡åŒ–çš„ï¼Œæ£€æŸ¥æ¨¡å‹é…ç½®å¯¹è±¡ä¸­æ˜¯å¦å­˜åœ¨ `quantization_config` å±æ€§ã€‚

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("{your_username}/bloom-560m-8bit", device_map="auto")
```

è¯·æ³¨æ„ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨ä¸éœ€è¦æŒ‡å®š `load_in_8bit=True` å‚æ•°ï¼Œä½†éœ€è¦ç¡®ä¿ `bitsandbytes` å’Œ `accelerate` å·²å®‰è£…ã€‚
æƒ…æ³¨æ„ï¼Œ`device_map` æ˜¯å¯é€‰çš„ï¼Œä½†è®¾ç½® `device_map = 'auto'` æ›´é€‚åˆç”¨äºæ¨ç†ï¼Œå› ä¸ºå®ƒå°†æ›´æœ‰æ•ˆåœ°è°ƒåº¦å¯ç”¨èµ„æºä¸Šçš„æ¨¡å‹ã€‚

### é«˜çº§ç”¨ä¾‹

æœ¬èŠ‚é¢å‘å¸Œæœ›æ¢ç´¢é™¤äº†åŠ è½½å’Œè¿è¡Œ 8 ä½æ¨¡å‹ä¹‹å¤–è¿˜èƒ½åšä»€ä¹ˆçš„è¿›é˜¶ç”¨æˆ·ã€‚

#### åœ¨ `cpu` å’Œ `gpu` ä¹‹é—´å¸è½½

æ­¤é«˜çº§ç”¨ä¾‹ä¹‹ä¸€æ˜¯èƒ½å¤ŸåŠ è½½æ¨¡å‹å¹¶å°†æƒé‡åˆ†æ´¾åˆ° `CPU` å’Œ `GPU` ä¹‹é—´ã€‚è¯·æ³¨æ„ï¼Œå°†åœ¨ CPU ä¸Šåˆ†æ´¾çš„æƒé‡ **ä¸ä¼š** è½¬æ¢ä¸º 8 ä½ï¼Œå› æ­¤ä¼šä¿ç•™ä¸º `float32`ã€‚æ­¤åŠŸèƒ½é€‚ç”¨äºæƒ³è¦é€‚åº”éå¸¸å¤§çš„æ¨¡å‹å¹¶å°†æ¨¡å‹åˆ†æ´¾åˆ° GPU å’Œ CPU ä¹‹é—´çš„ç”¨æˆ·ã€‚

é¦–å…ˆï¼Œä» `transformers` ä¸­åŠ è½½ä¸€ä¸ª [`BitsAndBytesConfig`]ï¼Œå¹¶å°†å±æ€§ `llm_int8_enable_fp32_cpu_offload` è®¾ç½®ä¸º `True`ï¼š


```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)
```

å‡è®¾æ‚¨æƒ³åŠ è½½ `bigscience/bloom-1b7` æ¨¡å‹ï¼Œæ‚¨çš„ GPUæ˜¾å­˜ä»…è¶³å¤Ÿå®¹çº³é™¤äº†`lm_head`å¤–çš„æ•´ä¸ªæ¨¡å‹ã€‚å› æ­¤ï¼Œæ‚¨å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ–¹å¼ç¼–å†™è‡ªå®šä¹‰çš„ device_mapï¼š

```python
device_map = {
    "transformer.word_embeddings": 0,
    "transformer.word_embeddings_layernorm": 0,
    "lm_head": "cpu",
    "transformer.h": 0,
    "transformer.ln_f": 0,
}
```

ç„¶åå¦‚ä¸‹åŠ è½½æ¨¡å‹ï¼š

```python
model_8bit = AutoModelForCausalLM.from_pretrained(
    "bigscience/bloom-1b7",
    device_map=device_map,
    quantization_config=quantization_config,
)
```

è¿™å°±æ˜¯å…¨éƒ¨å†…å®¹ï¼äº«å—æ‚¨çš„æ¨¡å‹å§ï¼

#### ä½¿ç”¨`llm_int8_threshold`

æ‚¨å¯ä»¥ä½¿ç”¨ `llm_int8_threshold` å‚æ•°æ¥æ›´æ”¹å¼‚å¸¸å€¼çš„é˜ˆå€¼ã€‚â€œå¼‚å¸¸å€¼â€æ˜¯ä¸€ä¸ªå¤§äºç‰¹å®šé˜ˆå€¼çš„`hidden state`å€¼ã€‚
è¿™å¯¹åº”äº`LLM.int8()`è®ºæ–‡ä¸­æè¿°çš„å¼‚å¸¸æ£€æµ‹çš„å¼‚å¸¸é˜ˆå€¼ã€‚ä»»ä½•é«˜äºæ­¤é˜ˆå€¼çš„`hidden state`å€¼éƒ½å°†è¢«è§†ä¸ºå¼‚å¸¸å€¼ï¼Œå¯¹è¿™äº›å€¼çš„æ“ä½œå°†åœ¨ fp16 ä¸­å®Œæˆã€‚å€¼é€šå¸¸æ˜¯æ­£æ€åˆ†å¸ƒçš„ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå¤§å¤šæ•°å€¼åœ¨ [-3.5, 3.5] èŒƒå›´å†…ï¼Œä½†æœ‰ä¸€äº›é¢å¤–çš„ç³»ç»Ÿå¼‚å¸¸å€¼ï¼Œå¯¹äºå¤§æ¨¡å‹æ¥è¯´ï¼Œå®ƒä»¬çš„åˆ†å¸ƒéå¸¸ä¸åŒã€‚è¿™äº›å¼‚å¸¸å€¼é€šå¸¸åœ¨åŒºé—´ [-60, -6] æˆ– [6, 60] å†…ã€‚Int8 é‡åŒ–å¯¹äºå¹…åº¦ä¸º ~5 çš„å€¼æ•ˆæœå¾ˆå¥½ï¼Œä½†è¶…å‡ºè¿™ä¸ªèŒƒå›´ï¼Œæ€§èƒ½å°±ä¼šæ˜æ˜¾ä¸‹é™ã€‚ä¸€ä¸ªå¥½çš„é»˜è®¤é˜ˆå€¼æ˜¯ 6ï¼Œä½†å¯¹äºæ›´ä¸ç¨³å®šçš„æ¨¡å‹ï¼ˆå°æ¨¡å‹ã€å¾®è°ƒï¼‰å¯èƒ½éœ€è¦æ›´ä½çš„é˜ˆå€¼ã€‚
è¿™ä¸ªå‚æ•°ä¼šå½±å“æ¨¡å‹çš„æ¨ç†é€Ÿåº¦ã€‚æˆ‘ä»¬å»ºè®®å°è¯•è¿™ä¸ªå‚æ•°ï¼Œä»¥æ‰¾åˆ°æœ€é€‚åˆæ‚¨çš„ç”¨ä¾‹çš„å‚æ•°ã€‚

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_id = "bigscience/bloom-1b7"

quantization_config = BitsAndBytesConfig(
    llm_int8_threshold=10,
)

model_8bit = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map=device_map,
    quantization_config=quantization_config,
)
tokenizer = AutoTokenizer.from_pretrained(model_id)
```

#### è·³è¿‡æŸäº›æ¨¡å—çš„è½¬æ¢

ä¸€äº›æ¨¡å‹æœ‰å‡ ä¸ªéœ€è¦ä¿æŒæœªè½¬æ¢çŠ¶æ€ä»¥ç¡®ä¿ç¨³å®šæ€§çš„æ¨¡å—ã€‚ä¾‹å¦‚ï¼ŒJukebox æ¨¡å‹æœ‰å‡ ä¸ª `lm_head` æ¨¡å—éœ€è¦è·³è¿‡ã€‚ä½¿ç”¨ `llm_int8_skip_modules` å‚æ•°è¿›è¡Œç›¸åº”æ“ä½œã€‚


```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_id = "bigscience/bloom-1b7"

quantization_config = BitsAndBytesConfig(
    llm_int8_skip_modules=["lm_head"],
)

model_8bit = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map=device_map,
    quantization_config=quantization_config,
)
tokenizer = AutoTokenizer.from_pretrained(model_id)
```

#### å¾®è°ƒå·²åŠ è½½ä¸º8ä½ç²¾åº¦çš„æ¨¡å‹

å€ŸåŠ©Hugging Faceç”Ÿæ€ç³»ç»Ÿä¸­é€‚é…å™¨ï¼ˆadaptersï¼‰çš„å®˜æ–¹æ”¯æŒï¼Œæ‚¨å¯ä»¥åœ¨8ä½ç²¾åº¦ä¸‹å¾®è°ƒæ¨¡å‹ã€‚è¿™ä½¿å¾—å¯ä»¥åœ¨å•ä¸ªGoogle Colabä¸­å¾®è°ƒå¤§æ¨¡å‹ï¼Œä¾‹å¦‚`flan-t5-large`æˆ–`facebook/opt-6.7b`ã€‚è¯·æŸ¥çœ‹[`peft`](https://github.com/huggingface/peft)åº“äº†è§£æ›´å¤šè¯¦æƒ…ã€‚

æ³¨æ„ï¼ŒåŠ è½½æ¨¡å‹è¿›è¡Œè®­ç»ƒæ—¶æ— éœ€ä¼ é€’`device_map`ã€‚å®ƒå°†è‡ªåŠ¨å°†æ‚¨çš„æ¨¡å‹åŠ è½½åˆ°GPUä¸Šã€‚å¦‚æœéœ€è¦ï¼Œæ‚¨å¯ä»¥å°†è®¾å¤‡æ˜ å°„ä¸ºç‰¹å®šè®¾å¤‡ï¼ˆä¾‹å¦‚`cuda:0`ã€`0`ã€`torch.device('cuda:0')`ï¼‰ã€‚è¯·æ³¨æ„ï¼Œ`device_map=auto`ä»…åº”ç”¨äºæ¨ç†ã€‚


### BitsAndBytesConfig

[[autodoc]] BitsAndBytesConfig


## ä½¿ç”¨ ğŸ¤— `optimum` è¿›è¡Œé‡åŒ–

è¯·æŸ¥çœ‹[Optimum æ–‡æ¡£](https://huggingface.co/docs/optimum/index)ä»¥äº†è§£æ›´å¤šå…³äº`optimum`æ”¯æŒçš„é‡åŒ–æ–¹æ³•ï¼Œå¹¶æŸ¥çœ‹è¿™äº›æ–¹æ³•æ˜¯å¦é€‚ç”¨äºæ‚¨çš„ç”¨ä¾‹ã€‚


============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\main_classes\text_generation.md
============================================================



# Generation

æ¯ä¸ªæ¡†æ¶éƒ½åœ¨å®ƒä»¬å„è‡ªçš„ `GenerationMixin` ç±»ä¸­å®ç°äº†æ–‡æœ¬ç”Ÿæˆçš„ `generate` æ–¹æ³•ï¼š

- PyTorch [`~generation.GenerationMixin.generate`] åœ¨ [`~generation.GenerationMixin`] ä¸­å®ç°ã€‚

æ— è®ºæ‚¨é€‰æ‹©å“ªä¸ªæ¡†æ¶ï¼Œéƒ½å¯ä»¥ä½¿ç”¨ [`~generation.GenerationConfig`] ç±»å®ä¾‹å¯¹ generate æ–¹æ³•è¿›è¡Œå‚æ•°åŒ–ã€‚æœ‰å…³ç”Ÿæˆæ–¹æ³•çš„æ§åˆ¶å‚æ•°çš„å®Œæ•´åˆ—è¡¨ï¼Œè¯·å‚é˜…æ­¤ç±»ã€‚

è¦äº†è§£å¦‚ä½•æ£€æŸ¥æ¨¡å‹çš„ç”Ÿæˆé…ç½®ã€é»˜è®¤å€¼æ˜¯ä»€ä¹ˆã€å¦‚ä½•ä¸´æ—¶æ›´æ”¹å‚æ•°ä»¥åŠå¦‚ä½•åˆ›å»ºå’Œä¿å­˜è‡ªå®šä¹‰ç”Ÿæˆé…ç½®ï¼Œè¯·å‚é˜… [æ–‡æœ¬ç”Ÿæˆç­–ç•¥æŒ‡å—](../generation_strategies)ã€‚è¯¥æŒ‡å—è¿˜è§£é‡Šäº†å¦‚ä½•ä½¿ç”¨ç›¸å…³åŠŸèƒ½ï¼Œå¦‚tokenæµã€‚

## GenerationConfig

[[autodoc]] generation.GenerationConfig
	- from_pretrained
	- from_model_config
	- save_pretrained

## GenerationMixin

[[autodoc]] generation.GenerationMixin
	- generate
	- compute_transition_scores

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\main_classes\tokenizer.md
============================================================



# Tokenizer

tokenizerè´Ÿè´£å‡†å¤‡è¾“å…¥ä»¥ä¾›æ¨¡å‹ä½¿ç”¨ã€‚è¯¥åº“åŒ…å«æ‰€æœ‰æ¨¡å‹çš„tokenizerã€‚å¤§å¤šæ•°tokenizeréƒ½æœ‰ä¸¤ç§ç‰ˆæœ¬ï¼šä¸€ä¸ªæ˜¯å®Œå…¨çš„ Python å®ç°ï¼Œå¦ä¸€ä¸ªæ˜¯åŸºäº Rust åº“ [ğŸ¤— Tokenizers](https://github.com/huggingface/tokenizers) çš„â€œFastâ€å®ç°ã€‚"Fast" å®ç°å…è®¸ï¼š

1. åœ¨æ‰¹é‡åˆ†è¯æ—¶æ˜¾è‘—æé€Ÿ
2. åœ¨åŸå§‹å­—ç¬¦ä¸²ï¼ˆå­—ç¬¦å’Œå•è¯ï¼‰å’Œtokenç©ºé—´ä¹‹é—´è¿›è¡Œæ˜ å°„çš„å…¶ä»–æ–¹æ³•ï¼ˆä¾‹å¦‚ï¼Œè·å–åŒ…å«ç»™å®šå­—ç¬¦çš„tokençš„ç´¢å¼•æˆ–ä¸ç»™å®štokenå¯¹åº”çš„å­—ç¬¦èŒƒå›´ï¼‰ã€‚

åŸºç±» [PreTrainedTokenizer] å’Œ [PreTrained TokenizerFast] å®ç°äº†åœ¨æ¨¡å‹è¾“å…¥ä¸­ç¼–ç å­—ç¬¦ä¸²è¾“å…¥çš„å¸¸ç”¨æ–¹æ³•ï¼ˆè§ä¸‹æ–‡ï¼‰ï¼Œå¹¶ä»æœ¬åœ°æ–‡ä»¶æˆ–ç›®å½•æˆ–ä»åº“æä¾›çš„é¢„è®­ç»ƒçš„ tokenizerï¼ˆä» HuggingFace çš„ AWS S3 å­˜å‚¨åº“ä¸‹è½½ï¼‰å®ä¾‹åŒ–/ä¿å­˜ python å’Œâ€œFastâ€ tokenizerã€‚å®ƒä»¬éƒ½ä¾èµ–äºåŒ…å«å¸¸ç”¨æ–¹æ³•çš„ [`~tokenization_utils_base.PreTrainedTokenizerBase`]ã€‚

å› æ­¤ï¼Œ[`PreTrainedTokenizer`] å’Œ [`PreTrainedTokenizerFast`] å®ç°äº†ä½¿ç”¨æ‰€æœ‰tokenizersçš„ä¸»è¦æ–¹æ³•ï¼š

- åˆ†è¯ï¼ˆå°†å­—ç¬¦ä¸²æ‹†åˆ†ä¸ºå­è¯æ ‡è®°å­—ç¬¦ä¸²ï¼‰ï¼Œå°†tokenså­—ç¬¦ä¸²è½¬æ¢ä¸ºidå¹¶è½¬æ¢å›æ¥ï¼Œä»¥åŠç¼–ç /è§£ç ï¼ˆå³æ ‡è®°åŒ–å¹¶è½¬æ¢ä¸ºæ•´æ•°ï¼‰ã€‚
- ä»¥ç‹¬ç«‹äºåº•å±‚ç»“æ„ï¼ˆBPEã€SentencePieceâ€¦â€¦ï¼‰çš„æ–¹å¼å‘è¯æ±‡è¡¨ä¸­æ·»åŠ æ–°tokensã€‚
- ç®¡ç†ç‰¹æ®Štokensï¼ˆå¦‚maskã€å¥é¦–ç­‰ï¼‰ï¼šæ·»åŠ å®ƒä»¬ï¼Œå°†å®ƒä»¬åˆ†é…ç»™tokenizerä¸­çš„å±æ€§ä»¥ä¾¿äºè®¿é—®ï¼Œå¹¶ç¡®ä¿å®ƒä»¬åœ¨æ ‡è®°è¿‡ç¨‹ä¸­ä¸ä¼šè¢«åˆ†å‰²ã€‚

[`BatchEncoding`] åŒ…å« [`~tokenization_utils_base.PreTrainedTokenizerBase`] çš„ç¼–ç æ–¹æ³•ï¼ˆ`__call__`ã€`encode_plus` å’Œ `batch_encode_plus`ï¼‰çš„è¾“å‡ºï¼Œå¹¶ä¸”æ˜¯ä» Python å­—å…¸æ´¾ç”Ÿçš„ã€‚å½“tokenizeræ˜¯çº¯ Python tokenizeræ—¶ï¼Œæ­¤ç±»çš„è¡Œä¸ºå°±åƒæ ‡å‡†çš„ Python å­—å…¸ä¸€æ ·ï¼Œå¹¶ä¿å­˜è¿™äº›æ–¹æ³•è®¡ç®—çš„å„ç§æ¨¡å‹è¾“å…¥ï¼ˆ`input_ids`ã€`attention_mask` ç­‰ï¼‰ã€‚å½“åˆ†è¯å™¨æ˜¯â€œFastâ€åˆ†è¯å™¨æ—¶ï¼ˆå³ç”± HuggingFace çš„ [tokenizers åº“](https://github.com/huggingface/tokenizers) æ”¯æŒï¼‰ï¼Œæ­¤ç±»è¿˜æä¾›äº†å‡ ç§é«˜çº§å¯¹é½æ–¹æ³•ï¼Œå¯ç”¨äºåœ¨åŸå§‹å­—ç¬¦ä¸²ï¼ˆå­—ç¬¦å’Œå•è¯ï¼‰ä¸tokenç©ºé—´ä¹‹é—´è¿›è¡Œæ˜ å°„ï¼ˆä¾‹å¦‚ï¼Œè·å–åŒ…å«ç»™å®šå­—ç¬¦çš„tokençš„ç´¢å¼•æˆ–ä¸ç»™å®štokenå¯¹åº”çš„å­—ç¬¦èŒƒå›´ï¼‰ã€‚


## PreTrainedTokenizer

[[autodoc]] PreTrainedTokenizer
    - __call__
    - add_tokens
    - add_special_tokens
    - apply_chat_template
    - batch_decode
    - decode
    - encode
    - push_to_hub
    - all

## PreTrainedTokenizerFast

[`PreTrainedTokenizerFast`] ä¾èµ–äº [tokenizers](https://huggingface.co/docs/tokenizers) åº“ã€‚å¯ä»¥éå¸¸ç®€å•åœ°å°†ä» ğŸ¤— tokenizers åº“è·å–çš„tokenizersåŠ è½½åˆ° ğŸ¤— transformers ä¸­ã€‚æŸ¥çœ‹ [ä½¿ç”¨ ğŸ¤— tokenizers çš„åˆ†è¯å™¨](../fast_tokenizers) é¡µé¢ä»¥äº†è§£å¦‚ä½•æ‰§è¡Œæ­¤æ“ä½œã€‚

[[autodoc]] PreTrainedTokenizerFast
    - __call__
    - add_tokens
    - add_special_tokens
    - apply_chat_template
    - batch_decode
    - decode
    - encode
    - push_to_hub
    - all

## BatchEncoding

[[autodoc]] BatchEncoding

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\main_classes\trainer.md
============================================================



# Trainer

[`Trainer`] ç±»æä¾›äº†ä¸€ä¸ª PyTorch çš„ APIï¼Œç”¨äºå¤„ç†å¤§å¤šæ•°æ ‡å‡†ç”¨ä¾‹çš„å…¨åŠŸèƒ½è®­ç»ƒã€‚å®ƒåœ¨å¤§å¤šæ•°[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples)ä¸­è¢«ä½¿ç”¨ã€‚

<Tip>

å¦‚æœä½ æƒ³è¦ä½¿ç”¨è‡ªå›å½’æŠ€æœ¯åœ¨æ–‡æœ¬æ•°æ®é›†ä¸Šå¾®è°ƒåƒ Llama-2 æˆ– Mistral è¿™æ ·çš„è¯­è¨€æ¨¡å‹ï¼Œè€ƒè™‘ä½¿ç”¨ [`trl`](https://github.com/huggingface/trl) çš„ [`~trl.SFTTrainer`]ã€‚[`~trl.SFTTrainer`] å°è£…äº† [`Trainer`]ï¼Œä¸“é—¨é’ˆå¯¹è¿™ä¸ªç‰¹å®šä»»åŠ¡è¿›è¡Œäº†ä¼˜åŒ–ï¼Œå¹¶æ”¯æŒåºåˆ—æ‰“åŒ…ã€LoRAã€é‡åŒ–å’Œ DeepSpeedï¼Œä»¥æœ‰æ•ˆæ‰©å±•åˆ°ä»»ä½•æ¨¡å‹å¤§å°ã€‚å¦ä¸€æ–¹é¢ï¼Œ[`Trainer`] æ˜¯ä¸€ä¸ªæ›´é€šç”¨çš„é€‰é¡¹ï¼Œé€‚ç”¨äºæ›´å¹¿æ³›çš„ä»»åŠ¡ã€‚

</Tip>

åœ¨å®ä¾‹åŒ–ä½ çš„ [`Trainer`] ä¹‹å‰ï¼Œåˆ›å»ºä¸€ä¸ª [`TrainingArguments`]ï¼Œä»¥ä¾¿åœ¨è®­ç»ƒæœŸé—´è®¿é—®æ‰€æœ‰å®šåˆ¶ç‚¹ã€‚

è¿™ä¸ª API æ”¯æŒåœ¨å¤šä¸ª GPU/TPU ä¸Šè¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒï¼Œæ”¯æŒ PyTorch çš„åŸç”Ÿ AMPã€‚

[`Trainer`] åŒ…å«åŸºæœ¬çš„è®­ç»ƒå¾ªç¯ï¼Œæ”¯æŒä¸Šè¿°åŠŸèƒ½ã€‚å¦‚æœéœ€è¦è‡ªå®šä¹‰è®­ç»ƒï¼Œä½ å¯ä»¥ç»§æ‰¿ `Trainer` å¹¶è¦†ç›–ä»¥ä¸‹æ–¹æ³•ï¼š

- **get_train_dataloader** -- åˆ›å»ºè®­ç»ƒ DataLoaderã€‚
- **get_eval_dataloader** -- åˆ›å»ºè¯„ä¼° DataLoaderã€‚
- **get_test_dataloader** -- åˆ›å»ºæµ‹è¯• DataLoaderã€‚
- **log** -- è®°å½•è§‚å¯Ÿè®­ç»ƒçš„å„ç§å¯¹è±¡çš„ä¿¡æ¯ã€‚
- **create_optimizer_and_scheduler** -- å¦‚æœå®ƒä»¬æ²¡æœ‰åœ¨åˆå§‹åŒ–æ—¶ä¼ é€’ï¼Œè¯·è®¾ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚è¯·æ³¨æ„ï¼Œä½ è¿˜å¯ä»¥å•ç‹¬ç»§æ‰¿æˆ–è¦†ç›– `create_optimizer` å’Œ `create_scheduler` æ–¹æ³•ã€‚
- **create_optimizer** -- å¦‚æœåœ¨åˆå§‹åŒ–æ—¶æ²¡æœ‰ä¼ é€’ï¼Œåˆ™è®¾ç½®ä¼˜åŒ–å™¨ã€‚
- **create_scheduler** -- å¦‚æœåœ¨åˆå§‹åŒ–æ—¶æ²¡æœ‰ä¼ é€’ï¼Œåˆ™è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚
- **compute_loss** - è®¡ç®—å•æ‰¹è®­ç»ƒè¾“å…¥çš„æŸå¤±ã€‚
- **training_step** -- æ‰§è¡Œä¸€æ­¥è®­ç»ƒã€‚
- **prediction_step** -- æ‰§è¡Œä¸€æ­¥è¯„ä¼°/æµ‹è¯•ã€‚
- **evaluate** -- è¿è¡Œè¯„ä¼°å¾ªç¯å¹¶è¿”å›æŒ‡æ ‡ã€‚
- **predict** -- è¿”å›åœ¨æµ‹è¯•é›†ä¸Šçš„é¢„æµ‹ï¼ˆå¦‚æœæœ‰æ ‡ç­¾ï¼Œåˆ™åŒ…æ‹¬æŒ‡æ ‡ï¼‰ã€‚

<Tip warning={true}>

[`Trainer`] ç±»è¢«ä¼˜åŒ–ç”¨äº ğŸ¤— Transformers æ¨¡å‹ï¼Œå¹¶åœ¨ä½ åœ¨å…¶ä»–æ¨¡å‹ä¸Šä½¿ç”¨æ—¶å¯èƒ½ä¼šæœ‰ä¸€äº›ä»¤äººæƒŠè®¶çš„ç»“æœã€‚å½“åœ¨ä½ è‡ªå·±çš„æ¨¡å‹ä¸Šä½¿ç”¨æ—¶ï¼Œè¯·ç¡®ä¿ï¼š

- ä½ çš„æ¨¡å‹å§‹ç»ˆè¿”å›å…ƒç»„æˆ– [`~utils.ModelOutput`] çš„å­ç±»ã€‚
- å¦‚æœæä¾›äº† `labels` å‚æ•°ï¼Œä½ çš„æ¨¡å‹å¯ä»¥è®¡ç®—æŸå¤±ï¼Œå¹¶ä¸”æŸå¤±ä½œä¸ºå…ƒç»„çš„ç¬¬ä¸€ä¸ªå…ƒç´ è¿”å›ï¼ˆå¦‚æœä½ çš„æ¨¡å‹è¿”å›å…ƒç»„ï¼‰ã€‚
- ä½ çš„æ¨¡å‹å¯ä»¥æ¥å—å¤šä¸ªæ ‡ç­¾å‚æ•°ï¼ˆåœ¨ [`TrainingArguments`] ä¸­ä½¿ç”¨ `label_names` å°†å®ƒä»¬çš„åç§°æŒ‡ç¤ºç»™ [`Trainer`]ï¼‰ï¼Œä½†å®ƒä»¬ä¸­æ²¡æœ‰ä¸€ä¸ªåº”è¯¥è¢«å‘½åä¸º `"label"`ã€‚

</Tip>

ä»¥ä¸‹æ˜¯å¦‚ä½•è‡ªå®šä¹‰ [`Trainer`] ä»¥ä½¿ç”¨åŠ æƒæŸå¤±çš„ç¤ºä¾‹ï¼ˆåœ¨è®­ç»ƒé›†ä¸å¹³è¡¡æ—¶å¾ˆæœ‰ç”¨ï¼‰ï¼š

```python
from torch import nn
from transformers import Trainer


class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop("labels")
        # forward pass
        outputs = model(**inputs)
        logits = outputs.get("logits")
        # compute custom loss (suppose one has 3 labels with different weights)
        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0, 3.0], device=model.device))
        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))
        return (loss, outputs) if return_outputs else loss
```

åœ¨ PyTorch [`Trainer`] ä¸­è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯è¡Œä¸ºçš„å¦ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨ [callbacks](callback)ï¼Œè¿™äº›å›è°ƒå¯ä»¥æ£€æŸ¥è®­ç»ƒå¾ªç¯çŠ¶æ€ï¼ˆç”¨äºè¿›åº¦æŠ¥å‘Šã€åœ¨ TensorBoard æˆ–å…¶ä»– ML å¹³å°ä¸Šè®°å½•æ—¥å¿—ç­‰ï¼‰å¹¶åšå‡ºå†³ç­–ï¼ˆæ¯”å¦‚æå‰åœæ­¢ï¼‰ã€‚


## Trainer

[[autodoc]] Trainer - all

## Seq2SeqTrainer

[[autodoc]] Seq2SeqTrainer - evaluate - predict

## TrainingArguments

[[autodoc]] TrainingArguments - all

## Seq2SeqTrainingArguments

[[autodoc]] Seq2SeqTrainingArguments - all

## Checkpoints

é»˜è®¤æƒ…å†µä¸‹ï¼Œ[`Trainer`] ä¼šå°†æ‰€æœ‰checkpointsä¿å­˜åœ¨ä½ ä½¿ç”¨çš„ [`TrainingArguments`] ä¸­è®¾ç½®çš„ `output_dir` ä¸­ã€‚è¿™äº›checkpointså°†ä½äºåä¸º `checkpoint-xxx` çš„å­æ–‡ä»¶å¤¹ä¸­ï¼Œxxx æ˜¯è®­ç»ƒçš„æ­¥éª¤ã€‚

ä»checkpointsæ¢å¤è®­ç»ƒå¯ä»¥é€šè¿‡è°ƒç”¨ [`Trainer.train`] æ—¶ä½¿ç”¨ä»¥ä¸‹ä»»ä¸€æ–¹å¼è¿›è¡Œï¼š

- `resume_from_checkpoint=True`ï¼Œè¿™å°†ä»æœ€æ–°çš„checkpointæ¢å¤è®­ç»ƒã€‚
- `resume_from_checkpoint=checkpoint_dir`ï¼Œè¿™å°†ä»æŒ‡å®šç›®å½•ä¸­çš„ç‰¹å®šcheckpointæ¢å¤è®­ç»ƒã€‚

æ­¤å¤–ï¼Œå½“ä½¿ç”¨ `push_to_hub=True` æ—¶ï¼Œä½ å¯ä»¥è½»æ¾å°†checkpointsä¿å­˜åœ¨ Model Hub ä¸­ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œä¿å­˜åœ¨è®­ç»ƒä¸­é—´è¿‡ç¨‹çš„checkpointsä¸­çš„æ‰€æœ‰æ¨¡å‹éƒ½ä¿å­˜åœ¨ä¸åŒçš„æäº¤ä¸­ï¼Œä½†ä¸åŒ…æ‹¬ä¼˜åŒ–å™¨çŠ¶æ€ã€‚ä½ å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ [`TrainingArguments`] çš„ `hub-strategy` å€¼ï¼š

- `"checkpoint"`: æœ€æ–°çš„checkpointä¹Ÿè¢«æ¨é€åˆ°ä¸€ä¸ªåä¸º last-checkpoint çš„å­æ–‡ä»¶å¤¹ä¸­ï¼Œè®©ä½ å¯ä»¥é€šè¿‡ `trainer.train(resume_from_checkpoint="output_dir/last-checkpoint")` è½»æ¾æ¢å¤è®­ç»ƒã€‚
- `"all_checkpoints"`: æ‰€æœ‰checkpointséƒ½åƒå®ƒä»¬å‡ºç°åœ¨è¾“å‡ºæ–‡ä»¶å¤¹ä¸­ä¸€æ ·è¢«æ¨é€ï¼ˆå› æ­¤ä½ å°†åœ¨æœ€ç»ˆå­˜å‚¨åº“ä¸­çš„æ¯ä¸ªæ–‡ä»¶å¤¹ä¸­è·å¾—ä¸€ä¸ªcheckpointæ–‡ä»¶å¤¹ï¼‰ã€‚

## Logging

é»˜è®¤æƒ…å†µä¸‹ï¼Œ[`Trainer`] å°†å¯¹ä¸»è¿›ç¨‹ä½¿ç”¨ `logging.INFO`ï¼Œå¯¹å‰¯æœ¬ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰ä½¿ç”¨ `logging.WARNING`ã€‚

å¯ä»¥é€šè¿‡ [`TrainingArguments`] çš„å‚æ•°è¦†ç›–è¿™äº›é»˜è®¤è®¾ç½®ï¼Œä½¿ç”¨å…¶ä¸­çš„ 5 ä¸ª `logging` çº§åˆ«ï¼š

- `log_level` - ç”¨äºä¸»è¿›ç¨‹
- `log_level_replica` - ç”¨äºå‰¯æœ¬

æ­¤å¤–ï¼Œå¦‚æœ [`TrainingArguments`] çš„ `log_on_each_node` è®¾ç½®ä¸º `False`ï¼Œåˆ™åªæœ‰ä¸»èŠ‚ç‚¹å°†ä½¿ç”¨å…¶ä¸»è¿›ç¨‹çš„æ—¥å¿—çº§åˆ«è®¾ç½®ï¼Œæ‰€æœ‰å…¶ä»–èŠ‚ç‚¹å°†ä½¿ç”¨å‰¯æœ¬çš„æ—¥å¿—çº§åˆ«è®¾ç½®ã€‚

è¯·æ³¨æ„ï¼Œ[`Trainer`] å°†åœ¨å…¶ [`Trainer.__init__`] ä¸­åˆ†åˆ«ä¸ºæ¯ä¸ªèŠ‚ç‚¹è®¾ç½® `transformers` çš„æ—¥å¿—çº§åˆ«ã€‚å› æ­¤ï¼Œå¦‚æœåœ¨åˆ›å»º [`Trainer`] å¯¹è±¡ä¹‹å‰è¦è°ƒç”¨å…¶ä»– `transformers` åŠŸèƒ½ï¼Œå¯èƒ½éœ€è¦æ›´æ—©åœ°è®¾ç½®è¿™ä¸€ç‚¹ï¼ˆè¯·å‚è§ä¸‹é¢çš„ç¤ºä¾‹ï¼‰ã€‚

ä»¥ä¸‹æ˜¯å¦‚ä½•åœ¨åº”ç”¨ç¨‹åºä¸­ä½¿ç”¨çš„ç¤ºä¾‹ï¼š

```python
[...]
logger = logging.getLogger(__name__)

# Setup logging
logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
    datefmt="%m/%d/%Y %H:%M:%S",
    handlers=[logging.StreamHandler(sys.stdout)],
)

# set the main code and the modules it uses to the same log-level according to the node
log_level = training_args.get_process_log_level()
logger.setLevel(log_level)
datasets.utils.logging.set_verbosity(log_level)
transformers.utils.logging.set_verbosity(log_level)

trainer = Trainer(...)
```

ç„¶åï¼Œå¦‚æœä½ åªæƒ³åœ¨ä¸»èŠ‚ç‚¹ä¸Šçœ‹åˆ°è­¦å‘Šï¼Œå¹¶ä¸”æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹ä¸æ‰“å°ä»»ä½•å¯èƒ½é‡å¤çš„è­¦å‘Šï¼Œå¯ä»¥è¿™æ ·è¿è¡Œï¼š

```bash
my_app.py ... --log_level warning --log_level_replica error
```

åœ¨å¤šèŠ‚ç‚¹ç¯å¢ƒä¸­ï¼Œå¦‚æœä½ ä¹Ÿä¸å¸Œæœ›æ¯ä¸ªèŠ‚ç‚¹çš„ä¸»è¿›ç¨‹çš„æ—¥å¿—é‡å¤è¾“å‡ºï¼Œä½ éœ€è¦å°†ä¸Šé¢çš„ä»£ç æ›´æ”¹ä¸ºï¼š

```bash
my_app.py ... --log_level warning --log_level_replica error --log_on_each_node 0
```

ç„¶åï¼Œåªæœ‰ç¬¬ä¸€ä¸ªèŠ‚ç‚¹çš„ä¸»è¿›ç¨‹å°†ä»¥ "warning" çº§åˆ«è®°å½•æ—¥å¿—ï¼Œä¸»èŠ‚ç‚¹ä¸Šçš„æ‰€æœ‰å…¶ä»–è¿›ç¨‹å’Œå…¶ä»–èŠ‚ç‚¹ä¸Šçš„æ‰€æœ‰è¿›ç¨‹å°†ä»¥ "error" çº§åˆ«è®°å½•æ—¥å¿—ã€‚

å¦‚æœä½ å¸Œæœ›åº”ç”¨ç¨‹åºå°½å¯èƒ½â€å®‰é™â€œï¼Œå¯ä»¥æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š


```bash
my_app.py ... --log_level error --log_level_replica error --log_on_each_node 0
```

(å¦‚æœåœ¨å¤šèŠ‚ç‚¹ç¯å¢ƒï¼Œæ·»åŠ  `--log_on_each_node 0`)


## éšæœºæ€§

å½“ä» [`Trainer`] ç”Ÿæˆçš„checkpointæ¢å¤è®­ç»ƒæ—¶ï¼Œç¨‹åºä¼šå°½ä¸€åˆ‡åŠªåŠ›å°† _python_ã€_numpy_ å’Œ _pytorch_ çš„ RNGï¼ˆéšæœºæ•°ç”Ÿæˆå™¨ï¼‰çŠ¶æ€æ¢å¤ä¸ºä¿å­˜æ£€æŸ¥ç‚¹æ—¶çš„çŠ¶æ€ï¼Œè¿™æ ·å¯ä»¥ä½¿â€œåœæ­¢å’Œæ¢å¤â€å¼è®­ç»ƒå°½å¯èƒ½æ¥è¿‘â€œéåœæ­¢å¼â€è®­ç»ƒã€‚

ç„¶è€Œï¼Œç”±äºå„ç§é»˜è®¤çš„éç¡®å®šæ€§ PyTorch è®¾ç½®ï¼Œè¿™å¯èƒ½æ— æ³•å®Œå…¨å®ç°ã€‚å¦‚æœä½ æƒ³è¦å®Œå…¨ç¡®å®šæ€§ï¼Œè¯·å‚é˜…[æ§åˆ¶éšæœºæº](https://pytorch.org/docs/stable/notes/randomness)ã€‚æ­£å¦‚æ–‡æ¡£ä¸­æ‰€è§£é‡Šçš„é‚£æ ·ï¼Œä½¿äº‹ç‰©å˜å¾—ç¡®å®šçš„ä¸€äº›è®¾ç½®ï¼ˆä¾‹å¦‚ `torch.backends.cudnn.deterministic`ï¼‰å¯èƒ½ä¼šå‡æ…¢é€Ÿåº¦ï¼Œå› æ­¤ä¸èƒ½é»˜è®¤æ‰§è¡Œï¼Œä½†å¦‚æœéœ€è¦ï¼Œä½ å¯ä»¥è‡ªè¡Œå¯ç”¨è¿™äº›è®¾ç½®ã€‚


## ç‰¹å®šGPUé€‰æ‹©

è®©æˆ‘ä»¬è®¨è®ºä¸€ä¸‹å¦‚ä½•å‘Šè¯‰ä½ çš„ç¨‹åºåº”è¯¥ä½¿ç”¨å“ªäº› GPU ä»¥åŠä½¿ç”¨çš„é¡ºåºã€‚

å½“ä½¿ç”¨ [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html) ä¸”ä»…ä½¿ç”¨éƒ¨åˆ† GPU æ—¶ï¼Œä½ åªéœ€æŒ‡å®šè¦ä½¿ç”¨çš„ GPU æ•°é‡ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æœ‰ 4 ä¸ª GPUï¼Œä½†åªæƒ³ä½¿ç”¨å‰ 2 ä¸ªï¼Œå¯ä»¥æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š


```bash
python -m torch.distributed.launch --nproc_per_node=2  trainer-program.py ...
```

å¦‚æœä½ å®‰è£…äº† [`accelerate`](https://github.com/huggingface/accelerate) æˆ– [`deepspeed`](https://github.com/deepspeedai/DeepSpeed)ï¼Œä½ è¿˜å¯ä»¥é€šè¿‡ä»¥ä¸‹ä»»ä¸€æ–¹æ³•å®ç°ç›¸åŒçš„æ•ˆæœï¼š


```bash
accelerate launch --num_processes 2 trainer-program.py ...
```

```bash
deepspeed --num_gpus 2 trainer-program.py ...
```

ä½ ä¸éœ€è¦ä½¿ç”¨ Accelerate æˆ– [Deepspeed é›†æˆ](Deepspeed) åŠŸèƒ½æ¥ä½¿ç”¨è¿™äº›å¯åŠ¨å™¨ã€‚

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œä½ å·²ç»èƒ½å¤Ÿå‘Šè¯‰ç¨‹åºè¦ä½¿ç”¨å¤šå°‘ä¸ª GPUã€‚ç°åœ¨è®©æˆ‘ä»¬è®¨è®ºå¦‚ä½•é€‰æ‹©ç‰¹å®šçš„ GPU å¹¶æ§åˆ¶å®ƒä»¬çš„é¡ºåºã€‚

ä»¥ä¸‹ç¯å¢ƒå˜é‡å¯å¸®åŠ©ä½ æ§åˆ¶ä½¿ç”¨å“ªäº› GPU ä»¥åŠå®ƒä»¬çš„é¡ºåºã€‚


**`CUDA_VISIBLE_DEVICES`**

å¦‚æœä½ æœ‰å¤šä¸ª GPUï¼Œæƒ³è¦ä»…ä½¿ç”¨å…¶ä¸­çš„ä¸€ä¸ªæˆ–å‡ ä¸ª GPUï¼Œè¯·å°†ç¯å¢ƒå˜é‡ `CUDA_VISIBLE_DEVICES` è®¾ç½®ä¸ºè¦ä½¿ç”¨çš„ GPU åˆ—è¡¨ã€‚

ä¾‹å¦‚ï¼Œå‡è®¾ä½ æœ‰ 4 ä¸ª GPUï¼š0ã€1ã€2 å’Œ 3ã€‚è¦ä»…åœ¨ç‰©ç† GPU 0 å’Œ 2 ä¸Šè¿è¡Œï¼Œä½ å¯ä»¥æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š


```bash
CUDA_VISIBLE_DEVICES=0,2 python -m torch.distributed.launch trainer-program.py ...
```

ç°åœ¨ï¼ŒPyTorch å°†åªçœ‹åˆ° 2 ä¸ª GPUï¼Œå…¶ä¸­ä½ çš„ç‰©ç† GPU 0 å’Œ 2 åˆ†åˆ«æ˜ å°„åˆ° `cuda:0` å’Œ `cuda:1`ã€‚

ä½ ç”šè‡³å¯ä»¥æ”¹å˜å®ƒä»¬çš„é¡ºåºï¼š


```bash
CUDA_VISIBLE_DEVICES=2,0 python -m torch.distributed.launch trainer-program.py ...
```

è¿™é‡Œï¼Œä½ çš„ç‰©ç† GPU 0 å’Œ 2 åˆ†åˆ«æ˜ å°„åˆ° `cuda:1` å’Œ `cuda:0`ã€‚

ä¸Šé¢çš„ä¾‹å­éƒ½æ˜¯é’ˆå¯¹ `DistributedDataParallel` ä½¿ç”¨æ¨¡å¼çš„ï¼Œä½†åŒæ ·çš„æ–¹æ³•ä¹Ÿé€‚ç”¨äº [`DataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html)ï¼š


```bash
CUDA_VISIBLE_DEVICES=2,0 python trainer-program.py ...
```

ä¸ºäº†æ¨¡æ‹Ÿæ²¡æœ‰ GPU çš„ç¯å¢ƒï¼Œåªéœ€å°†æ­¤ç¯å¢ƒå˜é‡è®¾ç½®ä¸ºç©ºå€¼ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```bash
CUDA_VISIBLE_DEVICES= python trainer-program.py ...
```

ä¸ä»»ä½•ç¯å¢ƒå˜é‡ä¸€æ ·ï¼Œä½ å½“ç„¶å¯ä»¥å°†å…¶exportåˆ°ç¯å¢ƒå˜é‡è€Œä¸æ˜¯å°†å…¶æ·»åŠ åˆ°å‘½ä»¤è¡Œï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š


```bash
export CUDA_VISIBLE_DEVICES=0,2
python -m torch.distributed.launch trainer-program.py ...
```

è¿™ç§æ–¹æ³•å¯èƒ½ä¼šä»¤äººå›°æƒ‘ï¼Œå› ä¸ºä½ å¯èƒ½ä¼šå¿˜è®°ä¹‹å‰è®¾ç½®äº†ç¯å¢ƒå˜é‡ï¼Œè¿›è€Œä¸æ˜ç™½ä¸ºä»€ä¹ˆä¼šä½¿ç”¨é”™è¯¯çš„ GPUã€‚å› æ­¤ï¼Œåœ¨åŒä¸€å‘½ä»¤è¡Œä¸­ä»…ä¸ºç‰¹å®šè¿è¡Œè®¾ç½®ç¯å¢ƒå˜é‡æ˜¯ä¸€ç§å¸¸è§åšæ³•ï¼Œæ­£å¦‚æœ¬èŠ‚å¤§å¤šæ•°ç¤ºä¾‹æ‰€ç¤ºã€‚


**`CUDA_DEVICE_ORDER`**

è¿˜æœ‰ä¸€ä¸ªé¢å¤–çš„ç¯å¢ƒå˜é‡ `CUDA_DEVICE_ORDER`ï¼Œç”¨äºæ§åˆ¶ç‰©ç†è®¾å¤‡çš„æ’åºæ–¹å¼ã€‚æœ‰ä¸¤ä¸ªé€‰æ‹©ï¼š

1. æŒ‰ PCIe æ€»çº¿ ID æ’åºï¼ˆä¸ nvidia-smi çš„é¡ºåºç›¸åŒ¹é…ï¼‰- è¿™æ˜¯é»˜è®¤é€‰é¡¹ã€‚


```bash
export CUDA_DEVICE_ORDER=PCI_BUS_ID
```

2. æŒ‰ GPU è®¡ç®—èƒ½åŠ›æ’åºã€‚

```bash
export CUDA_DEVICE_ORDER=FASTEST_FIRST
```

å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œä½ ä¸éœ€è¦å…³å¿ƒè¿™ä¸ªç¯å¢ƒå˜é‡ï¼Œä½†å¦‚æœä½ çš„è®¾ç½®ä¸å‡åŒ€ï¼Œé‚£ä¹ˆè¿™å°†éå¸¸æœ‰ç”¨ï¼Œä¾‹å¦‚ï¼Œæ‚¨çš„æ—§ GPU å’Œæ–° GPU ç‰©ç†ä¸Šå®‰è£…åœ¨ä¸€èµ·ï¼Œä½†è®©é€Ÿåº¦è¾ƒæ…¢çš„æ—§å¡æ’åœ¨è¿è¡Œçš„ç¬¬ä¸€ä½ã€‚è§£å†³è¿™ä¸ªé—®é¢˜çš„ä¸€ç§æ–¹æ³•æ˜¯äº¤æ¢å¡çš„ä½ç½®ã€‚ä½†å¦‚æœä¸èƒ½äº¤æ¢å¡ï¼ˆä¾‹å¦‚ï¼Œå¦‚æœè®¾å¤‡çš„æ•£çƒ­å—åˆ°å½±å“ï¼‰ï¼Œé‚£ä¹ˆè®¾ç½® `CUDA_DEVICE_ORDER=FASTEST_FIRST` å°†å§‹ç»ˆå°†è¾ƒæ–°ã€æ›´å¿«çš„å¡æ”¾åœ¨ç¬¬ä¸€ä½ã€‚ä½†è¿™å¯èƒ½ä¼šæœ‰ç‚¹æ··ä¹±ï¼Œå› ä¸º `nvidia-smi` ä»ç„¶ä¼šæŒ‰ç…§ PCIe é¡ºåºæŠ¥å‘Šå®ƒä»¬ã€‚

äº¤æ¢å¡çš„é¡ºåºçš„å¦ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨ï¼š


```bash
export CUDA_VISIBLE_DEVICES=1,0
```

åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬åªä½¿ç”¨äº† 2 ä¸ª GPUï¼Œä½†æ˜¯å½“ç„¶ï¼Œå¯¹äºè®¡ç®—æœºä¸Šæœ‰çš„ä»»ä½•æ•°é‡çš„ GPUï¼Œéƒ½é€‚ç”¨ç›¸åŒçš„æ–¹æ³•ã€‚

æ­¤å¤–ï¼Œå¦‚æœä½ è®¾ç½®äº†è¿™ä¸ªç¯å¢ƒå˜é‡ï¼Œæœ€å¥½å°†å…¶è®¾ç½®åœ¨ `~/.bashrc` æ–‡ä»¶æˆ–å…¶ä»–å¯åŠ¨é…ç½®æ–‡ä»¶ä¸­ï¼Œç„¶åå°±å¯ä»¥å¿˜è®°å®ƒäº†ã€‚


## Traineré›†æˆ

[`Trainer`] å·²ç»è¢«æ‰©å±•ï¼Œä»¥æ”¯æŒå¯èƒ½æ˜¾è‘—æé«˜è®­ç»ƒæ—¶é—´å¹¶é€‚åº”æ›´å¤§æ¨¡å‹çš„åº“ã€‚

ç›®å‰ï¼Œå®ƒæ”¯æŒç¬¬ä¸‰æ–¹è§£å†³æ–¹æ¡ˆ [DeepSpeed](https://github.com/deepspeedai/DeepSpeed) å’Œ [PyTorch FSDP](https://pytorch.org/docs/stable/fsdp.html)ï¼Œå®ƒä»¬å®ç°äº†è®ºæ–‡ [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models, by Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He](https://huggingface.co/papers/1910.02054) çš„éƒ¨åˆ†å†…å®¹ã€‚

æˆªè‡³æ’°å†™æœ¬æ–‡ï¼Œæ­¤æä¾›çš„æ”¯æŒæ˜¯æ–°çš„ä¸”å®éªŒæ€§çš„ã€‚å°½ç®¡æˆ‘ä»¬æ¬¢è¿å›´ç»• DeepSpeed å’Œ PyTorch FSDP çš„issuesï¼Œä½†æˆ‘ä»¬ä¸å†æ”¯æŒ FairScale é›†æˆï¼Œå› ä¸ºå®ƒå·²ç»é›†æˆåˆ°äº† PyTorch ä¸»çº¿ï¼ˆå‚è§ [PyTorch FSDP é›†æˆ](#pytorch-fully-sharded-data-parallel)ï¼‰ã€‚


<a id='zero-install-notes'></a>

### CUDAæ‹“å±•å®‰è£…æ³¨æ„äº‹é¡¹


æ’°å†™æ—¶ï¼ŒDeepspeed éœ€è¦åœ¨ä½¿ç”¨ä¹‹å‰ç¼–è¯‘ CUDA C++ ä»£ç ã€‚

è™½ç„¶æ‰€æœ‰å®‰è£…é—®é¢˜éƒ½åº”é€šè¿‡ [Deepspeed](https://github.com/deepspeedai/DeepSpeed/issues) çš„ GitHub Issueså¤„ç†ï¼Œä½†åœ¨æ„å»ºä¾èµ–CUDA æ‰©å±•çš„ä»»ä½• PyTorch æ‰©å±•æ—¶ï¼Œå¯èƒ½ä¼šé‡åˆ°ä¸€äº›å¸¸è§é—®é¢˜ã€‚

å› æ­¤ï¼Œå¦‚æœåœ¨æ‰§è¡Œä»¥ä¸‹æ“ä½œæ—¶é‡åˆ°ä¸ CUDA ç›¸å…³çš„æ„å»ºé—®é¢˜ï¼š


```bash
pip install deepspeed
```

è¯·é¦–å…ˆé˜…è¯»ä»¥ä¸‹è¯´æ˜ã€‚

åœ¨è¿™äº›è¯´æ˜ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†åœ¨ `pytorch` ä½¿ç”¨ CUDA `10.2` æ„å»ºæ—¶åº”é‡‡å–çš„æ“ä½œç¤ºä¾‹ã€‚å¦‚æœä½ çš„æƒ…å†µæœ‰æ‰€ä¸åŒï¼Œè¯·è®°å¾—å°†ç‰ˆæœ¬å·è°ƒæ•´ä¸ºæ‚¨æ‰€éœ€çš„ç‰ˆæœ¬ã€‚


#### å¯èƒ½çš„é—®é¢˜ #1

å°½ç®¡ PyTorch è‡ªå¸¦äº†å…¶è‡ªå·±çš„ CUDA å·¥å…·åŒ…ï¼Œä½†è¦æ„å»ºè¿™ä¸¤ä¸ªé¡¹ç›®ï¼Œä½ å¿…é¡»åœ¨æ•´ä¸ªç³»ç»Ÿä¸Šå®‰è£…ç›¸åŒç‰ˆæœ¬çš„ CUDAã€‚

ä¾‹å¦‚ï¼Œå¦‚æœä½ åœ¨ Python ç¯å¢ƒä¸­ä½¿ç”¨ `cudatoolkit==10.2` å®‰è£…äº† `pytorch`ï¼Œä½ è¿˜éœ€è¦åœ¨æ•´ä¸ªç³»ç»Ÿä¸Šå®‰è£… CUDA `10.2`ã€‚

ç¡®åˆ‡çš„ä½ç½®å¯èƒ½å› ç³»ç»Ÿè€Œå¼‚ï¼Œä½†åœ¨è®¸å¤š Unix ç³»ç»Ÿä¸Šï¼Œ`/usr/local/cuda-10.2` æ˜¯æœ€å¸¸è§çš„ä½ç½®ã€‚å½“ CUDA æ­£ç¡®è®¾ç½®å¹¶æ·»åŠ åˆ° `PATH` ç¯å¢ƒå˜é‡æ—¶ï¼Œå¯ä»¥é€šè¿‡æ‰§è¡Œä»¥ä¸‹å‘½ä»¤æ‰¾åˆ°å®‰è£…ä½ç½®ï¼š


```bash
which nvcc
```

å¦‚æœä½ å°šæœªåœ¨æ•´ä¸ªç³»ç»Ÿä¸Šå®‰è£… CUDAï¼Œè¯·é¦–å…ˆå®‰è£…ã€‚ä½ å¯ä»¥ä½¿ç”¨ä½ å–œæ¬¢çš„æœç´¢å¼•æ“æŸ¥æ‰¾è¯´æ˜ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ ä½¿ç”¨çš„æ˜¯ Ubuntuï¼Œä½ å¯èƒ½æƒ³æœç´¢ï¼š[ubuntu cuda 10.2 install](https://www.google.com/search?q=ubuntu+cuda+10.2+install)ã€‚


#### å¯èƒ½çš„é—®é¢˜ #2

å¦ä¸€ä¸ªå¯èƒ½çš„å¸¸è§é—®é¢˜æ˜¯ä½ å¯èƒ½åœ¨æ•´ä¸ªç³»ç»Ÿä¸Šå®‰è£…äº†å¤šä¸ª CUDA å·¥å…·åŒ…ã€‚ä¾‹å¦‚ï¼Œä½ å¯èƒ½æœ‰ï¼š


```bash
/usr/local/cuda-10.2
/usr/local/cuda-11.0
```

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ éœ€è¦ç¡®ä¿ `PATH` å’Œ `LD_LIBRARY_PATH` ç¯å¢ƒå˜é‡åŒ…å«æ‰€éœ€ CUDA ç‰ˆæœ¬çš„æ­£ç¡®è·¯å¾„ã€‚é€šå¸¸ï¼Œè½¯ä»¶åŒ…å®‰è£…ç¨‹åºå°†è®¾ç½®è¿™äº›å˜é‡ä»¥åŒ…å«æœ€æ–°å®‰è£…çš„ç‰ˆæœ¬ã€‚å¦‚æœé‡åˆ°æ„å»ºå¤±è´¥çš„é—®é¢˜ï¼Œä¸”æ˜¯å› ä¸ºåœ¨æ•´ä¸ªç³»ç»Ÿå®‰è£…ä½†è½¯ä»¶ä»æ‰¾ä¸åˆ°æ­£ç¡®çš„ CUDA ç‰ˆæœ¬ï¼Œè¿™æ„å‘³ç€ä½ éœ€è¦è°ƒæ•´è¿™ä¸¤ä¸ªç¯å¢ƒå˜é‡ã€‚

é¦–å…ˆï¼Œä½ ä»¥æŸ¥çœ‹å®ƒä»¬çš„å†…å®¹ï¼š


```bash
echo $PATH
echo $LD_LIBRARY_PATH
```

å› æ­¤ï¼Œæ‚¨å¯ä»¥äº†è§£å…¶ä¸­çš„å†…å®¹ã€‚

`LD_LIBRARY_PATH` å¯èƒ½æ˜¯ç©ºçš„ã€‚

`PATH` åˆ—å‡ºäº†å¯ä»¥æ‰¾åˆ°å¯æ‰§è¡Œæ–‡ä»¶çš„ä½ç½®ï¼Œè€Œ `LD_LIBRARY_PATH` ç”¨äºæŸ¥æ‰¾å…±äº«åº“ã€‚åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œè¾ƒæ—©çš„æ¡ç›®ä¼˜å…ˆäºè¾ƒåçš„æ¡ç›®ã€‚ `:` ç”¨äºåˆ†éš”å¤šä¸ªæ¡ç›®ã€‚

ç°åœ¨ï¼Œä¸ºäº†å‘Šè¯‰æ„å»ºç¨‹åºåœ¨å“ªé‡Œæ‰¾åˆ°ç‰¹å®šçš„ CUDA å·¥å…·åŒ…ï¼Œè¯·æ’å…¥æ‰€éœ€çš„è·¯å¾„ï¼Œè®©å…¶é¦–å…ˆåˆ—å‡ºï¼š


```bash
export PATH=/usr/local/cuda-10.2/bin:$PATH
export LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64:$LD_LIBRARY_PATH
```

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬æ²¡æœ‰è¦†ç›–ç°æœ‰å€¼ï¼Œè€Œæ˜¯åœ¨å‰é¢æ·»åŠ æ–°çš„å€¼ã€‚

å½“ç„¶ï¼Œæ ¹æ®éœ€è¦è°ƒæ•´ç‰ˆæœ¬å·å’Œå®Œæ•´è·¯å¾„ã€‚æ£€æŸ¥ä½ åˆ†é…çš„ç›®å½•æ˜¯å¦å®é™…å­˜åœ¨ã€‚`lib64` å­ç›®å½•æ˜¯å„ç§ CUDA `.so` å¯¹è±¡ï¼ˆå¦‚ `libcudart.so`ï¼‰çš„ä½ç½®ï¼Œè¿™ä¸ªåå­—å¯èƒ½åœ¨ä½ çš„ç³»ç»Ÿä¸­æ˜¯ä¸åŒçš„ï¼Œå¦‚æœæ˜¯ï¼Œè¯·è°ƒæ•´ä»¥åæ˜ å®é™…æƒ…å†µã€‚


#### å¯èƒ½çš„é—®é¢˜ #3

ä¸€äº›è¾ƒæ—§çš„ CUDA ç‰ˆæœ¬å¯èƒ½ä¼šæ‹’ç»ä½¿ç”¨æ›´æ–°çš„ç¼–è¯‘å™¨ã€‚ä¾‹å¦‚ï¼Œä½ å¯èƒ½æœ‰ `gcc-9`ï¼Œä½† CUDA å¯èƒ½éœ€è¦ `gcc-7`ã€‚

æœ‰å„ç§æ–¹æ³•å¯ä»¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚

å¦‚æœä½ å¯ä»¥å®‰è£…æœ€æ–°çš„ CUDA å·¥å…·åŒ…ï¼Œé€šå¸¸å®ƒåº”è¯¥æ”¯æŒæ›´æ–°çš„ç¼–è¯‘å™¨ã€‚

æˆ–è€…ï¼Œä½ å¯ä»¥åœ¨å·²ç»æ‹¥æœ‰çš„ç¼–è¯‘å™¨ç‰ˆæœ¬ä¹‹å¤–å®‰è£…è¾ƒä½ç‰ˆæœ¬ï¼Œæˆ–è€…ä½ å¯èƒ½å·²ç»å®‰è£…äº†å®ƒä½†å®ƒä¸æ˜¯é»˜è®¤çš„ç¼–è¯‘å™¨ï¼Œå› æ­¤æ„å»ºç³»ç»Ÿæ— æ³•æ‰¾åˆ°å®ƒã€‚å¦‚æœä½ å·²ç»å®‰è£…äº† `gcc-7` ä½†æ„å»ºç³»ç»Ÿæ‰¾ä¸åˆ°å®ƒï¼Œä»¥ä¸‹æ“ä½œå¯èƒ½ä¼šè§£å†³é—®é¢˜ï¼š


```bash
sudo ln -s /usr/bin/gcc-7  /usr/local/cuda-10.2/bin/gcc
sudo ln -s /usr/bin/g++-7  /usr/local/cuda-10.2/bin/g++
```

è¿™é‡Œï¼Œæˆ‘ä»¬æ­£åœ¨ä» `/usr/local/cuda-10.2/bin/gcc` åˆ›å»ºåˆ° `gcc-7` çš„è½¯é“¾æ¥ï¼Œç”±äº `/usr/local/cuda-10.2/bin/` åº”è¯¥åœ¨ `PATH` ç¯å¢ƒå˜é‡ä¸­ï¼ˆå‚è§å‰ä¸€ä¸ªé—®é¢˜çš„è§£å†³æ–¹æ¡ˆï¼‰ï¼Œå®ƒåº”è¯¥èƒ½å¤Ÿæ‰¾åˆ° `gcc-7`ï¼ˆå’Œ `g++7`ï¼‰ï¼Œç„¶åæ„å»ºå°†æˆåŠŸã€‚

ä¸å¾€å¸¸ä¸€æ ·ï¼Œè¯·ç¡®ä¿ç¼–è¾‘ç¤ºä¾‹ä¸­çš„è·¯å¾„ä»¥åŒ¹é…ä½ çš„æƒ…å†µã€‚



### PyTorchå®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œï¼ˆFSDP)

ä¸ºäº†åŠ é€Ÿåœ¨æ›´å¤§æ‰¹æ¬¡å¤§å°ä¸Šè®­ç»ƒåºå¤§æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®Œå…¨åˆ†ç‰‡çš„æ•°æ®å¹¶è¡Œæ¨¡å‹ã€‚è¿™ç§æ•°æ®å¹¶è¡ŒèŒƒä¾‹é€šè¿‡å¯¹ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦å’Œå‚æ•°è¿›è¡Œåˆ†ç‰‡ï¼Œå®ç°äº†åœ¨æ›´å¤šæ•°æ®å’Œæ›´å¤§æ¨¡å‹ä¸Šçš„è®­ç»ƒã€‚è¦äº†è§£æ›´å¤šä¿¡æ¯ä»¥åŠå…¶ä¼˜åŠ¿ï¼Œè¯·æŸ¥çœ‹[å®Œå…¨åˆ†ç‰‡çš„æ•°æ®å¹¶è¡Œåšå®¢](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)ã€‚æˆ‘ä»¬å·²ç»é›†æˆäº†æœ€æ–°çš„PyTorchå®Œå…¨åˆ†ç‰‡çš„æ•°æ®å¹¶è¡Œï¼ˆFSDPï¼‰è®­ç»ƒåŠŸèƒ½ã€‚æ‚¨åªéœ€é€šè¿‡é…ç½®å¯ç”¨å®ƒã€‚

**FSDPæ”¯æŒæ‰€éœ€çš„PyTorchç‰ˆæœ¬**: PyTorch Nightlyï¼ˆæˆ–è€…å¦‚æœä½ åœ¨å‘å¸ƒåé˜…è¯»è¿™ä¸ªï¼Œä½¿ç”¨1.12.0ç‰ˆæœ¬ï¼Œå› ä¸ºå¸¦æœ‰æ¿€æ´»çš„FSDPçš„æ¨¡å‹ä¿å­˜ä»…åœ¨æœ€è¿‘çš„ä¿®å¤ä¸­å¯ç”¨ã€‚


**ç”¨æ³•**:

- å¦‚æœä½ å°šæœªä½¿ç”¨è¿‡åˆ†å¸ƒå¼å¯åŠ¨å™¨ï¼Œç¡®ä¿ä½ å·²ç»æ·»åŠ äº†å®ƒ `-m torch.distributed.launch --nproc_per_node=NUMBER_OF_GPUS_YOU_HAVE`ã€‚

- **åˆ†ç‰‡ç­–ç•¥**ï¼š
  - FULL_SHARDï¼šåœ¨æ•°æ®å¹¶è¡Œçº¿ç¨‹/GPUä¹‹é—´ï¼Œå¯¹ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦å’Œæ¨¡å‹å‚æ•°è¿›è¡Œåˆ†ç‰‡ã€‚
    ä¸ºæ­¤ï¼Œè¯·åœ¨å‘½ä»¤è¡Œå‚æ•°ä¸­æ·»åŠ  `--fsdp full_shard`ã€‚
  - SHARD_GRAD_OPï¼šåœ¨æ•°æ®å¹¶è¡Œçº¿ç¨‹/GPUä¹‹é—´å¯¹ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦è¿›è¡Œåˆ†ç‰‡ã€‚
    ä¸ºæ­¤ï¼Œè¯·åœ¨å‘½ä»¤è¡Œå‚æ•°ä¸­æ·»åŠ  `--fsdp shard_grad_op`ã€‚
  - NO_SHARDï¼šä¸è¿›è¡Œåˆ†ç‰‡ã€‚ä¸ºæ­¤ï¼Œè¯·åœ¨å‘½ä»¤è¡Œå‚æ•°ä¸­æ·»åŠ  `--fsdp no_shard`ã€‚
- è¦å°†å‚æ•°å’Œæ¢¯åº¦å¸è½½åˆ°CPUï¼Œæ·»åŠ  `--fsdp "full_shard offload"` æˆ– `--fsdp "shard_grad_op offload"` åˆ°å‘½ä»¤è¡Œå‚æ•°ä¸­ã€‚
- è¦ä½¿ç”¨ `default_auto_wrap_policy` è‡ªåŠ¨é€’å½’åœ°ç”¨FSDPåŒ…è£…å±‚ï¼Œè¯·æ·»åŠ  `--fsdp "full_shard auto_wrap"` æˆ– `--fsdp "shard_grad_op auto_wrap"` åˆ°å‘½ä»¤è¡Œå‚æ•°ä¸­ã€‚
- è¦åŒæ—¶å¯ç”¨CPUå¸è½½å’Œè‡ªåŠ¨åŒ…è£…å±‚å·¥å…·ï¼Œè¯·æ·»åŠ  `--fsdp "full_shard offload auto_wrap"` æˆ– `--fsdp "shard_grad_op offload auto_wrap"` åˆ°å‘½ä»¤è¡Œå‚æ•°ä¸­ã€‚
- å…¶ä½™çš„FSDPé…ç½®é€šè¿‡ `--fsdp_config <path_to_fsdp_config.json>` ä¼ é€’ã€‚å®ƒå¯ä»¥æ˜¯FSDP jsoné…ç½®æ–‡ä»¶çš„ä½ç½®ï¼ˆä¾‹å¦‚ï¼Œ`fsdp_config.json`ï¼‰æˆ–å·²åŠ è½½çš„jsonæ–‡ä»¶ä½œä¸º `dict`ã€‚
  - å¦‚æœå¯ç”¨äº†è‡ªåŠ¨åŒ…è£…ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨åŸºäºtransformerçš„è‡ªåŠ¨åŒ…è£…ç­–ç•¥æˆ–åŸºäºå¤§å°çš„è‡ªåŠ¨åŒ…è£…ç­–ç•¥ã€‚
    - å¯¹äºåŸºäºtransformerçš„è‡ªåŠ¨åŒ…è£…ç­–ç•¥ï¼Œå»ºè®®åœ¨é…ç½®æ–‡ä»¶ä¸­æŒ‡å®š `fsdp_transformer_layer_cls_to_wrap`ã€‚å¦‚æœæœªæŒ‡å®šï¼Œåˆ™é»˜è®¤å€¼ä¸º `model._no_split_modules`ï¼ˆå¦‚æœå¯ç”¨ï¼‰ã€‚è¿™å°†æŒ‡å®šè¦åŒ…è£…çš„transformerå±‚ç±»åï¼ˆåŒºåˆ†å¤§å°å†™ï¼‰ï¼Œä¾‹å¦‚ [`BertLayer`]ã€[`GPTJBlock`]ã€[`T5Block`] ç­‰ã€‚è¿™å¾ˆé‡è¦ï¼Œå› ä¸ºå…±äº«æƒé‡çš„å­æ¨¡å—ï¼ˆä¾‹å¦‚ï¼Œembeddingå±‚ï¼‰ä¸åº”æœ€ç»ˆå‡ºç°åœ¨ä¸åŒçš„FSDPåŒ…è£…å•å…ƒä¸­ã€‚ä½¿ç”¨æ­¤ç­–ç•¥ï¼Œæ¯ä¸ªåŒ…è£…çš„å—å°†åŒ…å«å¤šå¤´æ³¨æ„åŠ›å’Œåé¢çš„å‡ ä¸ªMLPå±‚ã€‚å‰©ä½™çš„å±‚ï¼ŒåŒ…æ‹¬å…±äº«çš„embeddingå±‚ï¼Œéƒ½å°†è¢«æ–¹ä¾¿åœ°åŒ…è£…åœ¨åŒä¸€ä¸ªæœ€å¤–å±‚çš„FSDPå•å…ƒä¸­ã€‚å› æ­¤ï¼Œå¯¹äºåŸºäºtransformerçš„æ¨¡å‹ï¼Œè¯·ä½¿ç”¨è¿™ä¸ªæ–¹æ³•ã€‚
    - å¯¹äºåŸºäºå¤§å°çš„è‡ªåŠ¨åŒ…è£…ç­–ç•¥ï¼Œè¯·åœ¨é…ç½®æ–‡ä»¶ä¸­æ·»åŠ  `fsdp_min_num_params`ã€‚å®ƒæŒ‡å®šäº†FSDPè¿›è¡Œè‡ªåŠ¨åŒ…è£…çš„æœ€å°å‚æ•°æ•°é‡ã€‚
  - å¯ä»¥åœ¨é…ç½®æ–‡ä»¶ä¸­æŒ‡å®š `fsdp_backward_prefetch`ã€‚å®ƒæ§åˆ¶ä½•æ—¶é¢„å–ä¸‹ä¸€ç»„å‚æ•°ã€‚`backward_pre` å’Œ `backward_pos` æ˜¯å¯ç”¨çš„é€‰é¡¹ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜… `torch.distributed.fsdp.fully_sharded_data_parallel.BackwardPrefetch`
  - å¯ä»¥åœ¨é…ç½®æ–‡ä»¶ä¸­æŒ‡å®š `fsdp_forward_prefetch`ã€‚å®ƒæ§åˆ¶ä½•æ—¶é¢„å–ä¸‹ä¸€ç»„å‚æ•°ã€‚å¦‚æœæ˜¯`"True"`ï¼Œåœ¨æ‰§è¡Œå‰å‘ä¼ é€’æ—¶ï¼ŒFSDPæ˜ç¡®åœ°é¢„å–ä¸‹ä¸€æ¬¡å³å°†å‘ç”Ÿçš„å…¨å±€èšé›†ã€‚
  - å¯ä»¥åœ¨é…ç½®æ–‡ä»¶ä¸­æŒ‡å®š `limit_all_gathers`ã€‚å¦‚æœæ˜¯`"True"`ï¼ŒFSDPæ˜ç¡®åœ°åŒæ­¥CPUçº¿ç¨‹ï¼Œä»¥é˜²æ­¢å¤ªå¤šçš„è¿›è¡Œä¸­çš„å…¨å±€èšé›†ã€‚
  - å¯ä»¥åœ¨é…ç½®æ–‡ä»¶ä¸­æŒ‡å®š `activation_checkpointing`ã€‚å¦‚æœæ˜¯`"True"`ï¼ŒFSDP activation checkpointæ˜¯ä¸€ç§é€šè¿‡æ¸…é™¤æŸäº›å±‚çš„æ¿€æ´»å€¼å¹¶åœ¨åå‘ä¼ é€’æœŸé—´é‡æ–°è®¡ç®—å®ƒä»¬æ¥å‡å°‘å†…å­˜ä½¿ç”¨çš„æŠ€æœ¯ã€‚å®é™…ä¸Šï¼Œè¿™ä»¥æ›´å¤šçš„è®¡ç®—æ—¶é—´ä¸ºä»£ä»·å‡å°‘äº†å†…å­˜ä½¿ç”¨ã€‚


**éœ€è¦æ³¨æ„å‡ ä¸ªæ³¨æ„äº‹é¡¹**
- å®ƒä¸ `generate` ä¸å…¼å®¹ï¼Œå› æ­¤ä¸æ‰€æœ‰seq2seq/clmè„šæœ¬ï¼ˆç¿»è¯‘/æ‘˜è¦/clmç­‰ï¼‰ä¸­çš„ `--predict_with_generate` ä¸å…¼å®¹ã€‚è¯·å‚é˜…issue[#21667](https://github.com/huggingface/transformers/issues/21667)ã€‚


### PyTorch/XLA å®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œ

å¯¹äºæ‰€æœ‰TPUç”¨æˆ·ï¼Œæœ‰ä¸ªå¥½æ¶ˆæ¯ï¼PyTorch/XLAç°åœ¨æ”¯æŒFSDPã€‚æ‰€æœ‰æœ€æ–°çš„å®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œï¼ˆFSDPï¼‰è®­ç»ƒéƒ½å—æ”¯æŒã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[åœ¨äº‘ç«¯TPUä¸Šä½¿ç”¨FSDPæ‰©å±•PyTorchæ¨¡å‹](https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/)å’Œ[PyTorch/XLA FSDPçš„å®ç°](https://github.com/pytorch/xla/tree/master/torch_xla/distributed/fsdp)ã€‚ä½¿ç”¨å®ƒåªéœ€é€šè¿‡é…ç½®å¯ç”¨ã€‚

**éœ€è¦çš„ PyTorch/XLA ç‰ˆæœ¬ä»¥æ”¯æŒ FSDP**ï¼š>=2.0

**ç”¨æ³•**ï¼š

ä¼ é€’ `--fsdp "full shard"`ï¼ŒåŒæ—¶å¯¹ `--fsdp_config <path_to_fsdp_config.json>` è¿›è¡Œä»¥ä¸‹æ›´æ”¹ï¼š
- `xla` åº”è®¾ç½®ä¸º `True` ä»¥å¯ç”¨ PyTorch/XLA FSDPã€‚
- `xla_fsdp_settings` çš„å€¼æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œå­˜å‚¨ XLA FSDP å°è£…å‚æ•°ã€‚å®Œæ•´çš„é€‰é¡¹åˆ—è¡¨ï¼Œè¯·å‚è§[æ­¤å¤„](https://github.com/pytorch/xla/blob/master/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py)ã€‚
- `xla_fsdp_grad_ckpt`ã€‚å½“ `True` æ—¶ï¼Œåœ¨æ¯ä¸ªåµŒå¥—çš„ XLA FSDP å°è£…å±‚ä¸Šä½¿ç”¨æ¢¯åº¦checkpointã€‚è¯¥è®¾ç½®åªèƒ½åœ¨å°† xla æ ‡å¿—è®¾ç½®ä¸º trueï¼Œå¹¶é€šè¿‡ `fsdp_min_num_params` æˆ– `fsdp_transformer_layer_cls_to_wrap` æŒ‡å®šè‡ªåŠ¨åŒ…è£…ç­–ç•¥æ—¶ä½¿ç”¨ã€‚
- æ‚¨å¯ä»¥ä½¿ç”¨åŸºäºtransformerçš„è‡ªåŠ¨åŒ…è£…ç­–ç•¥æˆ–åŸºäºå¤§å°çš„è‡ªåŠ¨åŒ…è£…ç­–ç•¥ã€‚
  - å¯¹äºåŸºäºtransformerçš„è‡ªåŠ¨åŒ…è£…ç­–ç•¥ï¼Œå»ºè®®åœ¨é…ç½®æ–‡ä»¶ä¸­æŒ‡å®š `fsdp_transformer_layer_cls_to_wrap`ã€‚å¦‚æœæœªæŒ‡å®šï¼Œé»˜è®¤å€¼ä¸º `model._no_split_modules`ï¼ˆå¦‚æœå¯ç”¨ï¼‰ã€‚è¿™æŒ‡å®šäº†è¦åŒ…è£…çš„transformerå±‚ç±»ååˆ—è¡¨ï¼ˆåŒºåˆ†å¤§å°å†™ï¼‰ï¼Œä¾‹å¦‚ [`BertLayer`]ã€[`GPTJBlock`]ã€[`T5Block`] ç­‰ã€‚è¿™å¾ˆé‡è¦ï¼Œå› ä¸ºå…±äº«æƒé‡çš„å­æ¨¡å—ï¼ˆä¾‹å¦‚ï¼Œembeddingå±‚ï¼‰ä¸åº”æœ€ç»ˆå‡ºç°åœ¨ä¸åŒçš„FSDPåŒ…è£…å•å…ƒä¸­ã€‚ä½¿ç”¨æ­¤ç­–ç•¥ï¼Œæ¯ä¸ªåŒ…è£…çš„å—å°†åŒ…å«å¤šå¤´æ³¨æ„åŠ›å’Œåé¢çš„å‡ ä¸ªMLPå±‚ã€‚å‰©ä½™çš„å±‚ï¼ŒåŒ…æ‹¬å…±äº«çš„embeddingå±‚ï¼Œéƒ½å°†è¢«æ–¹ä¾¿åœ°åŒ…è£…åœ¨åŒä¸€ä¸ªæœ€å¤–å±‚çš„FSDPå•å…ƒä¸­ã€‚å› æ­¤ï¼Œå¯¹äºåŸºäºtransformerçš„æ¨¡å‹ï¼Œè¯·ä½¿ç”¨è¿™ä¸ªæ–¹æ³•ã€‚
  - å¯¹äºåŸºäºå¤§å°çš„è‡ªåŠ¨åŒ…è£…ç­–ç•¥ï¼Œè¯·åœ¨é…ç½®æ–‡ä»¶ä¸­æ·»åŠ  `fsdp_min_num_params`ã€‚å®ƒæŒ‡å®šäº†è‡ªåŠ¨åŒ…è£…çš„ FSDP çš„æœ€å°å‚æ•°æ•°é‡ã€‚


### åœ¨ Mac ä¸Šä½¿ç”¨ Trainer è¿›è¡ŒåŠ é€Ÿçš„ PyTorch è®­ç»ƒ

éšç€ PyTorch v1.12 ç‰ˆæœ¬çš„å‘å¸ƒï¼Œå¼€å‘äººå‘˜å’Œç ”ç©¶äººå‘˜å¯ä»¥åˆ©ç”¨ Apple Silicon GPU è¿›è¡Œæ˜¾è‘—æ›´å¿«çš„æ¨¡å‹è®­ç»ƒã€‚è¿™ä½¿å¾—å¯ä»¥åœ¨ Mac ä¸Šæœ¬åœ°æ‰§è¡ŒåŸå‹è®¾è®¡å’Œå¾®è°ƒç­‰æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹ã€‚Apple çš„ Metal Performance Shadersï¼ˆMPSï¼‰ä½œä¸º PyTorch çš„åç«¯å®ç°äº†è¿™ä¸€ç‚¹ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡æ–°çš„ `"mps"` è®¾å¤‡æ¥ä½¿ç”¨ã€‚
è¿™å°†åœ¨ MPS å›¾å½¢æ¡†æ¶ä¸Šæ˜ å°„è®¡ç®—å›¾å’Œç¥ç»å›¾å…ƒï¼Œå¹¶ä½¿ç”¨ MPS æä¾›çš„ä¼˜åŒ–å†…æ ¸ã€‚æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…å®˜æ–¹æ–‡æ¡£ [Introducing Accelerated PyTorch Training on Mac](https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/) å’Œ [MPS BACKEND](https://pytorch.org/docs/stable/notes/mps.html)ã€‚


<Tip warning={false}>

æˆ‘ä»¬å¼ºçƒˆå»ºè®®åœ¨ä½ çš„ MacOS æœºå™¨ä¸Šå®‰è£… PyTorch >= 1.13ï¼ˆåœ¨æ’°å†™æœ¬æ–‡æ—¶ä¸ºæœ€æ–°ç‰ˆæœ¬ï¼‰ã€‚å¯¹äºåŸºäº transformer çš„æ¨¡å‹ï¼Œ å®ƒæä¾›ä¸æ¨¡å‹æ­£ç¡®æ€§å’Œæ€§èƒ½æ”¹è¿›ç›¸å…³çš„é‡å¤§ä¿®å¤ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[pytorch/pytorch#82707](https://github.com/pytorch/pytorch/issues/82707)ã€‚

</Tip>

**ä½¿ç”¨ Apple Silicon èŠ¯ç‰‡è¿›è¡Œè®­ç»ƒå’Œæ¨ç†çš„å¥½å¤„**

1. ä½¿ç”¨æˆ·èƒ½å¤Ÿåœ¨æœ¬åœ°è®­ç»ƒæ›´å¤§çš„ç½‘ç»œæˆ–æ‰¹é‡æ•°æ®ã€‚
2. ç”±äºç»Ÿä¸€å†…å­˜æ¶æ„ï¼Œå‡å°‘æ•°æ®æ£€ç´¢å»¶è¿Ÿï¼Œå¹¶ä¸º GPU æä¾›å¯¹å®Œæ•´å†…å­˜å­˜å‚¨çš„ç›´æ¥è®¿é—®ã€‚ä»è€Œæé«˜ç«¯åˆ°ç«¯æ€§èƒ½ã€‚
3. é™ä½ä¸åŸºäºäº‘çš„å¼€å‘æˆ–éœ€è¦é¢å¤–æœ¬åœ° GPU çš„æˆæœ¬ã€‚

**å…ˆå†³æ¡ä»¶**ï¼šè¦å®‰è£…å¸¦æœ‰ mps æ”¯æŒçš„ torchï¼Œè¯·æŒ‰ç…§è¿™ç¯‡ç²¾å½©çš„ Medium æ–‡ç« æ“ä½œ [GPU-Acceleration Comes to PyTorch on M1 Macs](https://medium.com/towards-data-science/gpu-acceleration-comes-to-pytorch-on-m1-macs-195c399efcc1)ã€‚

**ç”¨æ³•**ï¼š
å¦‚æœå¯ç”¨ï¼Œ`mps` è®¾å¤‡å°†é»˜è®¤ä½¿ç”¨ï¼Œç±»ä¼¼äºä½¿ç”¨ `cuda` è®¾å¤‡çš„æ–¹å¼ã€‚å› æ­¤ï¼Œç”¨æˆ·æ— éœ€é‡‡å–ä»»ä½•æ“ä½œã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åœ¨ Apple Silicon GPU ä¸Šè¿è¡Œå®˜æ–¹çš„ Glue æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼ˆä»æ ¹æ–‡ä»¶å¤¹è¿è¡Œï¼‰ï¼š

```bash
export TASK_NAME=mrpc

python examples/pytorch/text-classification/run_glue.py \
  --model_name_or_path google-bert/bert-base-cased \
  --task_name $TASK_NAME \
  --do_train \
  --do_eval \
  --max_seq_length 128 \
  --per_device_train_batch_size 32 \
  --learning_rate 2e-5 \
  --num_train_epochs 3 \
  --output_dir /tmp/$TASK_NAME/ \
```

**éœ€è¦æ³¨æ„çš„ä¸€äº›æ³¨æ„äº‹é¡¹**

1. ä¸€äº› PyTorch æ“ä½œå°šæœªåœ¨ mps ä¸­å®ç°ï¼Œå°†å¼•å‘é”™è¯¯ã€‚è§£å†³æ­¤é—®é¢˜çš„ä¸€ç§æ–¹æ³•æ˜¯è®¾ç½®ç¯å¢ƒå˜é‡ `PYTORCH_ENABLE_MPS_FALLBACK=1`ï¼Œå®ƒå°†æŠŠè¿™äº›æ“ä½œå›é€€åˆ° CPU è¿›è¡Œã€‚ç„¶è€Œï¼Œå®ƒä»ç„¶ä¼šæŠ›å‡º UserWarning ä¿¡æ¯ã€‚
2. åˆ†å¸ƒå¼è®¾ç½® `gloo` å’Œ `nccl` åœ¨ `mps` è®¾å¤‡ä¸Šä¸èµ·ä½œç”¨ã€‚è¿™æ„å‘³ç€å½“å‰åªèƒ½ä½¿ç”¨ `mps` è®¾å¤‡ç±»å‹çš„å•ä¸ª GPUã€‚

æœ€åï¼Œè¯·è®°ä½ï¼ŒğŸ¤— `Trainer` ä»…é›†æˆäº† MPS åç«¯ï¼Œå› æ­¤å¦‚æœä½ åœ¨ä½¿ç”¨ MPS åç«¯æ—¶é‡åˆ°ä»»ä½•é—®é¢˜æˆ–æœ‰ç–‘é—®ï¼Œè¯·åœ¨ [PyTorch GitHub](https://github.com/pytorch/pytorch/issues) ä¸Šæäº¤é—®é¢˜ã€‚


## é€šè¿‡ Accelerate Launcher ä½¿ç”¨ Trainer

Accelerate ç°åœ¨æ”¯æŒ Trainerã€‚ç”¨æˆ·å¯ä»¥æœŸå¾…ä»¥ä¸‹å†…å®¹ï¼š
- ä»–ä»¬å¯ä»¥ç»§ç»­ä½¿ç”¨ Trainer çš„è¿­ä»£ï¼Œå¦‚ FSDPã€DeepSpeed ç­‰ï¼Œè€Œæ— éœ€åšä»»ä½•æ›´æ”¹ã€‚
- ç°åœ¨å¯ä»¥åœ¨ Trainer ä¸­ä½¿ç”¨ Accelerate Launcherï¼ˆå»ºè®®ä½¿ç”¨ï¼‰ã€‚

é€šè¿‡ Accelerate Launcher ä½¿ç”¨ Trainer çš„æ­¥éª¤ï¼š
1. ç¡®ä¿å·²å®‰è£… ğŸ¤— Accelerateï¼Œæ— è®ºå¦‚ä½•ï¼Œå¦‚æœæ²¡æœ‰å®ƒï¼Œä½ æ— æ³•ä½¿ç”¨ `Trainer`ã€‚å¦‚æœæ²¡æœ‰ï¼Œè¯·æ‰§è¡Œ `pip install accelerate`ã€‚ä½ å¯èƒ½è¿˜éœ€è¦æ›´æ–° Accelerate çš„ç‰ˆæœ¬ï¼š`pip install accelerate --upgrade`ã€‚
2. è¿è¡Œ `accelerate config` å¹¶å¡«å†™é—®é¢˜ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›åŠ é€Ÿé…ç½®çš„ç¤ºä¾‹ï¼š
   
  a. DDP å¤šèŠ‚ç‚¹å¤š GPU é…ç½®ï¼š

    ```yaml
    compute_environment: LOCAL_MACHINE                                                                                             
    distributed_type: MULTI_GPU                                                                                                    
    downcast_bf16: 'no'
    gpu_ids: all
    machine_rank: 0 #change rank as per the node
    main_process_ip: 192.168.20.1
    main_process_port: 9898
    main_training_function: main
    mixed_precision: fp16
    num_machines: 2
    num_processes: 8
    rdzv_backend: static
    same_network: true
    tpu_env: []
    tpu_use_cluster: false
    tpu_use_sudo: false
    use_cpu: false
    ```

  b. FSDP é…ç½®ï¼š

    ```yaml
    compute_environment: LOCAL_MACHINE
    distributed_type: FSDP
    downcast_bf16: 'no'
    fsdp_config:
      fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
      fsdp_backward_prefetch_policy: BACKWARD_PRE
      fsdp_forward_prefetch: true
      fsdp_offload_params: false
      fsdp_sharding_strategy: 1
      fsdp_state_dict_type: FULL_STATE_DICT
      fsdp_sync_module_states: true
      fsdp_transformer_layer_cls_to_wrap: BertLayer
      fsdp_use_orig_params: true
    machine_rank: 0
    main_training_function: main
    mixed_precision: bf16
    num_machines: 1
    num_processes: 2
    rdzv_backend: static
    same_network: true
    tpu_env: []
    tpu_use_cluster: false
    tpu_use_sudo: false
    use_cpu: false
    ```
 
  c. æŒ‡å‘æ–‡ä»¶çš„ DeepSpeed é…ç½®ï¼š

    ```yaml
    compute_environment: LOCAL_MACHINE
    deepspeed_config:
      deepspeed_config_file: /home/user/configs/ds_zero3_config.json
      zero3_init_flag: true
    distributed_type: DEEPSPEED
    downcast_bf16: 'no'
    machine_rank: 0
    main_training_function: main
    num_machines: 1
    num_processes: 4
    rdzv_backend: static
    same_network: true
    tpu_env: []
    tpu_use_cluster: false
    tpu_use_sudo: false
    use_cpu: false
    ```

  d. ä½¿ç”¨ accelerate æ’ä»¶çš„ DeepSpeed é…ç½®ï¼š

    ```yaml
    compute_environment: LOCAL_MACHINE                                                                                             
    deepspeed_config:                                                                                                              
      gradient_accumulation_steps: 1
      gradient_clipping: 0.7
      offload_optimizer_device: cpu
      offload_param_device: cpu
      zero3_init_flag: true
      zero_stage: 2
    distributed_type: DEEPSPEED
    downcast_bf16: 'no'
    machine_rank: 0
    main_training_function: main
    mixed_precision: bf16
    num_machines: 1
    num_processes: 4
    rdzv_backend: static
    same_network: true
    tpu_env: []
    tpu_use_cluster: false
    tpu_use_sudo: false
    use_cpu: false
    ```

3. ä½¿ç”¨accelerateé…ç½®æ–‡ä»¶å‚æ•°æˆ–å¯åŠ¨å™¨å‚æ•°ä»¥å¤–çš„å‚æ•°è¿è¡ŒTrainerè„šæœ¬ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªä½¿ç”¨ä¸Šè¿°FSDPé…ç½®ä»accelerateå¯åŠ¨å™¨è¿è¡Œ`run_glue.py`çš„ç¤ºä¾‹ã€‚

```bash
cd transformers

accelerate launch \
./examples/pytorch/text-classification/run_glue.py \
--model_name_or_path google-bert/bert-base-cased \
--task_name $TASK_NAME \
--do_train \
--do_eval \
--max_seq_length 128 \
--per_device_train_batch_size 16 \
--learning_rate 5e-5 \
--num_train_epochs 3 \
--output_dir /tmp/$TASK_NAME/ \
```

4. ä½ ä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨`accelerate launch`çš„cmdå‚æ•°ã€‚ä¸Šé¢çš„ç¤ºä¾‹å°†æ˜ å°„åˆ°ï¼š

```bash
cd transformers

accelerate launch --num_processes=2 \
--use_fsdp \
--mixed_precision=bf16 \
--fsdp_auto_wrap_policy=TRANSFORMER_BASED_WRAP  \
--fsdp_transformer_layer_cls_to_wrap="BertLayer" \
--fsdp_sharding_strategy=1 \
--fsdp_state_dict_type=FULL_STATE_DICT \
./examples/pytorch/text-classification/run_glue.py
--model_name_or_path google-bert/bert-base-cased \
--task_name $TASK_NAME \
--do_train \
--do_eval \
--max_seq_length 128 \
--per_device_train_batch_size 16 \
--learning_rate 5e-5 \
--num_train_epochs 3 \
--output_dir /tmp/$TASK_NAME/ \
```

æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜… ğŸ¤— Accelerate CLI æŒ‡å—ï¼š[å¯åŠ¨æ‚¨çš„ ğŸ¤— Accelerate è„šæœ¬](https://huggingface.co/docs/accelerate/basic_tutorials/launch)ã€‚

å·²ç§»åŠ¨çš„éƒ¨åˆ†ï¼š

[ <a href="./deepspeed#deepspeed-trainer-integration">DeepSpeed</a><a id="deepspeed"></a> | <a href="./deepspeed#deepspeed-installation">Installation</a><a id="installation"></a> | <a href="./deepspeed#deepspeed-multi-gpu">Deployment with multiple GPUs</a><a id="deployment-with-multiple-gpus"></a> | <a href="./deepspeed#deepspeed-one-gpu">Deployment with one GPU</a><a id="deployment-with-one-gpu"></a> | <a href="./deepspeed#deepspeed-notebook">Deployment in Notebooks</a><a id="deployment-in-notebooks"></a> | <a href="./deepspeed#deepspeed-config">Configuration</a><a id="configuration"></a> | <a href="./deepspeed#deepspeed-config-passing">Passing Configuration</a><a id="passing-configuration"></a> | <a href="./deepspeed#deepspeed-config-shared">Shared Configuration</a><a id="shared-configuration"></a> | <a href="./deepspeed#deepspeed-zero">ZeRO</a><a id="zero"></a> | <a href="./deepspeed#deepspeed-zero2-config">ZeRO-2 Config</a><a id="zero-2-config"></a> | <a href="./deepspeed#deepspeed-zero3-config">ZeRO-3 Config</a><a id="zero-3-config"></a> | <a href="./deepspeed#deepspeed-nvme">NVMe Support</a><a id="nvme-support"></a> | <a href="./deepspeed#deepspeed-zero2-zero3-performance">ZeRO-2 vs ZeRO-3 Performance</a><a id="zero-2-vs-zero-3-performance"></a> | <a href="./deepspeed#deepspeed-zero2-example">ZeRO-2 Example</a><a id="zero-2-example"></a> | <a href="./deepspeed#deepspeed-zero3-example">ZeRO-3 Example</a><a id="zero-3-example"></a> | <a href="./deepspeed#deepspeed-optimizer">Optimizer</a><a id="optimizer"></a> | <a href="./deepspeed#deepspeed-scheduler">Scheduler</a><a id="scheduler"></a> | <a href="./deepspeed#deepspeed-fp32">fp32 Precision</a><a id="fp32-precision"></a> | <a href="./deepspeed#deepspeed-amp">Automatic Mixed Precision</a><a id="automatic-mixed-precision"></a> | <a href="./deepspeed#deepspeed-bs">Batch Size</a><a id="batch-size"></a> | <a href="./deepspeed#deepspeed-grad-acc">Gradient Accumulation</a><a id="gradient-accumulation"></a> | <a href="./deepspeed#deepspeed-grad-clip">Gradient Clipping</a><a id="gradient-clipping"></a> | <a href="./deepspeed#deepspeed-weight-extraction">Getting The Model Weights Out</a><a id="getting-the-model-weights-out"></a>]


## é€šè¿‡ NEFTune æå‡å¾®è°ƒæ€§èƒ½

NEFTune æ˜¯ä¸€ç§æå‡èŠå¤©æ¨¡å‹æ€§èƒ½çš„æŠ€æœ¯ï¼Œç”± Jain ç­‰äººåœ¨è®ºæ–‡â€œNEFTune: Noisy Embeddings Improve Instruction Finetuningâ€ ä¸­å¼•å…¥ã€‚è¯¥æŠ€æœ¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å‘embeddingå‘é‡æ·»åŠ å™ªéŸ³ã€‚æ ¹æ®è®ºæ–‡æ‘˜è¦ï¼š

> ä½¿ç”¨ Alpaca å¯¹ LLaMA-2-7B è¿›è¡Œæ ‡å‡†å¾®è°ƒï¼Œå¯ä»¥åœ¨ AlpacaEval ä¸Šè¾¾åˆ° 29.79%ï¼Œè€Œä½¿ç”¨å¸¦æœ‰å™ªéŸ³embeddingçš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½æé«˜è‡³ 64.69%ã€‚NEFTune è¿˜åœ¨modern instructionæ•°æ®é›†ä¸Šå¤§å¤§ä¼˜äºåŸºçº¿ã€‚Evol-Instruct è®­ç»ƒçš„æ¨¡å‹è¡¨ç°æé«˜äº† 10%ï¼ŒShareGPT æé«˜äº† 8%ï¼ŒOpenPlatypus æé«˜äº† 8%ã€‚å³ä½¿åƒ LLaMA-2-Chat è¿™æ ·é€šè¿‡ RLHF è¿›ä¸€æ­¥ç»†åŒ–çš„å¼ºå¤§æ¨¡å‹ï¼Œé€šè¿‡ NEFTune çš„é¢å¤–è®­ç»ƒä¹Ÿèƒ½å—ç›Šã€‚

<div style="text-align: center">
<img src="https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/neft-screenshot.png">
</div>

è¦åœ¨ `Trainer` ä¸­ä½¿ç”¨å®ƒï¼Œåªéœ€åœ¨åˆ›å»º `TrainingArguments` å®ä¾‹æ—¶ä¼ é€’ `neftune_noise_alpha`ã€‚è¯·æ³¨æ„ï¼Œä¸ºäº†é¿å…ä»»ä½•æ„å¤–è¡Œä¸ºï¼ŒNEFTuneåœ¨è®­ç»ƒåè¢«ç¦æ­¢ï¼Œä»¥æ­¤æ¢å¤åŸå§‹çš„embeddingå±‚ã€‚

```python
from transformers import Trainer, TrainingArguments

args = TrainingArguments(..., neftune_noise_alpha=0.1)
trainer = Trainer(..., args=args)

...

trainer.train()
```

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\model_doc\bert.md
============================================================



<div style="float: right;">
    <div class="flex flex-wrap space-x-1">
        <img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white">
        <img alt="TensorFlow" src="https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white">
        <img alt="Flax" src="https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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
        ">
        <img alt="SDPA" src="https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white">
    </div>
</div>

# BERT

[BERT](https://huggingface.co/papers/1810.04805) æ˜¯ä¸€ä¸ªåœ¨æ— æ ‡ç­¾çš„æ–‡æœ¬æ•°æ®ä¸Šé¢„è®­ç»ƒçš„åŒå‘ transformerï¼Œç”¨äºé¢„æµ‹å¥å­ä¸­è¢«æ©ç çš„ï¼ˆmaskedï¼‰ tokenï¼Œä»¥åŠé¢„æµ‹ä¸€ä¸ªå¥å­æ˜¯å¦è·Ÿéšåœ¨å¦ä¸€ä¸ªå¥å­ä¹‹åã€‚å…¶ä¸»è¦æ€æƒ³æ˜¯ï¼Œåœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé€šè¿‡éšæœºæ©ç ä¸€äº› tokenï¼Œè®©æ¨¡å‹åˆ©ç”¨å·¦å³ä¸Šä¸‹æ–‡çš„ä¿¡æ¯é¢„æµ‹å®ƒä»¬ï¼Œä»è€Œè·å¾—æ›´å…¨é¢æ·±å…¥çš„ç†è§£ã€‚æ­¤å¤–ï¼ŒBERT å…·æœ‰å¾ˆå¼ºçš„é€šç”¨æ€§ï¼Œå…¶å­¦ä¹ åˆ°çš„è¯­è¨€è¡¨ç¤ºå¯ä»¥é€šè¿‡é¢å¤–çš„å±‚æˆ–å¤´è¿›è¡Œå¾®è°ƒï¼Œä»è€Œé€‚é…å…¶ä»–ä¸‹æ¸¸ NLP ä»»åŠ¡ã€‚

ä½ å¯ä»¥åœ¨ [BERT](https://huggingface.co/collections/google/bert-release-64ff5e7a4be99045d1896dbc) é›†åˆä¸‹æ‰¾åˆ° BERT çš„æ‰€æœ‰åŸå§‹ checkpointã€‚

> [!TIP]
> ç‚¹å‡»å³ä¾§è¾¹æ ä¸­çš„ BERT æ¨¡å‹ï¼Œä»¥æŸ¥çœ‹å°† BERT åº”ç”¨äºä¸åŒè¯­è¨€ä»»åŠ¡çš„æ›´å¤šç¤ºä¾‹ã€‚

ä¸‹é¢çš„ç¤ºä¾‹æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨ [`Pipeline`], [`AutoModel`] å’Œå‘½ä»¤è¡Œé¢„æµ‹ `[MASK]` tokenã€‚

<hfoptions id="usage">
<hfoption id="Pipeline">

```py
import torch
from transformers import pipeline

pipeline = pipeline(
    task="fill-mask",
    model="google-bert/bert-base-uncased",
    dtype=torch.float16,
    device=0
)
pipeline("Plants create [MASK] through a process known as photosynthesis.")
```

</hfoption>
<hfoption id="AutoModel">

```py
import torch
from transformers import AutoModelForMaskedLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(
    "google-bert/bert-base-uncased",
)
model = AutoModelForMaskedLM.from_pretrained(
    "google-bert/bert-base-uncased",
    dtype=torch.float16,
    device_map="auto",
    attn_implementation="sdpa"
)
inputs = tokenizer("Plants create [MASK] through a process known as photosynthesis.", return_tensors="pt").to("cuda")

with torch.no_grad():
    outputs = model(**inputs)
    predictions = outputs.logits

masked_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]
predicted_token_id = predictions[0, masked_index].argmax(dim=-1)
predicted_token = tokenizer.decode(predicted_token_id)

print(f"The predicted token is: {predicted_token}")
```

</hfoption>
<hfoption id="transformers">

```bash
echo -e "Plants create [MASK] through a process known as photosynthesis." | transformers run --task fill-mask --model google-bert/bert-base-uncased --device 0
```

</hfoption>
</hfoptions>

## æ³¨æ„

- è¾“å…¥å†…å®¹åº”åœ¨å³ä¾§è¿›è¡Œå¡«å……ï¼Œå› ä¸º BERT ä½¿ç”¨ç»å¯¹ä½ç½®åµŒå…¥ã€‚
## BertConfig

[[autodoc]] BertConfig
    - all

## BertTokenizer

[[autodoc]] BertTokenizer
    - get_special_tokens_mask
    - save_vocabulary

## BertTokenizerLegacy

[[autodoc]] BertTokenizerLegacy

## BertTokenizerFast

[[autodoc]] BertTokenizerFast

## BertModel

[[autodoc]] BertModel
    - forward

## BertForPreTraining

[[autodoc]] BertForPreTraining
    - forward

## BertLMHeadModel

[[autodoc]] BertLMHeadModel
    - forward

## BertForMaskedLM

[[autodoc]] BertForMaskedLM
    - forward

## BertForNextSentencePrediction

[[autodoc]] BertForNextSentencePrediction
    - forward

## BertForSequenceClassification

[[autodoc]] BertForSequenceClassification
    - forward

## BertForMultipleChoice

[[autodoc]] BertForMultipleChoice
    - forward

## BertForTokenClassification

[[autodoc]] BertForTokenClassification
    - forward

## BertForQuestionAnswering

[[autodoc]] BertForQuestionAnswering
    - forward

## Bert specific outputs

[[autodoc]] models.bert.modeling_bert.BertForPreTrainingOutput

============================================================
æ–‡ä»¶: ../34_Huggingface_Transformers_Lib/Code/transformers-main/docs/source/zh\tasks\asr.md
============================================================

# è‡ªåŠ¨è¯­éŸ³è¯†åˆ«

[[open-in-colab]]

<Youtube id="TksaY_FDgnk"/>

è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å°†è¯­éŸ³ä¿¡å·è½¬æ¢ä¸ºæ–‡æœ¬ï¼Œå°†ä¸€ç³»åˆ—éŸ³é¢‘è¾“å…¥æ˜ å°„åˆ°æ–‡æœ¬è¾“å‡ºã€‚
Siri å’Œ Alexa è¿™ç±»è™šæ‹ŸåŠ©æ‰‹ä½¿ç”¨ ASR æ¨¡å‹æ¥å¸®åŠ©ç”¨æˆ·æ—¥å¸¸ç”Ÿæ´»ï¼Œè¿˜æœ‰è®¸å¤šå…¶ä»–é¢å‘ç”¨æˆ·çš„æœ‰ç”¨åº”ç”¨ï¼Œå¦‚ä¼šè®®å®æ—¶å­—å¹•å’Œä¼šè®®çºªè¦ã€‚

æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ï¼š

1. åœ¨ [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) æ•°æ®é›†ä¸Šå¯¹
   [Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base) è¿›è¡Œå¾®è°ƒï¼Œä»¥å°†éŸ³é¢‘è½¬å½•ä¸ºæ–‡æœ¬ã€‚
2. ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæ¨æ–­ã€‚

<Tip>

å¦‚æœæ‚¨æƒ³æŸ¥çœ‹æ‰€æœ‰ä¸æœ¬ä»»åŠ¡å…¼å®¹çš„æ¶æ„å’Œæ£€æŸ¥ç‚¹ï¼Œæœ€å¥½æŸ¥çœ‹[ä»»åŠ¡é¡µ](https://huggingface.co/tasks/automatic-speech-recognition)ã€‚

</Tip>

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š

```bash
pip install transformers datasets evaluate jiwer
```

æˆ‘ä»¬é¼“åŠ±æ‚¨ç™»å½•è‡ªå·±çš„ Hugging Face è´¦æˆ·ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥ä¸Šä¼ å¹¶ä¸ç¤¾åŒºåˆ†äº«æ‚¨çš„æ¨¡å‹ã€‚
å‡ºç°æç¤ºæ—¶ï¼Œè¾“å…¥æ‚¨çš„ä»¤ç‰Œç™»å½•ï¼š

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## åŠ è½½ MInDS-14 æ•°æ®é›†

é¦–å…ˆä»ğŸ¤— Datasets åº“ä¸­åŠ è½½ [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14)
æ•°æ®é›†çš„ä¸€ä¸ªè¾ƒå°å­é›†ã€‚è¿™å°†è®©æ‚¨æœ‰æœºä¼šå…ˆè¿›è¡Œå®éªŒï¼Œç¡®ä¿ä¸€åˆ‡æ­£å¸¸ï¼Œç„¶åå†èŠ±æ›´å¤šæ—¶é—´åœ¨å®Œæ•´æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚

```py
>>> from datasets import load_dataset, Audio

>>> minds = load_dataset("PolyAI/minds14", name="en-US", split="train[:100]")
```

ä½¿ç”¨ [`~Dataset.train_test_split`] æ–¹æ³•å°†æ•°æ®é›†çš„ `train` æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š

```py
>>> minds = minds.train_test_split(test_size=0.2)
```

ç„¶åçœ‹çœ‹æ•°æ®é›†ï¼š

```py
>>> minds
DatasetDict({
    train: Dataset({
        features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],
        num_rows: 16
    })
    test: Dataset({
        features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],
        num_rows: 4
    })
})
```

è™½ç„¶æ•°æ®é›†åŒ…å« `lang_id` å’Œ `english_transcription` ç­‰è®¸å¤šæœ‰ç”¨çš„ä¿¡æ¯ï¼Œä½†åœ¨æœ¬æŒ‡å—ä¸­ï¼Œ
æ‚¨å°†ä¸“æ³¨äº `audio` å’Œ `transcription`ã€‚ä½¿ç”¨ [`~datasets.Dataset.remove_columns`] æ–¹æ³•åˆ é™¤å…¶ä»–åˆ—ï¼š

```py
>>> minds = minds.remove_columns(["english_transcription", "intent_class", "lang_id"])
```

å†çœ‹çœ‹ç¤ºä¾‹ï¼š

```py
>>> minds["train"][0]
{'audio': {'array': array([-0.00024414,  0.        ,  0.        , ...,  0.00024414,
          0.00024414,  0.00024414], dtype=float32),
  'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav',
  'sampling_rate': 8000},
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav',
 'transcription': "hi I'm trying to use the banking app on my phone and currently my checking and savings account balance is not refreshing"}
```

æœ‰ 2 ä¸ªå­—æ®µï¼š

- `audio`ï¼šç”±è¯­éŸ³ä¿¡å·å½¢æˆçš„ä¸€ç»´ `array`ï¼Œç”¨äºåŠ è½½å’Œé‡æ–°é‡‡æ ·éŸ³é¢‘æ–‡ä»¶ã€‚
- `transcription`ï¼šç›®æ ‡æ–‡æœ¬ã€‚

## é¢„å¤„ç†

ä¸‹ä¸€æ­¥æ˜¯åŠ è½½ä¸€ä¸ª Wav2Vec2 å¤„ç†å™¨æ¥å¤„ç†éŸ³é¢‘ä¿¡å·ï¼š

```py
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base")
```

MInDS-14 æ•°æ®é›†çš„é‡‡æ ·ç‡ä¸º 8000kHzï¼ˆæ‚¨å¯ä»¥åœ¨å…¶[æ•°æ®é›†å¡ç‰‡](https://huggingface.co/datasets/PolyAI/minds14)ä¸­æ‰¾åˆ°æ­¤ä¿¡æ¯ï¼‰ï¼Œ
è¿™æ„å‘³ç€æ‚¨éœ€è¦å°†æ•°æ®é›†é‡æ–°é‡‡æ ·ä¸º 16000kHz ä»¥ä½¿ç”¨é¢„è®­ç»ƒçš„ Wav2Vec2 æ¨¡å‹ï¼š

```py
>>> minds = minds.cast_column("audio", Audio(sampling_rate=16_000))
>>> minds["train"][0]
{'audio': {'array': array([-2.38064706e-04, -1.58618059e-04, -5.43987835e-06, ...,
          2.78103951e-04,  2.38446111e-04,  1.18740834e-04], dtype=float32),
  'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav',
  'sampling_rate': 16000},
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav',
 'transcription': "hi I'm trying to use the banking app on my phone and currently my checking and savings account balance is not refreshing"}
```

å¦‚æ‚¨åœ¨ä¸Šé¢çš„ `transcription` ä¸­æ‰€çœ‹åˆ°çš„ï¼Œæ–‡æœ¬åŒ…å«å¤§å°å†™å­—ç¬¦çš„æ··åˆã€‚
Wav2Vec2 åˆ†è¯å™¨ä»…è®­ç»ƒäº†å¤§å†™å­—ç¬¦ï¼Œå› æ­¤æ‚¨éœ€è¦ç¡®ä¿æ–‡æœ¬ä¸åˆ†è¯å™¨çš„è¯æ±‡è¡¨åŒ¹é…ï¼š

```py
>>> def uppercase(example):
...     return {"transcription": example["transcription"].upper()}


>>> minds = minds.map(uppercase)
```

ç°åœ¨åˆ›å»ºä¸€ä¸ªé¢„å¤„ç†å‡½æ•°ï¼Œè¯¥å‡½æ•°åº”è¯¥ï¼š

1. è°ƒç”¨ `audio` åˆ—ä»¥åŠ è½½å’Œé‡æ–°é‡‡æ ·éŸ³é¢‘æ–‡ä»¶ã€‚
2. ä»éŸ³é¢‘æ–‡ä»¶ä¸­æå– `input_values` å¹¶ä½¿ç”¨å¤„ç†å™¨å¯¹ `transcription` åˆ—æ‰§è¡Œ tokenizer æ“ä½œã€‚

```py
>>> def prepare_dataset(batch):
...     audio = batch["audio"]
...     batch = processor(audio["array"], sampling_rate=audio["sampling_rate"], text=batch["transcription"])
...     batch["input_length"] = len(batch["input_values"][0])
...     return batch
```

è¦åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šåº”ç”¨é¢„å¤„ç†å‡½æ•°ï¼Œå¯ä»¥ä½¿ç”¨ğŸ¤— Datasets çš„ [`~datasets.Dataset.map`] å‡½æ•°ã€‚
æ‚¨å¯ä»¥é€šè¿‡å¢åŠ  `num_proc` å‚æ•°æ¥åŠ é€Ÿ `map` çš„å¤„ç†è¿›ç¨‹æ•°é‡ã€‚
ä½¿ç”¨ [`~datasets.Dataset.remove_columns`] æ–¹æ³•åˆ é™¤ä¸éœ€è¦çš„åˆ—ï¼š

```py
>>> encoded_minds = minds.map(prepare_dataset, remove_columns=minds.column_names["train"], num_proc=4)
```

ğŸ¤— Transformers æ²¡æœ‰ç”¨äº ASR çš„æ•°æ®æ•´ç†å™¨ï¼Œå› æ­¤æ‚¨éœ€è¦è°ƒæ•´ [`DataCollatorWithPadding`] æ¥åˆ›å»ºä¸€ä¸ªç¤ºä¾‹æ‰¹æ¬¡ã€‚
å®ƒè¿˜ä¼šåŠ¨æ€åœ°å°†æ‚¨çš„æ–‡æœ¬å’Œæ ‡ç­¾å¡«å……åˆ°å…¶æ‰¹æ¬¡ä¸­æœ€é•¿å…ƒç´ çš„é•¿åº¦ï¼ˆè€Œä¸æ˜¯æ•´ä¸ªæ•°æ®é›†ï¼‰ï¼Œä»¥ä½¿å®ƒä»¬å…·æœ‰ç»Ÿä¸€çš„é•¿åº¦ã€‚
è™½ç„¶å¯ä»¥é€šè¿‡åœ¨ `tokenizer` å‡½æ•°ä¸­è®¾ç½® `padding=True` æ¥å¡«å……æ–‡æœ¬ï¼Œä½†åŠ¨æ€å¡«å……æ›´æœ‰æ•ˆã€‚

ä¸å…¶ä»–æ•°æ®æ•´ç†å™¨ä¸åŒï¼Œè¿™ä¸ªç‰¹å®šçš„æ•°æ®æ•´ç†å™¨éœ€è¦å¯¹ `input_values` å’Œ `labels` åº”ç”¨ä¸åŒçš„å¡«å……æ–¹æ³•ï¼š

```py
>>> import torch

>>> from dataclasses import dataclass, field
>>> from typing import Any, Dict, List, Optional, Union


>>> @dataclass
... class DataCollatorCTCWithPadding:
...     processor: AutoProcessor
...     padding: Union[bool, str] = "longest"

...     def __call__(self, features: list[dict[str, Union[list[int], torch.Tensor]]]) -> dict[str, torch.Tensor]:
...         # split inputs and labels since they have to be of different lengths and need
...         # different padding methods
...         input_features = [{"input_values": feature["input_values"][0]} for feature in features]
...         label_features = [{"input_ids": feature["labels"]} for feature in features]

...         batch = self.processor.pad(input_features, padding=self.padding, return_tensors="pt")

...         labels_batch = self.processor.pad(labels=label_features, padding=self.padding, return_tensors="pt")

...         # replace padding with -100 to ignore loss correctly
...         labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)

...         batch["labels"] = labels

...         return batch
```

ç°åœ¨å®ä¾‹åŒ–æ‚¨çš„ `DataCollatorForCTCWithPadding`ï¼š

```py
>>> data_collator = DataCollatorCTCWithPadding(processor=processor, padding="longest")
```

## è¯„ä¼°

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒ…å«ä¸€ä¸ªæŒ‡æ ‡é€šå¸¸æœ‰åŠ©äºè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚
æ‚¨å¯ä»¥é€šè¿‡ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) åº“å¿«é€ŸåŠ è½½ä¸€ä¸ªè¯„ä¼°æ–¹æ³•ã€‚
å¯¹äºè¿™ä¸ªä»»åŠ¡ï¼ŒåŠ è½½ [word error rate](https://huggingface.co/spaces/evaluate-metric/wer)ï¼ˆWERï¼‰æŒ‡æ ‡
ï¼ˆè¯·å‚é˜…ğŸ¤— Evaluate [å¿«é€Ÿä¸Šæ‰‹](https://huggingface.co/docs/evaluate/a_quick_tour)ä»¥äº†è§£å¦‚ä½•åŠ è½½å’Œè®¡ç®—æŒ‡æ ‡ï¼‰ï¼š

```py
>>> import evaluate

>>> wer = evaluate.load("wer")
```

ç„¶ååˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œå°†æ‚¨çš„é¢„æµ‹å’Œæ ‡ç­¾ä¼ é€’ç»™ [`~evaluate.EvaluationModule.compute`] æ¥è®¡ç®— WERï¼š

```py
>>> import numpy as np


>>> def compute_metrics(pred):
...     pred_logits = pred.predictions
...     pred_ids = np.argmax(pred_logits, axis=-1)

...     pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id

...     pred_str = processor.batch_decode(pred_ids)
...     label_str = processor.batch_decode(pred.label_ids, group_tokens=False)

...     wer = wer.compute(predictions=pred_str, references=label_str)

...     return {"wer": wer}
```

æ‚¨çš„ `compute_metrics` å‡½æ•°ç°åœ¨å·²ç»å‡†å¤‡å°±ç»ªï¼Œå½“æ‚¨è®¾ç½®å¥½è®­ç»ƒæ—¶å°†è¿”å›ç»™æ­¤å‡½æ•°ã€‚

## è®­ç»ƒ

<Tip>

å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰ä½¿ç”¨[`Trainer`]å¾®è°ƒæ¨¡å‹ï¼Œè¯·æŸ¥çœ‹è¿™é‡Œçš„åŸºæœ¬æ•™ç¨‹[here](../training#train-with-pytorch-trainer)ï¼

</Tip>

ç°åœ¨æ‚¨å·²ç»å‡†å¤‡å¥½å¼€å§‹è®­ç»ƒæ‚¨çš„æ¨¡å‹äº†ï¼ä½¿ç”¨ [`AutoModelForCTC`] åŠ è½½ Wav2Vec2ã€‚
ä½¿ç”¨ `ctc_loss_reduction` å‚æ•°æŒ‡å®šè¦åº”ç”¨çš„å‡å°‘æ–¹å¼ã€‚é€šå¸¸æœ€å¥½ä½¿ç”¨å¹³å‡å€¼è€Œä¸æ˜¯é»˜è®¤çš„æ±‚å’Œï¼š

```py
>>> from transformers import AutoModelForCTC, TrainingArguments, Trainer

>>> model = AutoModelForCTC.from_pretrained(
...     "facebook/wav2vec2-base",
...     ctc_loss_reduction="mean",
...     pad_token_id=processor.tokenizer.pad_token_id,
)
```

æ­¤æ—¶ï¼Œåªå‰©ä¸‹ 3 ä¸ªæ­¥éª¤ï¼š

1. åœ¨ [`TrainingArguments`] ä¸­å®šä¹‰æ‚¨çš„è®­ç»ƒå‚æ•°ã€‚å”¯ä¸€å¿…éœ€çš„å‚æ•°æ˜¯ `output_dir`ï¼Œç”¨äºæŒ‡å®šä¿å­˜æ¨¡å‹çš„ä½ç½®ã€‚
   æ‚¨å¯ä»¥é€šè¿‡è®¾ç½® `push_to_hub=True` å°†æ­¤æ¨¡å‹æ¨é€åˆ° Hubï¼ˆæ‚¨éœ€è¦ç™»å½•åˆ° Hugging Face æ‰èƒ½ä¸Šä¼ æ‚¨çš„æ¨¡å‹ï¼‰ã€‚
   åœ¨æ¯ä¸ª epoch ç»“æŸæ—¶ï¼Œ[`Trainer`] å°†è¯„ä¼° WER å¹¶ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹ã€‚
2. å°†è®­ç»ƒå‚æ•°ä¸æ¨¡å‹ã€æ•°æ®é›†ã€åˆ†è¯å™¨ã€æ•°æ®æ•´ç†å™¨å’Œ `compute_metrics` å‡½æ•°ä¸€èµ·ä¼ é€’ç»™ [`Trainer`]ã€‚
3. è°ƒç”¨ [`~Trainer.train`] æ¥å¾®è°ƒæ‚¨çš„æ¨¡å‹ã€‚

```py
>>> training_args = TrainingArguments(
...     output_dir="my_awesome_asr_mind_model",
...     per_device_train_batch_size=8,
...     gradient_accumulation_steps=2,
...     learning_rate=1e-5,
...     warmup_steps=500,
...     max_steps=2000,
...     gradient_checkpointing=True,
...     fp16=True,
...     group_by_length=True,
...     eval_strategy="steps",
...     per_device_eval_batch_size=8,
...     save_steps=1000,
...     eval_steps=1000,
...     logging_steps=25,
...     load_best_model_at_end=True,
...     metric_for_best_model="wer",
...     greater_is_better=False,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=encoded_minds["train"],
...     eval_dataset=encoded_minds["test"],
...     processing_class=processor,
...     data_collator=data_collator,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨ [`~transformers.Trainer.push_to_hub`] æ–¹æ³•å°†æ‚¨çš„æ¨¡å‹åˆ†äº«åˆ° Hubï¼Œæ–¹ä¾¿å¤§å®¶ä½¿ç”¨æ‚¨çš„æ¨¡å‹ï¼š

```py
>>> trainer.push_to_hub()
```

<Tip>

è¦æ·±å…¥äº†è§£å¦‚ä½•å¾®è°ƒæ¨¡å‹è¿›è¡Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼Œ
è¯·æŸ¥çœ‹è¿™ç¯‡åšå®¢[æ–‡ç« ](https://huggingface.co/blog/fine-tune-wav2vec2-english)ä»¥äº†è§£è‹±è¯­ ASRï¼Œ
è¿˜å¯ä»¥å‚é˜…[è¿™ç¯‡æ–‡ç« ](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2)ä»¥äº†è§£å¤šè¯­è¨€ ASRã€‚

</Tip>

## æ¨æ–­

å¾ˆå¥½ï¼Œç°åœ¨æ‚¨å·²ç»å¾®è°ƒäº†ä¸€ä¸ªæ¨¡å‹ï¼Œæ‚¨å¯ä»¥ç”¨å®ƒè¿›è¡Œæ¨æ–­äº†ï¼

åŠ è½½æ‚¨æƒ³è¦è¿è¡Œæ¨æ–­çš„éŸ³é¢‘æ–‡ä»¶ã€‚è¯·è®°ä½ï¼Œå¦‚æœéœ€è¦ï¼Œå°†éŸ³é¢‘æ–‡ä»¶çš„é‡‡æ ·ç‡é‡æ–°é‡‡æ ·ä¸ºä¸æ¨¡å‹åŒ¹é…çš„é‡‡æ ·ç‡ï¼

```py
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("PolyAI/minds14", "en-US", split="train")
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
>>> sampling_rate = dataset.features["audio"].sampling_rate
>>> audio_file = dataset[0]["audio"]["path"]
```

å°è¯•ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæ¨æ–­çš„æœ€ç®€å•æ–¹æ³•æ˜¯ä½¿ç”¨ [`pipeline`]ã€‚
ä½¿ç”¨æ‚¨çš„æ¨¡å‹å®ä¾‹åŒ–ä¸€ä¸ªç”¨äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«çš„ `pipeline`ï¼Œå¹¶å°†æ‚¨çš„éŸ³é¢‘æ–‡ä»¶ä¼ é€’ç»™å®ƒï¼š

```py
>>> from transformers import pipeline

>>> transcriber = pipeline("automatic-speech-recognition", model="stevhliu/my_awesome_asr_minds_model")
>>> transcriber(audio_file)
{'text': 'I WOUD LIKE O SET UP JOINT ACOUNT WTH Y PARTNER'}
```

<Tip>

è½¬å½•ç»“æœè¿˜ä¸é”™ï¼Œä½†å¯ä»¥æ›´å¥½ï¼å°è¯•ç”¨æ›´å¤šç¤ºä¾‹å¾®è°ƒæ‚¨çš„æ¨¡å‹ï¼Œä»¥è·å¾—æ›´å¥½çš„ç»“æœï¼

</Tip>

å¦‚æœæ‚¨æ„¿æ„ï¼Œæ‚¨ä¹Ÿå¯ä»¥æ‰‹åŠ¨å¤åˆ¶ `pipeline` çš„ç»“æœï¼š


åŠ è½½ä¸€ä¸ªå¤„ç†å™¨æ¥é¢„å¤„ç†éŸ³é¢‘æ–‡ä»¶å’Œè½¬å½•ï¼Œå¹¶å°† `input` è¿”å›ä¸º PyTorch å¼ é‡ï¼š

```py
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("stevhliu/my_awesome_asr_mind_model")
>>> inputs = processor(dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")
```

å°†æ‚¨çš„è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å¹¶è¿”å› logitsï¼š

```py
>>> from transformers import AutoModelForCTC

>>> model = AutoModelForCTC.from_pretrained("stevhliu/my_awesome_asr_mind_model")
>>> with torch.no_grad():
...     logits = model(**inputs).logits
```

è·å–å…·æœ‰æœ€é«˜æ¦‚ç‡çš„é¢„æµ‹ `input_ids`ï¼Œå¹¶ä½¿ç”¨å¤„ç†å™¨å°†é¢„æµ‹çš„ `input_ids` è§£ç å›æ–‡æœ¬ï¼š

```py
>>> import torch

>>> predicted_ids = torch.argmax(logits, dim=-1)
>>> transcription = processor.batch_decode(predicted_ids)
>>> transcription
['I WOUL LIKE O SET UP JOINT ACOUNT WTH Y PARTNER']
```

============================================================
å­—æ•°ç»Ÿè®¡æ±‡æ€»
============================================================

æŒ‰æ–‡ä»¶ç»Ÿè®¡:
----------------------------------------
accelerate.md:
  ä¸­æ–‡å­—ç¬¦: 673
  è‹±æ–‡å•è¯: 166
  æ€»å­—ç¬¦æ•°: 3197

add_new_pipeline.md:
  ä¸­æ–‡å­—ç¬¦: 1533
  è‹±æ–‡å•è¯: 424
  æ€»å­—ç¬¦æ•°: 7665

attention.md:
  ä¸­æ–‡å­—ç¬¦: 930
  è‹±æ–‡å•è¯: 55
  æ€»å­—ç¬¦æ•°: 1682

autoclass_tutorial.md:
  ä¸­æ–‡å­—ç¬¦: 745
  è‹±æ–‡å•è¯: 263
  æ€»å­—ç¬¦æ•°: 4050

bertology.md:
  ä¸­æ–‡å­—ç¬¦: 436
  è‹±æ–‡å•è¯: 114
  æ€»å­—ç¬¦æ•°: 1472

big_models.md:
  ä¸­æ–‡å­—ç¬¦: 957
  è‹±æ–‡å•è¯: 250
  æ€»å­—ç¬¦æ•°: 3918

chat_templating.md:
  ä¸­æ–‡å­—ç¬¦: 3188
  è‹±æ–‡å•è¯: 1086
  æ€»å­—ç¬¦æ•°: 14275

community.md:
  ä¸­æ–‡å­—ç¬¦: 1152
  è‹±æ–‡å•è¯: 2350
  æ€»å­—ç¬¦æ•°: 24406

contributing.md:
  ä¸­æ–‡å­—ç¬¦: 3559
  è‹±æ–‡å•è¯: 698
  æ€»å­—ç¬¦æ•°: 10074

create_a_model.md:
  ä¸­æ–‡å­—ç¬¦: 1988
  è‹±æ–‡å•è¯: 392
  æ€»å­—ç¬¦æ•°: 9356

custom_models.md:
  ä¸­æ–‡å­—ç¬¦: 2303
  è‹±æ–‡å•è¯: 484
  æ€»å­—ç¬¦æ•°: 9696

debugging.md:
  ä¸­æ–‡å­—ç¬¦: 1769
  è‹±æ–‡å•è¯: 564
  æ€»å­—ç¬¦æ•°: 9181

fast_tokenizers.md:
  ä¸­æ–‡å­—ç¬¦: 314
  è‹±æ–‡å•è¯: 194
  æ€»å­—ç¬¦æ•°: 2197

fsdp.md:
  ä¸­æ–‡å­—ç¬¦: 1142
  è‹±æ–‡å•è¯: 320
  æ€»å­—ç¬¦æ•°: 5080

generation_strategies.md:
  ä¸­æ–‡å­—ç¬¦: 2643
  è‹±æ–‡å•è¯: 780
  æ€»å­—ç¬¦æ•°: 12789

gguf.md:
  ä¸­æ–‡å­—ç¬¦: 540
  è‹±æ–‡å•è¯: 218
  æ€»å­—ç¬¦æ•°: 2659

hpo_train.md:
  ä¸­æ–‡å­—ç¬¦: 463
  è‹±æ–‡å•è¯: 254
  æ€»å­—ç¬¦æ•°: 4100

index.md:
  ä¸­æ–‡å­—ç¬¦: 537
  è‹±æ–‡å•è¯: 183
  æ€»å­—ç¬¦æ•°: 2059

installation.md:
  ä¸­æ–‡å­—ç¬¦: 1204
  è‹±æ–‡å•è¯: 418
  æ€»å­—ç¬¦æ•°: 5957

llm_tutorial.md:
  ä¸­æ–‡å­—ç¬¦: 1822
  è‹±æ–‡å•è¯: 787
  æ€»å­—ç¬¦æ•°: 10805

model_sharing.md:
  ä¸­æ–‡å­—ç¬¦: 1296
  è‹±æ–‡å•è¯: 300
  æ€»å­—ç¬¦æ•°: 4753

multilingual.md:
  ä¸­æ–‡å­—ç¬¦: 1114
  è‹±æ–‡å•è¯: 394
  æ€»å­—ç¬¦æ•°: 5822

peft.md:
  ä¸­æ–‡å­—ç¬¦: 790
  è‹±æ–‡å•è¯: 396
  æ€»å­—ç¬¦æ•°: 5733

performance.md:
  ä¸­æ–‡å­—ç¬¦: 818
  è‹±æ–‡å•è¯: 119
  æ€»å­—ç¬¦æ•°: 2062

perf_hardware.md:
  ä¸­æ–‡å­—ç¬¦: 1104
  è‹±æ–‡å•è¯: 404
  æ€»å­—ç¬¦æ•°: 5044

perf_infer_gpu_multi.md:
  ä¸­æ–‡å­—ç¬¦: 281
  è‹±æ–‡å•è¯: 213
  æ€»å­—ç¬¦æ•°: 2178

perf_torch_compile.md:
  ä¸­æ–‡å­—ç¬¦: 575
  è‹±æ–‡å•è¯: 1088
  æ€»å­—ç¬¦æ•°: 14930

perf_train_cpu.md:
  ä¸­æ–‡å­—ç¬¦: 530
  è‹±æ–‡å•è¯: 224
  æ€»å­—ç¬¦æ•°: 3085

perf_train_special.md:
  ä¸­æ–‡å­—ç¬¦: 429
  è‹±æ–‡å•è¯: 212
  æ€»å­—ç¬¦æ•°: 2333

philosophy.md:
  ä¸­æ–‡å­—ç¬¦: 1081
  è‹±æ–‡å•è¯: 189
  æ€»å­—ç¬¦æ•°: 2949

pipeline_tutorial.md:
  ä¸­æ–‡å­—ç¬¦: 1931
  è‹±æ–‡å•è¯: 888
  æ€»å­—ç¬¦æ•°: 10561

preprocessing.md:
  ä¸­æ–‡å­—ç¬¦: 2697
  è‹±æ–‡å•è¯: 873
  æ€»å­—ç¬¦æ•°: 16887

quicktour.md:
  ä¸­æ–‡å­—ç¬¦: 2831
  è‹±æ–‡å•è¯: 894
  æ€»å­—ç¬¦æ•°: 14799

run_scripts.md:
  ä¸­æ–‡å­—ç¬¦: 1291
  è‹±æ–‡å•è¯: 704
  æ€»å­—ç¬¦æ•°: 10260

serialization.md:
  ä¸­æ–‡å­—ç¬¦: 937
  è‹±æ–‡å•è¯: 396
  æ€»å­—ç¬¦æ•°: 5228

task_summary.md:
  ä¸­æ–‡å­—ç¬¦: 3582
  è‹±æ–‡å•è¯: 719
  æ€»å­—ç¬¦æ•°: 10982

tiktoken.md:
  ä¸­æ–‡å­—ç¬¦: 240
  è‹±æ–‡å•è¯: 206
  æ€»å­—ç¬¦æ•°: 2212

tokenizer_summary.md:
  ä¸­æ–‡å­—ç¬¦: 3960
  è‹±æ–‡å•è¯: 554
  æ€»å­—ç¬¦æ•°: 9391

training.md:
  ä¸­æ–‡å­—ç¬¦: 1556
  è‹±æ–‡å•è¯: 811
  æ€»å­—ç¬¦æ•°: 9645

audio_utils.md:
  ä¸­æ–‡å­—ç¬¦: 74
  è‹±æ–‡å•è¯: 134
  æ€»å­—ç¬¦æ•°: 1262

file_utils.md:
  ä¸­æ–‡å­—ç¬¦: 66
  è‹±æ–‡å•è¯: 139
  æ€»å­—ç¬¦æ•°: 1234

generation_utils.md:
  ä¸­æ–‡å­—ç¬¦: 416
  è‹±æ–‡å•è¯: 260
  æ€»å­—ç¬¦æ•°: 4413

image_processing_utils.md:
  ä¸­æ–‡å­—ç¬¦: 59
  è‹±æ–‡å•è¯: 134
  æ€»å­—ç¬¦æ•°: 1378

modeling_utils.md:
  ä¸­æ–‡å­—ç¬¦: 70
  è‹±æ–‡å•è¯: 119
  æ€»å­—ç¬¦æ•°: 987

pipelines_utils.md:
  ä¸­æ–‡å­—ç¬¦: 58
  è‹±æ–‡å•è¯: 140
  æ€»å­—ç¬¦æ•°: 1229

time_series_utils.md:
  ä¸­æ–‡å­—ç¬¦: 73
  è‹±æ–‡å•è¯: 122
  æ€»å­—ç¬¦æ•°: 983

tokenization_utils.md:
  ä¸­æ–‡å­—ç¬¦: 72
  è‹±æ–‡å•è¯: 134
  æ€»å­—ç¬¦æ•°: 1309

trainer_utils.md:
  ä¸­æ–‡å­—ç¬¦: 50
  è‹±æ–‡å•è¯: 130
  æ€»å­—ç¬¦æ•°: 1161

callback.md:
  ä¸­æ–‡å­—ç¬¦: 461
  è‹±æ–‡å•è¯: 330
  æ€»å­—ç¬¦æ•°: 3958

configuration.md:
  ä¸­æ–‡å­—ç¬¦: 98
  è‹±æ–‡å•è¯: 122
  æ€»å­—ç¬¦æ•°: 1056

data_collator.md:
  ä¸­æ–‡å­—ç¬¦: 124
  è‹±æ–‡å•è¯: 154
  æ€»å­—ç¬¦æ•°: 1923

deepspeed.md:
  ä¸­æ–‡å­—ç¬¦: 12756
  è‹±æ–‡å•è¯: 3070
  æ€»å­—ç¬¦æ•°: 60570

feature_extractor.md:
  ä¸­æ–‡å­—ç¬¦: 91
  è‹±æ–‡å•è¯: 133
  æ€»å­—ç¬¦æ•°: 1249

image_processor.md:
  ä¸­æ–‡å­—ç¬¦: 78
  è‹±æ–‡å•è¯: 128
  æ€»å­—ç¬¦æ•°: 1131

logging.md:
  ä¸­æ–‡å­—ç¬¦: 605
  è‹±æ–‡å•è¯: 227
  æ€»å­—ç¬¦æ•°: 3130

model.md:
  ä¸­æ–‡å­—ç¬¦: 938
  è‹±æ–‡å•è¯: 147
  æ€»å­—ç¬¦æ•°: 3100

optimizer_schedules.md:
  ä¸­æ–‡å­—ç¬¦: 56
  è‹±æ–‡å•è¯: 185
  æ€»å­—ç¬¦æ•°: 1870

output.md:
  ä¸­æ–‡å­—ç¬¦: 374
  è‹±æ–‡å•è¯: 264
  æ€»å­—ç¬¦æ•°: 4641

pipelines.md:
  ä¸­æ–‡å­—ç¬¦: 1234
  è‹±æ–‡å•è¯: 734
  æ€»å­—ç¬¦æ•°: 11109

processors.md:
  ä¸­æ–‡å­—ç¬¦: 692
  è‹±æ–‡å•è¯: 385
  æ€»å­—ç¬¦æ•°: 5011

quantization.md:
  ä¸­æ–‡å­—ç¬¦: 4077
  è‹±æ–‡å•è¯: 924
  æ€»å­—ç¬¦æ•°: 17776

text_generation.md:
  ä¸­æ–‡å­—ç¬¦: 157
  è‹±æ–‡å•è¯: 137
  æ€»å­—ç¬¦æ•°: 1345

tokenizer.md:
  ä¸­æ–‡å­—ç¬¦: 541
  è‹±æ–‡å•è¯: 179
  æ€»å­—ç¬¦æ•°: 2655

trainer.md:
  ä¸­æ–‡å­—ç¬¦: 5867
  è‹±æ–‡å•è¯: 1338
  æ€»å­—ç¬¦æ•°: 23923

bert.md:
  ä¸­æ–‡å­—ç¬¦: 247
  è‹±æ–‡å•è¯: 418
  æ€»å­—ç¬¦æ•°: 7245

asr.md:
  ä¸­æ–‡å­—ç¬¦: 1487
  è‹±æ–‡å•è¯: 719
  æ€»å­—ç¬¦æ•°: 11245

æ€»è®¡:
--------------------
æ–‡ä»¶æ€»æ•°: 66
æ€»ä¸­æ–‡å­—ç¬¦æ•°: 85762
æ€»è‹±æ–‡å•è¯æ•°: 30490
æ€»å­—ç¬¦æ•°: 460365
