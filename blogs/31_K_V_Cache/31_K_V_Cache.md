## 1. 背景：为什么需要 K V Cache？

**K V Cache 是专门用于 Transformer 的 Decoder（解码器）一侧的推理优化机制**。更准确地说，它是为了优化**自回归生成模型**（如 GPT、LLaMA 等 Decoder-only 架构）的**推理效率**而设计的。

在标准的 Transformer 自回归生成任务中（例如文本生成），模型会逐个生成 token。假设我们要生成一个长度为 $ L $ 的序列，模型会执行 $ L $ 次推理（forward passes）。

**如果不使用 K V Cache**：
- 每次生成第 $ t $ 个 token 时，模型需要处理从第 1 个到第 $ t $ 个 token 的整个输入序列。
- 自注意力机制的计算复杂度与序列长度的平方成正比（$ O(t^2) $）。
- 随着 $ t $ 增大，计算量和内存占用急剧增加，导致推理速度非常慢。

**K V Cache 的核心思想**：  

缓存前 $ t-1 $ 个 token 的 Key 和 Value 向量，在生成第 $ t $ 个 token 时，只计算当前新 token 的 Query、Key、Value，并复用之前缓存的 K 和 V。  这样，自注意力的计算量从 $ O(t^2) $ 降低到 $ O(t) $，大大加速了推理。

**KV Cache 的主要目的是为了加速推理（Inference），而不是训练（Training）。在训练阶段，通常不使用 KV Cache，或者其带来的加速效果非常有限，甚至可能因为额外的内存操作而拖慢速度。**

下面我们来详细解释其中的原因，以及 KV Cache 在训练和推理中扮演的不同角色。

### 1.1 核心概念：训练 vs. 推理

首先，我们需要明确深度学习模型生命周期的两个主要阶段：

- **训练（Training）：** 模型通过海量数据学习参数。在这个阶段，我们通常使用**批量（Batch）** 数据，并且利用**并行计算**（如 GPU 的矩阵乘法）来一次性计算整个序列的注意力，效率极高。
- **推理（Inference）：** 模型已经训练完成，用于实际生成内容（如聊天、写文章）。在自回归生成（Autoregressive Generation）中，我们是一个 token 一个 token 地生成，下一次的输入依赖于上一次的输出。

### 1.2 为什么 KV Cache 能加速推理？

在推理时，当我们使用标准的自回归生成（例如 GPT 生成文本）：

1. 输入是 `"我很喜欢"`，模型预测下一个 token 是 `"苹果"`。
2. 下一步的输入变成 `"我很喜欢苹果"`，模型预测下一个 token 是 `"。"`。

如果没有 KV Cache，在第二步计算 `"。"`的注意力时，模型需要重新计算 `"我"`、`"很"`、`"喜欢"`、`"苹果"`这些所有历史 token 的 Key 和 Value。这导致了巨大的计算冗余，因为历史 token 的 K 和 V 在第一步已经算过了。

**KV Cache 的作用**：它将之前所有时间步计算出的 Key 和 Value 张量**缓存**在内存中。当生成第 `t`个 token 时：

- 我们只需要计算**当前**新 token 的 Query。
- 从缓存中读取之前所有 `t-1`个 token 的 Key 和 Value。
- 将当前 Query 与缓存中的所有 Key 计算注意力分数，再与缓存中的所有 Value 加权求和。

这样，**避免了为每个新 token 重复计算历史 token 的 K 和 V**，从而将推理的计算复杂度从 O(n²) 降低到 O(n)（针对每个新生成的 token），极大地加速了长文本的生成。

### 1.3 为什么 KV Cache 通常不用于（全量）训练？

在标准的、全量的预训练（Pre-training）或微调（Fine-tuning）过程中，我们**不采用 KV Cache**，主要原因如下：

#### A. 并行计算的优势

训练时，我们一次性输入一个完整的序列（例如 2048 个 token 的批次）。Transformer 的自注意力机制可以利用**矩阵乘法**，一次性计算出整个序列中所有 token 之间的注意力。GPU 非常擅长这种大规模的并行计算，效率极高。如果使用 KV Cache 并逐个 token 地模拟自回归过程，我们就无法利用这种并行性，训练速度会变得极慢。

#### B. 内存瓶颈

KV Cache 需要存储所有历史 token 的 Key 和 Value。在训练中，批次（Batch Size）通常很大，序列长度也很长。如果为每个批次中的每个样本都维护一个独立的 KV Cache，**内存占用会爆炸式增长**。这比直接进行并行矩阵乘法所需的内存要大得多，很快就会导致 GPU 内存耗尽（OOM）。

#### C. 反向传播的复杂性

训练需要计算梯度并进行反向传播。如果使用 KV Cache 进行前向传播，那么反向传播路径将变得非常复杂，因为它需要穿过所有的缓存步骤。这会给计算图和自动微分带来巨大的负担，使得训练变得不稳定且难以优化。

### 1.4 特殊情况：训练时可能“类似”使用 KV Cache 的场景

虽然标准的全量训练不用，但在一些特定的训练技巧中，会用到与 KV Cache 思想类似的技术：

- **内存高效的注意力（Memory Efficient Attention）：** 像 FlashAttention 这样的算法，虽然其核心是优化 IO 效率，但在计算过程中，它也会将中间结果（可以类比为 K 和 V 的块）在 GPU 的 SRAM 中进行缓存和复用，以减少对全局内存的访问。这是一种更底层的优化，与推理时的 KV Cache 概念不同，但思想有相通之处。
- **验证（Validation）或 评估（Evaluation）：** 在训练过程中，我们会在验证集上评估模型的性能。如果验证时需要模型进行**自回归生成**（例如计算 BLEU 或 Rouge 分数），那么此时**会使用 KV Cache** 来加速生成过程。但这属于推理行为，而不是训练本身。

### 1.5 总结对比

| 特性              | 训练阶段 (Training)                                  | 推理阶段 (Inference)                                       |
| ----------------- | ---------------------------------------------------- | ---------------------------------------------------------- |
| **主要目标**      | 学习模型参数                                         | 利用训练好的模型生成内容                                   |
| **数据输入**      | 完整的批次数据，并行处理                             | 自回归，逐个 token 生成                                    |
| **注意力计算**    | 一次性并行计算整个序列的注意力                       | 逐步生成，依赖历史信息                                     |
| **KV Cache 作用** | **通常不使用**。并行计算已最优，缓存会导致内存爆炸。 | **核心加速技术**。避免重复计算历史 K/V，极大提升生成速度。 |
| **计算复杂度**    | 并行 O(n²)，但 GPU 效率高                            | 无缓存：O(n²) 每步；有缓存：O(n) 每步                      |

**结论：**

**KV Cache 是推理（Inference）的“杀手级”优化，但它并不是为训练（Training）设计的。** 试图在训练中使用 KV Cache 通常会导致速度变慢和内存不足。它们是针对模型生命周期中两个不同阶段的不同需求而采取的不同策略。

## 2. K V Cache 的原理

### 2.1 自注意力机制回顾
在 Transformer 的自注意力层中，对于输入序列 $ X \in \mathbb{R}^{t \times d_{\text{model}}} $，通过线性变换得到 Query、Key、Value 矩阵：
$$
Q = X W^Q, \quad K = X W^K, \quad V = X W^V
$$
注意力分数计算为：
$$
\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{Q K^T}{\sqrt{d_k}} \right) V
$$
其中 $ d_k $ 是 Key 的维度。

### 2.2 自回归生成的过程
假设我们要生成序列：$ [x_1, x_2, \dots, x_L] $。

1. **第一步（t=1）**：输入 $ x_1 $，计算 $ Q_1, K_1, V_1 $，注意力只依赖自身，得到输出 $ y_1 $。
2. **第二步（t=2）**：输入 $ x_1, x_2 $，需要重新计算 $ Q_{1:2}, K_{1:2}, V_{1:2} $，然后计算注意力。
3. **第三步（t=3）**：输入 $ x_1, x_2, x_3 $，重复上述过程……

可以看到，每次都要重新计算整个序列的 K 和 V，导致大量重复计算。

### 2.3 K V Cache 的工作方式
引入 K V Cache 后：

- 初始化时，缓存为空。
- 生成第 $ t $ 个 token 时：
  - 只计算当前新 token（$ x_t $）的 $ Q_t, K_t, V_t $。
  - 从缓存中读取前 $ t-1 $ 个 token 的 $ K_{1:t-1} $ 和 $ V_{1:t-1} $。
  - 将 $ K_t $ 和 $ V_t $ 追加到缓存中。
  - 注意力计算时，使用：
    $$
    K_{1:t} = [K_{1:t-1} \ (\text{cached}); \ K_t], \quad V_{1:t} = [V_{1:t-1} \ (\text{cached}); \ V_t]
    $$
    $$
    \text{Attention}(Q_t, K_{1:t}, V_{1:t}) = \text{softmax}\left( \frac{Q_t K_{1:t}^T}{\sqrt{d_k}} \right) V_{1:t}
    $$
  - 注意：这里的 $ Q_t $ 是当前 token 的 Query（形状为 $ 1 \times d_k $），而 $ K_{1:t} $ 是 $ t \times d_k $，所以 $ Q_t K_{1:t}^T $ 得到的是 $ 1 \times t $ 的注意力分数。

**关键点**：
- 每次只需计算新 token 的 K 和 V，计算量大幅减少。
- 缓存的大小随生成步数线性增长（$ O(t \cdot d_k) $），但避免了平方级的注意力计算。

## 3. K V Cache 的实现细节

### 3.1 缓存的数据结构
K V Cache 通常实现为两个张量：
- **key_cache**：形状为 `[num_layers, batch_size, seq_len, num_heads, head_dim]`
- **value_cache**：形状为 `[num_layers, batch_size, seq_len, num_heads, head_dim]`

在推理时，随着生成的进行，不断向这两个张量的 `seq_len` 维度追加新的 K 和 V。

**注意**：在批处理（batch inference）时，不同序列的生成长度可能不同，因此需要特殊处理（例如用掩码标记有效长度）。

### 3.2 实现步骤（伪代码）

```python

class TransformerWithKVCache:

    def __init__(self, model):
        self.model = model
        self.k_cache = []  # 按层存储
        self.v_cache = []  # 按层存储
        self.reset_cache()

    def reset_cache(self):
        # 清空缓存，通常在开始新序列时调用
        self.k_cache = [None] * self.model.num_layers
        self.v_cache = [None] * self.model.num_layers

    def generate_step(self, input_ids, layer_idx):
        # 输入: input_ids (batch_size, 1) 当前步的输入 token
        # 输出: logits (batch_size, vocab_size) 下一个 token 的概率分布

        # 1. 通过模型计算当前步的 hidden_states
        hidden_states = self.model.embed(input_ids)  # (batch_size, 1, d_model)

        for i in range(self.model.num_layers):
            # 2. 自注意力层
            # 计算当前步的 Q, K, V
            Q, K, V = self.model.layers[i].attention.project_qkv(hidden_states)

            # 3. 如果缓存不为空，则拼接
            if self.k_cache[i] is not None:
                K = torch.cat([self.k_cache[i], K], dim=1)  # 沿序列长度维度拼接
                V = torch.cat([self.v_cache[i], V], dim=1)

            # 4. 更新缓存
            self.k_cache[i] = K
            self.v_cache[i] = V

            # 5. 计算自注意力
            attn_output = self.model.layers[i].attention.forward(Q, K, V)

            # 6. 残差连接和前馈网络等后续处理...
            hidden_states = self.model.layers[i].ffn(attn_output)

        # 7. 输出投影得到 logits
        logits = self.model.lm_head(hidden_states)
        return logits
```

### 3.3 实际框架中的优化
在实际的深度学习框架（如 PyTorch、TensorFlow、JAX）中，K V Cache 的实现会进行更多优化：

1. **预分配内存**：为了避免频繁的内存重分配，通常会预先分配一个足够大的缓存空间，然后用掩码控制有效部分。
2. **内存布局优化**：将 K 和 V 的存储顺序调整为对硬件友好（例如使用连续内存）。
3. **批处理优化**：支持不同序列的并行生成，处理不同长度的缓存。
4. **量化**：为了减少内存占用，有时会对 K V Cache 进行量化（如 int8 量化）。

## 4. K V Cache 内存占用分析

**对于长序列，如果不使用K V Cache，在计算最后几个 token 时占用的峰值内存也通常远大于使用 K V Cache 时的内存占用。**

原因在于**计算图（Computational Graph）的依赖关系**和**中间激活（Activations）的存储**。

我们来详细分析一下这两种情况的内存占用模型。

### 4.1 不使用 K V Cache 时的内存占用分析

当**不使用 K V Cache** 时，生成第 $ t $ 个 token 的过程是：
- 输入：整个序列 $ x_1, x_2, \dots, x_t $。
- 计算：从头开始进行完整的前向传播。

此时，内存占用主要由两部分组成：
1.  **模型参数（Parameters）**：恒定大小，与序列长度无关。
2.  **中间激活（Activations）**：这是内存占用的大头。

##### 4.1.1 中间激活的存储
在深度学习框架（如 PyTorch、TensorFlow）中，为了支持反向传播（训练时）或仅仅为了完成复杂的前向计算（推理时），系统需要存储计算图中的**中间结果**（即激活值）。

对于 Transformer 模型，主要的内存消耗点包括：
- **自注意力计算中的 $ QK^T $ 矩阵**：形状为 $ [t, t] $，占用 $ O(t^2) $ 的空间。**这是最恐怖的部分**。
- 每一层的输入和输出激活：形状为 $ [t, d_{\text{model}}] $，占用 $ O(t) $ 的空间。
- 前馈网络（FFN）内部的中间结果。

**关键结论**：
- 即使你只关心最后一个 token 的输出，**计算过程本身**需要访问整个序列的所有 token，并产生巨大的中间矩阵（如 $ t \times t $ 的注意力分数矩阵）。
- 这些中间结果必须在内存中保留，直到计算完成。

##### 4.1.2 峰值内存分析
假设我们生成到第 $ L $ 个 token：
- 在计算第 $ L $ 个 token 时，输入序列长度为 $ L $。
- 此时，**注意力分数的矩阵**大小为 $ L \times L $。对于 $ L = 2048 $，这是一个 2048x2048 的浮点矩阵，仅这一项就占用约 `2048 * 2048 * 4 bytes ≈ 16 MB`（对于 float32）。这还只是**一层**的注意力分数。
- 如果模型有 32 层，并且所有中间结果都需要保存，这个内存占用会成倍增加。

**因此，不使用 K V Cache 时，峰值内存是 $ O(L^2) $ 级别的，并且发生在生成最后一个 token 的时候。**

### 4.2. 使用 K V Cache 时的内存占用分析

使用 **K V Cache** 后，生成第 $ t $ 个 token 的过程是：
- 输入：只有当前 token $ x_t $。
- 计算：只计算 $ Q_t $，并从缓存中读取 $ K_{1:t} $ 和 $ V_{1:t} $ 来计算注意力。

此时的内存占用也由两部分组成：
1.  **模型参数**：恒定。
2.  **K V Cache 本身**：存储了所有历史 token 的 Key 和 Value。形状为 $ [t, d_{\text{model}}] $（对于所有层和头），占用 $ O(t) $ 的空间。
3.  **中间激活**：由于输入序列长度在“逻辑上”是 $ t $，但在“计算上”只新增了一个 token，所以中间激活的大小被极大地压缩了。

##### 4.2.1 中间激活的优化
- **注意力计算**：我们不再需要计算和存储一个巨大的 $ t \times t $ 矩阵。我们只计算 $ Q_t $（形状为 $ [1, d_k] $）和 $ K_{1:t} $（形状为 $ [t, d_k] $）的点积，得到一个 $ [1, t] $ 的向量，然后做 softmax，再与 $ V_{1:t} $ 相乘。这个过程中，最大的中间张量是 $ [1, t] $ 的注意力权重，而不是 $ [t, t] $ 的矩阵。
- **其他层**：由于每一层的输入在序列维度上只有 1（当前 token），所以该层的所有中间激活（如 FFN 的输出）的形状都是 $ [1, d_{\text{model}}] $，而不是 $ [t, d_{\text{model}}] $。

**关键结论**：
- 使用 K V Cache 时，**中间激活的内存占用是常数 $ O(1) $**（相对于序列长度），因为它只与当前这一个 token 的计算有关。
- 总内存占用主要由 **K V Cache** 主导，而 K V Cache 的大小是 $ O(t) $ 的。

### 4.3. 内存占用对比

我们可以用一个简单的公式来对比两种方案的峰值内存（以最后一个 token 的计算为例）：

| 方案             | 模型参数 | K V Cache | 中间激活（峰值） | **总复杂度**   |
| :--------------- | :------- | :-------- | :--------------- | :------------- |
| **无 K V Cache** | $ O(1) $ | 0         | $ O(L^2) $       | **$ O(L^2) $** |
| **有 K V Cache** | $ O(1) $ | $ O(L) $  | $ O(1) $         | **$ O(L) $**   |

**举例说明**：
假设 $ L = 2048 $，模型隐藏维度 $ d_{\text{model}} = 4096 $，层数 $ N=32 $，注意力头数 $ H=32 $。
- **K V Cache 大小**：大约为 $ 2 \times N \times L \times d_{\text{model}} \times 4 $ 字节。代入数值约为 $ 2 \times 32 \times 2048 \times 4096 \times 4 \approx 2 \ \text{GB} $。
- **无 Cache 时的注意力分数矩阵**：仅一层的 $ QK^T $ 矩阵（float32）就占用 $ 2048^2 \times 4 \approx 16 \ \text{MB} $。32 层加起来，再加上其他中间激活，很容易达到数 GB 甚至数十 GB。

**结论**：当序列长度 $ L $ 较大时，平方项 $ L^2 $ 的增长速度远快于线性项 $ L $。因此，**不使用 K V Cache 时的峰值内存（主要由巨大的中间激活矩阵导致）会远远超过使用 K V Cache 时的内存（线性增长的缓存 + 很小的常数激活）**。

### 4.4. 总结

- **不使用 K V Cache**：虽然你只想要最后一个 token 的结果，但**计算过程**需要将整个序列通过所有层，产生并存储巨大的中间矩阵（如 $ L \times L $ 的注意力分数）。这导致峰值内存呈 **平方级 $ O(L^2) $** 增长。
- **使用 K V Cache**：计算过程被优化，只处理当前 token，中间激活很小且恒定。内存占用主要由存储历史信息的 **K V Cache** 主导，呈 **线性级 $ O(L) $** 增长。

因此，**K V Cache 不仅在计算速度上实现了从 $ O(L^2) $ 到 $ O(L) $ 的优化，在内存占用上也实现了从平方级到线性级的巨大优化**。这是它能成为推理加速核心技术的根本原因。

## 5. K V Cache 的优缺点

### 5.1 优点

- **大幅加速推理**：将自回归生成的计算复杂度从 $ O(L^2) $ 降低到 $ O(L) $。
- **内存访问优化**：减少了重复计算，提高了计算效率。

### 5.2 缺点

- **内存占用增加**：缓存需要存储所有历史 token 的 K 和 V，内存占用与序列长度成正比。对于长序列生成，这可能成为瓶颈。

  - **如果不做任何优化**（即不使用 K V Cache，也不做任何计算图重写），那么推理过程会产生$ O(L^2) $的中间激活，这是不可接受的。
  - **K V Cache 是一种计算图优化技术**。它通过重构计算过程，将必须存储在内存中的数据从巨大的、平方级的**中间激活**，转换成了小得多的、线性级的**缓存（K 和 V）**，**极大地降低了峰值内存**。

  **所谓的“内存占用增加”，是指我们主动维持了一份缓存，这份缓存是“持久性”的，它会随着生成的进行一直增长。** 但在不采用 K V Cache 的原始方案中，内存中充斥着大量“临时性”的、计算完就可以丢弃的中间结果，只是这些临时结果的体积太大了。

- **实现复杂性**：需要仔细管理缓存的生命周期、批处理中的不同序列长度等。

K V Cache 不是完美的，它用**可控的、线性增长的内存瓶颈**，替换了**不可控的、平方级爆炸的内存灾难**。在长序列生成中，这个线性增长的内存需求确实会成为限制因素，但这已经是在现有硬件和算法框架下所能达到的最佳权衡之一。

## 6. 扩展：多查询注意力（MQA）和分组查询注意力（GQA）

为了进一步减少 K V Cache 的内存占用，一些模型（如 LLaMA 2、PaLM）使用了 **MQA** 或 **GQA**：

- **MQA（Multi-Query Attention）**：多个 Query 头共享同一组 Key 和 Value 头。这样 K 和 V 的缓存大小减少为原来的 $ 1/\text{num\_heads} $。
- **GQA（Grouped-Query Attention）**：折中方案，将 Query 头分成若干组，每组共享 Key 和 Value 头。

这些方法在保持模型质量的同时，显著降低了 K V Cache 的内存占用。

## 7. 结论

K V Cache 是 Transformer 自回归生成任务中至关重要的推理优化技术，其核心思想是**避免重复计算历史 token 的 Key 和 Value**。通过缓存和复用，将计算复杂度从平方级降为线性级，实现了推理速度的成倍提升。

在实际应用中，需要权衡缓存的内存占用和计算效率，并结合 MQA/GQA 等进一步优化技术，以支持更长的序列生成和更高的吞吐量。