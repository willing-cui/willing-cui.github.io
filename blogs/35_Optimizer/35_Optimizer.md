深度学习优化器通过反向传播的梯度更新模型参数，以最小化损失函数。不同优化器在收敛速度、稳定性和泛化能力上各有侧重，选择时需结合任务特点。

<span class="image main">
<img class="main img-in-blog" style="max-width: 50%" src="./blogs/35_Optimizer/Gradient.webp" alt="Gradient" />
<i>Gradient. By <a href="//commons.wikimedia.org/wiki/User:MartinThoma" title="User:MartinThoma">MartinThoma</a> - <span class="int-own-work" lang="en">Own work</span>, <a href="http://creativecommons.org/publicdomain/zero/1.0/deed.en" title="Creative Commons Zero, Public Domain Dedication">CC0</a>, <a href="https://commons.wikimedia.org/w/index.php?curid=71375503">Link</a></i>
</span> 

## 1. 优化器对比

### 总览

| 优化器           | 核心思想                                                     | 优点                                        | 缺点                                            | 适用场景                                                |
| ---------------- | ------------------------------------------------------------ | ------------------------------------------- | ----------------------------------------------- | ------------------------------------------------------- |
| **SGD**          | 沿当前批次梯度方向更新参数。                                 | 实现简单、内存占用低、泛化能力强。          | 收敛慢、易震荡、对学习率敏感。                  | 小规模模型、数据量充足、追求高泛化精度的任务。          |
| **SGD+Momentum** | 引入动量项，累积历史梯度以平滑更新方向。                     | 减少震荡、加速收敛、有助于穿越平坦区域。    | 需手动调整动量因子和学习率。                    | 损失函数曲面复杂、存在沟壑和震荡的场景（如CV任务）。    |
| **AdaGrad**      | 为每个参数自适应调整学习率，学习率与历史梯度平方和成反比。   | 适合稀疏数据、自动调整学习率。              | 学习率过早衰减、内存占用高。                    | 稀疏数据任务（如NLP、推荐系统）。                       |
| **RMSProp**      | 改进AdaGrad，使用指数移动平均替代累积和，避免学习率过早衰减。 | 解决AdaGrad学习率衰减问题、适合非平稳目标。 | 仍需手动设置初始学习率、缺乏动量机制。          | 非平稳目标函数、RNN等序列模型。                         |
| **Adam**         | 结合Momentum和RMSProp，同时使用梯度的一阶矩和二阶矩。        | 收敛速度快、对超参数不敏感、适用范围广。    | 内存占用略高、在某些任务上泛化能力可能不如SGD。 | 绝大多数深度学习任务（如NLP、CV、RL）。                 |
| **AdamW**        | 在Adam基础上，将权重衰减与梯度更新解耦。                     | 泛化性能通常优于Adam。                      | 计算复杂度略高。                                | 需要良好泛化性能、强正则化需求的场景（如Transformer）。 |
| **Adadelta**     | 无需手动设置初始学习率，通过动态调整步长实现自适应。         | 无需初始学习率、学习率自适应更灵活。        | 超参数调节复杂、对某些任务效果不稳定。          | 稀疏或稠密数据、需自动调整学习率的场景。                |

### 优化器特性总结

| 特性       | SGD  | SGD+M | AdaGrad | RMSProp | Adam | AdamW |
| ---------- | ---- | ----- | ------- | ------- | ---- | ----- |
| 收敛速度   | 慢   | 中等  | 早期快  | 中等    | 快   | 快    |
| 内存占用   | 低   | 低    | 高      | 中等    | 中等 | 中等  |
| 超参数数   | 少   | 中    | 中      | 中      | 中   | 中    |
| 稀疏数据   | 差   | 差    | 优      | 良      | 良   | 良    |
| 非平稳目标 | 差   | 中    | 差      | 优      | 优   | 优    |
| 理论保证   | 强   | 强    | 弱      | 弱      | 弱   | 弱    |
| 调参难度   | 高   | 高    | 中      | 中      | 低   | 低    |

## 2. 优化器的作用与核心思想

优化器是深度学习训练的核心引擎，其目标是在高维参数空间中，通过反向传播计算的梯度信息，迭代更新模型参数 $\theta$ 以最小化损失函数 $L(\theta)$。

### 基本更新公式
$$
\theta\_{t+1} = \theta\_t - \eta \cdot \nabla L(\theta\_t)
$$
其中 $\eta$ 是学习率，$\nabla L(\theta\_t)$ 是损失函数关于参数的梯度。

优化器主要分为两类：
- **一阶优化器**：仅使用梯度的一阶矩（方向）信息，如 SGD、Momentum
- **自适应优化器**：为每个参数动态调整学习率，使用梯度的一阶和/或二阶矩信息

## 分类1：一阶优化器

### 1. SGD（随机梯度下降）

#### 数学原理

$$
\theta\_{t+1} = \theta\_t - \eta \cdot g\_t
$$
其中 $g\_t = \nabla L(\theta\_t)$ 是当前小批量的梯度。

**核心特点**：
- 每次更新仅使用当前批次的梯度信息
- 没有动量或历史信息的累积

#### 总结

**优点**：

1. 实现简单，计算开销小
2. 在凸优化问题上理论保证收敛
3. 在**泛化性能**上有时优于复杂优化器
4. 内存占用极低

**缺点**：
1. 收敛速度慢，特别是在平坦区域
2. 容易陷入**局部极小点或鞍点**
3. 对学习率 $\eta$ 高度敏感
4. 在非凸优化中表现不稳定
5. 梯度噪声大，更新方向震荡

**适用场景**：
- 小规模模型训练
- 数据量充足，可进行多轮训练
- 对模型最终精度要求极高的任务
- 需要良好泛化能力的场景

**调参建议**：
- 通常需要配合学习率衰减策略
- 初始学习率一般设置在 $0.01$ 到 $0.1$ 之间
- **批量大小不宜过小**，以减少梯度噪声

### 2. SGD with Momentum（带动量的SGD）

#### 数学原理

$$
\begin{aligned}
v\_{t+1} &= \gamma v\_t + \eta \cdot g\_t \\\\
\theta\_{t+1} &= \theta\_t - v\_{t+1}
\end{aligned}
$$
其中 $\gamma$ 是动量系数（通常取 $0.9$），$v\_t$ 是累积速度。

#### 物理意义

- 将参数更新看作物体在损失曲面上的运动
- 动量项 $\gamma v\_t$ 模拟物理中的**惯性**
- 帮助优化器**穿越平坦区域**和**狭窄峡谷**

#### 总结

**优点**：

1. 加速收敛，减少震荡
2. 有助于**逃离局部极小点和鞍点**
3. 更新方向更稳定
4. 在存在大量局部最优的问题上表现更好

**缺点**：

1. 引入额外的超参数 $\gamma$
2. 在最小点附近可能产生振荡
3. 动量过大可能导致错过最优解

**适用场景**：

- 损失函数曲面复杂、不平滑
- 存在大量局部最优的问题
- 高维非凸优化问题
- 计算机视觉中的卷积神经网络

**动量系数 $\gamma$ 的影响**：
- $\gamma=0$：退化为普通SGD
- $\gamma≈0.9$：标准配置
- $\gamma>0.99$：**强动量**，适合非常平坦的区域
- 实际中常使用 $\gamma=0.9$ 或 $0.99$

#### SGD提高模型的泛化能力？

随机梯度下降（SGD）优化器之所以被认为有助于提高模型的泛化能力，主要有以下几个关键机制：

1. **引入随机噪声（Stochastic Noise）**： **机制**：与批量梯度下降（BGD）使用整个训练集计算梯度不同，SGD每次只使用一个或一小批（mini-batch）样本来估计梯度。这种基于小批量的梯度计算本质上是真实梯度的一个**带噪声的估计**。 **作用**：这种噪声相当于一种隐式的**正则化**。在训练过程中，噪声会阻止优化算法精确地收敛到损失函数的某个尖锐的局部最小值或鞍点。相反，它倾向于使参数在平坦的极小值区域“徘徊”。 **泛化优势**：在深度学习中，**平坦的极小值（Flat Minima）** 通常被认为比**尖锐的极小值（Sharp Minima）** 具有更好的泛化能力。因为平坦区域对参数的小扰动不敏感，模型在未见过的测试数据上表现更稳健；而尖锐极小值虽然训练损失可能很低，但容易过拟合，对数据变化敏感。
2. **逃离局部极小值**： **机制**：由于SGD的梯度估计带有噪声，它具有一定的“随机波动”能力。 **作用**：这使得SGD有机会跳出一些泛化能力较差的局部极小值（可能是过拟合点），并有更高概率最终收敛到一个泛化能力更好的局部极小值或全局极小值附近。
3. **频繁的权重更新**： **机制**：SGD对每个批次都会进行一次权重更新，更新频率远高于BGD。 **作用**：这种频繁的更新使得优化路径更加“嘈杂”和探索性，有助于模型探索参数空间的不同区域，而不是直接沿着最陡下降方向快速收敛到最近的极小点。这种探索性有助于找到更稳健的解。
4. **与早停法（Early Stopping）的协同效应**： SGD通常需要更多次的迭代才能收敛。这使得我们可以更早地观察到验证集误差开始上升的拐点，从而有效地实施**早停法**。早停法本身就是一种强大的正则化技术，能防止模型在训练集上过度拟合。

**总结与对比**：

- **自适应优化器（如Adam, RMSprop）**：通常收敛速度更快，在训练初期能迅速降低损失。但它们有时会因自适应地调整每个参数的学习率，导致“过度适应”训练数据，从而收敛到泛化能力稍差的尖锐极小值。
- **SGD（通常带动量）**：虽然收敛速度可能较慢，但由于其固有的噪声和探索性，往往能找到更平坦的极小值，从而在测试集上获得更好的最终性能。这也是为什么在许多顶尖的计算机视觉和自然语言处理模型中，研究者们仍然倾向于使用SGD（带动量）而不是Adam来获得最佳泛化能力的原因。

**注意**：SGD的泛化优势并非绝对，它高度依赖于具体任务、模型架构、超参数（特别是学习率和批量大小）的设置。但在许多深度学习实践中，SGD（**特别是带动量的SGD**）确实被证明是获得最佳泛化性能的可靠选择。

## 分类2：自适应学习率优化器

### 3. AdaGrad（自适应梯度）

AdaGrad 的核心思想在于其“**参数级自适应学习率**”机制，这可以从其数学公式中清晰地推导出来。

#### 数学原理

对于第 $i$ 个参数 $\theta\_i$，在第 $t$ 次迭代时的更新公式为：

1.  **计算梯度**：$g\_{t,i} = \nabla\_{\theta\_i} L(\theta\_t)$
2.  **累积梯度平方**：$G\_{t,ii} = G\_{t-1,ii} + g\_{t,i}^2$
3.  **更新参数**：$\theta\_{t+1,i} = \theta\_{t,i} - \frac{\eta}{\sqrt{G\_{t,ii} + \epsilon}} \cdot g\_{t,i}$

其中，$\eta$ 是全局学习率，$\epsilon$ 是一个防止除零的小常数（如 $10^{-8}$）。

#### 核心思想

- **为每个参数独立调整学习率**：从更新公式可以看出，每个参数 $\theta\_i$ 都有自己独立的累积项 $G\_{t,ii}$。因此，每个参数的实际学习率是：
  $$
  \text{实际学习率} = \frac{\eta}{\sqrt{G\_{t,ii} + \epsilon}}
  $$

  由于 $G\_{t,ii}$ 是每个参数独有的，所以每个参数的学习率都是独立计算的，实现了“参数级”的自适应调整。

- **频繁更新的参数获得较小的学习率**：如果一个参数 $\theta\_i$ 频繁更新，意味着在多数迭代中 $g\_{t,i} \neq 0$。根据累积公式 $G\_{t,ii} = \sum\_{k=1}^t g\_{k,i}^2$，其累积值 $G\_{t,ii}$ 会随着迭代次数 $t$ 快速增大。

  由于 $G\_{t,ii}$ 位于分母，其增大将导致该参数的实际学习率 $\frac{\eta}{\sqrt{G\_{t,ii}}}$ 变小。因此，频繁更新的参数会自动获得较小的学习率，更新步伐更谨慎。

- **稀疏更新的参数获得较大的学习率**：相反，如果一个参数 $\theta\_i$ 对应稀疏特征，更新次数很少，那么其累积值 $G\_{t,ii}$ 的增长会非常缓慢。

  由于 $G\_{t,ii}$ 较小，该参数的实际学习率 $\frac{\eta}{\sqrt{G\_{t,ii}}}$ 会相对较大。当该参数偶尔出现梯度时，就能以较大的步长进行更新，从而加速学习。

#### 总结

**优点**：

1. 自动适应不同参数的更新频率

   - **早期训练**：所有参数学习率较大，快速学习

   - **中期训练**：频繁参数学习率衰减，稳定收敛

   - **后期训练**：稀疏参数仍有较大学习率，继续学习

2. 适合稀疏数据和稀疏梯度

3. 减少了手动调整学习率的需求

**缺点**：
1. 学习率单调递减，后期可能停止更新
   - $G\_{t,ii}$ 是单调递增的，导致 $\eta\_{t,i}$ 单调递减
   - 随着训练进行，所有参数的学习率都会趋向于零
   - 这在训练后期可能导致收敛过慢或提前停止
2. 需要**存储所有参数的梯度平方和**，内存占用大
3. 对初始学习率 $\eta$ 依然敏感
4. 累积梯度平方和可能导致数值问题

**适用场景**：
- 自然语言处理任务
- 推荐系统中的稀疏特征
- 具有不同更新频率的参数
- 文本分类、词向量训练

**改进变体**：
- AdaGrad存在的缺点引出了后续改进算法如 RMSProp 和 Adam，它们通过使用指数移动平均而不是累加和来解决这个问题：
  
  $$
  E[g^2]\_t = \beta E[g^2]\_{t-1} + (1-\beta) g\_t^2
  $$

  这样，$E[g^2]\_t$ 不会无限增长，学习率不会持续衰减到零。

### 4. RMSProp（均方根传播）

#### 数学原理

$$
\begin{aligned}
E[g^2]\_t &= \beta E[g^2]\_{t-1} + (1-\beta) g\_t^2 \\\\
\theta\_{t+1} &= \theta\_t - \frac{\eta}{\sqrt{E[g^2]\_t + \epsilon}} \odot g\_t
\end{aligned}
$$
其中 $\beta$ 是**衰减率**（通常取 $0.9$ 或 $0.99$）。

#### 总结

**核心改进**：

- 使用**指数移动平均**代替累加和
- 避免学习率过早衰减
- 能够处理非平稳目标函数

**优点**：
1. 解决AdaGrad学习率衰减问题
2. 适合在线学习和非平稳问题
3. 在循环神经网络中表现良好
4. 内存效率优于AdaGrad

**缺点**：
1. 仍然需要手动设置初始学习率
2. 缺乏动量机制，收敛可能不够平滑
3. 对超参数 $\beta$ 敏感

**适用场景**：
- 循环神经网络（RNN、LSTM）
- 非平稳目标函数
- 在线学习任务
- 语音识别、时间序列预测

**参数设置**：
- $\beta$ 通常取 $0.9$、$0.99$ 或 $0.999$
- 学习率 $\eta$ 通常比SGD小一个数量级
- $\epsilon$ 通常设为 $10^{-8}$

### 5. Adam（自适应矩估计）

Adam（Adaptive Moment Estimation）优化器结合了Momentum和RMSProp的优点，是目前深度学习中最常用的优化器之一。下面是其数学公式的详细拆解和解释。

Adam 的完整更新过程如下：

$$
\begin{aligned}
m\_t &= \beta\_1 m\_{t-1} + (1-\beta\_1) g\_t \\\\
v\_t &= \beta\_2 v\_{t-1} + (1-\beta\_2) g\_t^2 \\\\
\hat{m}\_t &= \frac{m\_t}{1-\beta\_1^t} \\\\
\hat{v}\_t &= \frac{v\_t}{1-\beta\_2^t} \\\\
\theta\_{t+1} &= \theta\_t - \frac{\eta}{\sqrt{\hat{v}\_t} + \epsilon} \hat{m}\_t
\end{aligned}
$$

其中：
*   $g\_t = \nabla\_\theta L(\theta\_t)$ 是当前小批量的梯度
*   $m\_t$ 是梯度的一阶矩（即 **期望值**，均值，动量）估计
*   $v\_t$ 是梯度的二阶矩（即 **变量平方的期望值**，未中心化方差）估计
*   $\hat{m}\_t$ 和 $\hat{v}\_t$ 是偏差校正后的一阶和二阶矩
*   $\beta\_1, \beta\_2$ 是衰减率（通常 $\beta\_1=0.9, \beta\_2=0.999$）
*   $\eta$ 是学习率
*   $\epsilon$ 是小常数（通常 $10^{-8}$）防止除零

#### 公式逐行解析

##### 1. 一阶矩估计（动量项）

$$
m\_t = \beta\_1 m\_{t-1} + (1-\beta\_1) g\_t
$$

*   **物理意义**：这是梯度的**指数移动平均**，相当于给梯度增加了动量。$\beta\_1$ 控制历史信息的衰减速度，$\beta\_1$ 越大，历史信息保留越多。
*   **展开形式**：
    $$
    m\_t = (1-\beta\_1)\sum\_{i=1}^t \beta\_1^{t-i} g\_i
    $$
*   **作用**：平滑梯度方向，减少震荡，帮助穿越平坦区域和逃离局部极小点。

**示例**：
假设 $\beta\_1 = 0.9$，当前梯度 $g\_t = 1.0$，上一时刻 $m\_{t-1} = 0.5$，则：
$$
m\_t = 0.9 \times 0.5 + 0.1 \times 1.0 = 0.45 + 0.1 = 0.55
$$
可以看到，$m\_t$ 是历史梯度与当前梯度的**加权平均**。

##### 2. 二阶矩估计（自适应学习率项）

$$
v\_t = \beta\_2 v\_{t-1} + (1-\beta\_2) g\_t^2
$$

*   **物理意义**：这是**梯度平方**的指数移动平均，用于估计梯度的二阶矩（方差）。
*   **展开形式**：
    $$
    v\_t = (1-\beta\_2)\sum\_{i=1}^t \beta\_2^{t-i} g\_i^2
    $$
*   **作用**：为每个参数计算自适应学习率。梯度大的方向 $v\_t$ 大，导致 $\frac{1}{\sqrt{v\_t}}$ 小，从而学习率小；梯度小的方向则相反。

**物理意义**：$v\_t$ 衡量了梯度大小的历史变化趋势。如果某个参数的梯度一直很大，$v\_t$ 就大，我们就会用较小的学习率来更新它，**避免震荡**。

##### 3. 偏差校正（Bias Correction）

$$
\hat{m}\_t = \frac{m\_t}{1-\beta\_1^t}, \quad \hat{v}\_t = \frac{v\_t}{1-\beta\_2^t}
$$

*   **问题来源**：在训练初期（$t$ 较小时），$m\_0$ 和 $v\_0$ 通常初始化为 0，这会导致估计值偏向 0。
*   **数学推导**：
    假设梯度是平稳的，$E[g\_t] = \mu$，则：
    $$
    E[m\_t] = E\left[(1-\beta\_1)\sum\_{i=1}^t \beta\_1^{t-i} g\_i\right] = \mu(1-\beta\_1^t)
    $$
    所以 $E[m\_t] = E[g\_t](1-\beta\_1^t)$，为了得到无偏估计，需要除以 $1-\beta\_1^t$。
*   **作用**：确保训练初期的更新步长不会过小，加速早期收敛。

**示例**：
当 $t=1$ 时，$\beta\_1=0.9$，则 $1-\beta\_1^1 = 0.1$，校正因子为 $\frac{1}{0.1}=10$，这放大了早期梯度的影响。

##### 4. 参数更新

$$
\theta\_{t+1} = \theta\_t - \frac{\eta}{\sqrt{\hat{v}\_t} + \epsilon} \hat{m}\_t
$$

*   **组合结构**：
    *   $\hat{m}\_t$：提供**平滑的梯度方向**（类似 Momentum）
    *   $\frac{\eta}{\sqrt{\hat{v}\_t} + \epsilon}$：提供**自适应学习率**（类似 RMSProp）
*   **特点**：每个参数有自己的学习率，且结合了动量机制。

**物理意义**：更新公式可以看作两个部分的组合：
1. **方向**：由 $\hat{m}\_t$ 决定，这是历史梯度的加权平均，方向更稳定
2. **步长**：由 $\frac{\eta}{\sqrt{\hat{v}\_t}}$ 决定，自动根据梯度大小调整

#### 加入L2正则化项

在实际应用中，为了**控制模型复杂度、防止过拟合**，通常会在损失函数中加入L2正则化项，**这也后续是AdamW优化的关键点**。此时，损失函数变为：

$$
L\_{\text{total}}(\theta) = L(\theta) + \frac{\lambda}{2} \|\theta\|\_2^2
$$

> 为什么**加入L2正则化项能控制模型复杂度、防止过拟合**？
>
> L2正则化在损失函数中增加了一项 $\frac{\lambda}{2} \|\theta\|\_2^2$，其中 $\lambda$​ 是正则化系数。这相当于在优化过程中对模型参数施加了"惩罚"。其效果是：
>
> - **参数收缩**：在每次参数更新时，会倾向于将参数**向0收缩**，即 $\theta \rightarrow (1−\eta \lambda)\theta$。这相当于对参数施加了衰减，使参数值变小。
> - **抑制过拟合**：过拟合通常表现为模型对训练数据中的噪声和细节过于敏感。通过将参数值变小，模型对输入特征的变化变得不那么敏感，从而降低了模型对训练数据中噪声的拟合能力，提高了泛化性能。

其中 $\lambda$ 是正则化系数。对损失函数求导，梯度变为：
$$
g\_t = \nabla\_\theta L(\theta\_t) + \lambda \theta\_t
$$

将 $g\_t$ 代入原始Adam公式，得到带L2正则化的Adam更新公式：

$$
\begin{aligned}
m\_t &= \beta\_1 m\_{t-1} + (1-\beta\_1) (\nabla\_\theta L(\theta\_t) + \lambda \theta\_t) \\\\
v\_t &= \beta\_2 v\_{t-1} + (1-\beta\_2) (\nabla\_\theta L(\theta\_t) + \lambda \theta\_t)^2 \\\\
\hat{m}\_t &= \frac{m\_t}{1-\beta\_1^t} \\\\
\hat{v}\_t &= \frac{v\_t}{1-\beta\_2^t} \\\\
\theta\_{t+1} &= \theta\_t - \frac{\eta}{\sqrt{\hat{v}\_t} + \epsilon} \hat{m}\_t
\end{aligned}
$$

#### 参数设置指南

| 参数 | 推荐值 | 作用 | 影响 | 调整建议 |
|------|--------|------|------|----------|
| $\beta\_1$ | 0.9 | 一阶矩衰减率 | 越大，动量越强，震荡越小 | 通常固定为 0.9 |
| $\beta\_2$ | 0.999 | 二阶矩衰减率 | 越大，自适应学习率越平滑 | 通常固定为 0.999 |
| $\eta$ | 0.001 | 学习率 | 控制整体更新步长 | 在 0.0001 到 0.01 之间调整 |
| $\epsilon$ | $10^{-8}$ | 数值稳定常数 | 防止除零 | 通常不需调整 |

#### 总结

**核心特性**：

- 结合了Momentum（一阶矩）和RMSProp（二阶矩）
- 对梯度进行偏差校正
- 自适应学习率 + 动量

**优点**：
1. 收敛速度快，尤其在前几轮
2. 对超参数相对鲁棒
3. 几乎不需要调参就能获得不错结果
4. 适合大规模数据和参数
5. 内存效率高（只需存储一阶和二阶矩）

**缺点**：
1. 泛化能力有时不如SGD
2. 可能收敛到尖锐的极小点
3. 在有些任务上表现不稳定
4. 需要更多的内存（相比SGD）

**适用场景**：
- 绝大多数深度学习任务
- 计算机视觉（CNN）
- 自然语言处理（Transformer）
- 强化学习
- 大规模预训练模型

### 6. AdamW（解耦权重衰减的Adam）

AdamW（Adam with Weight Decay）是 Adam 优化器的重要改进版本，它通过将权重衰减与梯度更新解耦，在多种深度学习任务中实现了更好的泛化性能。

####  AdamW 更新公式详解

AdamW 的完整更新过程如下：

$$
\begin{aligned}
m\_t &= \beta\_1 m\_{t-1} + (1-\beta\_1) g\_t \\\\
v\_t &= \beta\_2 v\_{t-1} + (1-\beta\_2) g\_t^2 \\\\
\hat{m}\_t &= \frac{m\_t}{1-\beta\_1^t} \\\\
\hat{v}\_t &= \frac{v\_t}{1-\beta\_2^t} \\\\
\theta\_{t+1} &= \theta\_t - \eta \left( \frac{\hat{m}\_t}{\sqrt{\hat{v}\_t} + \epsilon} + \lambda \theta\_t \right)
\end{aligned}
$$

其中：
*   $g\_t = \nabla\_\theta L(\theta\_t)$ 是当前小批量的梯度
*   $m\_t$ 是梯度的一阶矩（即 **期望值**，均值，动量）估计
*   $v\_t$ 是梯度的二阶矩（即 **变量平方的期望值**，未中心化方差）估计
*   $\hat{m}\_t$ 和 $\hat{v}\_t$ 是偏差校正后的一阶和二阶矩
*   $\beta\_1, \beta\_2$ 是衰减率（通常 $\beta\_1=0.9, \beta\_2=0.999$）
*   $\eta$ 是学习率
*   $\epsilon$ 是小常数（通常 $10^{-8}$）防止除零
*   $\lambda$ 是权重衰减系数

#### 公式逐行解析

##### 1. 一阶矩估计（动量项）

$$
m\_t = \beta\_1 m\_{t-1} + (1-\beta\_1) g\_t
$$

*   **物理意义**：这是梯度的指数移动平均（EMA），相当于给梯度增加了动量。$\beta\_1$ 控制历史信息的衰减速度，$\beta\_1$ 越大，历史信息保留越多。
*   **展开形式**：
    $$
    m\_t = (1-\beta\_1)\sum\_{i=1}^t \beta\_1^{t-i} g\_i
    $$
*   **作用**：平滑梯度方向，减少震荡，帮助穿越平坦区域和逃离局部极小点。

##### 2. 二阶矩估计（自适应学习率项）

$$
v\_t = \beta\_2 v\_{t-1} + (1-\beta\_2) g\_t^2
$$

*   **物理意义**：这是梯度平方的指数移动平均，用于估计梯度的二阶矩（方差）。
*   **展开形式**：
    $$
    v\_t = (1-\beta\_2)\sum\_{i=1}^t \beta\_2^{t-i} g\_i^2
    $$
*   **作用**：为每个参数计算自适应学习率。梯度大的方向学习率小，梯度小的方向学习率大。

##### 3. 偏差校正（Bias Correction）

$$
\hat{m}\_t = \frac{m\_t}{1-\beta\_1^t}, \quad \hat{v}\_t = \frac{v\_t}{1-\beta\_2^t}
$$

*   **问题来源**：在训练初期（$t$ 较小时），$m\_t$ 和 $v\_t$ 的初始值（通常为 0）会导致估计值偏向 0。
*   **数学推导**：假设梯度是平稳的，$E[g\_t] = \mu$，则：
    $$
    E[m\_t] = E\left[(1-\beta\_1)\sum\_{i=1}^t \beta\_1^{t-i} g\_i\right] = \mu(1-\beta\_1^t)
    $$
    所以 $E[m\_t] = E[g\_t](1-\beta\_1^t)$，为了得到无偏估计，需要除以 $1-\beta\_1^t$。
*   **作用**：确保训练初期的更新步长不会过小，加速早期收敛。

##### 4. 参数更新（AdamW的关键改进）

$$
\theta\_{t+1} = \theta\_t - \eta \left( \frac{\hat{m}\_t}{\sqrt{\hat{v}\_t} + \epsilon} + \lambda \theta\_t \right)
$$

这是 AdamW 的核心公式，与标准 Adam 有本质区别。

**传统 Adam 的参数更新公式**：
$$
\theta\_{t+1} = \theta\_t - \frac{\eta}{\sqrt{\hat{v}\_t} + \epsilon} (\hat{m}\_t + \lambda \theta\_t)
$$

**AdamW 的参数更新公式**：
$$
\theta\_{t+1} = \theta\_t - \eta \left( \frac{\hat{m}\_t}{\sqrt{\hat{v}\_t} + \epsilon} + \lambda \theta\_t \right)
$$

#### AdamW 的核心改进：权重衰减解耦

##### 1. 传统 Adam 的问题

在传统 Adam 中，权重衰减被添加到梯度中：
$$
\theta\_{t+1} = \theta\_t - \frac{\eta}{\sqrt{\hat{v}\_t} + \epsilon} (\hat{m}\_t + \lambda \theta\_t)
$$

这意味着：
*   权重衰减 $\lambda \theta\_t$ 被**自适应学习率** $\frac{\eta}{\sqrt{\hat{v}\_t} + \epsilon}$ 缩放
*   不同参数的权重衰减效果不同，取决于 $\hat{v}\_t$
*   这与 L2 正则化的原始含义不符

##### 2. AdamW 的解决方案

AdamW 将权重衰减与梯度更新解耦：
$$
\theta\_{t+1} = \theta\_t - \eta \left( \frac{\hat{m}\_t}{\sqrt{\hat{v}\_t} + \epsilon} + \lambda \theta\_t \right)
$$

这意味着：
*   权重衰减 $\lambda \theta\_t$ 直接作用于参数，不受自适应学习率影响
*   权重衰减的效果是**确定性的**，对所有参数一致
*   更符合 SGD with weight decay 的实现

####  数学对比分析

##### 更新公式对比

| 优化器 | 更新公式 | 权重衰减实现 |
|--------|---------|-------------|
| **SGD with Weight Decay** | $\theta\_{t+1} = \theta\_t - \eta (g\_t + \lambda \theta\_t)$ | 直接加到梯度 |
| **Adam (传统)** | $\theta\_{t+1} = \theta\_t - \frac{\eta}{\sqrt{\hat{v}\_t} + \epsilon} (\hat{m}\_t + \lambda \theta\_t)$ | 被自适应学习率缩放 |
| **AdamW** | $\theta\_{t+1} = \theta\_t - \eta \left( \frac{\hat{m}\_t}{\sqrt{\hat{v}\_t} + \epsilon} + \lambda \theta\_t \right)$ | 解耦，直接加到参数 |

##### 权重衰减效果对比

**传统 Adam**：
*   实际权重衰减：$\frac{\eta \lambda \theta\_t}{\sqrt{\hat{v}\_t} + \epsilon}$
*   问题：$\sqrt{\hat{v}\_t}$ 是随时间变化的，导致权重衰减强度不稳定

**AdamW**：
*   实际权重衰减：$\eta \lambda \theta\_t$
*   优点：权重衰减强度稳定，与 $\hat{v}\_t$ 无关

#### 为什么 AdamW 效果更好？

##### 1. 权重衰减的一致性

在 AdamW 中，权重衰减是**确定性的**：
*   每个参数在每个时间步都以 $\eta \lambda$ 的比例衰减
*   不受梯度历史的影响
*   这与 SGD 中的权重衰减行为一致

##### 2. 更好的泛化性能

实验表明 AdamW 通常有更好的泛化能力，原因包括：
*   **更稳定的正则化**：权重衰减不再与自适应学习率耦合
*   **更接近 L2 正则化**：实现了真正的权重衰减，而不是近似
*   **超参数更易调**：$\lambda$ 的作用更明确，与学习率 $\eta$ 解耦

##### 3. 在 Transformer 中的表现

Transformer 架构对正则化敏感：
*   传统 Adam 在 BERT/GPT 等模型中容易过拟合
*   AdamW 提供更稳定的权重衰减
*   在大规模预训练中表现显著优于传统 Adam

#### 参数设置指南

| 参数 | 推荐值 | 作用 | 影响 | 调整建议 |
|------|--------|------|------|----------|
| $\beta\_1$ | 0.9 | 一阶矩衰减率 | 动量强度 | 通常固定为 0.9 |
| $\beta\_2$ | 0.999 | 二阶矩衰减率 | 自适应学习率平滑度 | 通常固定为 0.999 |
| $\eta$ | 0.001 | 学习率 | 整体更新步长 | 在 0.0001 到 0.01 之间调整 |
| $\lambda$ | 0.01 | 权重衰减系数 | 正则化强度 | 根据任务调整，Transformer 常用 0.01 |
| $\epsilon$ | $10^{-8}$ | 数值稳定常数 | 防止除零 | 通常不需调整 |

**针对不同架构的典型设置**：
- **Transformer (BERT/GPT)**：$\eta = 0.0001$，$\lambda = 0.01$
- **卷积神经网络**：$\eta = 0.001$，$\lambda = 0.0001$
- **循环神经网络**：$\eta = 0.0001$，$\lambda = 0.001$

#### 总结

**核心改进**：

- 将权重衰减与梯度更新解耦
- 权重衰减直接作用于参数，而不是梯度
- 更符合L2正则化的原始形式

**优点**：
1. 泛化性能通常优于Adam
2. 权重衰减效果更稳定
3. 在Transformer等架构中表现更好
4. 超参数更容易调整

**缺点**：
1. 计算复杂度略高
2. 需要理解权重衰减与L2正则的区别
3. 在有些任务上优势不明显

**适用场景**：
- Transformer架构（BERT、GPT等）
- 需要强正则化的任务
- 图像分类、目标检测
- 大规模预训练和微调

**为什么有效**：
- 传统Adam将权重衰减混入梯度，与自适应学习率相互作用
- AdamW将权重衰减视为独立的惩罚项
- 更接近SGD with weight decay的效果

### 7. 其他优化器

#### AdaDelta
- 无需设置学习率
- 基于历史更新的自适应
- 适合对学习率敏感的任务

#### Nadam
- Adam + Nesterov加速梯度
- 结合了Nesterov动量和自适应学习率
- 在某些任务上收敛更快

#### Lookahead
- 外层循环探索，内层循环快速更新
- 对超参数更鲁棒
- 可与任意优化器结合

#### RAdam
- 修正Adam在训练初期的偏差
- 更稳定的训练过程
- 适合小批量训练

## 3. 实用选择指南

### 1. 根据任务类型选择
- **计算机视觉**：SGD+Momentum 或 AdamW
- **自然语言处理**：AdamW（Transformer）或 Adam（RNN）
- **推荐系统**：AdaGrad 或 FTRL
- **强化学习**：Adam 或 RMSProp
- **生成模型**：Adam 或 AdamW

### 2. 根据数据特性选择
- **数据量大、稠密**：Adam/AdamW
- **数据稀疏**：AdaGrad/FTRL
- **在线学习**：RMSProp/AdaDelta
- **小数据集**：SGD with Momentum

### 3. 根据模型大小选择
- **大模型**：Adam/AdamW（内存允许）
- **小模型**：SGD 或 SGD+Momentum
- **极大模型**：需考虑内存优化的变体

### 4. 训练阶段策略
- **预训练阶段**：通常使用 Adam/AdamW
- **微调阶段**：可切换到 SGD 提升泛化
- **收敛后期**：降低学习率，可能切换优化器

### 5. 实用技巧
1. **学习率调度**：配合余弦退火、热重启等策略
2. **权重衰减**：AdamW中更有效，SGD中可作为正则化
3. **梯度裁剪**：防止梯度爆炸，特别是RNN中
4. **批量大小**：影响优化器选择，大批量适合SGD
5. **热身阶段**：Adam在开始时需要少量热身迭代

### 6. 总结

- **通用首选**：**Adam** 或 **AdamW**。AdamW 在 Transformer 等现代架构中表现更佳。
- **追求极致泛化**：**SGD** 或 **SGD+Momentum**，配合学习率衰减策略。
- **稀疏数据任务**：**AdaGrad** 或 **RMSProp**。
- **非平稳目标/序列模型**：**RMSProp** 或 **Adadelta**。
- **快速实验**：从 **Adam** 开始，收敛后可视情况切换为 **SGD** 进行微调。

## 4. 高级话题与研究趋势

### 1. 优化器理论分析
- 收敛性证明：SGD在凸函数上有 $O(1/\sqrt{T})$ 收敛率，$T$ 指的是**迭代次数**。
  - 收敛率 $O(1/\sqrt{T})$ 的数学含义是：经过 $T$ 次迭代后，**期望误差**（expected error）以 $1/\sqrt{T}$ 的速度收敛到0。这意味着要达到$\epsilon$精度，需要 $O(1/\epsilon^2)$ 次迭代。 
  - 收敛速度是次线性的，后期收敛较慢。
  - 这是凸优化中SGD的理论最优收敛率。

- 泛化能力：SGD通常有更好的泛化理论保证
- 隐式正则化：不同优化器引入不同的正则化效果

### 2. 自适应优化器的问题
- **收敛到尖锐极小点**：可能影响泛化
- **训练-测试不匹配**：自适应机制在测试时消失
- **内存开销**：存储动量和方差增加内存需求

### 3. 最新研究趋势
- **二阶优化方法**：K-FAC、Shampoo等
- **分布式优化**：延迟补偿、压缩通信
- **元学习优化器**：学习如何优化
- **理论理解深化**：从动力系统、微分方程角度理解

### 4. 工程实践建议
1. **优先尝试 AdamW**，默认参数通常有效
2. 如果**过拟合**，尝试切换为 **SGD**
3. 使用**学习率查找器**确定合适的学习率
4. 监控梯度范数和参数更新的统计量
5. 在不同阶段使用不同优化器策略

## 5. 总结建议

### 新手建议
1. 从 **AdamW** 开始，使用默认参数
2. 设置合适的学习率（通常 0.001 或 0.0001）
3. 添加权重衰减（1e-4 到 1e-2）
4. 监控训练和验证损失

### 进阶建议
1. 比较不同优化器的验证集性能
2. 尝试 **SGD+Momentum** 配合余弦退火
3. 针对特定任务调优超参数
4. 考虑优化器组合策略

### 专家建议
1. 根据损失曲面特性选择优化器
2. 设计自适应的优化器调度策略
3. 考虑优化器的隐式正则化效应
4. 在计算资源限制下做出权衡

---

**最终建议**：没有绝对最优的优化器，最佳选择取决于具体任务、数据、模型架构和计算资源。实际应用中，建议在小规模实验上对比几种优化器，然后选择最适合当前任务的方案。