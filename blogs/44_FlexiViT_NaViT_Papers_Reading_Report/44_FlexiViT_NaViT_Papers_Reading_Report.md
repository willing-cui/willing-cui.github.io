## 1. 核心问题再理解

在深入技术细节前，先明确两个关键概念：

**块大小(patch size)**：ViT将图像分割成固定大小的块（如16×16像素），每个块通过线性投影变成token。块大小决定了：

- 输入序列长度：$\text{图像分辨率} \div \text{块大小}^2$
- 计算复杂度：序列长度影响注意力计算量
- 感受野：大块捕获全局信息，小块保留细节

**分辨率与长宽比**：传统ViT要求输入图像必须调整为固定尺寸（如224×224），这导致：

- 高分辨率图像被压缩，丢失细节
- 非方形图像被拉伸，几何失真
- 无法利用原始分辨率信息

## 2. FlexiViT 技术方案详解

### 2.1 为什么需要动态块大小？

**计算预算场景**：实际应用中，不同设备（手机、服务器）的计算能力不同，用户对延迟要求也不同。传统ViT只能通过模型压缩（剪枝、量化）来适应，但会损失性能。FlexiViT通过**改变块大小**来调整计算量，大块（如48×48）计算量小，小块（如8×8）计算量大但精度高。

### 2.2 块大小随机化训练

**训练过程**：每个训练批次，从预定义集合（如{8,10,12,...,48}）中随机采样一个块大小$p$，整个批次使用这个块大小。不同批次可能使用不同块大小。

**为什么能工作**：模型学习到"块大小无关"的特征表示。关键假设是：不同块大小提取的特征在语义空间中是相似的。例如，48×48块捕获的"猫头"特征，应该与8×8块捕获的"猫眼"特征在高层语义上一致。

### 2.3 伪逆重缩放(PI-resize)数学推导

这是FlexiViT最核心的技术，解决**块嵌入权重共享**问题。

**问题描述**：假设我们有一个训练好的ViT模型，其块嵌入层权重为$W \in \mathbb{R}^{d \times (p\_* \times p\_* \times 3)}$，其中$p\_*$是训练时的块大小。现在想用块大小$p$推理，需要将$W$调整到新维度$d \times (p \times p \times 3)$。

**传统方法的问题**：直接双线性插值会破坏权重矩阵的数学结构，导致性能下降。

**PI-resize解决方案**：

**步骤1：矩阵重塑**
将权重$W$重塑为三维张量$W\_{\text{reshaped}} \in \mathbb{R}^{d \times p\_* \times p\_* \times 3}$，每个通道对应RGB。

**步骤2：定义插值矩阵**
对于目标块大小$p$，定义插值矩阵$B\_p^{p\_*} \in \mathbb{R}^{p \times p\_*}$，表示从$p\_*$维到$p$维的插值操作。例如，如果$p=16$，$p\_*=8$，则$B\_{16}^8$表示从8×8上采样到16×16的插值矩阵。

**步骤3：伪逆重缩放公式**
对每个通道$c$，新权重计算为：
$$W\_{\text{new}}^{(c)} = \left( (B\_p^{p\_*})^T \right)^+ \cdot W\_{\text{old}}^{(c)} \cdot B\_p^{p\_*}$$
其中$(\cdot)^+$表示Moore-Penrose伪逆。

**为什么用伪逆**：伪逆提供了最小二乘意义下的最优解，确保重缩放后的权重在插值意义下最接近原始权重。

**实际实现**：训练时，对于每个随机采样的块大小$p$，都使用PI-resize从教师模型（固定块大小）的权重初始化当前块嵌入层。

### 2.4 知识蒸馏框架

**教师-学生设置**：

- 教师模型：使用固定块大小（如16）训练的标准ViT
- 学生模型：FlexiViT，使用随机块大小训练

**蒸馏损失**：
$$\mathcal{L}\_{\text{total}} = \mathcal{L}\_{\text{CE}} + \lambda \cdot \mathcal{L}\_{\text{KD}}$$
其中$\mathcal{L}\_{\text{KD}}$是KL散度损失，让学生模型的输出分布接近教师模型。

**为什么有效**：教师模型提供了强监督信号，帮助学生模型在不同块大小下保持一致的表示能力。

## 3. NaViT 技术方案详解

### 3.1 序列打包(Patch n' Pack)机制

**核心思想**：将多个图像的块序列拼接成一个长序列，类似NLP中的示例打包。

**具体步骤**：

1. 对每个图像，按实际分辨率分割成块，得到token序列
2. 将所有图像的序列拼接，用特殊token（如[SEP]）分隔
3. 输入到Transformer中处理

**示例**：

- 图像1：分辨率300×400，块大小16 → 序列长度(300/16)×(400/16)=18×25=450
- 图像2：分辨率224×224，块大小16 → 序列长度(224/16)²=196
- 打包后序列：450 + 196 + 2（分隔符）= 648个token

**优势**：

- 批次大小可变：可以打包不同数量的图像
- 计算效率：相比固定批次大小，减少padding浪费
- 支持任意分辨率：每个图像保持原始尺寸

### 3.2 因式分解位置嵌入

**问题**：传统ViT使用1D位置嵌入，假设序列是线性的。但图像是2D结构，且不同图像的分辨率不同，需要更灵活的位置编码。

**解决方案：分离x和y坐标**

**方法1：绝对位置嵌入**

- 定义两个独立的嵌入表：$\phi\_x: [0, W\_{\text{max}}] \rightarrow \mathbb{R}^D$ 和 $\phi\_y: [0, H\_{\text{max}}] \rightarrow \mathbb{R}^D$
- 对于位置$(x,y)$，位置编码为：$\phi(x,y) = \phi\_x(x) + \phi\_y(y)$
- 优点：简单，可处理任意分辨率
- 缺点：需要预定义最大尺寸，泛化性受限

**方法2：分数位置嵌入**

- 将绝对坐标归一化到[0,1]区间：$r\_x = x/W$, $r\_y = y/H$
- 使用正弦函数或可学习函数：$\phi(x,y) = \phi\_x(r\_x) + \phi\_y(r\_y)$
- 优点：完全分辨率无关，可处理任意尺寸
- 缺点：可能丢失绝对位置信息

**NaViT采用**：论文使用分数位置嵌入，因为需要处理任意分辨率。

### 3.3 连续令牌丢弃(CTD)

**动机**：高分辨率图像产生大量token，计算成本高。但并非所有token都同等重要。

**策略**：根据图像分辨率动态调整令牌保留率。高分辨率图像丢弃更多token，低分辨率保留更多。

**数学表达**：
保留率$r$与分辨率$s$的关系：
$$r = \min\left(1, \frac{s\_0}{s}\right)$$
其中$s\_0$是基准分辨率。

**实现**：在序列打包前，对每个图像的token序列按保留率$r$随机丢弃token。

**为什么有效**：高分辨率图像通常包含冗余信息（如天空、背景），丢弃部分token不会显著影响性能，但大幅降低计算量。

## 4. 关键差异对比（重新梳理）

### 4.1 解决的核心问题不同

| 维度             | FlexiViT                 | NaViT                     |
| ---------------- | ------------------------ | ------------------------- |
| **主要目标**     | 单一模型适应不同计算预算 | 处理任意分辨率/长宽比图像 |
| **核心机制**     | 动态块大小               | 序列打包                  |
| **计算调整方式** | 改变块大小（粗粒度）     | 改变序列长度（细粒度）    |

### 4.2 技术实现的关键点

**FlexiViT的关键挑战**：

- 块嵌入权重如何在不同块大小间共享
- 位置编码如何适应不同序列长度
- 如何保证不同块大小下特征一致性

**NaViT的关键挑战**：

- 如何高效处理可变长度序列
- 如何设计分辨率无关的位置编码
- 如何平衡不同分辨率图像的计算分配

### 4.3 训练策略的本质差异

**FlexiViT**：本质是**多任务学习**，每个块大小对应一个"子任务"，模型学习共享表示。

**NaViT**：本质是**序列建模**，将图像视为可变长度序列，类似NLP。

## 5. 为什么这些技术有效？

### 5.1 FlexiViT的有效性分析

**表示学习视角**：不同块大小提取的特征在高层语义空间应该对齐。例如：

- 48×48块：捕获整个猫头
- 16×16块：捕获猫眼
- 8×8块：捕获猫眼瞳孔

这些特征在"猫"这个类别上应该激活相似的神经元。PI-resize和知识蒸馏强制这种对齐。

**优化视角**：随机块大小训练相当于数据增强，增加模型鲁棒性。

### 5.2 NaViT的有效性分析

**序列建模视角**：图像patch序列与文本token序列本质相似，都是可变长度序列。NLP的序列打包技术自然迁移。

**位置编码视角**：图像是2D结构，x和y坐标独立，因式分解编码更合理。

**计算效率视角**：序列打包减少padding浪费，CTD根据内容复杂度分配计算，更高效。

## 6. 实际应用中的考虑

### 6.1 FlexiViT部署建议

**推理时块大小选择**：

- 高精度场景：使用小块（如8×8）
- 低延迟场景：使用大块（如48×48）
- 可以动态调整：根据设备性能或用户需求

**内存管理**：不同块大小对应的序列长度不同，需要动态分配内存。

### 6.2 NaViT部署建议

**批次打包策略**：

- 可以按分辨率分组打包，减少序列长度差异
- 设置最大序列长度，避免内存溢出

**CTD参数调优**：

- 基准分辨率$s\_0$需要根据任务调整
- 可以设计更复杂的丢弃策略（如基于注意力权重）

## 7. 总结：从技术角度理解

| 概念           | FlexiViT             | NaViT                     |
| -------------- | -------------------- | ------------------------- |
| **核心创新**   | 块嵌入权重动态调整   | 序列打包+因式分解位置编码 |
| **数学工具**   | 伪逆、插值理论       | 序列建模、位置编码理论    |
| **训练范式**   | 多任务学习           | 序列打包训练              |
| **适用场景**   | 计算预算敏感         | 分辨率敏感                |
| **实现复杂度** | 中等（需要插值操作） | 高（需要序列管理）        |

**关键启示**：两篇论文都体现了"一个模型，多种配置"的思想，但通过不同技术路径实现。FlexiViT更关注计算效率的灵活性，NaViT更关注输入分辨率的灵活性。两者可以结合使用，但需要解决技术兼容性问题。