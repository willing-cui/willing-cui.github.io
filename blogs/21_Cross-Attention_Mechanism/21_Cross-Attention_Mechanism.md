## 一、基本概念

交叉注意力机制（Cross-Attention）是注意力机制的一种重要变体，用于建模**两个不同序列之间的依赖关系**。与自注意力机制处理单一序列内部关系不同，交叉注意力允许一个序列（查询序列）主动"关注"另一个序列（键值序列）的相关部分，实现跨序列的信息交互。

## 二、核心公式

交叉注意力的核心计算公式如下：

$$
\text{CrossAttention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

其中：
- $Q \in \mathbb{R}^{n \times d_k}$：查询矩阵，来自序列A
- $K \in \mathbb{R}^{m \times d_k}$：键矩阵，来自序列B
- $V \in \mathbb{R}^{m \times d_v}$：值矩阵，来自序列B
- $d_k$：键向量的维度，用于缩放点积结果
- $n, m$：两个序列的长度

## 三、计算步骤详解

### 1. 线性变换生成三元组

首先对两个输入序列分别进行线性变换：

$$
Q = XW^Q, \quad K = YW^K, \quad V = YW^V
$$

其中 $X \in \mathbb{R}^{n \times d}$ 是序列A，$Y \in \mathbb{R}^{m \times d}$ 是序列B，$W^Q, W^K, W^V \in \mathbb{R}^{d \times d}$ 是可学习权重矩阵。

### 2. 计算注意力分数

通过查询向量与键向量的点积计算相似度：

$$
\text{Attention Scores} = \frac{QK^T}{\sqrt{d_k}}
$$

缩放因子 $\frac{1}{\sqrt{d_k}}$ 的作用是防止点积结果过大导致softmax梯度饱和。

### 3. 归一化权重

使用softmax函数将注意力分数转换为概率分布：

$$
\text{Attention Weights} = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)
$$

### 4. 加权求和

将注意力权重应用于值向量，得到最终输出：

$$
\text{Output} = \text{Attention Weights} \cdot V
$$

## 四、与自注意力的区别

| 特性           | 自注意力（Self-Attention） | 交叉注意力（Cross-Attention） |
| -------------- | -------------------------- | ----------------------------- |
| **输入来源**   | 同一序列                   | 两个不同序列                  |
| **Q/K/V来源**  | 都来自同一序列             | Q来自序列A，K/V来自序列B      |
| **应用场景**   | 序列内部依赖建模           | 跨序列关联建模                |
| **计算复杂度** | $O(n^2)$                   | $O(nm)$                       |
| **主要功能**   | 捕捉序列内部长距离依赖     | 融合不同序列的信息            |

## 五、应用场景

### 1. 机器翻译
在Transformer的编码器-解码器架构中，解码器通过交叉注意力访问编码器的输出表示，实现源语言和目标语言之间的信息传递。解码器的查询向量Q来自目标语言的已生成序列，而键值向量K和V来自编码器对源语言序列的编码结果。

### 2. 多模态任务
- **图文匹配**：文本特征作为查询，图像特征作为键值
- **视觉问答（VQA）**：问题文本查询图像特征
- **图像描述生成**：图像特征作为键值，文本描述作为查询

### 3. 文本摘要
原始文本作为键值，摘要文本作为查询，帮助模型识别输入文本中的关键信息。

### 4. 问答系统
问题作为查询，文档段落作为键值，动态选择最相关的段落内容。

## 六、设计变体

### 1. 单向交叉注意力
只有一方更新，另一方不动。适合信息检索类任务，如文本去匹配图像：

```python
text_new = CrossAttention(Q=text_emb, K=image_emb, V=image_emb)
```

### 2. 双向交叉注意力（Co-Attention）
双方都更新，一人问对方一次：

```python
Text = CrossAttention(Q=Text, K=Image, V=Image)
Image = CrossAttention(Q=Image, K=Text, V=Text)
```

### 3. 多轮交替堆叠
像Transformer堆多层一样交替进行，适合对齐过程复杂的任务。

### 4. Self + Cross混合
每一层既做模态内部的self-attention，也做跨模态的cross-attention。

## 七、多头交叉注意力

为了增强模型的表示能力，通常采用多头注意力机制：

$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O
$$

其中每个头：

$$
\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
$$

多头机制通过并行计算多个子空间的注意力，使模型能够从不同角度分析输入序列之间的相关性。

## 八、实现注意事项

### 1. 特征对齐
不同模态的原始特征维度往往不一致，必须先用线性映射将它们映射到统一维度：

```python
proj_text = nn.Linear(768, 512)
proj_img = nn.Linear(1024, 512)
```

### 2. 注意力掩码
在实际任务中，可能需要限制注意力范围，避免无关信息干扰：

```python
# 布尔掩码：True表示需要被mask的位置
attn_mask = torch.tensor(
    [[False, False, True],
	[False, False, True],
	[True, True, True]])
```

### 3. 融合策略
- **拼接（Concat）**：简单粗暴，把更新后的两模态特征拼在一起
- **加权融合**：学一个门控参数，动态调整两边贡献
- **共空间映射**：投到同一个隐空间，让它们直接可比

## 九、优势总结

1. **灵活性**：能够处理不同模态的数据，具有很高的灵活性
2. **高效性**：通过计算两个序列之间的相似度并分配权重，实现了信息的有效融合和提取
3. **可扩展性**：可以与其他深度学习技术结合使用，进一步提升性能
4. **动态性**：能够根据当前查询序列的内容动态调整关注程度

交叉注意力机制作为深度学习中的关键技术，在自然语言处理、计算机视觉以及多模态学习等领域展现出强大的潜力，是构建现代AI系统不可或缺的核心组件。