import sentencepiece as spm

# 综合符号列表
user_defined_symbols = [
    '。', '，', '、', '！', '？', '；', '：', '“', '”', '‘', '’', '（', '）', '【', '】', '《', '》', '『', '』', '「', '」', '〔', '〕', '…', '—', '～', '·', '．',
    '.', ',', '!', '?', ';', ':', '"', "'", '(', ')', '[', ']', '{', '}', '<', '>', '/', '\\', '|', '-', '_', '=', '+', '@', '#', '$', '%', '^', '&', '*',
    '×', '÷', '±', '≠', '≈', '≡', '≤', '≥', '≦', '≧', '≪', '≫', '∝', '∞',
    '∈', '∉', '⊂', '⊃', '⊆', '⊇', '∪', '∩', '∅', '∀', '∃', '¬', '∧', '∨', '⊕', '⊗', '⊥', '∥', '∠',
    'α', 'β', 'γ', 'δ', 'ε', 'ζ', 'η', 'θ', 'ι', 'κ', 'λ', 'μ', 'ν', 'ξ', 'ο', 'π', 'ρ', 'σ', 'τ', 'υ', 'φ', 'χ', 'ψ', 'ω',
    'Α', 'Β', 'Γ', 'Δ', 'Ε', 'Ζ', 'Η', 'Θ', 'Ι', 'Κ', 'Λ', 'Μ', 'Ν', 'Ξ', 'Ο', 'Π', 'Ρ', 'Σ', 'Τ', 'Υ', 'Φ', 'Χ', 'Ψ', 'Ω',
    '∑', '∏', '∫', '∬', '∭', '∮', '∯', '∰', '∇', '∂', '∆', '√', '∛', '∜', '‰', '‱',
    '→', '←', '↑', '↓', '↔', '↕', '⇒', '⇐', '⇔', '↦', '↣', '↪', '↩', '⇀', '⇁',
    '￥', '＄', '€', '£', '¥', '₩', '₽', '₹', '¢', '¤',
    '°', '℃', '℉', '㎜', '㎝', '㎞', '㎡', '㎥', '㎏', '㏄', '㏑', '㏒', 'µ', 'Å', 'Ω',
    '♂', '♀', '⚥', '☿', '♁', '♃', '♄', '♅', '♆', '♇', '♈', '♉', '♊', '♋', '♌', '♍', '♎', '♏', '♐', '♑', '♒', '♓',
    '■', '□', '▲', '△', '▼', '▽', '◆', '◇', '●', '○', '★', '☆', '♠', '♣', '♥', '♦',
    '零', '壹', '贰', '叁', '肆', '伍', '陆', '柒', '捌', '玖', '拾', '佰', '仟', '萬', '億',
    '甲', '乙', '丙', '丁', '戊', '己', '庚', '辛', '壬', '癸', '子', '丑', '寅', '卯', '辰', '巳', '午', '未', '申', '酉', '戌', '亥',
    '©', '®', '™', '§', '¶', '†', '‡', '※', '№', '♯',
]

# 定义训练参数
corpus_file = './dataset/wiki_corpus.txt'	# 语料文件
model_prefix = 'zh_wiki_spm'  # 模型输出文件的前缀
vocab_size = 30000            # 词表大小，根据你的需求调整，3万是一个常用起点
character_coverage = 0.9995   # 字符覆盖率，对于中文建议高一些

# 训练模型
model_prefix = 'zh_wiki_spm_comprehensive'
vocab_size = 30000
character_coverage = 0.9995  # 对中文很重要

# 训练模型
spm.SentencePieceTrainer.train(
    input=corpus_file,        # 输入语料文件
    model_prefix=model_prefix,# 输出模型前缀
    vocab_size=vocab_size,    # 词表大小
    model_type='bpe',         # 模型类型：'bpe', 'unigram', 'char', 'word'
    # 对于中文，BPE 或 Unigram 都是常用选择。Unigram 通常效果更好，BPE 更简单。
    # 这里我们使用 'bpe'
    character_coverage=character_coverage, # 字符覆盖率
    input_sentence_size=2000000,    # 加速和资源控制参数
    num_threads=12,           # 根据CPU核心数设置
    pad_id=0,                 # 定义特殊Token的ID
    unk_id=1,
    bos_id=2,
    eos_id=3,
    pad_piece='[PAD]',        # 定义特殊Token的字符串表示
    unk_piece='[UNK]',
    bos_piece='[BOS]',
    eos_piece='[EOS]',
    # 控制分词行为的重要参数
    # 将数字、标点也视为独立Token，而不是与字合并
    split_digits=True,        # 将数字拆分为单个数字
    user_defined_symbols=user_defined_symbols,  # 使用我们定义的全面符号列表
)

print("模型训练完成！")