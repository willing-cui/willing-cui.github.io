## 什么是Sinusoidal Positional Encoding

**Sinusoidal Positional Encoding（正弦位置编码）**是在著名的 Transformer 模型（比如 GPT、BERT 等）中被广泛应用的一种技术，其核心目的是**解决模型对输入序列中单词顺序的感知问题**。

### 1. 核心问题：为什么需要位置编码？

Transformer 模型的核心是 **自注意力机制**。自注意力机制有一个特点：它默认是**置换不变**的。

- **这是什么意思？** 意思是，如果你把输入句子的单词顺序打乱（例如，把 “我爱人工智能” 打乱成 “人工智能爱我”），自注意力机制在处理这两组乱序的单词时，每个单词计算出的初始表示会是几乎一样的。因为它只关心单词之间内容上的相关性，而完全忽略了它们的**位置信息**。
- **但这显然不对！** 在自然语言中，单词的顺序至关重要。“猫追老鼠” 和 “老鼠追猫” 的意思天差地别。

所以，我们必须想一种办法，明确地告诉 Transformer 模型每个单词在序列中的**绝对位置**（是第一个词还是第二个词）以及**相对位置**（两个词之间相隔多远）。

**位置编码就是解决这个问题的“药方”。**

### 2. 正弦位置编码是什么？

正弦位置编码是一种非常巧妙的方法，它不是通过模型学习得到，而是通过一个固定的、预先定义好的数学公式来生成的。这个公式使用了一组正弦和余弦函数。

- **它的样子：** 想象一个序列中的每个单词，除了它本身的词向量（比如一个 512 维的向量），还会被加上一个同样长度的“位置向量”。这个位置向量就是正弦位置编码。
- **向量的每一维** 都由一个不同频率的正弦或余弦波函数计算得出。

### 3. 计算公式

对于序列中位置为 `pos`的单词，我们计算其位置编码向量 `PE`的第 `i`个维度：

- 如果 `i`是偶数（`i % 2 == 0`）：$PE_{(pos,2i)}=\sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)$

- 如果 `i`是奇数（`i % 2 == 1`）：$PE_{(pos,2i+1)}=\cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)$

**参数解释：**

- `pos`：单词在序列中的位置（0, 1, 2, ...）。
- `i`：位置编码向量中的维度索引（0, 1, 2, ..., $d_{model/2}$）。注意，当 `i`是偶数时，使用正弦函数 `sin`；当 `i`是奇数时，使用余弦函数 `cos`。
- `d_model`：词向量的总维度（例如 512）。
- `10000^{2i / d_model}`：这个项定义了每个维度的“波长”。随着维度索引 `i`的增大，分母会以指数级增长，导致频率降低。这意味着**在编码向量的不同维度上，模型会学习到不同频率和波长的位置信号**

**简化理解：**

这个公式的本质是，为位置编码向量的**每一维**都分配一个不同频率的波形。

- 低频的波形（对应较小的 `i`）变化很慢，可以捕捉长距离的依赖关系。
- 高频的波形（对应较大的 `i`）变化很快，可以捕捉短距离的精细位置关系。

最终，每个位置 `pos`都获得了一个独一无二的、由不同频率波形值组合而成的“指纹”（即位置向量）。

**一个简单的例子:**

假设 `d_model = 4`，我们想计算位置 `pos=2`的位置编码。

- 对于 `i=0`（偶数索引）:`PE(2, 0) = sin(2 / (10000^(0/4))) = sin(2 / 1) = sin(2)`
- 对于 `i=1`（奇数索引）:`PE(2, 1) = cos(2 / (10000^(2/4))) = cos(2 / 10000^(0.5)) = cos(2 / 100)`
- 对于 `i=2`（偶数索引）:`PE(2, 2) = sin(2 / (10000^(4/4))) = sin(2 / 10000)`
- 对于 `i=3`（奇数索引）:`PE(2, 3) = cos(2 / (10000^(6/4))) = cos(2 / 10000^1.5)`

这样，我们就得到了一个 4 维的位置编码向量 `[sin(2), cos(2/100), sin(2/10000), cos(2/10000^1.5)]`。

### 4. 为什么这个设计如此巧妙？

正弦位置编码有两大绝佳特性，使其非常适合 Transformer：

#### a) 模型可以轻松学习相对位置

对于任意一个固定的偏移量 `k`，位置 `pos + k`的位置编码$PE_{pos+k}$可以通过一个**线性变换**从位置 `pos`的编码 $PE_{pos}$计算出来。

这是因为三角函数有和差化积公式：

$sin(pos+k)=sin(pos)cos(k)+cos(pos)sin(k) \\ cos(pos+k)=cos(pos)cos(k)−sin(pos)sin(k)$

这意味着，一旦模型学会了几个固定的变换矩阵，它就能很容易地推断出“距离我 `k`个位置的单词在哪里”，即使它从未在训练中见过这么长的序列。这赋予了 Transformer 强大的**外推能力**。

#### b. 编码值有界且易于扩展

正弦和余弦函数的值域是 [-1, 1]，这使得编码值是有界的，不会随着 `pos`的增大而爆炸。同时，由于正弦函数的周期性，理论上它可以表示比训练集中更长的序列位置（尽管在实际中，对很长的序列效果会打折扣）。

### 5. 在 Transformer 中是如何使用的？

过程非常简单直接：

1. 输入序列的每个单词被转换成一个词嵌入向量（Word Embedding）。
2. 根据单词的位置（`pos`），通过上述公式计算出对应的正弦位置编码向量。
3. 将**词嵌入向量**和**位置编码向量**直接**相加**，作为 Transformer 编码器的输入。

```
最终输入 = 词向量 + 位置向量
```

通过这种方式，Transformer 在处理每个单词时，就同时获得了它的**语义信息**（来自词向量）和**位置信息**（来自位置编码）。

### 总结

| 特性         | 解释                                                         |
| :----------- | :----------------------------------------------------------- |
| **目的**     | 让本身不具备位置感知能力的 Transformer 模型能够理解单词的顺序。 |
| **形式**     | 一个由正弦和余弦函数生成的、固定的向量，直接加到词向量上。   |
| **核心思想** | 用不同频率的波形为每个位置生成一个独一无二的、连续的“位置指纹”。 |
| **关键优势** | **能够轻松建模相对位置关系**，具有良好的外推性。             |
| **应用**     | 主要用于原始 Transformer 的编码器和解码器。后续的模型（如 BERT）也广泛使用它或它的变体。 |

值得一提的是，除了正弦这种**固定式**的位置编码，还有**可学习的位置编码**（让模型自己从数据中学习位置向量），以及为了处理更长序列而设计的各种**相对位置编码**（如 Transformer-XL、T5 等模型使用的）。但正弦位置编码因其简洁和有效，成为了一个里程碑式的基础方法。