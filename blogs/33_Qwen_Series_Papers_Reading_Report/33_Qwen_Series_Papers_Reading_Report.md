Qwen-VL系列是阿里巴巴集团推出的视觉-语言模型（LVLM）家族，历经Qwen-VL、Qwen2-VL、Qwen2.5-VL到Qwen3-VL四代迭代，在架构设计、核心能力、训练策略和实际性能上实现了持续突破。以下从**每代模型技术创新**、**性能提升**、**迭代改进对比**三个维度展开详细分析，结合官方技术报告数据与实验结果，全面呈现其演进路径。


## 一、各代Qwen-VL模型技术创新与性能提升
### 1. 第一代：Qwen-VL（2023年）——视觉-语言基础能力奠基
作为系列初代模型，Qwen-VL首次将Qwen-7B语言模型与视觉能力结合，核心目标是构建“文本+图像”双模态基础能力，填补开源LVLM在细粒度理解上的空白。

#### 核心技术创新
- **轻量化架构设计**：采用“视觉编码器+位置感知适配器+LLM”三段式结构，视觉编码器基于ViT-bigG（1.9B参数）初始化，通过**单层交叉注意力适配器**将图像特征压缩至固定长度256 tokens（解决长序列效率问题），并引入2D绝对位置编码保留细粒度空间信息，总参数仅9.6B（LLM占7.7B），兼顾性能与效率。
- **三段式训练 pipeline**：
  1. **预训练阶段**：冻结LLM，仅训练视觉编码器与适配器，使用1.4B清洗后的图像-文本对（77.3%英文+22.7%中文），输入分辨率224×224，优化文本生成交叉熵损失；
  2. **多任务预训练**：解锁LLM全参数，提升输入分辨率至448×448，融合7类任务数据（字幕生成、VQA、视觉定位、OCR等），强化细粒度理解；
  3. **指令微调（SFT）**：冻结视觉编码器，用350K多模态指令数据（含人工标注定位与多图像对话数据）训练LLM与适配器，生成交互式模型Qwen-VL-Chat。
- **细粒度能力突破**：通过“图像-字幕-边界框”三元组数据对齐，首次在开源LVLM中实现**视觉定位（Bounding Box生成）** 与**多语言OCR**（中英文为主），支持`<box>` `<ref>`等特殊令牌区分定位文本与普通文本。

#### 关键性能表现
- **基础视觉任务**：在零样本图像字幕任务中，Flickr30K数据集CIDEr得分85.8，超越Flamingo-80B（67.2）、BLIP-2（71.6）；VQAv2数据集准确率79.5，OKVQA准确率58.6，均为当时7B级开源模型SOTA。

- **细粒度任务**：RefCOCO+（testB）定位准确率77.21，GRIT文本定位准确率78.22，显著优于同期Shikra-13B（74.41/69.03）；OCR-VQA任务准确率75.7，支持PDF/HTML文档文本提取。

- **局限性**：仅支持静态图像，无视频理解能力；图像分辨率固定（448×448），动态分辨率适配差；上下文长度有限（2048 tokens），长文档/多图像处理能力弱。

  <span class="image main">
  <img class="main img-in-blog" style="max-width: 50%" src="./blogs/33_Qwen_VL_Paper_Reading_Report/Qwen-VL_Performance.webp" alt="Qwen-VL Performance" />
  <i>Qwen-VL在广泛任务中实现SOTA性能。From <a href="https://arxiv.org/pdf/2308.12966" >Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond</a></i>
  </span> 


### 2. 第二代：Qwen2-VL（2024年）——动态分辨率与视频理解升级
Qwen2-VL针对初代“固定分辨率+无视频能力”的核心痛点，提出“原生动态分辨率”与“统一图像-视频处理”架构，同时扩展模型规模覆盖边缘到高性能场景。

#### 核心技术创新
- **原生动态分辨率（Naive Dynamic Resolution）**：
  - 移除ViT绝对位置嵌入，引入**2D-RoPE（旋转位置编码）**，支持图像按原生分辨率输入（无需缩放/填充），动态生成视觉令牌数量（如224×224图像生成66 tokens，1260×1092图像生成更多 tokens）；
  - 推理时按GPU显存限制动态打包不同分辨率图像，通过MLP层压缩2×2相邻视觉补丁，平衡精度与效率。
- **多模态RoPE（M-RoPE）**：将位置编码拆解为“时间（t）、高度（h）、宽度（w）”三维分量，文本模态共享三维ID（等效1D-RoPE），图像模态固定tID、动态h/w ID，视频模态动态tID（每帧递增），实现“文本-图像-视频”统一位置建模。
- **统一图像-视频理解**：
  - 视频按2 FPS采样，用**3D卷积（深度2）** 处理连续两帧为“3D管”，减少视频令牌数量（如1小时视频令牌数控制在16384）；
  - 图像视为“两帧重复视频”，共享训练管道，支持20分钟以上长视频理解。
- **模型规模扩展**：推出2B/7B/72B三版本，视觉编码器统一为675M参数ViT，LLM基于Qwen2系列初始化，72B版本首次对标闭源模型（GPT-4o、Claude 3.5 Sonnet）。

#### 关键性能表现
- **动态分辨率优势**：在RealWorldQA（真实场景空间理解）数据集得分77.8，超越GPT-4o（75.4）；MMBench-EN测试集准确率86.5，与闭源模型持平。
- **视频理解**：EgoSchema（第一视角长视频）准确率77.9，超越GPT-4o（72.2）；Video-MME（1小时视频）无字幕场景得分71.2，接近Gemini 1.5-Pro（75.0）。
- **细粒度任务**：DocVQA测试集准确率96.5，InfoVQA准确率84.5，均为开源SOTA；多语言OCR支持10种语言（含日语、韩语、阿拉伯语），内部基准中韩语识别准确率94.5，超越GPT-4o（87.8）。
- **局限性**：视频时间编码依赖帧数量（非绝对时间），长视频（1小时以上）时间定位精度差；上下文长度仍限制在8192 tokens，长文档处理能力不足。


### 3. 第三代：Qwen2.5-VL（2025年）——长上下文与效率优化
Qwen2.5-VL聚焦“长视频/长文档理解”与“计算效率”，通过动态分辨率扩展、绝对时间编码与窗口注意力，实现“小时级视频+百页级文档”处理能力，同时保持小模型竞争力。

#### 核心技术创新
- **动态分辨率与帧率扩展**：
  - 空间维度：支持图像原生分辨率动态处理（最小100×28×28像素，最大16384×28×28像素），直接用图像实际尺寸表示边界框坐标（无需归一化），提升空间尺度感知；
  - 时间维度：引入**动态FPS采样**（视频按内容复杂度调整采样率）与**绝对时间编码**，将M-RoPE的tID与实际时间（秒）对齐，支持小时级视频（如3小时视频）的秒级事件定位。
- **高效视觉编码器**：
  - 从头训练**动态分辨率ViT**，引入**窗口注意力（Window Attention）**：仅4层用全注意力，其余层用112×112窗口注意力，计算复杂度从“补丁数平方”降至“线性”，显存占用减少30%；
  - 统一ViT架构与LLM设计：采用RMSNorm归一化与SwiGLU激活函数，提升跨模态兼容性。
- **超大规模训练数据**：预训练数据从1.2万亿tokens扩展至4.1万亿tokens，新增“长视频字幕”“3D空间推理”“Agent交互”数据（如手机/电脑UI操作轨迹），支持设备控制Agent能力。
- **多尺寸模型优化**：3B/7B/72B版本保持视觉编码器统一（1280维隐藏层，32层），通过MLP融合器动态匹配LLM维度（3B输出2048维，72B输出8192维），小模型在边缘设备仍保持强性能。

#### 关键性能表现
- **长上下文任务**：LongVideoBench（长视频QA）得分60.7，超越GPT-4o（66.7→注：原文可能存在笔误，实际Qwen2.5-VL-72B在LVBench得分47.3，显著超越GPT-4o的30.8）；MMLongBench-Doc（百页文档）准确率56.2，开源模型第一。
- **文档与OCR**：CC-OCR数据集准确率79.8，超越Gemini 1.5-Pro（73.0）；OCRBench_v2中英文赛道得分61.5/63.7，领先Gemini 1.5-Pro 9.6%/20.6%。
- **视频定位**：Charades-STA（视频事件定位）mIoU 50.9，超越GPT-4o（未公开，官方称领先）；支持视频帧级 timestamp输出（hmsf格式）。
- **小模型性能**：Qwen2.5-VL-7B在MMStar数据集得分63.9，超越InternVL2.5-78B（63.9 vs 63.8）；3B版本在DocVQA得分93.9，可部署于手机端。


### 4. 第四代：Qwen3-VL（2025年）——256K长上下文与多模态推理巅峰
Qwen3-VL作为当前系列旗舰，实现“文本-图像-视频”256K tokens interleaved上下文处理，通过MoE架构、3D grounding与工具集成，定位“通用智能Agent”场景。

#### 核心技术创新
- **架构革命性升级**：
  - **交错M-RoPE（Interleaved M-RoPE）**：将t/h/w分量均匀分布于高低频带，解决Qwen2.5-VL中时间维度频谱失衡问题，提升长视频（2小时以上）时间建模精度；
  - **DeepStack融合**：提取ViT中间层多尺度特征（低阶边缘→高阶语义），通过轻量级残差连接注入LLM对应层，强化视觉-语言对齐，InfoVQA准确率提升3.2%；
  - **文本时间戳编码**：用`<3.0 seconds>`等文本令牌标记视频帧组，替代M-RoPE时间编码，支持HMS/秒级时间格式，长视频时间定位误差降低40%。
- **模型规模与形态扩展**：
  - 推出“稠密模型（2B/4B/8B/32B）+MoE模型（30B-A3B/235B-A22B）”，235B MoE模型激活参数22B，平衡性能与 latency；
  - 首次支持**3D Grounding**：通过ARKitScenes/Hypersim数据集训练，输出9自由度3D边界框（x/y/z坐标+尺寸+旋转角），SUN RGB-D数据集mAP 39.4，超越Gemini 2.5-Pro（29.7）。
- **256K长上下文训练**：
  - 预训练分四阶段：S0（仅融合器训练，67B tokens）→S1（全参数训练，1T tokens，8K上下文）→S2（32K上下文）→S3（256K上下文，100B tokens）；
  - 后训练分“非推理（Instruct）”与“推理（Thinking）”版本：Thinking版本通过120K多步推理数据（如STEM解题链）训练，支持工具调用（如图像放大工具HRBench4K得分85.4）。
- **多模态代码与Agent能力**：新增“UI截图→HTML/CSS生成”“流程图→代码转换”数据，Design2Code任务准确率93.4；支持OSWorld/AndroidWorld环境的GUI操作，AndroidWorld成功率63.7，超越GPT-4o（34.5%）。

#### 关键性能表现
- **多模态推理**：MMMU（ college-level多学科）得分80.6（Thinking版本），接近GPT-5（84.2）；MathVista（视觉数学）得分85.8，超越Claude Opus 4.1（74.5）。
- **长上下文能力**：“Needle-in-a-Haystack”测试中，30分钟视频（256K tokens）目标定位准确率100%，1小时视频（512K tokens）准确率99.5%；MMLongBench-Doc得分57.0，开源SOTA。
- **多语言与细粒度**：支持39种语言OCR，32种语言准确率超70%；CountBench计数准确率93.7，超越Gemini 2.5-Pro（91.0）。
- **工具集成**：HRBench8K（高分辨率图像感知）得分82.4（+工具），超越人类标注精度（80.1）；多模态代码生成任务LiveCodeBench v6得分70.1，超越GPT-4o（58.6）。


## 二、各代模型迭代改进对比
### 1. 核心能力演进路径
| 能力维度         | Qwen-VL（2023）       | Qwen2-VL（2024）       | Qwen2.5-VL（2025）     | Qwen3-VL（2025）       |
|------------------|-----------------------|-----------------------|-----------------------|-----------------------|
| **输入模态**     | 静态图像（单图）      | 图像+视频（20分钟）   | 图像+视频（3小时）    | 图像+视频+3D点云（2小时） |
| **分辨率支持**   | 固定448×448           | 动态（224×224~1260×1092） | 动态（100×28~16384×28） | 动态（无上限，256K tokens） |
| **上下文长度**   | 2048 tokens           | 8192 tokens           | 32768 tokens          | 256K tokens（interleaved） |
| **核心能力**     | 基础VQA+OCR+定位      | 动态分辨率+视频理解   | 长视频+长文档+Agent   | 3D grounding+工具集成+多模态代码 |
| **多语言支持**   | 中英文                | 10种语言             | 12种语言             | 39种语言（OCR）       |

### 2. 架构与训练优化对比
| 技术模块         | Qwen-VL               | Qwen2-VL               | Qwen2.5-VL             | Qwen3-VL               |
|------------------|-----------------------|-----------------------|-----------------------|-----------------------|
| **视觉编码器**   | ViT-bigG（1.9B）      | ViT（675M，2D-RoPE）  | 动态ViT（窗口注意力）  | SigLIP-2（300M/400M，DeepStack） |
| **位置编码**     | 2D绝对编码            | M-RoPE（t/h/w拆分）   | 绝对时间M-RoPE        | 交错M-RoPE+文本时间戳 |
| **训练数据量**   | 1.4B图像-文本对       | 1.4万亿tokens         | 4.1万亿tokens         | 6.2万亿tokens（含4.1万亿多模态） |
| **模型规模**     | 9.6B（仅7B LLM）     | 2B/7B/72B             | 3B/7B/72B             | 2B/4B/8B/32B+30B-A3B/235B-A22B |
| **后训练策略**   | SFT（350K数据）       | SFT+蒸馏              | SFT+DPO               | SFT+蒸馏+RL（推理/通用） |

### 3. 关键性能迭代（72B/235B级旗舰模型）
|  benchmark       | Qwen-VL-7B            | Qwen2-VL-72B           | Qwen2.5-VL-72B         | Qwen3-VL-235B-A22B    |
|------------------|-----------------------|-----------------------|-----------------------|-----------------------|
| MMBench-EN（VQA）| 79.5                  | 86.5                  | 88.6                  | 89.3（Instruct）      |
| DocVQA（文档QA） | 65.1                  | 96.5                  | 96.4                  | 97.1                  |
| Video-MME（视频） | -                     | 71.2（无字幕）        | 73.3（无字幕）        | 79.2（无字幕）        |
| RefCOCO+（定位）  | 77.21                 | 85.6                  | 90.7                  | 91.9                  |
| MMMU（多学科）   | -                     | 64.5                  | 70.2                  | 80.6（Thinking）      |


## 三、总结：Qwen-VL系列演进核心趋势
1. **从“单模态增强”到“多模态融合”**：初代仅实现“文本+图像”基础对齐，四代已支持“图像-视频-3D-代码” interleaved处理，成为通用智能Agent；
2. **从“固定规模”到“弹性部署”**：模型规模从单一7B扩展至“2B（边缘）-235B MoE（高性能）”，覆盖手机、服务器、云端全场景；
3. **从“任务驱动”到“效率优先”**：通过窗口注意力、动态分辨率、MoE等技术，使72B模型推理效率提升2倍，同时保持SOTA性能；
4. **从“封闭能力”到“开放工具”**：从静态任务（VQA、字幕）走向“工具集成（图像放大、代码执行）+设备控制（手机/电脑UI）”，更贴近真实应用场景。

Qwen-VL系列的迭代印证了LVLM的发展方向：**以“高效架构+大规模数据+长上下文”为基础，逐步融合多模态感知与Agent决策能力，最终实现“感知-推理-行动”闭环**。

## 四、模型开源？

Qwen（通义千问）系列模型在**不公开完整训练/推理源码**的前提下，仍能被定义为“开源”的核心逻辑和实现方式，这本质是理解大模型领域“开源”的特殊形态。

### 1. 先明确：大模型领域的“开源”≠ 传统软件的“开源”
传统软件（如Linux、Python）的开源是**完整源代码公开+开源协议授权**，但大模型的“开源”是行业约定俗成的广义概念，核心包含三层：
1. **模型权重（Checkpoint）开源**：公开训练好的模型参数文件（这是Qwen的核心开源内容）；
2. **推理/微调接口开源**：提供适配主流框架（<a href="https://github.com/huggingface/transformers?tab=readme-ov-file">Hugging Face Transformers </a>）的轻量化调用代码；
3. **开源协议授权**：明确商用/非商用权限（Qwen采用Apache 2.0、MIT等开源协议）。

而训练源码、底层框架源码等并非大模型“开源”的必要条件——这也是Qwen“不公开完整源码但仍算开源”的核心原因。

### 2. Qwen实现“开源”的具体方式
Qwen的开源核心是“**权重+适配代码**”的轻量化开源模式，而非传统的全源码开源，具体拆解为4个关键环节：

#### 2.1 核心：公开模型权重文件（最关键的“开源资产”）
Qwen在Hugging Face、ModelScope等平台公开的核心是**模型权重文件**（后缀为`.bin`/`.safetensors`），这是模型的“大脑”——包含预训练后所有层的参数（如注意力层权重、全连接层权重）。
- 权重文件是训练的最终产物，用户无需训练源码，即可通过权重文件加载模型进行推理、微调、续训；
- 权重文件的大小：例如Qwen2.5-7B约13GB，Qwen2.5-14B约26GB，均可直接下载使用。

#### 2.2 适配：提供“桥接代码”（而非完整源码）
Qwen不公开训练/底层实现的完整源码，但会提供**适配主流框架的轻量化调用代码**（通过Hugging Face的`trust_remote_code=True`加载）：
- 这些代码仅包含**模型结构定义**（如Qwen的Transformer变体、RoPE位置编码实现）、**tokenizer编码逻辑**、**基础推理接口**；
- 代码体积极小（通常几百行），核心作用是让Transformers框架能正确解析权重文件，而非复刻训练过程；
- 示例：你之前加载Qwen2.5时设置`trust_remote_code=True`，本质是从模型仓库下载这份桥接代码，而非完整源码。

#### 2.3 协议：明确开源授权（合规性保障）
Qwen的权重和适配代码均通过开源协议授权（如Apache 2.0、MIT），这是“开源”的法律基础：
- Apache 2.0协议：允许商用、修改、分发，仅需保留版权声明，无闭源衍生限制；
- 对比闭源模型（如GPT-4、Claude）：Qwen无需申请API密钥、无调用次数限制，用户可本地部署，这是“开源”的核心特征。

#### 2.4 兼容：适配主流开源生态（降低使用门槛）
Qwen的权重和适配代码完全兼容Hugging Face、Transformers、Accelerate等开源生态：
- 用户无需适配Qwen的专属框架，直接用熟悉的Transformers API即可调用（如`AutoModelForCausalLM.from_pretrained`）；
- 即使没有官方训练源码，用户也能基于Transformers/DeepSpeed等开源框架，实现微调、续训（如你之前问的Qwen2.5续训）。

### 3. 为什么Qwen不公开完整训练源码？（行业共性原因）
Qwen不公开训练源码并非“伪开源”，而是大模型行业的普遍做法，核心原因有3点：
1. **训练源码无通用价值**：大模型训练源码高度定制化——依赖特定的集群架构（如阿里云飞天）、数据流水线、分布式策略（如DeepSpeed ZeRO-4）、硬件适配（如A100/NVLink），公开后普通用户也无法直接复用（需千万级硬件成本）；
2. **保护核心技术壁垒**：训练源码中包含批量大小调度、学习率策略、数据清洗逻辑、稳定性优化等核心技术，这些是厂商的差异化竞争力，不会公开；
3. **降低用户使用成本**：普通用户的核心需求是“使用模型”而非“复刻训练”，公开权重+适配代码已能满足99%的场景（推理、微调、部署），无需公开复杂的训练源码。

### 4. 验证：Qwen“开源”的可落地性（用户视角）
即使没有完整源码，你仍能基于Qwen的开源内容完成所有核心操作：
| 操作场景      | 是否需要完整源码 | 实现方式                                                     |
| ------------- | ---------------- | ------------------------------------------------------------ |
| 本地推理      | 否               | 下载权重+Transformers调用代码，直接`model.generate()`        |
| 指令微调/续训 | 否               | 基于Transformers/Accelerate，用户自己编写续训代码即可（无需官方训练源码） |
| 模型部署      | 否               | 通过vLLM/TensorRT-LLM加载权重，适配后部署为API服务           |
| 模型量化      | 否               | 用BitsAndBytes/GPTQ量化权重，降低部署显存需求                |
| 复刻训练      | 是               | 但普通用户无此需求（训练7B模型需数千卡时，成本超百万）       |

### 总结
1. Qwen的“开源”核心是**模型权重+轻量化适配代码**的开源，而非传统的全源码开源，这是大模型领域的行业惯例；
2. 不公开训练源码不影响核心使用场景（推理、微调、部署），且能保护厂商的核心技术壁垒；
3. 判定Qwen是否“开源”的关键：是否公开可本地部署的权重、是否有开源协议授权、是否兼容主流开源生态——这三点Qwen均满足。

简单来说，Qwen的开源逻辑是：**给你“能用的模型”，而非“造模型的工具”** ——这既符合大模型行业的开源定义，也能最大化满足用户的实际需求。

