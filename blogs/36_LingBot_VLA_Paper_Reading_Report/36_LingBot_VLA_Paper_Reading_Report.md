## 摘要
LingBot-VLA是由Ant Group开发的先进视觉-语言-动作（VLA）基础模型，通过约20,000小时的真实世界数据和创新的模型架构设计，在机器人操作任务中展现出卓越的性能和泛化能力。本报告从模型设计、训练优化、实验评估等多维度进行全面分析。

## 1. 引言与背景

### 1.1 VLA模型的发展现状
Vision-Language-Action（VLA）基础模型作为机器人执行多样化操作任务的重要方法，通过大规模预训练获得可泛化技能，能够快速适应不同任务和机器人平台。然而，当前研究缺乏关于真实机器人性能如何随预训练数据规模扩展的全面实证研究，同时社区也缺乏能够**高效处理海量数据**的优化训练代码库。

### 1.2 研究挑战
- **数据规模效应**：缺乏对VLA模型在真实世界数据上扩展规律的深入理解
- **训练效率瓶颈**：大规模VLA模型在多节点集群上的训练面临数据I/O和通信开销挑战
- **评估标准不统一**：需要建立跨平台、大规模的真实世界评估基准

## 2. 模型架构设计创新

### 2.1 整体架构概览
LingBot-VLA采用**混合变换器**（Mixture-of-Transformers，MoT）架构，整合预训练的视觉语言模型（Qwen2.5 VL）与专门设计的动作生成模块。

<span class="image main">
<img class="main img-in-blog" style="max-width: 90%" src="./blogs/36_LingBot_VLA_Paper_Reading_Report/LingBot_Diagram.webp" alt="LingBot Diagram" />
<i>LingBot-VLA 架构概览。From <a href="https://arxiv.org/pdf/2601.18692" target="_blank" rel="noopener noreferrer">APragmatic VLA Foundation Model</a></i>
</span> 

#### 2.1.1 Mixture-of-Transformers（MoT）架构概述与设计理念

Mixture-of-Transformers（MoT）是LingBot-VLA的核心创新架构，旨在解决传统VLA模型在跨模态融合和计算效率方面的挑战。该架构采用**分而治之**的策略，将复杂的多模态处理任务分解为专门的子网络，同时通过**统一的注意力机制**保持模态间的协同。

#### 2.1.2 核心组件架构

##### 2.1.2.1 视觉-语言处理分支
MoT架构继承并优化了预训练的视觉语言模型（VLM）作为语义骨干网络。具体采用Qwen2.5 VL模型，该分支负责：

- **多视图图像编码**：处理三视角操作图像 $I\_t^{1,2,3}$
- **语言指令理解**：解析任务指令 $T\_t$
- **跨模态对齐**：建立视觉特征与语言描述的语义关联

##### 2.1.2.2 动作专家模块
专门设计的动作生成头，采用条件流匹配（Flow Matching）技术进行连续动作建模：

$$
\mathcal{L}\_{FM} = \mathbb{E}\_{s\sim\mathcal{U}[0,1],A\_t,\epsilon}\left\|v\_{\theta}\left(A\_{t,s},O\_t,s\right)-\left(A\_t-\epsilon\right)\right\|^{2}
$$

其中动作专家 $v\_{\theta}$ 被训练来预测条件向量场，目标速度由从线性概率路径导出的理想向量场 $A\_t - \epsilon$ 给出。

#### 2.1.3 模态交互机制

##### 2.1.3.1 共享自注意力层
MoT的关键创新在于通过**共享自注意力机制**实现层间统一序列建模。这种设计确保：

- **语义先验持续指导**：VLM的高维语义先验在所有层提供连续指导
- **模态特异性处理**：通过保持模态特定的处理来减轻跨模态干扰
- **信息流控制**：防止未来动作令牌信息泄露到当前观测表示中

##### 2.1.3.2 块级因果注意力
受 $\pi\_0$论文（<a href="https://arxiv.org/abs/2410.24164" target="\_blank" rel="noopener noreferrer">pi0: A Vision-Language-Action Flow Model for General Robot Control</a>）的启发，MoT实现**块级因果注意力**用于建模联合序列 $\left[O\_t, A\_t\right]$。序列被划分为三个功能块：

1. **视觉语言条件块**：$\left[I\_t^1, I\_t^2, I\_t^3, T\_t\right]$
2. **机器人状态块**：$\left[s\_t\right]$  
3. **动作序列块**：$\left[a\_t, a\_{t+1},\ldots, a\_{t+T-1}\right]$

在这些**块之间**应用**因果掩码**，使得每个块中的令牌（Token）只能关注自身及前驱块中的令牌。相反，同一**块内**的所有令牌采用**双向注意力**并可相互关注。

#### 2.1.4 训练优化策略

##### 2.1.4.1 分布式训练优化
针对VLA模型的中等参数数量，MoT架构在GPU内存占用和训练吞吐量之间实现最佳权衡：

**分布式策略**：
- 采用完全分片数据并行（FSDP）——Zero冗余优化器的高效PyTorch实现
- 受HSDP方法启发，为动作专家模块构建特定的"分片组"
- 有效减轻与过度参数分片相关的通信开销

**混合精度策略**：
- 在torch.float32中执行归约确保数值稳定性
- 使用torch.bfloat16进行存储和通信

##### 2.1.4.2 算子级优化
MoT架构中的多模态融合本质上是稀疏注意力过程，因此采用：

**计算优化**：
- 利用FlexAttention优化稀疏注意力计算
- 应用算子融合（通过torch.compile）减少内核启动开销
- 最大化内存带宽利用率

#### 2.1.5 空间感知增强

##### 2.1.5.1 深度信息整合
为显式捕捉操作环境中的空间感知并进一步增强机器人执行的鲁棒性，MoT采用视觉蒸馏方法：

- 应用对应于三视图操作图像的可学习查询（queries） $\left[Q\_t^1, Q\_t^2, Q\_t^3\right]$
- 这些查询由VLM处理，然后与来自LingBot-Depth的深度令牌 $\left[D\_t^1, D\_t^2, D\_t^3\right]$ **对齐**

##### 2.1.5.2 蒸馏对齐
通过最小化**蒸馏损失** $\mathcal{L}\_{\text{distill}}$ 对齐VLM可学习查询和LingBot-Depth令牌：

$$
\mathcal{L}\_{\text{distill}} = \mathbb{E}\_{Q\_t}\left|\operatorname{Proj}(Q\_t)-D\_t\right|
$$

其中 $\operatorname{Proj}(\cdot)$ 是应用交叉注意力进行维度对齐的投影层。这种整合将几何信息注入到LingBot-VLA模型中，为复杂操作任务实现精确感知。

#### 2.1.6 与现有架构的对比优势

##### 2.1.6.1 传统VLA架构的局限性
传统VLA模型通常采用端到端的单一Transformer架构，存在以下问题：
- **模态干扰**：视觉、语言、动作模态在统一空间中产生相互干扰
- **计算效率低**：所有模态通过相同的网络路径，计算资源利用不均衡
- **优化困难**：不同模态的梯度信号可能相互冲突

##### 2.1.6.2 MoT的创新优势

**模态专业化**：
- 视觉语言分支专注于高维语义特征提取
- 动作专家专注于运动轨迹生成和优化
- 各子网络针对特定任务进行优化，提升整体效率

**灵活扩展性**：
- 可以独立更新或替换特定模块（如更换更强的VLM骨干）
- 支持模块化扩展，便于集成新的传感器模态
- 适应不同的机器人平台和任务需求

#### 2.1.7 实际部署性能

##### 2.1.7.1 训练效率表现
在8-GPU训练设置下，MoT架构实现了显著的性能提升：
- **吞吐量**：达到每GPU每秒261个样本
- **加速比**：相比现有VLA导向代码库实现1.5~2.8倍加速
- **内存效率**：通过参数分片和混合精度优化内存使用

##### 2.1.7.2 泛化能力验证
在GM-100基准测试中，MoT架构展现出卓越的跨平台泛化能力：

- **多平台一致性**：在Agibot G1、AgileX和Galaxea R1Pro三个平台上均表现优异
- **任务多样性**：成功处理100个不同的操作任务
- **零样本迁移**：在未见过的任务和环境中展现良好的适应能力

### 2.2 核心架构组件

#### 2.2.1 多模态条件编码
模型通过统一的VLM编码多视角操作图像和相关任务指令，建立多模态条件以支持后续动作生成：
$$O\_t=\left[I\_{t}^{1}, I\_{t}^{2}, I\_{t}^{3},T\_t,s\_t\right]$$

其中包含双臂机器人三视角操作图像、任务指令和机器人状态的令牌表示。

#### 2.2.2 动作序列建模
对应的动作序列表示为：
$$A\_t=\left[a\_t,a\_{t+1},\ldots,a\_{t+T-1}\right]$$

其中$T$表示动作块长度（预训练阶段设置为50），即预测轨迹的时间范围。

### 2.3 关键技术创新点

#### 2.3.1 条件流匹配技术
采用Flow Matching进行连续动作建模，定义条件分布$p\left(A\_{t}\mid O\_{t}\right)$通过条件流匹配。对于流时间步$s\in[0,1]$，通过高斯噪声$\epsilon\sim\mathcal{N}(0,I)$与真实动作$A\_t$的线性插值获得中间动作$A\_{t, s}=s A\_{t}+(1-s)\epsilon$。

#### 2.3.2 块级因果注意力机制
实现块级因果注意力用于建模联合序列$\left[O\_t, A\_t\right]$，将序列划分为三个功能块：
- 视觉语言条件块：$\left[I\_t^1, I\_t^2, I\_t^3, T\_t\right]$
- 机器人状态块：$\left[s\_t\right]$
- 动作序列块：$\left[a\_t, a\_{t+1},\ldots, a\_{t+T-1}\right]$

这种配置确保动作专家可以利用所有可用的观测知识，同时防止未来动作令牌信息泄露到当前观测表示中。

#### 2.3.3 空间感知增强
通过视觉蒸馏方法显式捕捉操作环境中的空间感知，将VLM可学习查询与LingBot-Depth的深度令牌对齐，最小化蒸馏损失$\mathcal{L}\_{\text{distill}}$，将几何信息注入到LingBot-VLA模型中。

## 3. 训练基础设施优化

### 3.1 分布式训练策略
由于动作数据本质上是高频的，建立包含分布式训练和算子优化的高效流水线至关重要：

**内存优化策略**：
- 采用完全分片数据并行（FSDP），Zero冗余优化器的高效PyTorch实现
- 构建动作专家模块特定的"分片组"，减轻过度参数分片带来的通信开销
- 混合精度策略：在torch.float32中执行归约确保数值稳定性，使用torch.bfloat16进行存储和通信

**计算优化**：
- 利用FlexAttention优化稀疏注意力计算
- 应用算子融合（通过torch.compile）减少内核启动开销，最大化内存带宽利用率

### 3.2 训练效率成果
在8-GPU训练设置下，代码库实现了**每GPU每秒261个样本的吞吐量**，相比现有VLA导向代码库实现了1.5~2.8倍（取决于依赖的VLM基础模型）的加速。


## 4. 数据策略与预处理

### 4.1 大规模真实世界数据集
预训练数据集基于从9个流行双臂机器人配置收集的大规模遥操作数据，包括：
- AgiBot G1、AgileX、Galaxea R1Lite、Galaxea R1Pro
- Realman Rs-02、Leju KUAVO 4 Pro、Qinglong、ARX Lift2、Bimanual Franka


### 4.2 数据标注流程
为获得精确的语言指令，执行以下标注：
1. **视频分段**：根据预定义的原子动作，由人工标注员将多视角视频联合分解为片段
2. **指令标注**：使用Qwen3-VL-235B-A22B对包含机器人完整运动轨迹的视频和每个原子动作的视频片段进行精确的任务和子任务指令标注

### 4.3 数据多样性分析
通过词云分析预训练数据集和基准测试中原子动作的分布，定量分析显示测试集中约50%的原子动作在训练集前100个最常见动作中不存在，确保了测试集的多样性和模型泛化能力的严格评估。


## 5. 实验评估与结果分析

### 5.1 大规模真实世界基准测试
在GM-100基准上进行系统评估，包含100个多样化操作任务和39,000次专家演示，通过22,500次试验比较不同模型在3个商业平台上的表现。

**评估指标**：
- **成功率（SR）**：模型在3分钟时间限制内完成所有任务步骤的试验比例
- **进度分数（PS）**：通过顺序子任务检查点跟踪部分任务完成情况

### 5.2 跨平台性能比较
实验结果（表1）显示，LingBot-VLA在三个机器人平台上均显著优于现有基线模型：

**关键发现**：
- 不含深度信息的版本在SR和PS指标上已显著优于WALL-OSS和GR00T N1.6
- 通过整合基于深度的空间信息，LingBot-VLA在三个平台上平均SR提高$4.28\%$，PS提高$7.76\%$
- 在Galaxea R1Pro平台上，GR00T N1.6表现出与$\pi\_{0.5}$相当的性能，表明预训练可以显著增强在下游任务上的性能

### 5.3 仿真环境评估
在RoboTwin 2.0套件中的50个代表性操作任务上进行评估，结果显示：
- 在清洁环境中，不含深度的LingBot-VLA比$\pi\_{0.5}$基线绝对成功率提高超过$3.76\%$
- 在随机化场景中，绝对成功率提高$8.58\%$
- 通过整合深度信息，在清洁和随机化配置中分别超过基线模型$5.82\%$和$9.92\%$

### 5.4 数据效率分析
在有限的数据预算下（每任务仅80次演示），LingBot-VLA在使用完整130次演示集的$\pi\_{0.5}$上在进度率和成功率方面均表现更优。随着训练后数据量的增加，LingBot-VLA和$\pi\_{0.5}$之间的性能差距显著扩大，展示了卓越的数据效率和可扩展性。


## 6. 扩展性分析与未来方向

### 6.1 数据规模扩展规律
通过在不同数据规模（从3,000小时到20,000小时）上的实验，证明了下游任务成功率随着预训练数据量和多样性的增加而持续显著提高。即使在20,000小时标记处，这种扩展行为也未显示饱和迹象，表明VLA性能继续受益于数据量的增加。


### 6.2 实际部署价值
LingBot-VLA通过结合卓越性能、广泛泛化能力和计算效率，为现实世界机器人应用提供了理想的基础模型。其优化的代码库显著缩短了训练周期并减少了计算开销，从而降低了总体成本。

## 7. 结论与贡献

LingBot-VLA通过系统性的架构创新和大规模真实世界数据训练，在VLA基础模型领域实现了重要突破。主要贡献包括：

1. **架构创新**：提出混合变换器架构，实现视觉-语言与动作模态的高效协同
2. **训练优化**：开发高性能开源代码库，显著提升训练效率和可扩展性
3. **评估基准**：建立大规模跨平台评估标准，推动领域健康发展
4. **开源贡献**：公开发布代码、基础模型和基准数据，促进社区发展

未来工作将重点通过整合单臂和移动机器人数据来扩展模型的多功能性，为在无约束环境中实现更多样化和移动操作能力铺平道路。