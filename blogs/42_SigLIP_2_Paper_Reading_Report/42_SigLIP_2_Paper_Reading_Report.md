## 核心速览

### 研究背景

1. **研究问题**：这篇文章要解决的问题是如何改进多语言视觉-语言编码器（Vision-Language Encoders），使其在零样本分类、图像-文本检索和视觉语言模型（VLMs）的视觉表示提取等核心能力上超越现有的SigLIP模型，并在定位和**密集预测**任务上有显著提升。

   >  **密集预测（Dense Prediction）** 是计算机视觉中的一种任务类型，指对输入图像的每个像素（或每个位置）都输出一个预测结果，形成与输入空间分辨率相同的密集输出。与只输出单个标签或边界框的分类/检测任务不同，密集预测要求对图像中的每个细节位置都进行精细化的理解。
   >
   > #### 核心特征
   >
   > | 特征维度       | 描述                                                         |
   > | -------------- | ------------------------------------------------------------ |
   > | **输出分辨率** | 与输入图像保持相同或相近的空间尺寸（如512×512输入→512×512输出） |
   > | **预测粒度**   | 像素级或超像素级，每个位置都有独立预测                       |
   > | **任务类型**   | 语义分割、实例分割、深度估计、表面法线估计、光流估计等       |
   > | **输出形式**   | 通常为与输入同尺寸的预测图（如分割mask、深度图、法线图）     |

1. **研究难点**：该问题的研究难点包括：如何在多语言数据上进行训练以提高模型的泛化能力；如何在不增加计算复杂度的情况下提高模型的性能；如何在**保持输入原始宽高比的同时支持多种分辨率**。
2. **相关工作**：该问题的研究相关工作包括CLIP和ALIGN等对比图像-文本嵌入模型的成功应用，以及在其基础上提出的重新描述图像、添加仅图像自监督损失和训练小型解码器等改进方法。此外，还有一些开源社区发布的模型检查点，但这些工作并没有将最新的所有改进集成到一个模型中。

### 研究方法

<span class="image main">
<img class="main img-in-blog" style="max-width: 60%" src="./blogs/42_SigLIP_2_Paper_Reading_Report/SigLIP2.webp" alt="SigLIP 2" />
<i>SigLIP 2 在 SigLIP 的 sigmoid 损失函数基础上，加入了 LocCa 的基于图像描述的预训练方法，以及 SILC 和 TIPS 的自蒸馏和掩码预测方法（在训练的最后 20% 阶段）。对于某些变体，该方法还包含通过数据整理或适应原生宽高比和可变序列长度进行微调。</i>
</span> 

这篇论文提出了SigLIP 2，一种新的多语言视觉-语言编码器家族，通过结合多种独立开发的技术来扩展原始的SigLIP模型。具体来说，

1. **预训练**：首先，使用图像编码器和语言解码器的组合进行预训练，以改进OCR能力和定位。预训练的损失函数结合了SigLIP和LocCa的损失，使用逻辑回归（sigmoid损失）进行二分类。
2. **自监督学习**：其次，引入自监督学习技术，包括自蒸馏和掩码预测，以提高密集预测任务的性能。自蒸馏通过教师模型和学生模型的参数更新来实现，掩码预测则通过将学生网络中的部分图像块替换为掩码标记来训练。
3. **多分辨率支持**：此外，提出了一个名为**NaFlex的变体**，支持多种分辨率并保持输入的原始宽高比。NaFlex通过调整输入图像的大小和序列长度来实现这一点，并使用双线性插值来处理不同的序列长度。
4. **数据混合**：最后，在训练过程中使用一个更加多样化的数据混合，包括去偏置技术，以提高模型的多语言理解和公平性。

### 实验设计

1. **数据集**：使用WebLI数据集，包含100亿张图像和120亿个替代文本，覆盖109种语言。训练数据混合了90%的英文网页和10%的非英文网页。
2. **模型架构**：采用标准的ViT架构，固定分辨率为256，序列长度为64，并使用多语言Gemma分词器。对于g大小的视觉编码器，搭配一个So400m大小的文本编码器。
3. **优化器**：使用Adam优化器，学习率为10^-3，解耦权重衰减为10^-4，梯度裁剪为范数1。批量大小为32k，使用余弦调度，训练400亿个样本。
4. **训练步骤**：分为两个阶段，第一阶段结合SigLIP和LocCa的损失进行预训练，第二阶段在预训练的基础上加入自蒸馏和掩码预测的损失进行微调。

### 结果与分析

1. **零样本分类和检索**：SigLIP 2在所有模型规模上均优于其SigLIP对应模型，特别是在B规模模型上，由于蒸馏技术的应用，性能提升显著。 
2. **多语言检索**：在Crossmodal-3600数据集上，SigLIP 2的召回率超过了SigLIP，尽管在英文视觉-语言任务上表现稍差于mSigLIP。
3. **VLMs中的视觉表示**：SigLIP 2作为视觉编码器在多个下游任务中表现优异，特别是在分割、深度估计和表面法线估计任务上，显著优于其他开源模型。
4. **定位任务**：在指代表达理解任务上，SigLIP 2显著优于SigLIP、CLIP和仅通过图像标题预训练的模型。
5. **文化多样性和公平性**：SigLIP 2在文化多样性方面表现更好，特别是在地理定位任务上，10次射击的准确率从SigLIP的36.2%提高到SigLIP 2的44.4%。此外，SigLIP 2在表示偏见方面也显著优于SigLIP。

### 总体结论

这篇论文提出了SigLIP 2，一种新的多语言视觉-语言编码器家族，通过结合预训练、自监督学习和多分辨率支持等技术，显著提高了模型在零样本分类、图像-文本检索和视觉语言模型中的视觉表示提取等核心能力。此外，SigLIP 2在定位和密集预测任务上也有显著提升，并且在文化多样性和公平性方面表现更好。

## 论文评价

### 优点与创新

1. **强大的多语言视觉-语言编码器**：SigLIP 2在英语为主的视觉-语言任务上表现出色，同时在单一模型上实现了多语言基准测试的强结果，使其能够在广泛的语种和文化背景下使用。
2. **密集特征**：通过结合自监督损失和基于解码器的损失，SigLIP 2生成了更好的密集特征（例如用于分割和深度估计），并提高了定位任务的性能。
3. **向后兼容性**：SigLIP 2设计为与SigLIP向后兼容，允许现有用户仅通过替换模型权重和现在支持多语言的tokenizer来获得各种任务的改进。
4. **原生宽高比和多分辨率支持**：SigLIP 2还包括**NaFlex变体**，支持多个分辨率并保留图像的原生宽高比，这对于文档理解等敏感应用潜在有益。
5. **强大的小模型**：SigLIP 2通过使用主动数据策划的蒸馏技术优化了较小模型（B/16和B/32）的性能。
6. **多样化的训练数据混合**：通过包含去偏技术，SigLIP 2在多语言理解方面取得了显著改进，并提高了公平性。
7. **多种模型检查点**：为了允许用户在推理成本和性能之间进行权衡，发布了四种大小的模型检查点：ViT-B(86M)，L(303M)，So400m(400M)，和g(1B)。

### 不足与反思

1. **局限性**：论文中没有明确提到具体的局限性，但可以推测在处理某些特定类型的数据或任务时，SigLIP 2可能仍然存在一些局限性。
2. **下一步工作**：论文没有详细讨论未来的工作方向，但可以考虑进一步优化模型架构、训练策略或在更多类型的任务和语言上进行扩展。

## 关键问题及回答

**问题1：SigLIP 2在多语言视觉-语言编码器方面的主要技术创新有哪些？**

1. **预训练**：SigLIP 2使用了图像编码器和语言解码器的组合进行预训练，以改进OCR能力和定位。预训练的损失函数结合了SigLIP和LocCa的损失，使用逻辑回归（sigmoid损失）进行二分类。
2. **自监督学习**：引入了自监督学习技术，包括自蒸馏和掩码预测，以提高密集预测任务的性能。自蒸馏通过教师模型和学生模型的参数更新来实现，掩码预测则通过将学生网络中的部分图像块替换为掩码标记来训练。
3. **多分辨率支持**：提出了一个名为**NaFlex**的变体，支持多种分辨率并保持输入的原始宽高比。**NaFlex**通过调整输入图像的大小和序列长度来实现这一点，并使用双线性插值来处理不同的序列长度。
4. **数据混合**：在训练过程中使用一个更加多样化的数据混合，包括去偏置技术，以提高模型的多语言理解和公平性。

**问题2：SigLIP 2在定位任务上有哪些改进？这些改进是如何实现的？**

SigLIP 2在定位任务上的改进主要体现在指代表达理解任务上。具体实现方式如下：

1. **预训练阶段**：使用LocCa损失函数，通过附加一个标准的Transformer解码器到未池化的视觉编码器表示上，进行自动指代表达预测和基于区域的字幕预测。LocCa的损失函数包括字幕目标、区域字幕目标和区域-字幕对的自动注释。
2. **自蒸馏阶段**：在预训练的基础上，加入自蒸馏和掩码预测的损失进行微调。自蒸馏通过教师模型和学生模型的参数更新来实现，掩码预测则通过将学生网络中的部分图像块替换为掩码标记来训练。

这些改进使得SigLIP 2在指代表达理解任务上显著优于SigLIP、CLIP和仅通过图像标题预训练的模型。

**问题3：SigLIP 2在多语言理解和公平性方面有哪些改进？**

1. **数据混合**：SigLIP 2在训练过程中使用了一个更加多样化的数据混合，包括90%的英文网页和10%的非英文网页，以提高模型的多语言理解能力。
2. **去偏置技术**：为了解决训练数据中可能存在的偏见问题，SigLIP 2集成了数据去偏置技术，以减少在性别代表性、职业与性别的关联等方面的偏见。
3. **评估结果**：在文化多样性方面，SigLIP 2在地理定位任务上表现更好，10次射击的准确率从SigLIP的36.2%提高到SigLIP 2的44.4%。此外，SigLIP 2在表示偏见方面也显著优于SigLIP，表示偏见降低了约30%。

这些改进使得SigLIP 2在多语言理解和公平性方面表现更好。